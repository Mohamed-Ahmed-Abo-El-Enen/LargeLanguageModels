{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMRgkwntNaaXkr4ukB8YGBx"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRcummk-pRln"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "HF_HOME = '/content/transformers_cache/huggingface'\n",
    "HF_DATASETS_CACHE = '/content/huggingface/datasets'\n",
    "TRANSFORMERS_CACHE = '/content/huggingface/models'\n",
    "\n",
    "\n",
    "def re_direct_hf_cache():\n",
    "    Path(HF_HOME).mkdir(parents=True, exist_ok=True)\n",
    "    Path(HF_DATASETS_CACHE).mkdir(parents=True, exist_ok=True)\n",
    "    Path(TRANSFORMERS_CACHE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    os.environ['HF_HOME'] = HF_HOME\n",
    "    os.environ['HF_DATASETS_CACHE'] = HF_DATASETS_CACHE\n",
    "    os.environ['TRANSFORMERS_CACHE'] = TRANSFORMERS_CACHE\n",
    "\n",
    "re_direct_hf_cache()"
   ],
   "metadata": {
    "id": "sZQOA3yrpXdU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# quantize_args = {\n",
    "#     \"device\": \"cuda:0\",\n",
    "#     \"double_quant\": True,\n",
    "#     \"quant_type\": \"nf4\",\n",
    "#     \"bits\": 4,\n",
    "#     \"bf16\": False,\n",
    "#     \"fp16\": True,\n",
    "#     \"cache_dir\": None\n",
    "# }\n",
    "\n",
    "# compute_dtype = (torch.float16 if quantize_args[\"fp16\"] else (torch.bfloat16 if quantize_args[\"bf16\"] else torch.float32))\n",
    "\n",
    "# bnb_model_from_pretrained_args = {}\n",
    "# bnb_model_from_pretrained_args[\"device_map\"]={\"\": quantize_args[\"device\"]}\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     llm_int8_skip_modules=[\"mm_projector\"],\n",
    "#     llm_int8_threshold=6.0,\n",
    "#     llm_int8_has_fp16_weight=False,\n",
    "#     bnb_4bit_compute_dtype=compute_dtype,\n",
    "#     bnb_4bit_use_double_quant=quantize_args[\"double_quant\"],\n",
    "#     bnb_4bit_quant_type=quantize_args[\"quant_type\"]  # {'fp4', 'nf4'}\n",
    "# )\n",
    "\n",
    "# if quantize_args[\"bits\"] == 4:\n",
    "#     #bnb_model_from_pretrained_args[\"load_in_4bit\"] = True\n",
    "#     quantization_config.load_in_4bit = True\n",
    "# else:\n",
    "#     #bnb_params[\"load_in_8bit\"] = True\n",
    "#     quantization_config.load_in_8bit = True\n",
    "\n",
    "# bnb_model_from_pretrained_args[\"quantization_config\"] = quantization_config"
   ],
   "metadata": {
    "id": "TGgSuwXxpYKz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub.hf_api import HfFolder\n",
    "\n",
    "HfFolder.save_token('YOUR HUGGING FAC API KEY')"
   ],
   "metadata": {
    "id": "LwS2wd44pZkJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             # cache_dir=quantize_args[\"cache_dir\"],\n",
    "                                             # torch_dtype=(torch.bfloat16 if quantize_args[\"bf16\"] else None),\n",
    "                                             # **bnb_model_from_pretrained_args\n",
    "                                             )\n",
    "# model.config.use_cache = False"
   ],
   "metadata": {
    "id": "wOMdGA3WpbLt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "import transformers\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
   ],
   "metadata": {
    "id": "uqxlvWxEpdqh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    print(f\"Adding pad token as '<pad>'\")\n",
    "    smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict=dict(pad_token=\"<pad>\"),\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ],
   "metadata": {
    "id": "iqrPuounpfPe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(model.config.pad_token_id, tokenizer.pad_token_id)"
   ],
   "metadata": {
    "id": "hcMYQ3nYpgxi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "new_model = \"/content/Llama-3-8B-Instruct\"\n",
    "\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ],
   "metadata": {
    "id": "OqkL4mplpjT1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                  # low_cpu_mem_usage=True,\n",
    "                                                  # return_dict=True,\n",
    "                                                  # torch_dtype=torch.float16,\n",
    "                                                  # device_map=\"auto\"\n",
    "                                                  )\n",
    "\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "# merged_model.save_pretrained(\"merged_model\", safe_serialiaztion=True)"
   ],
   "metadata": {
    "id": "0uQwH3WHpkw4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "merged_model.push_to_hub(\"Llama-3-8B-Instruct_pad_token\")\n",
    "tokenizer.push_to_hub(\"Llama-3-8B-Instruct_pad_token\")"
   ],
   "metadata": {
    "id": "wlFz_vdPpmJX"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
