{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSFT6Y7LCfd2",
        "outputId": "c4cdc8b7-be74-4f65-cd01-ebe25607af0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.25-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.25 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.25)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.9-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.26)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.6-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.66.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.11.14)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.16-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.14.0)\n",
            "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.7-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Downloading llama_index-0.12.25-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.9-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_cloud-0.1.16-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.3/251.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading llama_cloud_services-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: striprtf, python-dotenv, pypdf, llama-cloud, llama-index-readers-file, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed llama-cloud-0.1.16 llama-cloud-services-0.6.7 llama-index-0.12.25 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.9 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.6 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 pypdf-5.4.0 python-dotenv-1.0.1 striprtf-0.0.26\n",
            "Requirement already satisfied: llama-index-core in /usr/local/lib/python3.11/dist-packages (0.12.25)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.11.14)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.18.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core) (1.3.1)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: llama-index-llms-openai in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai) (0.12.25)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai) (1.66.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.11.14)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.17.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (24.2)\n",
            "Requirement already satisfied: llama-cloud-services in /usr/local/lib/python3.11/dist-packages (0.6.7)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services) (8.1.8)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.16 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services) (0.1.16)\n",
            "Requirement already satisfied: llama-index-core>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services) (0.12.25)\n",
            "Requirement already satisfied: pydantic!=2.10 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services) (2.10.6)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.16->llama-cloud-services) (2025.1.31)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.16->llama-cloud-services) (0.28.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama-cloud-services) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (3.11.14)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (2025.3.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (11.1.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama-cloud-services) (1.17.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10->llama-cloud-services) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10->llama-cloud-services) (2.27.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-cloud-services) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-cloud-services) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-cloud-services) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-cloud-services) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-cloud-services) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-cloud-services) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama-cloud-services) (1.18.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.16->llama-cloud-services) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.16->llama-cloud-services) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.16->llama-cloud-services) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.16->llama-cloud-services) (0.14.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama-cloud-services) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama-cloud-services) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-cloud-services) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama-cloud-services) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama-cloud-services) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core>=0.11.0->llama-cloud-services) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core>=0.11.0->llama-cloud-services) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.11.0->llama-cloud-services) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.16->llama-cloud-services) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-core\n",
        "!pip install llama-index-embeddings-openai llama-index-llms-openai\n",
        "!pip install llama-cloud-services"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/data\""
      ],
      "metadata": {
        "id": "NegTC1bJC6L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://www.dropbox.com/scl/fi/sterddtajrf844ytvwlim/the-amazon-nova-family-of-models-technical-report-and-model-card.pdf?rlkey=0if0ct5diw70jifr9m8fikpsc&dl=0' -O '/content/data/nova_technical_report.pdf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HgB0ZpdbB-G",
        "outputId": "840f2ec1-824b-4fa5-ed2f-28ea752722b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-24 09:11:56--  https://www.dropbox.com/scl/fi/sterddtajrf844ytvwlim/the-amazon-nova-family-of-models-technical-report-and-model-card.pdf?rlkey=0if0ct5diw70jifr9m8fikpsc&dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucc185e2fc79ecba4e590c7c745e.dl.dropboxusercontent.com/cd/0/inline/Cmdox7vqhL8hoCTR_mWHdliyjXtNtdF6Aq9-sHDhNpCSi73i61ZSptYnbBsnBxJ6W0HP6VULtucUjnQjaUWa32xxGAbLhSxlDG0qCOnryhneE5szRbCgMfTnv3sX24DXy_k/file# [following]\n",
            "--2025-03-24 09:11:59--  https://ucc185e2fc79ecba4e590c7c745e.dl.dropboxusercontent.com/cd/0/inline/Cmdox7vqhL8hoCTR_mWHdliyjXtNtdF6Aq9-sHDhNpCSi73i61ZSptYnbBsnBxJ6W0HP6VULtucUjnQjaUWa32xxGAbLhSxlDG0qCOnryhneE5szRbCgMfTnv3sX24DXy_k/file\n",
            "Resolving ucc185e2fc79ecba4e590c7c745e.dl.dropboxusercontent.com (ucc185e2fc79ecba4e590c7c745e.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucc185e2fc79ecba4e590c7c745e.dl.dropboxusercontent.com (ucc185e2fc79ecba4e590c7c745e.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CmeKuYc-bUD_a_RbVGILCpogAg-hBCJ6omG7K43d-j1_4U67GE0LflQIJ9Lqk45wMmyp25CGscyiO7fTW0o0Eu_5vIQOLM2Pk4HsmBOIspzn-KU9agUnP1aY0_vlRUIYdGlqC3MJ5yIMFczvA6WToDN60R_H27EVYrY81oLNbW2zoUi6iPzCvzYpMbPsSP0H4hRG4PdZycQ_xAEP4A57R855CkAmmYBDIVQXxlUoHcJbtuJwikLfEdTzOtbV1AHa1GfwXxlWbNRU79NKVFFReAM3UTczwzdNH23VYRJVltlsJtw4MD5cXpcsBeoDAFbpTNwbv7FRaAI-moHBlR_15ksajfNz3hM_GbgLIO7Fhbx3jA/file [following]\n",
            "--2025-03-24 09:12:00--  https://ucc185e2fc79ecba4e590c7c745e.dl.dropboxusercontent.com/cd/0/inline2/CmeKuYc-bUD_a_RbVGILCpogAg-hBCJ6omG7K43d-j1_4U67GE0LflQIJ9Lqk45wMmyp25CGscyiO7fTW0o0Eu_5vIQOLM2Pk4HsmBOIspzn-KU9agUnP1aY0_vlRUIYdGlqC3MJ5yIMFczvA6WToDN60R_H27EVYrY81oLNbW2zoUi6iPzCvzYpMbPsSP0H4hRG4PdZycQ_xAEP4A57R855CkAmmYBDIVQXxlUoHcJbtuJwikLfEdTzOtbV1AHa1GfwXxlWbNRU79NKVFFReAM3UTczwzdNH23VYRJVltlsJtw4MD5cXpcsBeoDAFbpTNwbv7FRaAI-moHBlR_15ksajfNz3hM_GbgLIO7Fhbx3jA/file\n",
            "Reusing existing connection to ucc185e2fc79ecba4e590c7c745e.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21225801 (20M) [application/pdf]\n",
            "Saving to: ‘/content/data/nova_technical_report.pdf’\n",
            "\n",
            "/content/data/nova_ 100%[===================>]  20.24M  66.2MB/s    in 0.3s    \n",
            "\n",
            "2025-03-24 09:12:00 (66.2 MB/s) - ‘/content/data/nova_technical_report.pdf’ saved [21225801/21225801]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ihPA9WwiDHON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] =  userdata.get('LLAMA_CLOUD_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "W6axiqBgDTXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import Settings\n",
        "\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "42n42c39GZIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using LlamaParse with Auto-Mode"
      ],
      "metadata": {
        "id": "emqnzfKwW8Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cloud_services import LlamaParse\n",
        "\n",
        "file_path = \"/content/data/handwritten.pdf\"\n",
        "file_path = \"/content/data/nova_technical_report.pdf\"\n",
        "\n",
        "documents = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    auto_mode=True,\n",
        "    auto_mode_trigger_on_image_in_page=True,\n",
        "    auto_mode_trigger_on_table_in_page=True,\n",
        "    auto_mode_trigger_on_text_in_page=\"<text_on_page>\",\n",
        "    auto_mode_trigger_on_regexp_in_page=\"<regexp_on_page>\"\n",
        "\n",
        ").load_data(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HjD5220G9M8",
        "outputId": "60fc69ca-40f9-4a8a-b559-d688245ae8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id b296151c-dd7f-4b80-a701-36fb60f2e106\n",
            "......."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_documents = LlamaParse(result_type=\"markdown\").load_data(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4afDh4N0JU9W",
        "outputId": "9b9cb13b-a56e-47c0-a2c5-22f8d972c6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 1b4ca43f-1c3a-4f3a-ab3e-5fe059882906\n",
            "....."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating page nodes from our parsed documents"
      ],
      "metadata": {
        "id": "-BhlEw9xW1Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "def get_page_nodes(docs, seperator=\"\\n---\\n\"):\n",
        "    \"\"\"Split each document into page node, by separator.\"\"\"\n",
        "    nodes = []\n",
        "    for doc in docs:\n",
        "        doc_chunks = doc.text.split(seperator)\n",
        "        for doc_chunk in doc_chunks:\n",
        "            node = TextNode(\n",
        "                text=doc_chunk,\n",
        "                metadata=deepcopy(doc.metadata))\n",
        "            nodes.append(node)\n",
        "\n",
        "    return nodes"
      ],
      "metadata": {
        "id": "G92xnsD3Vqqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_nodes = get_page_nodes(documents)\n",
        "base_page_nodes = get_page_nodes(base_documents)"
      ],
      "metadata": {
        "id": "QgxpTo-pXJWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triggering on images"
      ],
      "metadata": {
        "id": "KjzJ_ZBBZS5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(page_nodes[0].get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UzdSUz8ZNjF",
        "outputId": "36f3f03f-f0a0-4c40-bdf0-2867ff232946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     The Amazon Nova Family of Models:\n",
            "                       Technical Report and Model Card\n",
            "                                 Amazon Artificial General Intelligence\n",
            "                       Nova\n",
            "                       Lite                                                     Nova\n",
            "                       Nova                <l>             <l >                 Micro     Ix          </>\n",
            "                       Pro    AIx\n",
            "</>\n",
            "=\n",
            "                      Nova                                                      Nova\n",
            "                     Canvas                                                      Reel\n",
            "                               Figure 1: The Amazon Nova family of models\n",
            "                                               Abstract\n",
            "       We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver\n",
            "       frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable\n",
            "       multimodal model with the best combination of accuracy, speed, and cost for a wide range of\n",
            "       tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing\n",
            "       images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our\n",
            "       lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that\n",
            "       creates professional grade images with rich customization controls. Amazon Nova Reel is a video\n",
            "       generation model offering high-quality outputs, customization, and motion control. Our models\n",
            "       were built responsibly and with a commitment to customer trust, security, and reliability. We report\n",
            "       benchmarking results for core capabilities, agentic performance, long context, functional adaptation,\n",
            "       runtime performance, and human evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_page_nodes[0].get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ZapWp7aM2G",
        "outputId": "4e192ed3-4f00-4f23-f45f-bbf521f7e388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# The Amazon Nova Family of Models: Technical Report and Model Card\n",
            "\n",
            "# Amazon Artificial General Intelligence\n",
            "\n",
            "# Abstract\n",
            "\n",
            "We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triggering on tables"
      ],
      "metadata": {
        "id": "xTdyKj0ja6Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(page_nodes[10].get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsFB1nRVabb2",
        "outputId": "d9140934-7dfd-4fec-d4ae-4bdbb1e9b04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# The Amazon Nova Family of Models\n",
            "\n",
            "| Nova Micro | Nova Lite | Nova Pro |\n",
            "|------------|-----------|----------|\n",
            "| [Heatmap representation of recall performance for Nova Micro with context lengths 32k, 64k, 128k and needle positions from 10 to 100] | [Heatmap representation of recall performance for Nova Lite with context lengths 32k, 64k, 128k, 200k, 256k, 300k and needle positions from 10 to 100] | [Heatmap representation of recall performance for Nova Pro with context lengths 32k, 64k, 128k, 200k, 256k, 300k and needle positions from 10 to 100] |\n",
            "\n",
            "Figure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.\n",
            "\n",
            "| | SQuALITY | LVBench |\n",
            "|---|----------|---------|\n",
            "| | ROUGE-L | accuracy |\n",
            "| Nova Pro | 19.8 ±8.7 | 41.6 ±2.5 |\n",
            "| Nova Lite | 19.2 ±8.6 | 40.4 ±2.4 |\n",
            "| Nova Micro | 18.8 ±8.6 | - |\n",
            "| Claude 3.5 Sonnet (Jun) | 13.4 ±7.5 | - |\n",
            "| Gemini 1.5 Pro (001) | - | 33.1 ±2.3 |\n",
            "| Gemini 1.5 Pro (002) | 19.1 ±8.6 M | - |\n",
            "| Gemini 1.5 Flash (002) | 18.1 ±8.4 M | - |\n",
            "| GPT-4o | 18.8 ±8.6 | 30.8 ±2.3 |\n",
            "| Llama 3 - 70B | 16.4 ±8.1 | - |\n",
            "| Llama 3 - 8B | 15.3 ±7.9 | - |\n",
            "\n",
            "Table 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For SQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3 report [45]. Gemini results were measured by us² (M). For LVBench, Gemini and GPT-4o numbers were taken from the corresponding benchmark leaderboard [77].\n",
            "\n",
            "Results for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering task, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other models. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any depth across both text and multimodal understanding use cases, delivering high accuracy and reliability.\n",
            "\n",
            "## 2.4 Functional expertise\n",
            "\n",
            "In addition to core capabilities, foundation models must perform well in particular specialties and domains. Across our many areas of performance analyses, we have selected four domains for which to present benchmarking results: Software engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can be found in Appendix B.3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_page_nodes[10].get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bt9QwSIalpJ",
        "outputId": "493bfe73-5921-41d3-ed9a-c20814981ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# The Amazon Nova Family of Models\n",
            "\n",
            "|Nova Micro|Nova Lite|Nova Pro|\n",
            "|---|---|---|\n",
            "|10|10|10|\n",
            "|20|20|20|\n",
            "|30|30|30|\n",
            "|40|40|40|\n",
            "|50|50|50|\n",
            "|60|60|60|\n",
            "|70|70|70|\n",
            "|80|80|80|\n",
            "|90|90|90|\n",
            "|100|100|100|\n",
            "\n",
            "Context Length\n",
            "\n",
            "Figure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.\n",
            "\n",
            "| |SQuALITY|LVBench|\n",
            "|---|---|---|\n",
            "|Nova Pro|19.8 ±8.7|41.6 ±2.5|\n",
            "|Nova Lite|19.2 ±8.6|40.4 ±2.4|\n",
            "|Nova Micro|18.8 ±8.6|-|\n",
            "|Claude 3.5 Sonnet (Jun)|13.4 ±7.5|-|\n",
            "|Gemini 1.5 Pro (001)|-|33.1 ±2.3|\n",
            "|Gemini 1.5 Pro (002)|19.1 ±8.6 M|-|\n",
            "|Gemini 1.5 Flash (002)|18.1 ±8.4 M|-|\n",
            "|GPT-4o|18.8 ±8.6|30.8 ±2.3|\n",
            "|Llama 3 - 70B|16.4 ±8.1|-|\n",
            "|Llama 3 - 8B|15.3 ±7.9|-|\n",
            "\n",
            "Table 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For SQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3 report [45]. Gemini results were measured by us2 (M). For LVBench, Gemini and GPT-4o numbers were taken from the corresponding benchmark leaderboard [77].\n",
            "\n",
            "Results for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering task, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other models. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any depth across both text and multimodal understanding use cases, delivering high accuracy and reliability.\n",
            "\n",
            "# 2.4 Functional expertise\n",
            "\n",
            "In addition to core capabilities, foundation models must perform well in particular specialties and domains. Across our many areas of performance analyses, we have selected four domains for which to present benchmarking results: Software engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can be found in Appendix B.3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rendering charts"
      ],
      "metadata": {
        "id": "U-6G67G5cxXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(page_nodes[13].get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNSS-SzEaiIj",
        "outputId": "89339910-fce9-431e-a29b-6a474140aa67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# The Amazon Nova Family of Models\n",
            "\n",
            "| Model | Time to First Token (sec) | Output Tokens per Second | Total Response Time (sec) |\n",
            "|-------|---------------------------|--------------------------|---------------------------|\n",
            "| Llama 2 7B | 0.29 | 283 | 0.7 |\n",
            "| Nova MICRO | 0.32 | 210 | 0.8 |\n",
            "| Gemini 1.5 Pro (Fast) | 0.35 | 190 | 0.9 |\n",
            "| Gemini 1.5 Pro (Fast) | 0.35 | 163 | 1.0 |\n",
            "| Mistral 8x7B | 0.36 | 157 | 1.0 |\n",
            "| Llama 2 13B | 0.36 | 157 | 1.1 |\n",
            "| Nova Lite | 0.37 | 124 | 1.2 |\n",
            "| Nova Pro | 0.38 | 115 | 1.3 |\n",
            "| Llama 2 70B | 0.42 | 113 | 1.4 |\n",
            "| GPT-3.5 | 0.42 | 100 | 1.5 |\n",
            "| Llama 2 34B | 0.46 | 73 | 1.7 |\n",
            "| Mistral Large 2 (Nov) | 0.53 | 64 | 2.4 |\n",
            "| GPT-4 Turbo | 0.62 | 58 | 2.7 |\n",
            "| Llama 2 34B8 | 0.72 | 57 | 2.8 |\n",
            "| Claude 2.1 (Fast) | 0.72 | 40 | 2.9 |\n",
            "| Claude 2.1 (Standard) | 0.87 | 35 | 3.4 |\n",
            "| Gemini 1.5 Pro (Fast) | 0.98 | 29 | 4.0 |\n",
            "\n",
            "Figure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of input and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov 29th, 2024).\n",
            "\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_page_nodes[13].get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlbFtJ0bdDJY",
        "outputId": "dccbddc4-7ff8-45a5-b8b6-cfbe1df050e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# The Amazon Nova Family of Models\n",
            "\n",
            "# Model Family\n",
            "\n",
            "|Meta|Amazon|Google|Mistral AI|OpenAI|Anthropic|\n",
            "|---|---|---|---|---|---|\n",
            "|1.0| | | |0.98| |\n",
            "|80.8| | |0.72|0.72|0.87|\n",
            "|80.6| |0.46|0.53|0.62| |\n",
            "|20.4|0.29|0.32|0.35|0.35|0.36|\n",
            "|0.36|0.37|0.38|0.42|0.42| |\n",
            "|2|20.2| | | | |\n",
            "|0.0|1~|9|1|1|8|\n",
            "|2|1|8|1ᴬ⁸|1|9|\n",
            "|1|2|1⁵|2|8|1~|\n",
            "|8|Tv|2|81|A|8|\n",
            "|3|2|8|2|2| |\n",
            "|0'| |J| |3|3|\n",
            "|A|K| |{|J| |\n",
            "|J| |8| |283| |\n",
            "|8²⁵⁰|210|190|200|163|157|\n",
            "|157|8|150| |124|115|\n",
            "|113|100| |73|64|58|\n",
            "|57|40|35|3|50|1|\n",
            "|9|9|2|2|{|2|\n",
            "|8|2|8|1⁵⁸|3|1|\n",
            "|2|8|1|1|~|3|\n",
            "|1|2|8|~|3|8|\n",
            "|4.0| | | |4.0| |\n",
            "|83.5| | |2.8|2.9|3.4|\n",
            "|23.0| |2.4|2.7| | |\n",
            "|L2.5| | | | | |\n",
            "|2.0|1.3|1.4|1.5|1.7| |\n",
            "|1.5|0.8|0.9|1.0|1.0|1,.2|\n",
            "|2|1.0|0.7|0.5|0.0|8|\n",
            "|{|0|2|2|1|g|\n",
            "|58|~|9|8|1|2|\n",
            "|1|2|8|1|K|2|\n",
            "|3|2|8|2|8|{|\n",
            "|3|8|2|3|8| |\n",
            "\n",
            "Figure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of input and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov 29th, 2024).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text-only pages"
      ],
      "metadata": {
        "id": "z7abZJJkdOFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(page_nodes[2].get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhd6LsiPdH4s",
        "outputId": "8d9c3821-f175-422f-d9aa-e3616bd5ab0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# The Amazon Nova Family of Models\n",
            "\n",
            "# 1. Introduction\n",
            "\n",
            "This document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance.\n",
            "\n",
            "# 1.1 Amazon Nova Pro, Lite, and Micro\n",
            "\n",
            "Key capabilities of Amazon Nova Pro, Lite, and Micro include:\n",
            "\n",
            "- Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard). Our multimodal models, Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output. These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA), Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web) in their respective intelligence tiers. These models are the first to offer video understanding capabilities on Amazon Bedrock, enabling deeper insights from multimedia content.\n",
            "- Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one of the fastest models in their respective intelligence tiers.\n",
            "- Agentic Workflows: Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the developer’s data.\n",
            "- Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro, Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\n",
            "- Price-Performance: Each model was optimized to deliver exceptional price-performance value, offering state-of-the-art performance on key benchmarks at low cost.\n",
            "\n",
            "Amazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series of training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal data. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular emphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning stages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and reward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via methods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that the final models are aligned with human preferences in both quality and responsibility.\n",
            "\n",
            "# 1.2 Amazon Nova Canvas and Reel\n",
            "\n",
            "Amazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images and videos, for a wide range of applications such as advertising, marketing, and entertainment.\n",
            "\n",
            "Amazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\n",
            "\n",
            "- Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to 2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or color palette, or to generate variations of an image.\n",
            "- Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting through natural language mask prompts. These mask prompts describe the specific area of the input image that needs to be repainted. The user can also easily change a background with the background removal feature, leaving the subject of the image unchanged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_page_nodes[2].get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3CY7YDddQ38",
        "outputId": "ecc8ece9-f547-441d-b44a-1b41281be7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# The Amazon Nova Family of Models\n",
            "\n",
            "# 1 Introduction\n",
            "\n",
            "This document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance.\n",
            "\n",
            "# 1.1 Amazon Nova Pro, Lite, and Micro\n",
            "\n",
            "Key capabilities of Amazon Nova Pro, Lite, and Micro include:\n",
            "\n",
            "- Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard). Our multimodal models, Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output. These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA), Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web) in their respective intelligence tiers. These models are the first to offer video understanding capabilities on Amazon Bedrock, enabling deeper insights from multimedia content.\n",
            "- Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one of the fastest models in their respective intelligence tiers.\n",
            "- Agentic Workflows: Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the developer’s data.\n",
            "- Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro, Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\n",
            "- Price-Performance: Each model was optimized to deliver exceptional price-performance value, offering state-of-the-art performance on key benchmarks at low cost.\n",
            "\n",
            "Amazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series of training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal data. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular emphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning stages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and reward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via methods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that the final models are aligned with human preferences in both quality and responsibility.\n",
            "\n",
            "# 1.2 Amazon Nova Canvas and Reel\n",
            "\n",
            "Amazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images and videos, for a wide range of applications such as advertising, marketing, and entertainment.\n",
            "\n",
            "Amazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\n",
            "\n",
            "- Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to 2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or color palette, or to generate variations of an image.\n",
            "- Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting through natural language mask prompts. These mask prompts describe the specific area of the input image that needs to be repainted. The user can also easily change a background with the background removal feature, leaving the subject of the image unchanged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triggering on strings"
      ],
      "metadata": {
        "id": "S3h4UXhfdX30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    auto_mode=True,\n",
        "    auto_mode_trigger_on_text_in_page=\"agents\"\n",
        ")\n",
        "\n",
        "agent_documents = agent_parser.load_data(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gf0W9A2dTxL",
        "outputId": "7a571d06-6508-4883-add4-e7e8b60c6a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 1be3e512-c246-4b2d-a0d2-0100fece3d16\n",
            "....."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_output = agent_parser.get_json_result(file_path)\n",
        "json_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbBgDlsUeVgY",
        "outputId": "1629ce22-de26-41ba-f4ad-7cca8b1219e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 52fb78c8-3815-4cca-b12c-4c7c947d36ce\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'pages': [{'page': 1,\n",
              "    'text': '                     The Amazon Nova Family of Models:\\n                       Technical Report and Model Card\\n                                 Amazon Artificial General Intelligence\\n                       Nova\\n                       Lite                                                     Nova\\n                       Nova                <l>             <l >                 Micro     Ix          </>\\n                       Pro    AIx\\n</>\\n=\\n                      Nova                                                      Nova\\n                     Canvas                                                      Reel\\n                               Figure 1: The Amazon Nova family of models\\n                                               Abstract\\n       We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver\\n       frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable\\n       multimodal model with the best combination of accuracy, speed, and cost for a wide range of\\n       tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing\\n       images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our\\n       lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that\\n       creates professional grade images with rich customization controls. Amazon Nova Reel is a video\\n       generation model offering high-quality outputs, customization, and motion control. Our models\\n       were built responsibly and with a commitment to customer trust, security, and reliability. We report\\n       benchmarking results for core capabilities, agentic performance, long context, functional adaptation,\\n       runtime performance, and human evaluation.',\n",
              "    'md': '# The Amazon Nova Family of Models: Technical Report and Model Card\\n\\n# Amazon Artificial General Intelligence\\n\\n# Abstract\\n\\nWe present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.\\n\\n# Figure 1: The Amazon Nova family of models',\n",
              "    'images': [{'name': 'img_p0_1.png',\n",
              "      'height': 1540,\n",
              "      'width': 1952,\n",
              "      'x': 72,\n",
              "      'y': 225.5598,\n",
              "      'original_width': 1952,\n",
              "      'original_height': 1540,\n",
              "      'ocr': [{'x': 945,\n",
              "        'y': 471,\n",
              "        'w': 240,\n",
              "        'h': 98,\n",
              "        'confidence': '0.9999932646751404',\n",
              "        'text': 'Nova'},\n",
              "       {'x': 974,\n",
              "        'y': 590,\n",
              "        'w': 180,\n",
              "        'h': 96,\n",
              "        'confidence': '0.9999980926513672',\n",
              "        'text': 'Lite'},\n",
              "       {'x': 946,\n",
              "        'y': 774,\n",
              "        'w': 241,\n",
              "        'h': 99,\n",
              "        'confidence': '0.9999923706054688',\n",
              "        'text': 'Nova'},\n",
              "       {'x': 1802,\n",
              "        'y': 860,\n",
              "        'w': 94,\n",
              "        'h': 50,\n",
              "        'confidence': '0.1643972396850586',\n",
              "        'text': '<l>'},\n",
              "       {'x': 987,\n",
              "        'y': 897,\n",
              "        'w': 158,\n",
              "        'h': 92,\n",
              "        'confidence': '0.9999774252650832',\n",
              "        'text': 'Pro'},\n",
              "       {'x': 1283,\n",
              "        'y': 931,\n",
              "        'w': 40,\n",
              "        'h': 42,\n",
              "        'confidence': '0.9993208608148194',\n",
              "        'text': 'A'},\n",
              "       {'x': 1307,\n",
              "        'y': 941,\n",
              "        'w': 60,\n",
              "        'h': 72,\n",
              "        'confidence': '0.463554484308421',\n",
              "        'text': 'Ix'},\n",
              "       {'x': 66,\n",
              "        'y': 1028,\n",
              "        'w': 94,\n",
              "        'h': 54,\n",
              "        'confidence': '0.31367990980962945',\n",
              "        'text': '</>'},\n",
              "       {'x': 61,\n",
              "        'y': 1313,\n",
              "        'w': 86,\n",
              "        'h': 116,\n",
              "        'confidence': '0.9766882835297253',\n",
              "        'text': '='}]},\n",
              "     {'name': 'img_p0_2.png',\n",
              "      'height': 554,\n",
              "      'width': 1928,\n",
              "      'x': 329.401,\n",
              "      'y': 278.37412,\n",
              "      'original_width': 1928,\n",
              "      'original_height': 554,\n",
              "      'ocr': [{'x': 905,\n",
              "        'y': 144,\n",
              "        'w': 236,\n",
              "        'h': 92,\n",
              "        'confidence': '0.9999958872795105',\n",
              "        'text': 'Nova'},\n",
              "       {'x': 894,\n",
              "        'y': 259,\n",
              "        'w': 261,\n",
              "        'h': 102,\n",
              "        'confidence': '0.6382414265111352',\n",
              "        'text': 'Micro'},\n",
              "       {'x': 1267,\n",
              "        'y': 317,\n",
              "        'w': 60,\n",
              "        'h': 72,\n",
              "        'confidence': '0.463554484308421',\n",
              "        'text': 'Ix'},\n",
              "       {'x': 58,\n",
              "        'y': 366,\n",
              "        'w': 94,\n",
              "        'h': 50,\n",
              "        'confidence': '0.16095946729183197',\n",
              "        'text': '<l >'},\n",
              "       {'x': 1776,\n",
              "        'y': 372,\n",
              "        'w': 98,\n",
              "        'h': 52,\n",
              "        'confidence': '0.17453782055061207',\n",
              "        'text': '</>'}]},\n",
              "     {'name': 'img_p0_3.png',\n",
              "      'height': 572,\n",
              "      'width': 1944,\n",
              "      'x': 72,\n",
              "      'y': 398.66996,\n",
              "      'original_width': 1944,\n",
              "      'original_height': 572,\n",
              "      'ocr': [{'x': 895,\n",
              "        'y': 125,\n",
              "        'w': 241,\n",
              "        'h': 98,\n",
              "        'confidence': '0.7646325282365729',\n",
              "        'text': 'Nova'},\n",
              "       {'x': 844,\n",
              "        'y': 244,\n",
              "        'w': 345,\n",
              "        'h': 104,\n",
              "        'confidence': '0.9997395603041859',\n",
              "        'text': 'Canvas'}]},\n",
              "     {'name': 'img_p0_4.png',\n",
              "      'height': 572,\n",
              "      'width': 1944,\n",
              "      'x': 329.401,\n",
              "      'y': 398.66996,\n",
              "      'original_width': 1944,\n",
              "      'original_height': 572,\n",
              "      'ocr': [{'x': 897,\n",
              "        'y': 127,\n",
              "        'w': 239,\n",
              "        'h': 97,\n",
              "        'confidence': '0.9999892711639404',\n",
              "        'text': 'Nova'},\n",
              "       {'x': 917,\n",
              "        'y': 249,\n",
              "        'w': 202,\n",
              "        'h': 92,\n",
              "        'confidence': '0.9999985098838806',\n",
              "        'text': 'Reel'}]}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models: Technical Report and Model Card',\n",
              "      'md': '# The Amazon Nova Family of Models: Technical Report and Model Card',\n",
              "      'bBox': {'x': 169, 'y': 96.78, 'w': 285, 'h': 326.22}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Amazon Artificial General Intelligence',\n",
              "      'md': '# Amazon Artificial General Intelligence',\n",
              "      'bBox': {'x': 223, 'y': 179.04, 'w': 165, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Abstract',\n",
              "      'md': '# Abstract',\n",
              "      'bBox': {'x': 283, 'y': 524.04, 'w': 44, 'h': 12}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.',\n",
              "      'md': 'We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.',\n",
              "      'bBox': {'x': 107, 'y': 276, 'w': 398, 'h': 393.04}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Figure 1: The Amazon Nova family of models',\n",
              "      'md': '# Figure 1: The Amazon Nova family of models',\n",
              "      'bBox': {'x': 169, 'y': 276, 'w': 285, 'h': 206.04}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 2,\n",
              "    'text': '                                   The Amazon Nova Family of Models\\nContents\\n1  Introduction                                                                                                        3\\n   1.1  Amazon Nova Pro, Lite, and Micro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       3\\n   1.2  Amazon Nova Canvas and Reel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        3\\n2  Amazon Nova Pro, Lite, and Micro Evaluations                                                                        5\\n   2.1  Core capability public benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      5\\n        2.1.1      Core capability text benchmarks and results . . . . . . . . . . . . . . . . . . . . . . . . . . .   5\\n        2.1.2        Core capability multimodal benchmarks and results . . . . . . . . . . . . . . . . . . . . . . .   7\\n   2.2  Agentic workflows    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   8\\n        2.2.1      Agentic text benchmarks and results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   9\\n        2.2.2        Agentic multimodal benchmarks and results . . . . . . . . . . . . . . . . . . . . . . . . . . .   9\\n   2.3  Long context   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  10\\n   2.4  Functional expertise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  11\\n        2.4.1     Software engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12\\n        2.4.2   Financial analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12\\n        2.4.3   Retrieval augmented generation     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12\\n   2.5  Runtime performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     13\\n3  Amazon Nova Canvas Evaluation                                                                                      15\\n   3.1  Automated metrics    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  15\\n   3.2  Human evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    15\\n4  Amazon Nova Reel Evaluation                                                                                        16\\n   4.1  Human evaluation metrics     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  16\\n   4.2  Dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  16\\n   4.3  Implementation details & results   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\\n5  Responsible AI                                                                                                     17\\n   5.1  Defining our RAI objectives    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\\n   5.2  Ensuring adherence to RAI objectives     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  18\\n   5.3  RAI Evaluation   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19\\n   5.4  Red Teaming    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19\\n        5.4.1     Internal Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19\\n        5.4.2   External Red Teaming     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  20\\n        5.4.3        Automated Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21\\n6  Training Infrastructure                                                                                            21\\nA  Amazon Nova Canvas Capabilities                                                                                    28\\nB  Prompts and Scoring                                                                                                30\\nC  Qualitative examples of multimodal intelligence                                                                    39\\nD  Correspondence and Contributors                                                                                    43\\n                                                   2',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# Contents\\n\\n1. Introduction\\n2. 1. Amazon Nova Pro, Lite, and Micro\\n2. Amazon Nova Canvas and Reel\\n\\nAmazon Nova Pro, Lite, and Micro Evaluations\\n3. 1. Core capability public benchmarks\\n2. 1. Core capability text benchmarks and results\\n2. Core capability multimodal benchmarks and results\\n\\nAgentic workflows\\n3. 1. Agentic text benchmarks and results\\n2. Agentic multimodal benchmarks and results\\n\\nLong context\\n4. Functional expertise\\n5. 1. Software engineering\\n2. Financial analysis\\n3. Retrieval augmented generation\\n\\nRuntime performance\\n\\nAmazon Nova Canvas Evaluation\\n4. 1. Automated metrics\\n2. Human evaluation\\n\\nAmazon Nova Reel Evaluation\\n5. 1. Human evaluation metrics\\n2. Dataset\\n3. Implementation details & results\\n\\nResponsible AI\\n6. 1. Defining our RAI objectives\\n2. Ensuring adherence to RAI objectives\\n3. RAI Evaluation\\n4. Red Teaming\\n\\nTraining Infrastructure\\n7. A Amazon Nova Canvas Capabilities\\n8. B Prompts and Scoring\\n9. C Qualitative examples of multimodal intelligence\\n10. D Correspondence and Contributors',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 72, 'y': 34.04, 'w': 306, 'h': 541}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Contents',\n",
              "      'md': '# Contents',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 46, 'h': 483.99}},\n",
              "     {'type': 'text',\n",
              "      'value': '1. Introduction\\n2. 1. Amazon Nova Pro, Lite, and Micro\\n2. Amazon Nova Canvas and Reel\\n\\nAmazon Nova Pro, Lite, and Micro Evaluations\\n3. 1. Core capability public benchmarks\\n2. 1. Core capability text benchmarks and results\\n2. Core capability multimodal benchmarks and results\\n\\nAgentic workflows\\n3. 1. Agentic text benchmarks and results\\n2. Agentic multimodal benchmarks and results\\n\\nLong context\\n4. Functional expertise\\n5. 1. Software engineering\\n2. Financial analysis\\n3. Retrieval augmented generation\\n\\nRuntime performance\\n\\nAmazon Nova Canvas Evaluation\\n4. 1. Automated metrics\\n2. Human evaluation\\n\\nAmazon Nova Reel Evaluation\\n5. 1. Human evaluation metrics\\n2. Dataset\\n3. Implementation details & results\\n\\nResponsible AI\\n6. 1. Defining our RAI objectives\\n2. Ensuring adherence to RAI objectives\\n3. RAI Evaluation\\n4. Red Teaming\\n\\nTraining Infrastructure\\n7. A Amazon Nova Canvas Capabilities\\n8. B Prompts and Scoring\\n9. C Qualitative examples of multimodal intelligence\\n10. D Correspondence and Contributors',\n",
              "      'md': '1. Introduction\\n2. 1. Amazon Nova Pro, Lite, and Micro\\n2. Amazon Nova Canvas and Reel\\n\\nAmazon Nova Pro, Lite, and Micro Evaluations\\n3. 1. Core capability public benchmarks\\n2. 1. Core capability text benchmarks and results\\n2. Core capability multimodal benchmarks and results\\n\\nAgentic workflows\\n3. 1. Agentic text benchmarks and results\\n2. Agentic multimodal benchmarks and results\\n\\nLong context\\n4. Functional expertise\\n5. 1. Software engineering\\n2. Financial analysis\\n3. Retrieval augmented generation\\n\\nRuntime performance\\n\\nAmazon Nova Canvas Evaluation\\n4. 1. Automated metrics\\n2. Human evaluation\\n\\nAmazon Nova Reel Evaluation\\n5. 1. Human evaluation metrics\\n2. Dataset\\n3. Implementation details & results\\n\\nResponsible AI\\n6. 1. Defining our RAI objectives\\n2. Ensuring adherence to RAI objectives\\n3. RAI Evaluation\\n4. Red Teaming\\n\\nTraining Infrastructure\\n7. A Amazon Nova Canvas Capabilities\\n8. B Prompts and Scoring\\n9. C Qualitative examples of multimodal intelligence\\n10. D Correspondence and Contributors',\n",
              "      'bBox': {'x': 72, 'y': 93.04, 'w': 468, 'h': 657}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 3,\n",
              "    'text': '                                      The Amazon Nova Family of Models\\n1   Introduction\\nThis document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier\\nintelligence and industry-leading price performance.\\n1.1   Amazon Nova Pro, Lite, and Micro\\nKey capabilities of Amazon Nova Pro, Lite, and Micro include:\\n       •  Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and\\n          process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in\\n          its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning\\n          (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard).                 Our multimodal models,\\n          Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output.\\n          These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA),\\n          Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web)\\n          in their respective intelligence tiers. These models are the first to offer video understanding capabilities on\\n          Amazon Bedrock, enabling deeper insights from multimedia content.\\n       •  Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one\\n          of the fastest models in their respective intelligence tiers.\\n       •  Agentic Workflows:        Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down\\n          and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel\\n          at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the\\n          developer’s data.\\n       •  Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro,\\n          Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also\\n          run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\\n       •  Price-Performance:    Each model was optimized to deliver exceptional price-performance value, offering\\n          state-of-the-art performance on key benchmarks at low cost.\\nAmazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series\\nof training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal\\ndata. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source\\ndatasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular\\nemphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian,\\nSimplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning\\nstages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and\\nreward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via\\nmethods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that\\nthe final models are aligned with human preferences in both quality and responsibility.\\n1.2   Amazon Nova Canvas and Reel\\n    Amazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images\\nand videos, for a wide range of applications such as advertising, marketing, and entertainment.\\nAmazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\\n       •  Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to\\n          2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M\\n          pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or\\n          color palette, or to generate variations of an image.\\n       •  Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting\\n          through natural language mask prompts. These mask prompts describe the specific area of the input image that\\n          needs to be repainted. The user can also easily change a background with the background removal feature,\\n          leaving the subject of the image unchanged.\\n                                                        3',\n",
              "    'md': \"The Amazon Nova Family of Models\\n\\n# 1 Introduction\\n\\nThis document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance.\\n\\n## 1.1 Amazon Nova Pro, Lite, and Micro\\n\\nKey capabilities of Amazon Nova Pro, Lite, and Micro include:\\n\\n- Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard). Our multimodal models, Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output. These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA), Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web) in their respective intelligence tiers. These models are the first to offer video understanding capabilities on Amazon Bedrock, enabling deeper insights from multimedia content.\\n\\n- Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one of the fastest models in their respective intelligence tiers.\\n\\n- Agentic Workflows: Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the developer's data.\\n\\n- Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro, Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\\n\\n- Price-Performance: Each model was optimized to deliver exceptional price-performance value, offering state-of-the-art performance on key benchmarks at low cost.\\n\\nAmazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series of training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal data. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular emphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning stages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and reward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via methods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that the final models are aligned with human preferences in both quality and responsibility.\\n\\n## 1.2 Amazon Nova Canvas and Reel\\n\\nAmazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images and videos, for a wide range of applications such as advertising, marketing, and entertainment.\\n\\nAmazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\\n\\n- Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to 2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or color palette, or to generate variations of an image.\\n\\n- Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting through natural language mask prompts. These mask prompts describe the specific area of the input image that needs to be repainted. The user can also easily change a background with the background removal feature, leaving the subject of the image unchanged.\",\n",
              "    'images': [{'name': 'page_3.jpg',\n",
              "      'height': 792,\n",
              "      'width': 612,\n",
              "      'x': 0,\n",
              "      'y': 0,\n",
              "      'original_width': 1236,\n",
              "      'original_height': 1600,\n",
              "      'type': 'full_page_screenshot'}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'text',\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': 'The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '1 Introduction',\n",
              "      'md': '# 1 Introduction',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 82, 'h': 12}},\n",
              "     {'type': 'text',\n",
              "      'value': 'This document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance.',\n",
              "      'md': 'This document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance.',\n",
              "      'bBox': {'x': 71, 'y': 97.04, 'w': 468, 'h': 21}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 2,\n",
              "      'value': '1.1 Amazon Nova Pro, Lite, and Micro',\n",
              "      'md': '## 1.1 Amazon Nova Pro, Lite, and Micro',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 171, 'h': 74.99}},\n",
              "     {'type': 'text',\n",
              "      'value': \"Key capabilities of Amazon Nova Pro, Lite, and Micro include:\\n\\n- Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard). Our multimodal models, Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output. These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA), Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web) in their respective intelligence tiers. These models are the first to offer video understanding capabilities on Amazon Bedrock, enabling deeper insights from multimedia content.\\n\\n- Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one of the fastest models in their respective intelligence tiers.\\n\\n- Agentic Workflows: Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the developer's data.\\n\\n- Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro, Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\\n\\n- Price-Performance: Each model was optimized to deliver exceptional price-performance value, offering state-of-the-art performance on key benchmarks at low cost.\\n\\nAmazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series of training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal data. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular emphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning stages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and reward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via methods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that the final models are aligned with human preferences in both quality and responsibility.\",\n",
              "      'md': \"Key capabilities of Amazon Nova Pro, Lite, and Micro include:\\n\\n- Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard). Our multimodal models, Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output. These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA), Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web) in their respective intelligence tiers. These models are the first to offer video understanding capabilities on Amazon Bedrock, enabling deeper insights from multimedia content.\\n\\n- Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one of the fastest models in their respective intelligence tiers.\\n\\n- Agentic Workflows: Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the developer's data.\\n\\n- Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro, Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\\n\\n- Price-Performance: Each model was optimized to deliver exceptional price-performance value, offering state-of-the-art performance on key benchmarks at low cost.\\n\\nAmazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series of training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal data. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular emphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning stages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and reward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via methods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that the final models are aligned with human preferences in both quality and responsibility.\",\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 470, 'h': 471.99}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 2,\n",
              "      'value': '1.2 Amazon Nova Canvas and Reel',\n",
              "      'md': '## 1.2 Amazon Nova Canvas and Reel',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 155, 'h': 497.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Amazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images and videos, for a wide range of applications such as advertising, marketing, and entertainment.\\n\\nAmazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\\n\\n- Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to 2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or color palette, or to generate variations of an image.\\n\\n- Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting through natural language mask prompts. These mask prompts describe the specific area of the input image that needs to be repainted. The user can also easily change a background with the background removal feature, leaving the subject of the image unchanged.',\n",
              "      'md': 'Amazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images and videos, for a wide range of applications such as advertising, marketing, and entertainment.\\n\\nAmazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\\n\\n- Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to 2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or color palette, or to generate variations of an image.\\n\\n- Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting through natural language mask prompts. These mask prompts describe the specific area of the input image that needs to be repainted. The user can also easily change a background with the background removal feature, leaving the subject of the image unchanged.',\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 469, 'h': 649.99}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': True,\n",
              "    'parsingMode': 'premium',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 4,\n",
              "    'text': '                                            The Amazon Nova Family of Models\\n      Amazon Nova Reel offers the following functionalities:\\n             • Generate videos from a text prompt: Amazon Nova Reel can generate high-quality videos of 6-second duration\\n               (720p resolution at 24 frames per second) from a text prompt.\\n             •          Generate videos from a reference image and a prompt: Amazon Nova Reel brings images to motion and\\n               generates videos that are guided by the input image and a text prompt.\\n             •    Camera motion control using a text prompt: With camera motion control in Amazon Nova Reel, the user can\\n                 guide camera motion with text prompts like “zoom” and “dolly forward” to get the exact visual needed for\\n                 each video. Amazon Nova Reel supports more than 20 camera motions. For more details, please refer to our\\n               prompting guide1.\\n             Amazon Nova Canvas and Reel are latent diffusion models [61] where a Variational AutoEncoder (VAE) [41] maps\\n     the image or video frames to latent variables on which the diffusion process happens. A text encoder tokenizes input\\n     text prompts into tokens which are then passed to the diffusion model as a conditioning signal. At inference time, a\\n     latent variable is initialized with random noise sampled from a Gaussian distribution, which is then denoised by the\\n trained diffusion model iteratively into a clean latent variable. The clean latent variable is decoded back to images or\\n      video frames by the decoder of the VAE. Both models underwent a two-phased approach of pretraining and fine-tuning.\\nPretraining data were sourced from a variety of sources, including licensed data, proprietary data, open source datasets,\\n      and publicly available data where appropriate.    Our highly scalable data filtering, deduplication, and enrichment\\n      pipelines were based on AWS EMR [2] and AWS Batch [1], as well as other AWS services.\\n         1https://docs.aws.amazon.com/nova/latest/userguide\\n                                                              4',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\nAmazon Nova Reel offers the following functionalities:\\n\\n- Generate videos from a text prompt: Amazon Nova Reel can generate high-quality videos of 6-second duration (720p resolution at 24 frames per second) from a text prompt.\\n- Generate videos from a reference image and a prompt: Amazon Nova Reel brings images to motion and generates videos that are guided by the input image and a text prompt.\\n- Camera motion control using a text prompt: With camera motion control in Amazon Nova Reel, the user can guide camera motion with text prompts like “zoom” and “dolly forward” to get the exact visual needed for each video. Amazon Nova Reel supports more than 20 camera motions. For more details, please refer to our prompting guide1.\\n\\nAmazon Nova Canvas and Reel are latent diffusion models[61] where a Variational AutoEncoder (VAE)[41] maps the image or video frames to latent variables on which the diffusion process happens. A text encoder tokenizes input text prompts into tokens which are then passed to the diffusion model as a conditioning signal. At inference time, a latent variable is initialized with random noise sampled from a Gaussian distribution, which is then denoised by the trained diffusion model iteratively into a clean latent variable. The clean latent variable is decoded back to images or video frames by the decoder of the VAE. Both models underwent a two-phased approach of pretraining and fine-tuning. Pretraining data were sourced from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. Our highly scalable data filtering, deduplication, and enrichment pipelines were based on AWS EMR[2] and AWS Batch[1], as well as other AWS services.\\n\\n1https://docs.aws.amazon.com/nova/latest/userguide',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Amazon Nova Reel offers the following functionalities:\\n\\n- Generate videos from a text prompt: Amazon Nova Reel can generate high-quality videos of 6-second duration (720p resolution at 24 frames per second) from a text prompt.\\n- Generate videos from a reference image and a prompt: Amazon Nova Reel brings images to motion and generates videos that are guided by the input image and a text prompt.\\n- Camera motion control using a text prompt: With camera motion control in Amazon Nova Reel, the user can guide camera motion with text prompts like “zoom” and “dolly forward” to get the exact visual needed for each video. Amazon Nova Reel supports more than 20 camera motions. For more details, please refer to our prompting guide1.\\n\\nAmazon Nova Canvas and Reel are latent diffusion models[61] where a Variational AutoEncoder (VAE)[41] maps the image or video frames to latent variables on which the diffusion process happens. A text encoder tokenizes input text prompts into tokens which are then passed to the diffusion model as a conditioning signal. At inference time, a latent variable is initialized with random noise sampled from a Gaussian distribution, which is then denoised by the trained diffusion model iteratively into a clean latent variable. The clean latent variable is decoded back to images or video frames by the decoder of the VAE. Both models underwent a two-phased approach of pretraining and fine-tuning. Pretraining data were sourced from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. Our highly scalable data filtering, deduplication, and enrichment pipelines were based on AWS EMR[2] and AWS Batch[1], as well as other AWS services.\\n\\n1https://docs.aws.amazon.com/nova/latest/userguide',\n",
              "      'md': 'Amazon Nova Reel offers the following functionalities:\\n\\n- Generate videos from a text prompt: Amazon Nova Reel can generate high-quality videos of 6-second duration (720p resolution at 24 frames per second) from a text prompt.\\n- Generate videos from a reference image and a prompt: Amazon Nova Reel brings images to motion and generates videos that are guided by the input image and a text prompt.\\n- Camera motion control using a text prompt: With camera motion control in Amazon Nova Reel, the user can guide camera motion with text prompts like “zoom” and “dolly forward” to get the exact visual needed for each video. Amazon Nova Reel supports more than 20 camera motions. For more details, please refer to our prompting guide1.\\n\\nAmazon Nova Canvas and Reel are latent diffusion models[61] where a Variational AutoEncoder (VAE)[41] maps the image or video frames to latent variables on which the diffusion process happens. A text encoder tokenizes input text prompts into tokens which are then passed to the diffusion model as a conditioning signal. At inference time, a latent variable is initialized with random noise sampled from a Gaussian distribution, which is then denoised by the trained diffusion model iteratively into a clean latent variable. The clean latent variable is decoded back to images or video frames by the decoder of the VAE. Both models underwent a two-phased approach of pretraining and fine-tuning. Pretraining data were sourced from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. Our highly scalable data filtering, deduplication, and enrichment pipelines were based on AWS EMR[2] and AWS Batch[1], as well as other AWS services.\\n\\n1https://docs.aws.amazon.com/nova/latest/userguide',\n",
              "      'bBox': {'x': 71, 'y': 72.04, 'w': 470, 'h': 678}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://docs.aws.amazon.com/nova/latest/userguide',\n",
              "      'text': 'https://docs.aws.amazon.com/nova/latest/userguide'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 5,\n",
              "    'text': '                                           The Amazon Nova Family of Models\\n     2   Amazon Nova Pro, Lite, and Micro Evaluations\\n     In this section, we report benchmarking results for Amazon Nova models and for select publicly-available models,\\n     including by citing existing public results and by measuring their performance.2 In cases for which the result is a\\n     simple average of binary scores, we assume a Gaussian distribution for the sample and approximate the 95% confidence\\n     interval as:                            CI(S) = 1.96 × r S × (1 − S)                                         (1)\\n                                                                     N\\n     where CI  is the 95% confidence interval, S is the measured score for the benchmark, and N is the sample size [48, 45].\\n     2.1  Core capability public benchmarks\\n     We evaluate Amazon Nova models on a suite of automated public benchmarks to assess core capabilities, including for\\n     both text-only (Section 2.1.1) and multimodal (Section 2.1.2) use cases.\\n     2.1.1  Core capability text benchmarks and results\\n        We evaluate select core capabilities of Amazon Nova models on a variety of public text-only benchmarks, spanning\\n     general knowledge, reasoning, language understanding, multilinguality, and instruction following.\\n         The following list briefly describes our selected text-only benchmarks. The prompts used for evaluation of each\\n     benchmark are summarized in Appendix B.1.\\n            •  MMLU [36]: Massive Multitask Language Understanding (MMLU) is a multiple-choice question answering\\n               benchmark that covers 57 subject areas across STEM, humanities, and social sciences. Subjects include law,\\n               physics, mathematics, computer science, history, and more. The difficulty levels vary from elementary level to\\n               advanced professional level, focusing on both world knowledge and problem solving abilities. We use 0-shot\\n               Chain-of-Thought (CoT) [79] for prompting and report the macro average exact match accuracy across all\\n               subjects.\\n            •  ARC-C [22]: The AI2’s Reasoning Challenge (ARC) is a multiple-choice question-answering dataset, which\\n               contains science questions from grade 3 to grade 9 exams. We use 0-shot CoT for prompting and report exact\\n               match accuracy.\\n            •  DROP [26]: Discrete Reasoning Over Paragraphs (DROP) is a crowdsourced reading comprehension dataset\\n               that requires reasoning and operating over multiple input positions from the reference text. We use 0-shot CoT\\n               for prompting and report f1 score.\\n            • GPQA [64]: Graduate-level Google-Proof Question and Answering (GPQA) is a challenging and high-quality\\n               multiple-choice question answering benchmark written by domain experts who have or are pursuing PhDs in\\n               biology, physics, and chemistry. We use 0-shot CoT for prompting and report exact match accuracy on the\\n               main set.\\n            •  MATH [37]: MATH is a mathematics problem solving benchmark, consisting of problems from mathematics\\n               competitions including the American Mathematics Competitions (AMC 10 and AMC 12), the American\\n               Invitational Mathematics Examination (AIME) and more. We use 0-shot CoT for prompting and report the\\n               exact match accuracy on the MATH5k set.\\n                     • GSM8K [23]: Grade School Math 8K (GSM8K) is a math benchmark consisting of 8,500 high-quality and\\n               diverse grade school math problems. The benchmark tests basic mathematical problem solving capabilities,\\n               requiring multi-step reasoning. We use 0-shot CoT for prompting and report the exact match accuracy on the\\n               test set containing 1,319 samples.\\n            •  IFEval [89]: IFeval is an instruction-following benchmark, which evaluates a model’s capability of following\\n        “verifiable instructions” such as “mention the keyword of AI at least 3 times”. The dataset contains 25 types of\\n               verifiable instructions and in total 541 prompts, where each prompt contains one or more verifiable instructions\\n               in natural language. We report the instruction-level accuracy under loose constraints.\\n            •  BBH [72]: Big Bench Hard (BBH) is a diverse benchmark consisting of an aggregate of 23 diverse subjects that\\n               cover algorithmic and NLP tasks ranging from casual logic tasks to word sorting and movie recommendations.\\n               The tasks are both multiple choice and open generation tasks. We report the macro average exact match\\n               accuracy across the subjects.\\n2Results measured internally by Amazon for evaluation purposes after Amazon Nova models completed training using (i) the\\n     Bedrock API for Claude and Meta models or (ii) the OpenAI API or Gemini API, as applicable.\\n                                                            5',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# 2 Amazon Nova Pro, Lite, and Micro Evaluations\\n\\nIn this section, we report benchmarking results for Amazon Nova models and for select publicly-available models, including by citing existing public results and by measuring their performance.2 In cases for which the result is a simple average of binary scores, we assume a Gaussian distribution for the sample and approximate the 95% confidence interval as:\\n\\nCI(S) = 1.96 × r S × (1 − S)\\nN\\n\\nwhere CI is the 95% confidence interval, S is the measured score for the benchmark, and N is the sample size [48, 45].\\n\\n# 2.1 Core capability public benchmarks\\n\\nWe evaluate Amazon Nova models on a suite of automated public benchmarks to assess core capabilities, including for both text-only (Section 2.1.1) and multimodal (Section 2.1.2) use cases.\\n\\n# 2.1.1 Core capability text benchmarks and results\\n\\nWe evaluate select core capabilities of Amazon Nova models on a variety of public text-only benchmarks, spanning general knowledge, reasoning, language understanding, multilinguality, and instruction following.\\n\\nThe following list briefly describes our selected text-only benchmarks. The prompts used for evaluation of each benchmark are summarized in Appendix B.1.\\n\\n- MMLU [36]: Massive Multitask Language Understanding (MMLU) is a multiple-choice question answering benchmark that covers 57 subject areas across STEM, humanities, and social sciences. Subjects include law, physics, mathematics, computer science, history, and more. The difficulty levels vary from elementary level to advanced professional level, focusing on both world knowledge and problem solving abilities. We use 0-shot Chain-of-Thought (CoT) [79] for prompting and report the macro average exact match accuracy across all subjects.\\n- ARC-C [22]: The AI2’s Reasoning Challenge (ARC) is a multiple-choice question-answering dataset, which contains science questions from grade 3 to grade 9 exams. We use 0-shot CoT for prompting and report exact match accuracy.\\n- DROP [26]: Discrete Reasoning Over Paragraphs (DROP) is a crowdsourced reading comprehension dataset that requires reasoning and operating over multiple input positions from the reference text. We use 0-shot CoT for prompting and report f1 score.\\n- GPQA [64]: Graduate-level Google-Proof Question and Answering (GPQA) is a challenging and high-quality multiple-choice question answering benchmark written by domain experts who have or are pursuing PhDs in biology, physics, and chemistry. We use 0-shot CoT for prompting and report exact match accuracy on the main set.\\n- MATH [37]: MATH is a mathematics problem solving benchmark, consisting of problems from mathematics competitions including the American Mathematics Competitions (AMC 10 and AMC 12), the American Invitational Mathematics Examination (AIME) and more. We use 0-shot CoT for prompting and report the exact match accuracy on the MATH5k set.\\n- GSM8K [23]: Grade School Math 8K (GSM8K) is a math benchmark consisting of 8,500 high-quality and diverse grade school math problems. The benchmark tests basic mathematical problem solving capabilities, requiring multi-step reasoning. We use 0-shot CoT for prompting and report the exact match accuracy on the test set containing 1,319 samples.\\n- IFEval [89]: IFeval is an instruction-following benchmark, which evaluates a model’s capability of following “verifiable instructions” such as “mention the keyword of AI at least 3 times”. The dataset contains 25 types of verifiable instructions and in total 541 prompts, where each prompt contains one or more verifiable instructions in natural language. We report the instruction-level accuracy under loose constraints.\\n- BBH [72]: Big Bench Hard (BBH) is a diverse benchmark consisting of an aggregate of 23 diverse subjects that cover algorithmic and NLP tasks ranging from casual logic tasks to word sorting and movie recommendations. The tasks are both multiple choice and open generation tasks. We report the macro average exact match accuracy across the subjects.\\n\\nResults measured internally by Amazon for evaluation purposes after Amazon Nova models completed training using (i) the Bedrock API for Claude and Meta models or (ii) the OpenAI API or Gemini API, as applicable.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 129}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2 Amazon Nova Pro, Lite, and Micro Evaluations',\n",
              "      'md': '# 2 Amazon Nova Pro, Lite, and Micro Evaluations',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 277, 'h': 92.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'In this section, we report benchmarking results for Amazon Nova models and for select publicly-available models, including by citing existing public results and by measuring their performance.2 In cases for which the result is a simple average of binary scores, we assume a Gaussian distribution for the sample and approximate the 95% confidence interval as:\\n\\nCI(S) = 1.96 × r S × (1 − S)\\nN\\n\\nwhere CI is the 95% confidence interval, S is the measured score for the benchmark, and N is the sample size [48, 45].',\n",
              "      'md': 'In this section, we report benchmarking results for Amazon Nova models and for select publicly-available models, including by citing existing public results and by measuring their performance.2 In cases for which the result is a simple average of binary scores, we assume a Gaussian distribution for the sample and approximate the 95% confidence interval as:\\n\\nCI(S) = 1.96 × r S × (1 − S)\\nN\\n\\nwhere CI is the 95% confidence interval, S is the measured score for the benchmark, and N is the sample size [48, 45].',\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 471, 'h': 679.99}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2.1 Core capability public benchmarks',\n",
              "      'md': '# 2.1 Core capability public benchmarks',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 277, 'h': 131.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We evaluate Amazon Nova models on a suite of automated public benchmarks to assess core capabilities, including for both text-only (Section 2.1.1) and multimodal (Section 2.1.2) use cases.',\n",
              "      'md': 'We evaluate Amazon Nova models on a suite of automated public benchmarks to assess core capabilities, including for both text-only (Section 2.1.1) and multimodal (Section 2.1.2) use cases.',\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 469, 'h': 185.99}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2.1.1 Core capability text benchmarks and results',\n",
              "      'md': '# 2.1.1 Core capability text benchmarks and results',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 277, 'h': 185.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We evaluate select core capabilities of Amazon Nova models on a variety of public text-only benchmarks, spanning general knowledge, reasoning, language understanding, multilinguality, and instruction following.\\n\\nThe following list briefly describes our selected text-only benchmarks. The prompts used for evaluation of each benchmark are summarized in Appendix B.1.\\n\\n- MMLU [36]: Massive Multitask Language Understanding (MMLU) is a multiple-choice question answering benchmark that covers 57 subject areas across STEM, humanities, and social sciences. Subjects include law, physics, mathematics, computer science, history, and more. The difficulty levels vary from elementary level to advanced professional level, focusing on both world knowledge and problem solving abilities. We use 0-shot Chain-of-Thought (CoT) [79] for prompting and report the macro average exact match accuracy across all subjects.\\n- ARC-C [22]: The AI2’s Reasoning Challenge (ARC) is a multiple-choice question-answering dataset, which contains science questions from grade 3 to grade 9 exams. We use 0-shot CoT for prompting and report exact match accuracy.\\n- DROP [26]: Discrete Reasoning Over Paragraphs (DROP) is a crowdsourced reading comprehension dataset that requires reasoning and operating over multiple input positions from the reference text. We use 0-shot CoT for prompting and report f1 score.\\n- GPQA [64]: Graduate-level Google-Proof Question and Answering (GPQA) is a challenging and high-quality multiple-choice question answering benchmark written by domain experts who have or are pursuing PhDs in biology, physics, and chemistry. We use 0-shot CoT for prompting and report exact match accuracy on the main set.\\n- MATH [37]: MATH is a mathematics problem solving benchmark, consisting of problems from mathematics competitions including the American Mathematics Competitions (AMC 10 and AMC 12), the American Invitational Mathematics Examination (AIME) and more. We use 0-shot CoT for prompting and report the exact match accuracy on the MATH5k set.\\n- GSM8K [23]: Grade School Math 8K (GSM8K) is a math benchmark consisting of 8,500 high-quality and diverse grade school math problems. The benchmark tests basic mathematical problem solving capabilities, requiring multi-step reasoning. We use 0-shot CoT for prompting and report the exact match accuracy on the test set containing 1,319 samples.\\n- IFEval [89]: IFeval is an instruction-following benchmark, which evaluates a model’s capability of following “verifiable instructions” such as “mention the keyword of AI at least 3 times”. The dataset contains 25 types of verifiable instructions and in total 541 prompts, where each prompt contains one or more verifiable instructions in natural language. We report the instruction-level accuracy under loose constraints.\\n- BBH [72]: Big Bench Hard (BBH) is a diverse benchmark consisting of an aggregate of 23 diverse subjects that cover algorithmic and NLP tasks ranging from casual logic tasks to word sorting and movie recommendations. The tasks are both multiple choice and open generation tasks. We report the macro average exact match accuracy across the subjects.\\n\\nResults measured internally by Amazon for evaluation purposes after Amazon Nova models completed training using (i) the Bedrock API for Claude and Meta models or (ii) the OpenAI API or Gemini API, as applicable.',\n",
              "      'md': 'We evaluate select core capabilities of Amazon Nova models on a variety of public text-only benchmarks, spanning general knowledge, reasoning, language understanding, multilinguality, and instruction following.\\n\\nThe following list briefly describes our selected text-only benchmarks. The prompts used for evaluation of each benchmark are summarized in Appendix B.1.\\n\\n- MMLU [36]: Massive Multitask Language Understanding (MMLU) is a multiple-choice question answering benchmark that covers 57 subject areas across STEM, humanities, and social sciences. Subjects include law, physics, mathematics, computer science, history, and more. The difficulty levels vary from elementary level to advanced professional level, focusing on both world knowledge and problem solving abilities. We use 0-shot Chain-of-Thought (CoT) [79] for prompting and report the macro average exact match accuracy across all subjects.\\n- ARC-C [22]: The AI2’s Reasoning Challenge (ARC) is a multiple-choice question-answering dataset, which contains science questions from grade 3 to grade 9 exams. We use 0-shot CoT for prompting and report exact match accuracy.\\n- DROP [26]: Discrete Reasoning Over Paragraphs (DROP) is a crowdsourced reading comprehension dataset that requires reasoning and operating over multiple input positions from the reference text. We use 0-shot CoT for prompting and report f1 score.\\n- GPQA [64]: Graduate-level Google-Proof Question and Answering (GPQA) is a challenging and high-quality multiple-choice question answering benchmark written by domain experts who have or are pursuing PhDs in biology, physics, and chemistry. We use 0-shot CoT for prompting and report exact match accuracy on the main set.\\n- MATH [37]: MATH is a mathematics problem solving benchmark, consisting of problems from mathematics competitions including the American Mathematics Competitions (AMC 10 and AMC 12), the American Invitational Mathematics Examination (AIME) and more. We use 0-shot CoT for prompting and report the exact match accuracy on the MATH5k set.\\n- GSM8K [23]: Grade School Math 8K (GSM8K) is a math benchmark consisting of 8,500 high-quality and diverse grade school math problems. The benchmark tests basic mathematical problem solving capabilities, requiring multi-step reasoning. We use 0-shot CoT for prompting and report the exact match accuracy on the test set containing 1,319 samples.\\n- IFEval [89]: IFeval is an instruction-following benchmark, which evaluates a model’s capability of following “verifiable instructions” such as “mention the keyword of AI at least 3 times”. The dataset contains 25 types of verifiable instructions and in total 541 prompts, where each prompt contains one or more verifiable instructions in natural language. We report the instruction-level accuracy under loose constraints.\\n- BBH [72]: Big Bench Hard (BBH) is a diverse benchmark consisting of an aggregate of 23 diverse subjects that cover algorithmic and NLP tasks ranging from casual logic tasks to word sorting and movie recommendations. The tasks are both multiple choice and open generation tasks. We report the macro average exact match accuracy across the subjects.\\n\\nResults measured internally by Amazon for evaluation purposes after Amazon Nova models completed training using (i) the Bedrock API for Claude and Meta models or (ii) the OpenAI API or Gemini API, as applicable.',\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 470, 'h': 679.99}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 6,\n",
              "    'text': '                                                 The Amazon Nova Family of Models\\n                                          MMLU   ARC-C        DROP   GPQA        MATH      GSM8k     IFEval      BBH\\n                                                                                                    instruction-\\n                               tok/     accuracy  accuracy  F1-score  accuracy accuracy   accuracy   level     accuracy\\n                                sec                                                                  loose\\n                                                                                                    accuracy\\n Nova Pro                      100        85.9   94.8         85.4    46.9       76.6       94.8      92.1       86.9\\n                                                 ±1.3      ±0.7       ±4.6       ±1.2       ±1.2      ±1.8\\n Nova Lite                     157        80.5   92.4         80.2    42.0       73.3       94.5      89.7       82.4\\n                                                 ±1.5         ±0.8    ±4.6       ±1.2       ±1.2      ±2.1\\n Nova Micro                    210        77.6   90.2         79.3    40.0       69.3       92.3      87.2       79.5\\n                                                 ±1.7         ±0.8    ±4.5       ±1.3       ±1.4      ±2.3\\n                                         0-shot  0-shot      6-shot   0-shot    0-shot     0-shot    0-shot     3-shot\\n                                        CoT                   CoT       CoT      CoT        CoT                CoT\\n Claude 3.5 Sonnet (Oct)        57        89.3   96.3ᴹ        88.3    58.0ᴹ      78.3     96.5ᴹ      90.2*       93.2\\n                                                 ±1.1         ±0.6    ±4.6       ±1.1       ±1.0      ±2.0\\n Claude 3.5 Haiku               64        80.3   90.9ᴹ     83.1       37.5ᴹ      69.4     93.8ᴹ      85.9*       86.6\\n                                                 ±1.6         ±0.8    ±4.5       ±1.3       ±1.3      ±2.4\\n                                         0-shot   25-shot    3-shot   0-shot    0-shot     0-shot    0-shot     3-shot\\n                                        CoT                           CoT        CoT        CoT                CoT\\n Gemini 1.5 Pro (002)           58        85.9   95.4ᴹ        74.9    55.1ᴹ      86.5       90.8     91.7ᴹ       89.2\\n                                                 ±1.2         ±0.9    ±4.6       ±0.9       ±1.6      ±1.9\\n Gemini 1.5 Flash (002)        190        78.9   94.3ᴹ        78.4    45.1ᴹ      77.9       86.2     91.6ᴹ       85.5\\n                                                 ±1.3         ±0.8    ±4.6       ±1.2       ±1.9      ±1.9\\n Gemini 1.5 Flash 8B (001)     283        68.1   88.7ᴹ     68.1ᴹ      33.5ᴹ      58.7     84.5ᴹ      86.1ᴹ       69.5\\n                                                 ±1.8         ±0.9    ±4.4       ±1.4       ±2.0      ±2.3\\n                                         5-shot   25-shot    3-shot   0-shot    4-shot    11-shot   0-shot      3-shot\\n GPT-4o                        163        88.7   96.2ᴹ        83.4    48.4ᴹ      76.6     92.6ᴹ      89.8ᴹ    83.0ᴹ\\n                                                 ±1.1      ±0.7       ±4.6       ±1.2       ±1.4      ±2.1\\n GPT-4o Mini                   113        82.0   92.3ᴹ        79.7    41.7ᴹ      70.2     86.4ᴹ      87.4ᴹ    81.0ᴹ\\n                                                 ±1.5         ±0.8    ±4.6       ±1.3       ±1.8      ±2.3\\n                                         0-shot   25-shot    3-shot   0-shot    0-shot     0-shot   0-shot      3-shot\\n                                                                                 CoT        CoT\\n Llama 3.2 90B                  40        86.0   94.8          -      46.7       68.0     95.1       90.9ᴹ        -\\n                                                 ±1.3                 ±4.6       ±1.3       ±1.2      ±2.0\\n Llama 3.2 11B                 124        73.0   83.4          -      32.8       51.9       84.5     85.0ᴹ        -\\n                                                 ±2.1                 ±4.3       ±1.4       ±2.0      ±2.4\\n Llama 3.1 8B                  157        73.0   83.4          -      30.4       51.9       84.5     85.0ᴹ        -\\n                                                 ±2.1                 ±4.3       ±1.4       ±2.0      ±2.4\\n                                         0-shot   25-shot      -      0-shot    0-shot     8-shot     -         -\\n                                        CoT                                      CoT        CoT\\nTable 1: Quantitative results on core capability benchmarks (MMLU [36], ARC-C [22], DROP [26], GPQA [64],\\nMATH [37]), GSM8K [23], IFEval [89] and BigBench-Hard (BBH) [72]). Unless otherwise noted, all reference\\nnumbers are taken from the original technical reports and websites for Claude models [14, 11], GPT4 models [58, 57],\\nLlama models [45] and Gemini models [32]. Results marked with M were measured by us2. Claude numbers for\\nIFEval (taken from [14]) are marked with an asterisk (∗), as the scoring methodology is unspecified in the report.\\nToken generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from\\nSection 2.5.\\n                                                         6',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n| |tok/ sec|accuracy|accuracy|F1-score|accuracy|accuracy|accuracy|instruction- level accuracy| |\\n|---|---|---|---|---|---|---|---|---|---|\\n|Nova Pro|100|85.9 ±1.3|94.8 ±0.7|85.4 ±4.6|46.9 ±1.2|76.6 ±1.2|94.8 ±1.8|92.1|86.9|\\n|Nova Lite|157|80.5 ±1.5|92.4 ±0.8|80.2 ±4.6|42.0 ±1.2|73.3 ±1.2|94.5 ±2.1|89.7|82.4|\\n|Nova Micro|210|77.6 ±1.7|90.2 ±0.8|79.3 ±4.5|40.0 ±1.3|69.3 ±1.4|92.3 ±2.3|87.2|79.5|\\n\\n| |0-shot|0-shot|6-shot|0-shot|0-shot|0-shot|0-shot|3-shot| |\\n|---|---|---|---|---|---|---|---|---|---|\\n|Claude 3.5 Sonnet (Oct)|57|89.3 ±1.1|96.3ᴹ ±0.6|88.3 ±4.6|58.0ᴹ ±1.1|78.3 ±1.0|96.5ᴹ ±2.0|90.2*|93.2|\\n|Claude 3.5 Haiku|64|80.3 ±1.6|90.9ᴹ ±0.8|83.1 ±4.5|37.5ᴹ ±1.3|69.4 ±1.3|93.8ᴹ ±2.4|85.9*|86.6|\\n\\n| |0-shot|25-shot|3-shot|0-shot|0-shot|0-shot|0-shot|3-shot| |\\n|---|---|---|---|---|---|---|---|---|---|\\n|Gemini 1.5 Pro (002)|58|85.9 ±1.2|95.4ᴹ ±0.9|74.9 ±4.6|55.1ᴹ ±0.9|86.5 ±1.6|90.8 ±1.9|91.7ᴹ|89.2|\\n|Gemini 1.5 Flash (002)|190|78.9 ±1.3|94.3ᴹ ±0.8|78.4 ±4.6|45.1ᴹ ±1.2|77.9 ±1.9|86.2 ±1.9|91.6ᴹ|85.5|\\n|Gemini 1.5 Flash 8B (001)|283|68.1 ±1.8|88.7ᴹ ±0.9|68.1ᴹ ±4.4|33.5ᴹ ±1.4|58.7 ±2.0|84.5ᴹ ±2.3|86.1ᴹ|69.5|\\n\\n| |5-shot|25-shot|3-shot|0-shot|4-shot|11-shot|0-shot|3-shot| |\\n|---|---|---|---|---|---|---|---|---|---|\\n|GPT-4o|163|88.7 ±1.1|96.2ᴹ ±0.7|83.4 ±4.6|48.4ᴹ ±1.2|76.6 ±1.4|92.6ᴹ ±2.1|89.8ᴹ|83.0ᴹ|\\n|GPT-4o Mini|113|82.0 ±1.5|92.3ᴹ ±0.8|79.7 ±4.6|41.7ᴹ ±1.3|70.2 ±1.8|86.4ᴹ ±2.3|87.4ᴹ|81.0ᴹ|\\n\\n| |0-shot|25-shot|3-shot|0-shot|0-shot|0-shot|0-shot|3-shot|\\n|---|---|---|---|---|---|---|---|---|\\n|Llama 3.2 90B|40|86.0 ±1.3|-|46.7|68.0 ±1.3|95.1 ±1.2|90.9ᴹ|-|\\n|Llama 3.2 11B|124|73.0 ±2.1|-|32.8|51.9 ±1.4|84.5 ±2.0|85.0ᴹ|-|\\n|Llama 3.1 8B|157|73.0 ±2.1|-|30.4|51.9 ±1.4|84.5 ±2.0|85.0ᴹ|-|\\n\\nTable 1: Quantitative results on core capability benchmarks (MMLU [36], ARC-C [22], DROP [26], GPQA [64], MATH [37]), GSM8K [23], IFEval [89] and BigBench-Hard (BBH) [72]). Unless otherwise noted, all reference numbers are taken from the original technical reports and websites for Claude models [14, 11], GPT4 models [58, 57], Llama models [45] and Gemini models [32]. Results marked with M were measured by us. Claude numbers for IFEval (taken from [14]) are marked with an asterisk (∗), as the scoring methodology is unspecified in the report. Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['',\n",
              "        'tok/ sec',\n",
              "        'accuracy',\n",
              "        'accuracy',\n",
              "        'F1-score',\n",
              "        'accuracy',\n",
              "        'accuracy',\n",
              "        'accuracy',\n",
              "        'instruction- level accuracy',\n",
              "        ''],\n",
              "       ['Nova Pro',\n",
              "        '100',\n",
              "        '85.9 ±1.3',\n",
              "        '94.8 ±0.7',\n",
              "        '85.4 ±4.6',\n",
              "        '46.9 ±1.2',\n",
              "        '76.6 ±1.2',\n",
              "        '94.8 ±1.8',\n",
              "        '92.1',\n",
              "        '86.9'],\n",
              "       ['Nova Lite',\n",
              "        '157',\n",
              "        '80.5 ±1.5',\n",
              "        '92.4 ±0.8',\n",
              "        '80.2 ±4.6',\n",
              "        '42.0 ±1.2',\n",
              "        '73.3 ±1.2',\n",
              "        '94.5 ±2.1',\n",
              "        '89.7',\n",
              "        '82.4'],\n",
              "       ['Nova Micro',\n",
              "        '210',\n",
              "        '77.6 ±1.7',\n",
              "        '90.2 ±0.8',\n",
              "        '79.3 ±4.5',\n",
              "        '40.0 ±1.3',\n",
              "        '69.3 ±1.4',\n",
              "        '92.3 ±2.3',\n",
              "        '87.2',\n",
              "        '79.5']],\n",
              "      'md': '| |tok/ sec|accuracy|accuracy|F1-score|accuracy|accuracy|accuracy|instruction- level accuracy| |\\n|Nova Pro|100|85.9 ±1.3|94.8 ±0.7|85.4 ±4.6|46.9 ±1.2|76.6 ±1.2|94.8 ±1.8|92.1|86.9|\\n|Nova Lite|157|80.5 ±1.5|92.4 ±0.8|80.2 ±4.6|42.0 ±1.2|73.3 ±1.2|94.5 ±2.1|89.7|82.4|\\n|Nova Micro|210|77.6 ±1.7|90.2 ±0.8|79.3 ±4.5|40.0 ±1.3|69.3 ±1.4|92.3 ±2.3|87.2|79.5|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"\",\"tok/ sec\",\"accuracy\",\"accuracy\",\"F1-score\",\"accuracy\",\"accuracy\",\"accuracy\",\"instruction- level accuracy\",\"\"\\n\"Nova Pro\",\"100\",\"85.9 ±1.3\",\"94.8 ±0.7\",\"85.4 ±4.6\",\"46.9 ±1.2\",\"76.6 ±1.2\",\"94.8 ±1.8\",\"92.1\",\"86.9\"\\n\"Nova Lite\",\"157\",\"80.5 ±1.5\",\"92.4 ±0.8\",\"80.2 ±4.6\",\"42.0 ±1.2\",\"73.3 ±1.2\",\"94.5 ±2.1\",\"89.7\",\"82.4\"\\n\"Nova Micro\",\"210\",\"77.6 ±1.7\",\"90.2 ±0.8\",\"79.3 ±4.5\",\"40.0 ±1.3\",\"69.3 ±1.4\",\"92.3 ±2.3\",\"87.2\",\"79.5\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '6-shot',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '3-shot',\n",
              "        ''],\n",
              "       ['Claude 3.5 Sonnet (Oct)',\n",
              "        '57',\n",
              "        '89.3 ±1.1',\n",
              "        '96.3ᴹ ±0.6',\n",
              "        '88.3 ±4.6',\n",
              "        '58.0ᴹ ±1.1',\n",
              "        '78.3 ±1.0',\n",
              "        '96.5ᴹ ±2.0',\n",
              "        '90.2*',\n",
              "        '93.2'],\n",
              "       ['Claude 3.5 Haiku',\n",
              "        '64',\n",
              "        '80.3 ±1.6',\n",
              "        '90.9ᴹ ±0.8',\n",
              "        '83.1 ±4.5',\n",
              "        '37.5ᴹ ±1.3',\n",
              "        '69.4 ±1.3',\n",
              "        '93.8ᴹ ±2.4',\n",
              "        '85.9*',\n",
              "        '86.6']],\n",
              "      'md': '| |0-shot|0-shot|6-shot|0-shot|0-shot|0-shot|0-shot|3-shot| |\\n|Claude 3.5 Sonnet (Oct)|57|89.3 ±1.1|96.3ᴹ ±0.6|88.3 ±4.6|58.0ᴹ ±1.1|78.3 ±1.0|96.5ᴹ ±2.0|90.2*|93.2|\\n|Claude 3.5 Haiku|64|80.3 ±1.6|90.9ᴹ ±0.8|83.1 ±4.5|37.5ᴹ ±1.3|69.4 ±1.3|93.8ᴹ ±2.4|85.9*|86.6|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"\",\"0-shot\",\"0-shot\",\"6-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"3-shot\",\"\"\\n\"Claude 3.5 Sonnet (Oct)\",\"57\",\"89.3 ±1.1\",\"96.3ᴹ ±0.6\",\"88.3 ±4.6\",\"58.0ᴹ ±1.1\",\"78.3 ±1.0\",\"96.5ᴹ ±2.0\",\"90.2*\",\"93.2\"\\n\"Claude 3.5 Haiku\",\"64\",\"80.3 ±1.6\",\"90.9ᴹ ±0.8\",\"83.1 ±4.5\",\"37.5ᴹ ±1.3\",\"69.4 ±1.3\",\"93.8ᴹ ±2.4\",\"85.9*\",\"86.6\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['',\n",
              "        '0-shot',\n",
              "        '25-shot',\n",
              "        '3-shot',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '3-shot',\n",
              "        ''],\n",
              "       ['Gemini 1.5 Pro (002)',\n",
              "        '58',\n",
              "        '85.9 ±1.2',\n",
              "        '95.4ᴹ ±0.9',\n",
              "        '74.9 ±4.6',\n",
              "        '55.1ᴹ ±0.9',\n",
              "        '86.5 ±1.6',\n",
              "        '90.8 ±1.9',\n",
              "        '91.7ᴹ',\n",
              "        '89.2'],\n",
              "       ['Gemini 1.5 Flash (002)',\n",
              "        '190',\n",
              "        '78.9 ±1.3',\n",
              "        '94.3ᴹ ±0.8',\n",
              "        '78.4 ±4.6',\n",
              "        '45.1ᴹ ±1.2',\n",
              "        '77.9 ±1.9',\n",
              "        '86.2 ±1.9',\n",
              "        '91.6ᴹ',\n",
              "        '85.5'],\n",
              "       ['Gemini 1.5 Flash 8B (001)',\n",
              "        '283',\n",
              "        '68.1 ±1.8',\n",
              "        '88.7ᴹ ±0.9',\n",
              "        '68.1ᴹ ±4.4',\n",
              "        '33.5ᴹ ±1.4',\n",
              "        '58.7 ±2.0',\n",
              "        '84.5ᴹ ±2.3',\n",
              "        '86.1ᴹ',\n",
              "        '69.5']],\n",
              "      'md': '| |0-shot|25-shot|3-shot|0-shot|0-shot|0-shot|0-shot|3-shot| |\\n|Gemini 1.5 Pro (002)|58|85.9 ±1.2|95.4ᴹ ±0.9|74.9 ±4.6|55.1ᴹ ±0.9|86.5 ±1.6|90.8 ±1.9|91.7ᴹ|89.2|\\n|Gemini 1.5 Flash (002)|190|78.9 ±1.3|94.3ᴹ ±0.8|78.4 ±4.6|45.1ᴹ ±1.2|77.9 ±1.9|86.2 ±1.9|91.6ᴹ|85.5|\\n|Gemini 1.5 Flash 8B (001)|283|68.1 ±1.8|88.7ᴹ ±0.9|68.1ᴹ ±4.4|33.5ᴹ ±1.4|58.7 ±2.0|84.5ᴹ ±2.3|86.1ᴹ|69.5|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"\",\"0-shot\",\"25-shot\",\"3-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"3-shot\",\"\"\\n\"Gemini 1.5 Pro (002)\",\"58\",\"85.9 ±1.2\",\"95.4ᴹ ±0.9\",\"74.9 ±4.6\",\"55.1ᴹ ±0.9\",\"86.5 ±1.6\",\"90.8 ±1.9\",\"91.7ᴹ\",\"89.2\"\\n\"Gemini 1.5 Flash (002)\",\"190\",\"78.9 ±1.3\",\"94.3ᴹ ±0.8\",\"78.4 ±4.6\",\"45.1ᴹ ±1.2\",\"77.9 ±1.9\",\"86.2 ±1.9\",\"91.6ᴹ\",\"85.5\"\\n\"Gemini 1.5 Flash 8B (001)\",\"283\",\"68.1 ±1.8\",\"88.7ᴹ ±0.9\",\"68.1ᴹ ±4.4\",\"33.5ᴹ ±1.4\",\"58.7 ±2.0\",\"84.5ᴹ ±2.3\",\"86.1ᴹ\",\"69.5\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['',\n",
              "        '5-shot',\n",
              "        '25-shot',\n",
              "        '3-shot',\n",
              "        '0-shot',\n",
              "        '4-shot',\n",
              "        '11-shot',\n",
              "        '0-shot',\n",
              "        '3-shot',\n",
              "        ''],\n",
              "       ['GPT-4o',\n",
              "        '163',\n",
              "        '88.7 ±1.1',\n",
              "        '96.2ᴹ ±0.7',\n",
              "        '83.4 ±4.6',\n",
              "        '48.4ᴹ ±1.2',\n",
              "        '76.6 ±1.4',\n",
              "        '92.6ᴹ ±2.1',\n",
              "        '89.8ᴹ',\n",
              "        '83.0ᴹ'],\n",
              "       ['GPT-4o Mini',\n",
              "        '113',\n",
              "        '82.0 ±1.5',\n",
              "        '92.3ᴹ ±0.8',\n",
              "        '79.7 ±4.6',\n",
              "        '41.7ᴹ ±1.3',\n",
              "        '70.2 ±1.8',\n",
              "        '86.4ᴹ ±2.3',\n",
              "        '87.4ᴹ',\n",
              "        '81.0ᴹ']],\n",
              "      'md': '| |5-shot|25-shot|3-shot|0-shot|4-shot|11-shot|0-shot|3-shot| |\\n|GPT-4o|163|88.7 ±1.1|96.2ᴹ ±0.7|83.4 ±4.6|48.4ᴹ ±1.2|76.6 ±1.4|92.6ᴹ ±2.1|89.8ᴹ|83.0ᴹ|\\n|GPT-4o Mini|113|82.0 ±1.5|92.3ᴹ ±0.8|79.7 ±4.6|41.7ᴹ ±1.3|70.2 ±1.8|86.4ᴹ ±2.3|87.4ᴹ|81.0ᴹ|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"\",\"5-shot\",\"25-shot\",\"3-shot\",\"0-shot\",\"4-shot\",\"11-shot\",\"0-shot\",\"3-shot\",\"\"\\n\"GPT-4o\",\"163\",\"88.7 ±1.1\",\"96.2ᴹ ±0.7\",\"83.4 ±4.6\",\"48.4ᴹ ±1.2\",\"76.6 ±1.4\",\"92.6ᴹ ±2.1\",\"89.8ᴹ\",\"83.0ᴹ\"\\n\"GPT-4o Mini\",\"113\",\"82.0 ±1.5\",\"92.3ᴹ ±0.8\",\"79.7 ±4.6\",\"41.7ᴹ ±1.3\",\"70.2 ±1.8\",\"86.4ᴹ ±2.3\",\"87.4ᴹ\",\"81.0ᴹ\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['',\n",
              "        '0-shot',\n",
              "        '25-shot',\n",
              "        '3-shot',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '0-shot',\n",
              "        '3-shot'],\n",
              "       ['Llama 3.2 90B',\n",
              "        '40',\n",
              "        '86.0 ±1.3',\n",
              "        '-',\n",
              "        '46.7',\n",
              "        '68.0 ±1.3',\n",
              "        '95.1 ±1.2',\n",
              "        '90.9ᴹ',\n",
              "        '-'],\n",
              "       ['Llama 3.2 11B',\n",
              "        '124',\n",
              "        '73.0 ±2.1',\n",
              "        '-',\n",
              "        '32.8',\n",
              "        '51.9 ±1.4',\n",
              "        '84.5 ±2.0',\n",
              "        '85.0ᴹ',\n",
              "        '-'],\n",
              "       ['Llama 3.1 8B',\n",
              "        '157',\n",
              "        '73.0 ±2.1',\n",
              "        '-',\n",
              "        '30.4',\n",
              "        '51.9 ±1.4',\n",
              "        '84.5 ±2.0',\n",
              "        '85.0ᴹ',\n",
              "        '-']],\n",
              "      'md': '| |0-shot|25-shot|3-shot|0-shot|0-shot|0-shot|0-shot|3-shot|\\n|Llama 3.2 90B|40|86.0 ±1.3|-|46.7|68.0 ±1.3|95.1 ±1.2|90.9ᴹ|-|\\n|Llama 3.2 11B|124|73.0 ±2.1|-|32.8|51.9 ±1.4|84.5 ±2.0|85.0ᴹ|-|\\n|Llama 3.1 8B|157|73.0 ±2.1|-|30.4|51.9 ±1.4|84.5 ±2.0|85.0ᴹ|-|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"\",\"0-shot\",\"25-shot\",\"3-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"3-shot\"\\n\"Llama 3.2 90B\",\"40\",\"86.0 ±1.3\",\"-\",\"46.7\",\"68.0 ±1.3\",\"95.1 ±1.2\",\"90.9ᴹ\",\"-\"\\n\"Llama 3.2 11B\",\"124\",\"73.0 ±2.1\",\"-\",\"32.8\",\"51.9 ±1.4\",\"84.5 ±2.0\",\"85.0ᴹ\",\"-\"\\n\"Llama 3.1 8B\",\"157\",\"73.0 ±2.1\",\"-\",\"30.4\",\"51.9 ±1.4\",\"84.5 ±2.0\",\"85.0ᴹ\",\"-\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 1: Quantitative results on core capability benchmarks (MMLU [36], ARC-C [22], DROP [26], GPQA [64], MATH [37]), GSM8K [23], IFEval [89] and BigBench-Hard (BBH) [72]). Unless otherwise noted, all reference numbers are taken from the original technical reports and websites for Claude models [14, 11], GPT4 models [58, 57], Llama models [45] and Gemini models [32]. Results marked with M were measured by us. Claude numbers for IFEval (taken from [14]) are marked with an asterisk (∗), as the scoring methodology is unspecified in the report. Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "      'md': 'Table 1: Quantitative results on core capability benchmarks (MMLU [36], ARC-C [22], DROP [26], GPQA [64], MATH [37]), GSM8K [23], IFEval [89] and BigBench-Hard (BBH) [72]). Unless otherwise noted, all reference numbers are taken from the original technical reports and websites for Claude models [14, 11], GPT4 models [58, 57], Llama models [45] and Gemini models [32]. Results marked with M were measured by us. Claude numbers for IFEval (taken from [14]) are marked with an asterisk (∗), as the scoring methodology is unspecified in the report. Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "      'bBox': {'x': 71, 'y': 106.03, 'w': 470, 'h': 644.01}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 7,\n",
              "    'text': '                                     The Amazon Nova Family of Models\\nTable 1 summarizes the quantitative results of Nova models and select public models on the aforementioned benchmarks\\nfor core capabilities. When available, we reference the highest publicly-reported numbers for each benchmark from the\\nofficial technical reports and websites for Claude, Gemini, OpenAI and Llama family of models. Amazon Nova Pro,\\nLite, and Micro demonstrate strong performance across all benchmarks, showcasing their advanced core intelligence,\\nparticularly Amazon Nova Micro and Lite on math, reasoning, and instruction following benchmarks.\\nWe also evaluate the translation capabilities of Nova models. Flores200 [73, 34, 35], or simply Flores, is a machine\\ntranslation benchmark consisting of translations from 842 distinct web articles, which tests the translation capabilities\\nbetween English and non-English languages. Sentences are 21 words long on average. We use a 0-shot setup and\\nreport the macro average of two metrics, spBleu and COMET22 score [63] across a set of languages (Arabic, German,\\nSpanish, French, Hindi, Italian, Japanese, Korean, Portuguese, Hebrew, Turkish, Simplified Chinese, Russian, Dutch)\\nfor translation from and into English. The prompts used for evaluation are summarized in Appendix B.1. Table 2\\nsummarizes our quantitative results on Flores, demonstrating strong multilingual performance on translation for Amazon\\nNova Pro, Lite, and Micro.\\n                                                                FLORES (0-shot)\\n                                                     en → Set1                      Set1 → en\\n                                  tok/sec     spBleu (↑)   COMET22 (↑)       spBleu (↑)    COMET22 (↑)\\n       Nova Pro                     100         43.4           89.1            44.4            89.0\\n       Nova Lite                    157         41.5           88.8            43.1            88.8\\n       Nova Micro                   210         40.2           88.5            42.6            88.7\\n       Claude 3.5 Sonnet (Oct)       57         42.5ᴹ          89.4ᴹ           43.5ᴹ           89.1ᴹ\\n       Claude 3.5 Haiku              64         40.0ᴹ          88.5ᴹ           40.2ᴹ           88.3ᴹ\\n       Gemini 1.5 Pro (002)          57         43.0M*         89.1M*          45.6M*          89.1M*\\n       Gemini 1.5 Flash (002)       190         40.0M*         88.5M*          42.9M*          88.8M*\\n       Gemini 1.5 Flash 8B (001)    283         38.2M*         88.0M*          41.4M*          88.5M*\\n       GPT-4o                       163         43.1M*         89.2M*          43.9M*          89.0M*\\n       GPT-4o Mini                  113         41.1M*         88.7M*          41.9M*          88.7M*\\n       Llama 3.2 90B                 40         39.7ᴹ          88.2ᴹ           43.7ᴹ           88.5ᴹ\\n       Llama 3.2 11B                124         33.0ᴹ          85.7ᴹ           36.3ᴹ           86.3ᴹ\\n       Llama 3.1 8B                 157         32.7ᴹ          85.5ᴹ           36.5ᴹ           86.5ᴹ\\nTable 2: Quantitative results on Flores200 [34], a machine translation benchmark. Set1 refers to {de, es, fr, it, pt, ja,\\nar, hi, ru, nl, tr, he, ko, zh}. Results marked with M were measured by us.2. Results marked with an asterisk (∗) were\\nobtained using an alternate prompt which can be found in Appendix B.1 Token generation speed in tokens per second\\n(tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n2.1.2  Core capability multimodal benchmarks and results\\nIn this section we evaluate the multimodal capabilities of Amazon Nova models on a diverse set of public benchmarks.\\nOur selection of multimodal benchmarks aims to probe for various capabilities, including natural image understanding,\\ndocument understanding with charts and graphs, text understanding, and temporal reasoning in videos.    For all\\nbenchmarks, we follow the suggested metrics and choice of data split for evaluation. The following list briefly describes\\nthe selected benchmarks.\\n      •    MMMU [85]: The Massive Multi-discipline Multimodal Understanding benchmark consists of college-level\\n          multiple-choice and open-ended questions from 30 different disciplines. We use Chain-of-Thought (CoT)\\n         prompting for this benchmark and report accuracy.\\n      •  ChartQA [50]: The 2,500 questions of this benchmark cover three different types of charts (bar, line and pie)\\n   and require strong visual, logical, and arithmetical reasoning capabilities. We evaluate on the test set and\\n         report relaxed accuracy.\\n      • DocVQA [51]: This benchmark probes capabilities on document analysis and recognition, including Optical\\n       Character Recognition (OCR). The 5,349 questions contain images from a diverse set of documents, ranging\\n                                                      7',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\nTable 1 summarizes the quantitative results of Nova models and select public models on the aforementioned benchmarks for core capabilities. When available, we reference the highest publicly-reported numbers for each benchmark from the official technical reports and websites for Claude, Gemini, OpenAI and Llama family of models. Amazon Nova Pro, Lite, and Micro demonstrate strong performance across all benchmarks, showcasing their advanced core intelligence, particularly Amazon Nova Micro and Lite on math, reasoning, and instruction following benchmarks.\\n\\nWe also evaluate the translation capabilities of Nova models. Flores200 [73, 34, 35], or simply Flores, is a machine translation benchmark consisting of translations from 842 distinct web articles, which tests the translation capabilities between English and non-English languages. Sentences are 21 words long on average. We use a 0-shot setup and report the macro average of two metrics, spBleu and COMET22 score [63] across a set of languages (Arabic, German, Spanish, French, Hindi, Italian, Japanese, Korean, Portuguese, Hebrew, Turkish, Simplified Chinese, Russian, Dutch) for translation from and into English. The prompts used for evaluation are summarized in Appendix B.1. Table 2 summarizes our quantitative results on Flores, demonstrating strong multilingual performance on translation for Amazon Nova Pro, Lite, and Micro.\\n\\n# FLORES (0-shot)\\n\\n| | |en → Set1| |Set1 → en| |\\n|---|---|---|---|---|---|\\n| |tok/sec|spBleu (↑)|COMET22 (↑)|spBleu (↑)|COMET22 (↑)|\\n|Nova Pro|100|43.4|89.1|44.4|89.0|\\n|Nova Lite|157|41.5|88.8|43.1|88.8|\\n|Nova Micro|210|40.2|88.5|42.6|88.7|\\n|Claude 3.5 Sonnet (Oct)|57|42.5ᴹ|89.4ᴹ|43.5ᴹ|89.1ᴹ|\\n|Claude 3.5 Haiku|64|40.0ᴹ|88.5ᴹ|40.2ᴹ|88.3ᴹ|\\n|Gemini 1.5 Pro (002)|57|43.0M*|89.1M*|45.6M*|89.1M*|\\n|Gemini 1.5 Flash (002)|190|40.0M*|88.5M*|42.9M*|88.8M*|\\n|Gemini 1.5 Flash 8B (001)|283|38.2M*|88.0M*|41.4M*|88.5M*|\\n|GPT-4o|163|43.1M*|89.2M*|43.9M*|89.0M*|\\n|GPT-4o Mini|113|41.1M*|88.7M*|41.9M*|88.7M*|\\n|Llama 3.2 90B|40|39.7ᴹ|88.2ᴹ|43.7ᴹ|88.5ᴹ|\\n|Llama 3.2 11B|124|33.0ᴹ|85.7ᴹ|36.3ᴹ|86.3ᴹ|\\n|Llama 3.1 8B|157|32.7ᴹ|85.5ᴹ|36.5ᴹ|86.5ᴹ|\\n\\nTable 2: Quantitative results on Flores200 [34], a machine translation benchmark. Set1 refers to {de, es, fr, it, pt, ja, ar, hi, ru, nl, tr, he, ko, zh}. Results marked with M were measured by us. Results marked with an asterisk (∗) were obtained using an alternate prompt which can be found in Appendix B.1 Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n\\n# 2.1.2 Core capability multimodal benchmarks and results\\n\\nIn this section we evaluate the multimodal capabilities of Amazon Nova models on a diverse set of public benchmarks. Our selection of multimodal benchmarks aims to probe for various capabilities, including natural image understanding, document understanding with charts and graphs, text understanding, and temporal reasoning in videos. For all benchmarks, we follow the suggested metrics and choice of data split for evaluation. The following list briefly describes the selected benchmarks.\\n\\n- MMMU [85]: The Massive Multi-discipline Multimodal Understanding benchmark consists of college-level multiple-choice and open-ended questions from 30 different disciplines. We use Chain-of-Thought (CoT) prompting for this benchmark and report accuracy.\\n- ChartQA [50]: The 2,500 questions of this benchmark cover three different types of charts (bar, line and pie) and require strong visual, logical, and arithmetical reasoning capabilities. We evaluate on the test set and report relaxed accuracy.\\n- DocVQA [51]: This benchmark probes capabilities on document analysis and recognition, including Optical Character Recognition (OCR). The 5,349 questions contain images from a diverse set of documents, ranging.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 1 summarizes the quantitative results of Nova models and select public models on the aforementioned benchmarks for core capabilities. When available, we reference the highest publicly-reported numbers for each benchmark from the official technical reports and websites for Claude, Gemini, OpenAI and Llama family of models. Amazon Nova Pro, Lite, and Micro demonstrate strong performance across all benchmarks, showcasing their advanced core intelligence, particularly Amazon Nova Micro and Lite on math, reasoning, and instruction following benchmarks.\\n\\nWe also evaluate the translation capabilities of Nova models. Flores200 [73, 34, 35], or simply Flores, is a machine translation benchmark consisting of translations from 842 distinct web articles, which tests the translation capabilities between English and non-English languages. Sentences are 21 words long on average. We use a 0-shot setup and report the macro average of two metrics, spBleu and COMET22 score [63] across a set of languages (Arabic, German, Spanish, French, Hindi, Italian, Japanese, Korean, Portuguese, Hebrew, Turkish, Simplified Chinese, Russian, Dutch) for translation from and into English. The prompts used for evaluation are summarized in Appendix B.1. Table 2 summarizes our quantitative results on Flores, demonstrating strong multilingual performance on translation for Amazon Nova Pro, Lite, and Micro.',\n",
              "      'md': 'Table 1 summarizes the quantitative results of Nova models and select public models on the aforementioned benchmarks for core capabilities. When available, we reference the highest publicly-reported numbers for each benchmark from the official technical reports and websites for Claude, Gemini, OpenAI and Llama family of models. Amazon Nova Pro, Lite, and Micro demonstrate strong performance across all benchmarks, showcasing their advanced core intelligence, particularly Amazon Nova Micro and Lite on math, reasoning, and instruction following benchmarks.\\n\\nWe also evaluate the translation capabilities of Nova models. Flores200 [73, 34, 35], or simply Flores, is a machine translation benchmark consisting of translations from 842 distinct web articles, which tests the translation capabilities between English and non-English languages. Sentences are 21 words long on average. We use a 0-shot setup and report the macro average of two metrics, spBleu and COMET22 score [63] across a set of languages (Arabic, German, Spanish, French, Hindi, Italian, Japanese, Korean, Portuguese, Hebrew, Turkish, Simplified Chinese, Russian, Dutch) for translation from and into English. The prompts used for evaluation are summarized in Appendix B.1. Table 2 summarizes our quantitative results on Flores, demonstrating strong multilingual performance on translation for Amazon Nova Pro, Lite, and Micro.',\n",
              "      'bBox': {'x': 71, 'y': 72.04, 'w': 470, 'h': 678}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'FLORES (0-shot)',\n",
              "      'md': '# FLORES (0-shot)',\n",
              "      'bBox': {'x': 346, 'y': 232.04, 'w': 75, 'h': 10}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['', '', 'en → Set1', '', 'Set1 → en', ''],\n",
              "       ['',\n",
              "        'tok/sec',\n",
              "        'spBleu (↑)',\n",
              "        'COMET22 (↑)',\n",
              "        'spBleu (↑)',\n",
              "        'COMET22 (↑)'],\n",
              "       ['Nova Pro', '100', '43.4', '89.1', '44.4', '89.0'],\n",
              "       ['Nova Lite', '157', '41.5', '88.8', '43.1', '88.8'],\n",
              "       ['Nova Micro', '210', '40.2', '88.5', '42.6', '88.7'],\n",
              "       ['Claude 3.5 Sonnet (Oct)', '57', '42.5ᴹ', '89.4ᴹ', '43.5ᴹ', '89.1ᴹ'],\n",
              "       ['Claude 3.5 Haiku', '64', '40.0ᴹ', '88.5ᴹ', '40.2ᴹ', '88.3ᴹ'],\n",
              "       ['Gemini 1.5 Pro (002)', '57', '43.0M*', '89.1M*', '45.6M*', '89.1M*'],\n",
              "       ['Gemini 1.5 Flash (002)',\n",
              "        '190',\n",
              "        '40.0M*',\n",
              "        '88.5M*',\n",
              "        '42.9M*',\n",
              "        '88.8M*'],\n",
              "       ['Gemini 1.5 Flash 8B (001)',\n",
              "        '283',\n",
              "        '38.2M*',\n",
              "        '88.0M*',\n",
              "        '41.4M*',\n",
              "        '88.5M*'],\n",
              "       ['GPT-4o', '163', '43.1M*', '89.2M*', '43.9M*', '89.0M*'],\n",
              "       ['GPT-4o Mini', '113', '41.1M*', '88.7M*', '41.9M*', '88.7M*'],\n",
              "       ['Llama 3.2 90B', '40', '39.7ᴹ', '88.2ᴹ', '43.7ᴹ', '88.5ᴹ'],\n",
              "       ['Llama 3.2 11B', '124', '33.0ᴹ', '85.7ᴹ', '36.3ᴹ', '86.3ᴹ'],\n",
              "       ['Llama 3.1 8B', '157', '32.7ᴹ', '85.5ᴹ', '36.5ᴹ', '86.5ᴹ']],\n",
              "      'md': '| | |en → Set1| |Set1 → en| |\\n| |tok/sec|spBleu (↑)|COMET22 (↑)|spBleu (↑)|COMET22 (↑)|\\n|Nova Pro|100|43.4|89.1|44.4|89.0|\\n|Nova Lite|157|41.5|88.8|43.1|88.8|\\n|Nova Micro|210|40.2|88.5|42.6|88.7|\\n|Claude 3.5 Sonnet (Oct)|57|42.5ᴹ|89.4ᴹ|43.5ᴹ|89.1ᴹ|\\n|Claude 3.5 Haiku|64|40.0ᴹ|88.5ᴹ|40.2ᴹ|88.3ᴹ|\\n|Gemini 1.5 Pro (002)|57|43.0M*|89.1M*|45.6M*|89.1M*|\\n|Gemini 1.5 Flash (002)|190|40.0M*|88.5M*|42.9M*|88.8M*|\\n|Gemini 1.5 Flash 8B (001)|283|38.2M*|88.0M*|41.4M*|88.5M*|\\n|GPT-4o|163|43.1M*|89.2M*|43.9M*|89.0M*|\\n|GPT-4o Mini|113|41.1M*|88.7M*|41.9M*|88.7M*|\\n|Llama 3.2 90B|40|39.7ᴹ|88.2ᴹ|43.7ᴹ|88.5ᴹ|\\n|Llama 3.2 11B|124|33.0ᴹ|85.7ᴹ|36.3ᴹ|86.3ᴹ|\\n|Llama 3.1 8B|157|32.7ᴹ|85.5ᴹ|36.5ᴹ|86.5ᴹ|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"\",\"\",\"en → Set1\",\"\",\"Set1 → en\",\"\"\\n\"\",\"tok/sec\",\"spBleu (↑)\",\"COMET22 (↑)\",\"spBleu (↑)\",\"COMET22 (↑)\"\\n\"Nova Pro\",\"100\",\"43.4\",\"89.1\",\"44.4\",\"89.0\"\\n\"Nova Lite\",\"157\",\"41.5\",\"88.8\",\"43.1\",\"88.8\"\\n\"Nova Micro\",\"210\",\"40.2\",\"88.5\",\"42.6\",\"88.7\"\\n\"Claude 3.5 Sonnet (Oct)\",\"57\",\"42.5ᴹ\",\"89.4ᴹ\",\"43.5ᴹ\",\"89.1ᴹ\"\\n\"Claude 3.5 Haiku\",\"64\",\"40.0ᴹ\",\"88.5ᴹ\",\"40.2ᴹ\",\"88.3ᴹ\"\\n\"Gemini 1.5 Pro (002)\",\"57\",\"43.0M*\",\"89.1M*\",\"45.6M*\",\"89.1M*\"\\n\"Gemini 1.5 Flash (002)\",\"190\",\"40.0M*\",\"88.5M*\",\"42.9M*\",\"88.8M*\"\\n\"Gemini 1.5 Flash 8B (001)\",\"283\",\"38.2M*\",\"88.0M*\",\"41.4M*\",\"88.5M*\"\\n\"GPT-4o\",\"163\",\"43.1M*\",\"89.2M*\",\"43.9M*\",\"89.0M*\"\\n\"GPT-4o Mini\",\"113\",\"41.1M*\",\"88.7M*\",\"41.9M*\",\"88.7M*\"\\n\"Llama 3.2 90B\",\"40\",\"39.7ᴹ\",\"88.2ᴹ\",\"43.7ᴹ\",\"88.5ᴹ\"\\n\"Llama 3.2 11B\",\"124\",\"33.0ᴹ\",\"85.7ᴹ\",\"36.3ᴹ\",\"86.3ᴹ\"\\n\"Llama 3.1 8B\",\"157\",\"32.7ᴹ\",\"85.5ᴹ\",\"36.5ᴹ\",\"86.5ᴹ\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 2: Quantitative results on Flores200 [34], a machine translation benchmark. Set1 refers to {de, es, fr, it, pt, ja, ar, hi, ru, nl, tr, he, ko, zh}. Results marked with M were measured by us. Results marked with an asterisk (∗) were obtained using an alternate prompt which can be found in Appendix B.1 Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "      'md': 'Table 2: Quantitative results on Flores200 [34], a machine translation benchmark. Set1 refers to {de, es, fr, it, pt, ja, ar, hi, ru, nl, tr, he, ko, zh}. Results marked with M were measured by us. Results marked with an asterisk (∗) were obtained using an alternate prompt which can be found in Appendix B.1 Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "      'bBox': {'x': 71, 'y': 262.03, 'w': 470, 'h': 257}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2.1.2 Core capability multimodal benchmarks and results',\n",
              "      'md': '# 2.1.2 Core capability multimodal benchmarks and results',\n",
              "      'bBox': {'x': 72, 'y': 546.04, 'w': 250, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'In this section we evaluate the multimodal capabilities of Amazon Nova models on a diverse set of public benchmarks. Our selection of multimodal benchmarks aims to probe for various capabilities, including natural image understanding, document understanding with charts and graphs, text understanding, and temporal reasoning in videos. For all benchmarks, we follow the suggested metrics and choice of data split for evaluation. The following list briefly describes the selected benchmarks.\\n\\n- MMMU [85]: The Massive Multi-discipline Multimodal Understanding benchmark consists of college-level multiple-choice and open-ended questions from 30 different disciplines. We use Chain-of-Thought (CoT) prompting for this benchmark and report accuracy.\\n- ChartQA [50]: The 2,500 questions of this benchmark cover three different types of charts (bar, line and pie) and require strong visual, logical, and arithmetical reasoning capabilities. We evaluate on the test set and report relaxed accuracy.\\n- DocVQA [51]: This benchmark probes capabilities on document analysis and recognition, including Optical Character Recognition (OCR). The 5,349 questions contain images from a diverse set of documents, ranging.',\n",
              "      'md': 'In this section we evaluate the multimodal capabilities of Amazon Nova models on a diverse set of public benchmarks. Our selection of multimodal benchmarks aims to probe for various capabilities, including natural image understanding, document understanding with charts and graphs, text understanding, and temporal reasoning in videos. For all benchmarks, we follow the suggested metrics and choice of data split for evaluation. The following list briefly describes the selected benchmarks.\\n\\n- MMMU [85]: The Massive Multi-discipline Multimodal Understanding benchmark consists of college-level multiple-choice and open-ended questions from 30 different disciplines. We use Chain-of-Thought (CoT) prompting for this benchmark and report accuracy.\\n- ChartQA [50]: The 2,500 questions of this benchmark cover three different types of charts (bar, line and pie) and require strong visual, logical, and arithmetical reasoning capabilities. We evaluate on the test set and report relaxed accuracy.\\n- DocVQA [51]: This benchmark probes capabilities on document analysis and recognition, including Optical Character Recognition (OCR). The 5,349 questions contain images from a diverse set of documents, ranging.',\n",
              "      'bBox': {'x': 72, 'y': 564.04, 'w': 470, 'h': 156}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 8,\n",
              "    'text': '                                                  The Amazon Nova Family of Models\\n                                    MMMU            Chart         Doc           Text          VATEX         Ego\\n                                     (CoT)           QAC          VQA             VQA                      Schema\\n                                      val            test          test           val          test         test\\n                            tok/    accuracy       relaxed        ANLS          weighted      CIDEr       accuracy\\n                            sec                    accuracy                     accuracy\\n Amazon Nova Pro            100     61.7 ±3.2     89.2 ±1.2       93.5          81.5           77.8      72.1 ±5.4\\n Amazon Nova Lite           157     56.2 ±3.2     86.8 ±1.3       92.4          80.2           77.8      71.4 ±5.4\\n Claude 3.5 Sonnet (Oct)     57     70.4 ±3.0     90.8 ±1.1       94.2          61.7ᴹ           -            -\\n Claude 3 Haiku              64     50.2 ±3.3     82.0 ±1.5       88.8            -             -            -\\n Gemini 1.5 Pro (001)        58     65.9 ±3.1 E   87.2 ±1.3      93.1ᴮ          78.7          64.6ᴬ      72.2 ±5.4\\n Gemini 1.5 Flash (001)     190     62.3 ±3.2 E   85.4 ±1.4      89.9ᴮ          78.7           57.1      65.7 ±5.7\\n Gemini 1.5 Flash 8B (001)  283     53.7 ±3.3 F   78.2 ±1.6 G     73.6          66.7          53.2ᴬ          -\\n GPT-4o (May)                -      69.1 ±3.0     85.7 ±1.4       92.8          77.2D,M         -        72.2 ±5.4\\n GPT-4o Mini (Jul)          113     59.4 ±3.2     79.2 ±1.6 M      -            70.3ᴹ           -            -\\n Llama 3.2 90B               40     60.3 ±3.2     85.5 ±1.4       90.1          80.7ᴹ           -            -\\n Llama 3.2 11B              124     50.7 ±3.3     83.4 ±1.5       88.4          71.3ᴹ           -            -\\nTable 3: Quantitative results on four image understanding benchmarks (MMMU [85], ChartQA [50], DocVQA [51],\\nTextVQA [70]) and 2 video understanding benchmarks (VATEX [78] and EgoSchema [49]). Higher numbers are better\\nfor all benchmarks (↑). Unless otherwise noted, all evaluations are 0-shot and reference numbers are taken from the\\noriginal technical reports and websites for Claude models [11, 12], GPT4 models [56, 55], Llama models [45, 53] and\\nGemini models [32, 33]. Remarks: (A) 4-shot evaluation; (B) External Optical Character Recognition (OCR) was used;\\n(C) All models except Amazon Nova use CoT; (D) GPT-4o (Nov); (E) Gemini 1.5 Flash/Pro (002) models; (F) Reported\\nin [33]; (G) Reported in [4]; (M) Claude 3.5 Sonnet and Llama 3.2 results for TextVQA as well as GPT4o and GPT4o\\nmini results on ChartQA, TextVQA and VATEX were measured by us.2 Token generation speed in tokens per second\\n(tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n              from 1940 to 2020 and covering multiple industries. We report Average Normalized Levenshtein Similarity\\n         (ANLS).\\n       •     TextVQA [70]: The 5,000 samples of this dataset focus specifically on text-reading capabilities (OCR) in\\n         natural images. We report weighted accuracy on the validation set.\\n       •         VATEX [78]: This video captioning benchmark covers a diverse set of human activities. We evaluate on\\n         the public test set containing videos with a length of around 10 seconds. The CIDEr [75] score is used for\\n         evaluation.\\n       •   EgoSchema [49]: The unique characteristic of this long-form video question answering benchmark is its high\\n                                                  “certificate length” [15], which is, loosely speaking, the time it takes a human to verify the video description.\\n               The videos cover a broad range of natural human activities and come with human-curated multiple-choice\\n         question-answer pairs.\\nTable 3 summarizes our quantitative results on multiple image and video understanding benchmarks. Amazon Nova\\nPro and Lite achieve high scores across all benchmarks. Chart understanding on ChartQA and video understanding on\\nVATEX stand out, where Nova models rank either first or second. We provide the prompt templates for all benchmarks\\nin Appendix B.2, as well as qualitative examples in Appendix C.\\n2.2   Agentic workflows\\nAmazon Nova Pro, Lite, and Micro models can be used as agents. An agent considers a suite of tools and APIs, reasons\\nabout the user’s request and past conversational history, chooses if a tool should be used and, if so, decides which tool\\nto use, invokes the tool, assesses the outcome from the tool, and then communicates back with the user [83, 67, 46, 60].\\nTo this end, we evaluated our Nova models on agentic workflows that require textual understanding and visual reasoning.\\nFor textual understanding (Section 2.2.1), we used the Berkeley Function Calling Leaderboard benchmark to test our\\nmodels’ capabilities in function calling and orchestrating real-world applications. For visual reasoning (Section 2.2.2),\\n                                                         8',\n",
              "    'md': 'The Amazon Nova Family of Models\\n\\n| Model | tok/sec | MMMU (CoT) accuracy | Chart QA^C relaxed accuracy | Doc VQA ANLS | Text VQA weighted accuracy | VATEX CIDEr | Ego Schema accuracy |\\n|-------|---------|----------------------|----------------------------|---------------|----------------------------|-------------|----------------------|\\n| Amazon Nova Pro | 100 | 61.7 ±3.2 | 89.2 ±1.2 | 93.5 | 81.5 | 77.8 | 72.1 ±5.4 |\\n| Amazon Nova Lite | 157 | 56.2 ±3.2 | 86.8 ±1.3 | 92.4 | 80.2 | 77.8 | 71.4 ±5.4 |\\n| Claude 3.5 Sonnet (Oct) | 57 | 70.4 ±3.0 | 90.8 ±1.1 | 94.2 | 61.7^M | - | - |\\n| Claude 3 Haiku | 64 | 50.2 ±3.3 | 82.0 ±1.5 | 88.8 | - | - | - |\\n| Gemini 1.5 Pro (001) | 58 | 65.9 ±3.1^E | 87.2 ±1.3 | 93.1^B | 78.7 | 64.6^A | 72.2 ±5.4 |\\n| Gemini 1.5 Flash (001) | 190 | 62.3 ±3.2^E | 85.4 ±1.4 | 89.9^B | 78.7 | 57.1 | 65.7 ±5.7 |\\n| Gemini 1.5 Flash 8B (001) | 283 | 53.7 ±3.3^F | 78.2 ±1.6^G | 73.6 | 66.7 | 53.2^A | - |\\n| GPT-4o (May) | - | 69.1 ±3.0 | 85.7 ±1.4 | 92.8 | 77.2^D,M | - | 72.2 ±5.4 |\\n| GPT-4o Mini (Jul) | 113 | 59.4 ±3.2 | 79.2 ±1.6^M | - | 70.3^M | - | - |\\n| Llama 3.2 90B | 40 | 60.3 ±3.2 | 85.5 ±1.4 | 90.1 | 80.7^M | - | - |\\n| Llama 3.2 11B | 124 | 50.7 ±3.3 | 83.4 ±1.5 | 88.4 | 71.3^M | - | - |\\n\\nTable 3: Quantitative results on four image understanding benchmarks (MMMU [85], ChartQA [50], DocVQA [51], TextVQA [70]) and 2 video understanding benchmarks (VATEX [78] and EgoSchema [49]). Higher numbers are better for all benchmarks (↑). Unless otherwise noted, all evaluations are 0-shot and reference numbers are taken from the original technical reports and websites for Claude models [11, 12], GPT4 models [56, 55], Llama models [45, 53] and Gemini models [32, 33]. Remarks: (A) 4-shot evaluation; (B) External Optical Character Recognition (OCR) was used; (C) All models except Amazon Nova use CoT; (D) GPT-4o (Nov); (E) Gemini 1.5 Flash/Pro (002) models; (F) Reported in [33]; (G) Reported in [4]; (M) Claude 3.5 Sonnet and Llama 3.2 results for TextVQA as well as GPT4o and GPT4o mini results on ChartQA, TextVQA and VATEX were measured by us.² Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n\\nfrom 1940 to 2020 and covering multiple industries. We report Average Normalized Levenshtein Similarity (ANLS).\\n\\n- TextVQA [70]: The 5,000 samples of this dataset focus specifically on text-reading capabilities (OCR) in natural images. We report weighted accuracy on the validation set.\\n\\n- VATEX [78]: This video captioning benchmark covers a diverse set of human activities. We evaluate on the public test set containing videos with a length of around 10 seconds. The CIDEr [75] score is used for evaluation.\\n\\n- EgoSchema [49]: The unique characteristic of this long-form video question answering benchmark is its high \"certificate length\" [15], which is, loosely speaking, the time it takes a human to verify the video description. The videos cover a broad range of natural human activities and come with human-curated multiple-choice question-answer pairs.\\n\\nTable 3 summarizes our quantitative results on multiple image and video understanding benchmarks. Amazon Nova Pro and Lite achieve high scores across all benchmarks. Chart understanding on ChartQA and video understanding on VATEX stand out, where Nova models rank either first or second. We provide the prompt templates for all benchmarks in Appendix B.2, as well as qualitative examples in Appendix C.\\n\\n## 2.2 Agentic workflows\\n\\nAmazon Nova Pro, Lite, and Micro models can be used as agents. An agent considers a suite of tools and APIs, reasons about the user\\'s request and past conversational history, chooses if a tool should be used and, if so, decides which tool to use, invokes the tool, assesses the outcome from the tool, and then communicates back with the user [83, 67, 46, 60].\\n\\nTo this end, we evaluated our Nova models on agentic workflows that require textual understanding and visual reasoning. For textual understanding (Section 2.2.1), we used the Berkeley Function Calling Leaderboard benchmark to test our models\\' capabilities in function calling and orchestrating real-world applications. For visual reasoning (Section 2.2.2),',\n",
              "    'images': [{'name': 'page_8.jpg',\n",
              "      'height': 792,\n",
              "      'width': 612,\n",
              "      'x': 0,\n",
              "      'y': 0,\n",
              "      'original_width': 1236,\n",
              "      'original_height': 1600,\n",
              "      'type': 'full_page_screenshot'}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'text',\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': 'The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       ['Model',\n",
              "        'tok/sec',\n",
              "        'MMMU (CoT) accuracy',\n",
              "        'Chart QA^C relaxed accuracy',\n",
              "        'Doc VQA ANLS',\n",
              "        'Text VQA weighted accuracy',\n",
              "        'VATEX CIDEr',\n",
              "        'Ego Schema accuracy'],\n",
              "       ['-------',\n",
              "        '---------',\n",
              "        '----------------------',\n",
              "        '----------------------------',\n",
              "        '---------------',\n",
              "        '----------------------------',\n",
              "        '-------------',\n",
              "        '----------------------'],\n",
              "       ['Amazon Nova Pro',\n",
              "        '100',\n",
              "        '61.7 ±3.2',\n",
              "        '89.2 ±1.2',\n",
              "        '93.5',\n",
              "        '81.5',\n",
              "        '77.8',\n",
              "        '72.1 ±5.4'],\n",
              "       ['Amazon Nova Lite',\n",
              "        '157',\n",
              "        '56.2 ±3.2',\n",
              "        '86.8 ±1.3',\n",
              "        '92.4',\n",
              "        '80.2',\n",
              "        '77.8',\n",
              "        '71.4 ±5.4'],\n",
              "       ['Claude 3.5 Sonnet (Oct)',\n",
              "        '57',\n",
              "        '70.4 ±3.0',\n",
              "        '90.8 ±1.1',\n",
              "        '94.2',\n",
              "        '61.7^M',\n",
              "        '-',\n",
              "        '-'],\n",
              "       ['Claude 3 Haiku',\n",
              "        '64',\n",
              "        '50.2 ±3.3',\n",
              "        '82.0 ±1.5',\n",
              "        '88.8',\n",
              "        '-',\n",
              "        '-',\n",
              "        '-'],\n",
              "       ['Gemini 1.5 Pro (001)',\n",
              "        '58',\n",
              "        '65.9 ±3.1^E',\n",
              "        '87.2 ±1.3',\n",
              "        '93.1^B',\n",
              "        '78.7',\n",
              "        '64.6^A',\n",
              "        '72.2 ±5.4'],\n",
              "       ['Gemini 1.5 Flash (001)',\n",
              "        '190',\n",
              "        '62.3 ±3.2^E',\n",
              "        '85.4 ±1.4',\n",
              "        '89.9^B',\n",
              "        '78.7',\n",
              "        '57.1',\n",
              "        '65.7 ±5.7'],\n",
              "       ['Gemini 1.5 Flash 8B (001)',\n",
              "        '283',\n",
              "        '53.7 ±3.3^F',\n",
              "        '78.2 ±1.6^G',\n",
              "        '73.6',\n",
              "        '66.7',\n",
              "        '53.2^A',\n",
              "        '-'],\n",
              "       ['GPT-4o (May)',\n",
              "        '-',\n",
              "        '69.1 ±3.0',\n",
              "        '85.7 ±1.4',\n",
              "        '92.8',\n",
              "        '77.2^D,M',\n",
              "        '-',\n",
              "        '72.2 ±5.4'],\n",
              "       ['GPT-4o Mini (Jul)',\n",
              "        '113',\n",
              "        '59.4 ±3.2',\n",
              "        '79.2 ±1.6^M',\n",
              "        '-',\n",
              "        '70.3^M',\n",
              "        '-',\n",
              "        '-'],\n",
              "       ['Llama 3.2 90B',\n",
              "        '40',\n",
              "        '60.3 ±3.2',\n",
              "        '85.5 ±1.4',\n",
              "        '90.1',\n",
              "        '80.7^M',\n",
              "        '-',\n",
              "        '-'],\n",
              "       ['Llama 3.2 11B',\n",
              "        '124',\n",
              "        '50.7 ±3.3',\n",
              "        '83.4 ±1.5',\n",
              "        '88.4',\n",
              "        '71.3^M',\n",
              "        '-',\n",
              "        '-']],\n",
              "      'md': 'The Amazon Nova Family of Models\\n| Model | tok/sec | MMMU (CoT) accuracy | Chart QA^C relaxed accuracy | Doc VQA ANLS | Text VQA weighted accuracy | VATEX CIDEr | Ego Schema accuracy |\\n|-------|---------|----------------------|----------------------------|---------------|----------------------------|-------------|----------------------|\\n| Amazon Nova Pro | 100 | 61.7 ±3.2 | 89.2 ±1.2 | 93.5 | 81.5 | 77.8 | 72.1 ±5.4 |\\n| Amazon Nova Lite | 157 | 56.2 ±3.2 | 86.8 ±1.3 | 92.4 | 80.2 | 77.8 | 71.4 ±5.4 |\\n| Claude 3.5 Sonnet (Oct) | 57 | 70.4 ±3.0 | 90.8 ±1.1 | 94.2 | 61.7^M | - | - |\\n| Claude 3 Haiku | 64 | 50.2 ±3.3 | 82.0 ±1.5 | 88.8 | - | - | - |\\n| Gemini 1.5 Pro (001) | 58 | 65.9 ±3.1^E | 87.2 ±1.3 | 93.1^B | 78.7 | 64.6^A | 72.2 ±5.4 |\\n| Gemini 1.5 Flash (001) | 190 | 62.3 ±3.2^E | 85.4 ±1.4 | 89.9^B | 78.7 | 57.1 | 65.7 ±5.7 |\\n| Gemini 1.5 Flash 8B (001) | 283 | 53.7 ±3.3^F | 78.2 ±1.6^G | 73.6 | 66.7 | 53.2^A | - |\\n| GPT-4o (May) | - | 69.1 ±3.0 | 85.7 ±1.4 | 92.8 | 77.2^D,M | - | 72.2 ±5.4 |\\n| GPT-4o Mini (Jul) | 113 | 59.4 ±3.2 | 79.2 ±1.6^M | - | 70.3^M | - | - |\\n| Llama 3.2 90B | 40 | 60.3 ±3.2 | 85.5 ±1.4 | 90.1 | 80.7^M | - | - |\\n| Llama 3.2 11B | 124 | 50.7 ±3.3 | 83.4 ±1.5 | 88.4 | 71.3^M | - | - |',\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\"Model\",\"tok/sec\",\"MMMU (CoT) accuracy\",\"Chart QA^C relaxed accuracy\",\"Doc VQA ANLS\",\"Text VQA weighted accuracy\",\"VATEX CIDEr\",\"Ego Schema accuracy\"\\n\"-------\",\"---------\",\"----------------------\",\"----------------------------\",\"---------------\",\"----------------------------\",\"-------------\",\"----------------------\"\\n\"Amazon Nova Pro\",\"100\",\"61.7 ±3.2\",\"89.2 ±1.2\",\"93.5\",\"81.5\",\"77.8\",\"72.1 ±5.4\"\\n\"Amazon Nova Lite\",\"157\",\"56.2 ±3.2\",\"86.8 ±1.3\",\"92.4\",\"80.2\",\"77.8\",\"71.4 ±5.4\"\\n\"Claude 3.5 Sonnet (Oct)\",\"57\",\"70.4 ±3.0\",\"90.8 ±1.1\",\"94.2\",\"61.7^M\",\"-\",\"-\"\\n\"Claude 3 Haiku\",\"64\",\"50.2 ±3.3\",\"82.0 ±1.5\",\"88.8\",\"-\",\"-\",\"-\"\\n\"Gemini 1.5 Pro (001)\",\"58\",\"65.9 ±3.1^E\",\"87.2 ±1.3\",\"93.1^B\",\"78.7\",\"64.6^A\",\"72.2 ±5.4\"\\n\"Gemini 1.5 Flash (001)\",\"190\",\"62.3 ±3.2^E\",\"85.4 ±1.4\",\"89.9^B\",\"78.7\",\"57.1\",\"65.7 ±5.7\"\\n\"Gemini 1.5 Flash 8B (001)\",\"283\",\"53.7 ±3.3^F\",\"78.2 ±1.6^G\",\"73.6\",\"66.7\",\"53.2^A\",\"-\"\\n\"GPT-4o (May)\",\"-\",\"69.1 ±3.0\",\"85.7 ±1.4\",\"92.8\",\"77.2^D,M\",\"-\",\"72.2 ±5.4\"\\n\"GPT-4o Mini (Jul)\",\"113\",\"59.4 ±3.2\",\"79.2 ±1.6^M\",\"-\",\"70.3^M\",\"-\",\"-\"\\n\"Llama 3.2 90B\",\"40\",\"60.3 ±3.2\",\"85.5 ±1.4\",\"90.1\",\"80.7^M\",\"-\",\"-\"\\n\"Llama 3.2 11B\",\"124\",\"50.7 ±3.3\",\"83.4 ±1.5\",\"88.4\",\"71.3^M\",\"-\",\"-\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 686}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 3: Quantitative results on four image understanding benchmarks (MMMU [85], ChartQA [50], DocVQA [51], TextVQA [70]) and 2 video understanding benchmarks (VATEX [78] and EgoSchema [49]). Higher numbers are better for all benchmarks (↑). Unless otherwise noted, all evaluations are 0-shot and reference numbers are taken from the original technical reports and websites for Claude models [11, 12], GPT4 models [56, 55], Llama models [45, 53] and Gemini models [32, 33]. Remarks: (A) 4-shot evaluation; (B) External Optical Character Recognition (OCR) was used; (C) All models except Amazon Nova use CoT; (D) GPT-4o (Nov); (E) Gemini 1.5 Flash/Pro (002) models; (F) Reported in [33]; (G) Reported in [4]; (M) Claude 3.5 Sonnet and Llama 3.2 results for TextVQA as well as GPT4o and GPT4o mini results on ChartQA, TextVQA and VATEX were measured by us.² Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n\\nfrom 1940 to 2020 and covering multiple industries. We report Average Normalized Levenshtein Similarity (ANLS).\\n\\n- TextVQA [70]: The 5,000 samples of this dataset focus specifically on text-reading capabilities (OCR) in natural images. We report weighted accuracy on the validation set.\\n\\n- VATEX [78]: This video captioning benchmark covers a diverse set of human activities. We evaluate on the public test set containing videos with a length of around 10 seconds. The CIDEr [75] score is used for evaluation.\\n\\n- EgoSchema [49]: The unique characteristic of this long-form video question answering benchmark is its high \"certificate length\" [15], which is, loosely speaking, the time it takes a human to verify the video description. The videos cover a broad range of natural human activities and come with human-curated multiple-choice question-answer pairs.\\n\\nTable 3 summarizes our quantitative results on multiple image and video understanding benchmarks. Amazon Nova Pro and Lite achieve high scores across all benchmarks. Chart understanding on ChartQA and video understanding on VATEX stand out, where Nova models rank either first or second. We provide the prompt templates for all benchmarks in Appendix B.2, as well as qualitative examples in Appendix C.',\n",
              "      'md': 'Table 3: Quantitative results on four image understanding benchmarks (MMMU [85], ChartQA [50], DocVQA [51], TextVQA [70]) and 2 video understanding benchmarks (VATEX [78] and EgoSchema [49]). Higher numbers are better for all benchmarks (↑). Unless otherwise noted, all evaluations are 0-shot and reference numbers are taken from the original technical reports and websites for Claude models [11, 12], GPT4 models [56, 55], Llama models [45, 53] and Gemini models [32, 33]. Remarks: (A) 4-shot evaluation; (B) External Optical Character Recognition (OCR) was used; (C) All models except Amazon Nova use CoT; (D) GPT-4o (Nov); (E) Gemini 1.5 Flash/Pro (002) models; (F) Reported in [33]; (G) Reported in [4]; (M) Claude 3.5 Sonnet and Llama 3.2 results for TextVQA as well as GPT4o and GPT4o mini results on ChartQA, TextVQA and VATEX were measured by us.² Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n\\nfrom 1940 to 2020 and covering multiple industries. We report Average Normalized Levenshtein Similarity (ANLS).\\n\\n- TextVQA [70]: The 5,000 samples of this dataset focus specifically on text-reading capabilities (OCR) in natural images. We report weighted accuracy on the validation set.\\n\\n- VATEX [78]: This video captioning benchmark covers a diverse set of human activities. We evaluate on the public test set containing videos with a length of around 10 seconds. The CIDEr [75] score is used for evaluation.\\n\\n- EgoSchema [49]: The unique characteristic of this long-form video question answering benchmark is its high \"certificate length\" [15], which is, loosely speaking, the time it takes a human to verify the video description. The videos cover a broad range of natural human activities and come with human-curated multiple-choice question-answer pairs.\\n\\nTable 3 summarizes our quantitative results on multiple image and video understanding benchmarks. Amazon Nova Pro and Lite achieve high scores across all benchmarks. Chart understanding on ChartQA and video understanding on VATEX stand out, where Nova models rank either first or second. We provide the prompt templates for all benchmarks in Appendix B.2, as well as qualitative examples in Appendix C.',\n",
              "      'bBox': {'x': 71, 'y': 73.03, 'w': 470, 'h': 677}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 2,\n",
              "      'value': '2.2 Agentic workflows',\n",
              "      'md': '## 2.2 Agentic workflows',\n",
              "      'bBox': {'x': 72, 'y': 630.04, 'w': 101, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': \"Amazon Nova Pro, Lite, and Micro models can be used as agents. An agent considers a suite of tools and APIs, reasons about the user's request and past conversational history, chooses if a tool should be used and, if so, decides which tool to use, invokes the tool, assesses the outcome from the tool, and then communicates back with the user [83, 67, 46, 60].\\n\\nTo this end, we evaluated our Nova models on agentic workflows that require textual understanding and visual reasoning. For textual understanding (Section 2.2.1), we used the Berkeley Function Calling Leaderboard benchmark to test our models' capabilities in function calling and orchestrating real-world applications. For visual reasoning (Section 2.2.2),\",\n",
              "      'md': \"Amazon Nova Pro, Lite, and Micro models can be used as agents. An agent considers a suite of tools and APIs, reasons about the user's request and past conversational history, chooses if a tool should be used and, if so, decides which tool to use, invokes the tool, assesses the outcome from the tool, and then communicates back with the user [83, 67, 46, 60].\\n\\nTo this end, we evaluated our Nova models on agentic workflows that require textual understanding and visual reasoning. For textual understanding (Section 2.2.1), we used the Berkeley Function Calling Leaderboard benchmark to test our models' capabilities in function calling and orchestrating real-world applications. For visual reasoning (Section 2.2.2),\",\n",
              "      'bBox': {'x': 71, 'y': 73.03, 'w': 470, 'h': 677}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': True,\n",
              "    'parsingMode': 'premium',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 9,\n",
              "    'text': '                                     The Amazon Nova Family of Models\\nwe evaluate on three benchmarks that require image understanding capabilities for correct function calling. We highlight\\nthat both Amazon Nova Pro and Lite models set a new state of the art on these challenging benchmarks.\\n2.2.1  Agentic text benchmarks and results\\nTable 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL).3 Stemming from the\\nGorilla project [60], the revamped BFCL [81] benchmark evaluates a model’s ability to accurately call and utilize\\nreal-world functions, or tools, based on a user’s natural language request. Amazon Nova models particularly excel in\\nthe Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models.\\nAmazon Nova Lite and Micro also had the lowest latency of the selected models.\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names\\nand argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based\\non manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call’s\\naccuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\nTo measure the rate of hallucination, Irrelevance measures the model’s ability to recognize that it does not have the\\nappropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of\\nirrelevance, measures the model’s ability to recognize it indeed does have the functions necessary to help the user (but\\ndoes not verify function signature accuracy). For both metrics, higher numbers are better.\\n                             Overall  Latency        Non-Live          Live      Multi-      Hallucination\\n                                                                                 Turn\\n                             accuracy seconds     AST      execution overall   overall   relevance   irrelevance\\n                               (↑)      (↓)        (↑)       (↑)       (↑)       (↑)        (↑)        (↑)\\n Nova Pro                      68.4     1.0       90.1      89.8       71.5       45.1      95.1       65.1\\n Nova Lite                     66.6     0.6       87.5      86.4       66.0      50.3       97.6       49.1\\n Nova Micro                    56.2     0.5       87.2      89.7       67.4      15.5       87.8       57.6\\n Claude Sonnet 3.5 (Jun)       61.3     3.9       70.0      66.3       74.7      40.0       68.3       74.6\\n Claude Haiku 3              40.4       1.5       41.7      47.5       57.7      20.6       97.6       29.4\\n Gemini 1.5 Pro (002)          59.8     3.0       88.0      91.4       74.3      16.3       75.6       75.1\\n Gemini 1.5 Flash (002)        55.3     1.1       79.7      80.6       73.2      12.5       78.1       75.7\\n Llama 3.2 90Bᴬ                54.3     N/A       88.9      89.3       61.1      14.3       92.7       58.4\\n Llama 3.2 11Bᴬ              49.9       N/A       83.6      87.3       57.9      10.5       78.1       41.6\\n GPT-4o (Aug)                  68.9     1.5       85.9      85.6       75.4      45.3       63.4       82.9\\n GPT-4o-mini (Jul)             60.7     1.6       84.3      84.1       70.2      28.3       80.5       71.8\\nTable 4: Results on the Berkeley Function Calling Leaderboard (BFCL) v3 as of the Nov 17th, 2024 update. We include\\nthe latest versions of the models available on the leaderboard at that time. (A) We use leaderboard results for Llama 3.1\\n8B and 70B for Llama 3.2 11B and 90B, respectively, given the shared text LLM.\\n2.2.2  Agentic multimodal benchmarks and results\\nThe Amazon Nova Pro and Lite models provide native support for multimodal inputs, including agentic workflows. In\\nthis section, we present results from our models on three different benchmarks that require agents to navigate websites\\nto solve real-world tasks. Websites are typically represented as screenshots in these datasets to correctly convey all style\\nelements and visual data as rendered in a standard web browser.\\n      •  VisualWebBench [43]: This benchmark includes seven core tasks related to web browsing, including captioning,\\n         question answering, OCR, action prediction, and grounding. All models are evaluated on 1,536 samples that\\n         span more than 100 websites from 12 domains. The final metric is the average over different metrics for the\\n         individual core tasks.\\n                             3BFCL is a fast-moving, live benchmark. We report results using the state of the repository and website leaderboard as of Nov\\n17th, 2024 (commit 8226d).\\n                                                      9',\n",
              "    'md': \"The Amazon Nova Family of Models\\n\\nwe evaluate on three benchmarks that require image understanding capabilities for correct function calling. We highlight that both Amazon Nova Pro and Lite models set a new state of the art on these challenging benchmarks.\\n\\n## 2.2.1 Agentic text benchmarks and results\\n\\nTable 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL)³. Stemming from the Gorilla project [60], the revamped BFCL [81] benchmark evaluates a model's ability to accurately call and utilize real-world functions, or tools, based on a user's natural language request. Amazon Nova models particularly excel in the Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models. Amazon Nova Lite and Micro also had the lowest latency of the selected models.\\n\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names and argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based on manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call's accuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\n\\nTo measure the rate of hallucination, Irrelevance measures the model's ability to recognize that it does not have the appropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of irrelevance, measures the model's ability to recognize it indeed does have the functions necessary to help the user (but does not verify function signature accuracy). For both metrics, higher numbers are better.\\n\\n| Model | Overall accuracy (↑) | Latency seconds (↓) | Non-Live AST (↑) | Non-Live execution (↑) | Live overall (↑) | Multi-Turn overall (↑) | Hallucination relevance (↑) | Hallucination irrelevance (↑) |\\n|---|---|---|---|---|---|---|---|---|\\n| Nova Pro | 68.4 | 1.0 | 90.1 | 89.8 | 71.5 | 45.1 | 95.1 | 65.1 |\\n| Nova Lite | 66.6 | 0.6 | 87.5 | 86.4 | 66.0 | 50.3 | 97.6 | 49.1 |\\n| Nova Micro | 56.2 | 0.5 | 87.2 | 89.7 | 67.4 | 15.5 | 87.8 | 57.6 |\\n| Claude Sonnet 3.5 (Jun) | 61.3 | 3.9 | 70.0 | 66.3 | 74.7 | 40.0 | 68.3 | 74.6 |\\n| Claude Haiku 3 | 40.4 | 1.5 | 41.7 | 47.5 | 57.7 | 20.6 | 97.6 | 29.4 |\\n| Gemini 1.5 Pro (002) | 59.8 | 3.0 | 88.0 | 91.4 | 74.3 | 16.3 | 75.6 | 75.1 |\\n| Gemini 1.5 Flash (002) | 55.3 | 1.1 | 79.7 | 80.6 | 73.2 | 12.5 | 78.1 | 75.7 |\\n| Llama 3.2 90B^A | 54.3 | N/A | 88.9 | 89.3 | 61.1 | 14.3 | 92.7 | 58.4 |\\n| Llama 3.2 11B^A | 49.9 | N/A | 83.6 | 87.3 | 57.9 | 10.5 | 78.1 | 41.6 |\\n| GPT-4o (Aug) | 68.9 | 1.5 | 85.9 | 85.6 | 75.4 | 45.3 | 63.4 | 82.9 |\\n| GPT-4o-mini (Jul) | 60.7 | 1.6 | 84.3 | 84.1 | 70.2 | 28.3 | 80.5 | 71.8 |\\n\\nTable 4: Results on the Berkeley Function Calling Leaderboard (BFCL) v3 as of the Nov 17th, 2024 update. We include the latest versions of the models available on the leaderboard at that time. (A) We use leaderboard results for Llama 3.1 8B and 70B for Llama 3.2 11B and 90B, respectively, given the shared text LLM.\\n\\n## 2.2.2 Agentic multimodal benchmarks and results\\n\\nThe Amazon Nova Pro and Lite models provide native support for multimodal inputs, including agentic workflows. In this section, we present results from our models on three different benchmarks that require agents to navigate websites to solve real-world tasks. Websites are typically represented as screenshots in these datasets to correctly convey all style elements and visual data as rendered in a standard web browser.\\n\\n- VisualWebBench [43]: This benchmark includes seven core tasks related to web browsing, including captioning, question answering, OCR, action prediction, and grounding. All models are evaluated on 1,536 samples that span more than 100 websites from 12 domains. The final metric is the average over different metrics for the individual core tasks.\\n\\n³BFCL is a fast-moving, live benchmark. We report results using the state of the repository and website leaderboard as of Nov 17th, 2024 (commit 8226d).\",\n",
              "    'images': [{'name': 'page_9.jpg',\n",
              "      'height': 792,\n",
              "      'width': 612,\n",
              "      'x': 0,\n",
              "      'y': 0,\n",
              "      'original_width': 1236,\n",
              "      'original_height': 1600,\n",
              "      'type': 'full_page_screenshot'}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'text',\n",
              "      'value': 'The Amazon Nova Family of Models\\n\\nwe evaluate on three benchmarks that require image understanding capabilities for correct function calling. We highlight that both Amazon Nova Pro and Lite models set a new state of the art on these challenging benchmarks.',\n",
              "      'md': 'The Amazon Nova Family of Models\\n\\nwe evaluate on three benchmarks that require image understanding capabilities for correct function calling. We highlight that both Amazon Nova Pro and Lite models set a new state of the art on these challenging benchmarks.',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 468, 'h': 321}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 2,\n",
              "      'value': '2.2.1 Agentic text benchmarks and results',\n",
              "      'md': '## 2.2.1 Agentic text benchmarks and results',\n",
              "      'bBox': {'x': 72, 'y': 108.04, 'w': 184, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': \"Table 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL)³. Stemming from the Gorilla project [60], the revamped BFCL [81] benchmark evaluates a model's ability to accurately call and utilize real-world functions, or tools, based on a user's natural language request. Amazon Nova models particularly excel in the Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models. Amazon Nova Lite and Micro also had the lowest latency of the selected models.\\n\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names and argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based on manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call's accuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\n\\nTo measure the rate of hallucination, Irrelevance measures the model's ability to recognize that it does not have the appropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of irrelevance, measures the model's ability to recognize it indeed does have the functions necessary to help the user (but does not verify function signature accuracy). For both metrics, higher numbers are better.\",\n",
              "      'md': \"Table 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL)³. Stemming from the Gorilla project [60], the revamped BFCL [81] benchmark evaluates a model's ability to accurately call and utilize real-world functions, or tools, based on a user's natural language request. Amazon Nova models particularly excel in the Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models. Amazon Nova Lite and Micro also had the lowest latency of the selected models.\\n\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names and argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based on manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call's accuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\n\\nTo measure the rate of hallucination, Irrelevance measures the model's ability to recognize that it does not have the appropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of irrelevance, measures the model's ability to recognize it indeed does have the functions necessary to help the user (but does not verify function signature accuracy). For both metrics, higher numbers are better.\",\n",
              "      'bBox': {'x': 71, 'y': 160.04, 'w': 471, 'h': 209}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       [],\n",
              "       [],\n",
              "       [],\n",
              "       [],\n",
              "       ['Model',\n",
              "        'Overall accuracy (↑)',\n",
              "        'Latency seconds (↓)',\n",
              "        'Non-Live AST (↑)',\n",
              "        'Non-Live execution (↑)',\n",
              "        'Live overall (↑)',\n",
              "        'Multi-Turn overall (↑)',\n",
              "        'Hallucination relevance (↑)',\n",
              "        'Hallucination irrelevance (↑)'],\n",
              "       ['---', '---', '---', '---', '---', '---', '---', '---', '---'],\n",
              "       ['Nova Pro',\n",
              "        '68.4',\n",
              "        '1.0',\n",
              "        '90.1',\n",
              "        '89.8',\n",
              "        '71.5',\n",
              "        '45.1',\n",
              "        '95.1',\n",
              "        '65.1'],\n",
              "       ['Nova Lite',\n",
              "        '66.6',\n",
              "        '0.6',\n",
              "        '87.5',\n",
              "        '86.4',\n",
              "        '66.0',\n",
              "        '50.3',\n",
              "        '97.6',\n",
              "        '49.1'],\n",
              "       ['Nova Micro',\n",
              "        '56.2',\n",
              "        '0.5',\n",
              "        '87.2',\n",
              "        '89.7',\n",
              "        '67.4',\n",
              "        '15.5',\n",
              "        '87.8',\n",
              "        '57.6'],\n",
              "       ['Claude Sonnet 3.5 (Jun)',\n",
              "        '61.3',\n",
              "        '3.9',\n",
              "        '70.0',\n",
              "        '66.3',\n",
              "        '74.7',\n",
              "        '40.0',\n",
              "        '68.3',\n",
              "        '74.6'],\n",
              "       ['Claude Haiku 3',\n",
              "        '40.4',\n",
              "        '1.5',\n",
              "        '41.7',\n",
              "        '47.5',\n",
              "        '57.7',\n",
              "        '20.6',\n",
              "        '97.6',\n",
              "        '29.4'],\n",
              "       ['Gemini 1.5 Pro (002)',\n",
              "        '59.8',\n",
              "        '3.0',\n",
              "        '88.0',\n",
              "        '91.4',\n",
              "        '74.3',\n",
              "        '16.3',\n",
              "        '75.6',\n",
              "        '75.1'],\n",
              "       ['Gemini 1.5 Flash (002)',\n",
              "        '55.3',\n",
              "        '1.1',\n",
              "        '79.7',\n",
              "        '80.6',\n",
              "        '73.2',\n",
              "        '12.5',\n",
              "        '78.1',\n",
              "        '75.7'],\n",
              "       ['Llama 3.2 90B^A',\n",
              "        '54.3',\n",
              "        'N/A',\n",
              "        '88.9',\n",
              "        '89.3',\n",
              "        '61.1',\n",
              "        '14.3',\n",
              "        '92.7',\n",
              "        '58.4'],\n",
              "       ['Llama 3.2 11B^A',\n",
              "        '49.9',\n",
              "        'N/A',\n",
              "        '83.6',\n",
              "        '87.3',\n",
              "        '57.9',\n",
              "        '10.5',\n",
              "        '78.1',\n",
              "        '41.6'],\n",
              "       ['GPT-4o (Aug)',\n",
              "        '68.9',\n",
              "        '1.5',\n",
              "        '85.9',\n",
              "        '85.6',\n",
              "        '75.4',\n",
              "        '45.3',\n",
              "        '63.4',\n",
              "        '82.9'],\n",
              "       ['GPT-4o-mini (Jul)',\n",
              "        '60.7',\n",
              "        '1.6',\n",
              "        '84.3',\n",
              "        '84.1',\n",
              "        '70.2',\n",
              "        '28.3',\n",
              "        '80.5',\n",
              "        '71.8']],\n",
              "      'md': \"Table 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL)³. Stemming from the Gorilla project [60], the revamped BFCL [81] benchmark evaluates a model's ability to accurately call and utilize real-world functions, or tools, based on a user's natural language request. Amazon Nova models particularly excel in the Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models. Amazon Nova Lite and Micro also had the lowest latency of the selected models.\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names and argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based on manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call's accuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\n\\nTo measure the rate of hallucination, Irrelevance measures the model's ability to recognize that it does not have the appropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of irrelevance, measures the model's ability to recognize it indeed does have the functions necessary to help the user (but does not verify function signature accuracy). For both metrics, higher numbers are better.\\n\\n| Model | Overall accuracy (↑) | Latency seconds (↓) | Non-Live AST (↑) | Non-Live execution (↑) | Live overall (↑) | Multi-Turn overall (↑) | Hallucination relevance (↑) | Hallucination irrelevance (↑) |\\n|---|---|---|---|---|---|---|---|---|\\n| Nova Pro | 68.4 | 1.0 | 90.1 | 89.8 | 71.5 | 45.1 | 95.1 | 65.1 |\\n| Nova Lite | 66.6 | 0.6 | 87.5 | 86.4 | 66.0 | 50.3 | 97.6 | 49.1 |\\n| Nova Micro | 56.2 | 0.5 | 87.2 | 89.7 | 67.4 | 15.5 | 87.8 | 57.6 |\\n| Claude Sonnet 3.5 (Jun) | 61.3 | 3.9 | 70.0 | 66.3 | 74.7 | 40.0 | 68.3 | 74.6 |\\n| Claude Haiku 3 | 40.4 | 1.5 | 41.7 | 47.5 | 57.7 | 20.6 | 97.6 | 29.4 |\\n| Gemini 1.5 Pro (002) | 59.8 | 3.0 | 88.0 | 91.4 | 74.3 | 16.3 | 75.6 | 75.1 |\\n| Gemini 1.5 Flash (002) | 55.3 | 1.1 | 79.7 | 80.6 | 73.2 | 12.5 | 78.1 | 75.7 |\\n| Llama 3.2 90B^A | 54.3 | N/A | 88.9 | 89.3 | 61.1 | 14.3 | 92.7 | 58.4 |\\n| Llama 3.2 11B^A | 49.9 | N/A | 83.6 | 87.3 | 57.9 | 10.5 | 78.1 | 41.6 |\\n| GPT-4o (Aug) | 68.9 | 1.5 | 85.9 | 85.6 | 75.4 | 45.3 | 63.4 | 82.9 |\\n| GPT-4o-mini (Jul) | 60.7 | 1.6 | 84.3 | 84.1 | 70.2 | 28.3 | 80.5 | 71.8 |\",\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\\n\\n\\n\\n\"Model\",\"Overall accuracy (↑)\",\"Latency seconds (↓)\",\"Non-Live AST (↑)\",\"Non-Live execution (↑)\",\"Live overall (↑)\",\"Multi-Turn overall (↑)\",\"Hallucination relevance (↑)\",\"Hallucination irrelevance (↑)\"\\n\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\\n\"Nova Pro\",\"68.4\",\"1.0\",\"90.1\",\"89.8\",\"71.5\",\"45.1\",\"95.1\",\"65.1\"\\n\"Nova Lite\",\"66.6\",\"0.6\",\"87.5\",\"86.4\",\"66.0\",\"50.3\",\"97.6\",\"49.1\"\\n\"Nova Micro\",\"56.2\",\"0.5\",\"87.2\",\"89.7\",\"67.4\",\"15.5\",\"87.8\",\"57.6\"\\n\"Claude Sonnet 3.5 (Jun)\",\"61.3\",\"3.9\",\"70.0\",\"66.3\",\"74.7\",\"40.0\",\"68.3\",\"74.6\"\\n\"Claude Haiku 3\",\"40.4\",\"1.5\",\"41.7\",\"47.5\",\"57.7\",\"20.6\",\"97.6\",\"29.4\"\\n\"Gemini 1.5 Pro (002)\",\"59.8\",\"3.0\",\"88.0\",\"91.4\",\"74.3\",\"16.3\",\"75.6\",\"75.1\"\\n\"Gemini 1.5 Flash (002)\",\"55.3\",\"1.1\",\"79.7\",\"80.6\",\"73.2\",\"12.5\",\"78.1\",\"75.7\"\\n\"Llama 3.2 90B^A\",\"54.3\",\"N/A\",\"88.9\",\"89.3\",\"61.1\",\"14.3\",\"92.7\",\"58.4\"\\n\"Llama 3.2 11B^A\",\"49.9\",\"N/A\",\"83.6\",\"87.3\",\"57.9\",\"10.5\",\"78.1\",\"41.6\"\\n\"GPT-4o (Aug)\",\"68.9\",\"1.5\",\"85.9\",\"85.6\",\"75.4\",\"45.3\",\"63.4\",\"82.9\"\\n\"GPT-4o-mini (Jul)\",\"60.7\",\"1.6\",\"84.3\",\"84.1\",\"70.2\",\"28.3\",\"80.5\",\"71.8\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 636}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 4: Results on the Berkeley Function Calling Leaderboard (BFCL) v3 as of the Nov 17th, 2024 update. We include the latest versions of the models available on the leaderboard at that time. (A) We use leaderboard results for Llama 3.1 8B and 70B for Llama 3.2 11B and 90B, respectively, given the shared text LLM.',\n",
              "      'md': 'Table 4: Results on the Berkeley Function Calling Leaderboard (BFCL) v3 as of the Nov 17th, 2024 update. We include the latest versions of the models available on the leaderboard at that time. (A) We use leaderboard results for Llama 3.1 8B and 70B for Llama 3.2 11B and 90B, respectively, given the shared text LLM.',\n",
              "      'bBox': {'x': 71, 'y': 514.04, 'w': 469, 'h': 236}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 2,\n",
              "      'value': '2.2.2 Agentic multimodal benchmarks and results',\n",
              "      'md': '## 2.2.2 Agentic multimodal benchmarks and results',\n",
              "      'bBox': {'x': 72, 'y': 576.04, 'w': 218, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'The Amazon Nova Pro and Lite models provide native support for multimodal inputs, including agentic workflows. In this section, we present results from our models on three different benchmarks that require agents to navigate websites to solve real-world tasks. Websites are typically represented as screenshots in these datasets to correctly convey all style elements and visual data as rendered in a standard web browser.\\n\\n- VisualWebBench [43]: This benchmark includes seven core tasks related to web browsing, including captioning, question answering, OCR, action prediction, and grounding. All models are evaluated on 1,536 samples that span more than 100 websites from 12 domains. The final metric is the average over different metrics for the individual core tasks.\\n\\n³BFCL is a fast-moving, live benchmark. We report results using the state of the repository and website leaderboard as of Nov 17th, 2024 (commit 8226d).',\n",
              "      'md': 'The Amazon Nova Pro and Lite models provide native support for multimodal inputs, including agentic workflows. In this section, we present results from our models on three different benchmarks that require agents to navigate websites to solve real-world tasks. Websites are typically represented as screenshots in these datasets to correctly convey all style elements and visual data as rendered in a standard web browser.\\n\\n- VisualWebBench [43]: This benchmark includes seven core tasks related to web browsing, including captioning, question answering, OCR, action prediction, and grounding. All models are evaluated on 1,536 samples that span more than 100 websites from 12 domains. The final metric is the average over different metrics for the individual core tasks.\\n\\n³BFCL is a fast-moving, live benchmark. We report results using the state of the repository and website leaderboard as of Nov 17th, 2024 (commit 8226d).',\n",
              "      'bBox': {'x': 71, 'y': 300.03, 'w': 470, 'h': 420}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': True,\n",
              "    'parsingMode': 'premium',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 10,\n",
              "    'text': '                                                      The Amazon Nova Family of Models\\n               •  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the\\n            original website screenshots, making it multimodal. An agent needs to select an element and pick one of three\\n             elementary actions (click, type, or select) alongside a value for some actions. We report micro average over\\n          the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the\\n                  predicted value, are correct.\\n               •  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24],\\n                 and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an\\n            instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location\\n         of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding\\n                  box.\\n                                                      Table 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results.\\n         Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve\\n         high scores on all three benchmarks.\\n                                                      VisualWebBench        MM-Mind2Web   GroundUI-1K\\n                                                        compositeᴰ         step accuracy     accuracy\\n                     Nova Pro                         79.7                 63.7                 81.4\\n                     Nova Lite                        77.7                 60.7                 80.2\\n                     Claude 3.5 Sonnet (Oct)          76.7ᴹ                61.6ᴹ                16.3\\n                     GPT-4o (Nov)                     77.5ᴹ                55.0ᴹ                13.4C\\n                     GPT-4o Mini (Jul)                71.3ᴹ                58.6ᴹ                 7.2ᴹ\\n                     GPT-4 (Apr)                      64.6                 36.8ᴬ                  -\\n                     Gemini 1.5 Pro (002)             76.4ᴹ                58.4ᴹ                35.2ᴮ\\n                     Gemini 1.5 Flash (002)           76.1ᴹ                46.2ᴹ                59.9ᴹ\\n                     Gemini 1.0 Pro (001)             48.0                 17.9ᴬ                  -\\n                     Llama 3.2 90B                    73.2ᴹ                21.6ᴹ                 8.3ᴹ\\n                     Llama 3.2 11B                    65.1ᴹ                22.1ᴹ                 3.7ᴹ\\n             Table 5: Quantitative results on three multi-modal agentic benchmarks: VisualWebBench [43], MM-Mind2Web [86]\\n               and GroundUI-1K [87]. Reference numbers are taken from the corresponding benchmark papers [43, 86, 87] and\\n    leaderboard [3]. Remarks: (A) uses in-context learning (ICL) (please note that Amazon Nova models do not need to rely\\n      on in-context examples); (B) Gemini 1.5 Pro (001); (C) GPT-4o (May); (D) Macro average over individual metrics; (M)\\n         Measured by us.2\\n         2.3  Long context\\n         We evaluate Amazon Nova Pro, Lite, and Micro on tasks that require the models to understand and reason over long\\ncontext. These skills are crucial for tasks such as long multi-turn conversations, reasoning over long lists of retrieved\\n              documents, or understanding long videos. Amazon Nova Micro, Lite, and Pro models support context lengths of\\n         128k, 300k, and 300k tokens, respectively. We used the following benchmarks to evaluate our models’ long context\\n         performance:\\n               •    Text Needle-in-a-Haystack (NIAH): Following [40], we assessed each model’s ability to locate specific\\n                  information (the “needle”) within extensive contexts (the “haystack”). This “needle-in-a-haystack” test\\n              evaluates the model’s performance on context lengths starting at 32k, allowing us to measure its ability to\\n                  accurately retrieve information across varying lengths of input context.\\n               • SQuALITY [76] (ZeroScrolls Benchmark [69]): Focused on query-based summarization of literary stories,\\n                  this task evaluates the model’s capacity to generate relevant summaries from large contexts.\\n               •    LVBench [77]: This multimodal benchmark includes questions about YouTube videos4 from various domains\\n           such as TV series, sports, broadcasts, and surveillance footage. The LVBench dataset consists of 99 videos and\\n                  1,549 questions, covering six different types of tasks such as reasoning, event understanding and summarization.\\n            4https://huggingface.co/datasets/AIWinter/LVBench\\n                                                              10',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n•  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the original website screenshots, making it multimodal. An agent needs to select an element and pick one of three elementary actions (click, type, or select) alongside a value for some actions. We report micro average over the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the predicted value, are correct.\\n\\n•  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24], and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding box.\\n\\nTable 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results. Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve high scores on all three benchmarks.\\n\\n| |Model|Results| |\\n|---|---|---|---|\\n|VisualWebBench compositeᴰ|MM-Mind2Web step accuracy|GroundUI-1K accuracy| |\\n|Nova Pro|79.7|63.7|81.4|\\n|Nova Lite|77.7|60.7|80.2|\\n|Claude 3.5 Sonnet (Oct)|76.7ᴹ|61.6ᴹ|16.3|\\n|GPT-4o (Nov)|77.5ᴹ|55.0ᴹ|13.4C|\\n|GPT-4o Mini (Jul)|71.3ᴹ|58.6ᴹ|7.2ᴹ|\\n|GPT-4 (Apr)|64.6|36.8ᴬ|-|\\n|Gemini 1.5 Pro (002)|76.4ᴹ|58.4ᴹ|35.2ᴮ|\\n|Gemini 1.5 Flash (002)|76.1ᴹ|46.2ᴹ|59.9ᴹ|\\n|Gemini 1.0 Pro (001)|48.0|17.9ᴬ|-|\\n|Llama 3.2 90B|73.2ᴹ|21.6ᴹ|8.3ᴹ|\\n|Llama 3.2 11B|65.1ᴹ|22.1ᴹ|3.7ᴹ|\\n\\nTable 5: Quantitative results on three multi-modal agentic benchmarks: VisualWebBench [43], MM-Mind2Web [86] and GroundUI-1K [87]. Reference numbers are taken from the corresponding benchmark papers [43, 86, 87] and leaderboard [3]. Remarks: (A) uses in-context learning (ICL) (please note that Amazon Nova models do not need to rely on in-context examples); (B) Gemini 1.5 Pro (001); (C) GPT-4o (May); (D) Macro average over individual metrics; (M) Measured by us.\\n\\n# 2.3  Long context\\n\\nWe evaluate Amazon Nova Pro, Lite, and Micro on tasks that require the models to understand and reason over long context. These skills are crucial for tasks such as long multi-turn conversations, reasoning over long lists of retrieved documents, or understanding long videos. Amazon Nova Micro, Lite, and Pro models support context lengths of 128k, 300k, and 300k tokens, respectively. We used the following benchmarks to evaluate our models’ long context performance:\\n\\n•    Text Needle-in-a-Haystack (NIAH): Following [40], we assessed each model’s ability to locate specific information (the “needle”) within extensive contexts (the “haystack”). This “needle-in-a-haystack” test evaluates the model’s performance on context lengths starting at 32k, allowing us to measure its ability to accurately retrieve information across varying lengths of input context.\\n\\n• SQuALITY [76] (ZeroScrolls Benchmark [69]): Focused on query-based summarization of literary stories, this task evaluates the model’s capacity to generate relevant summaries from large contexts.\\n\\n•    LVBench [77]: This multimodal benchmark includes questions about YouTube videos from various domains such as TV series, sports, broadcasts, and surveillance footage. The LVBench dataset consists of 99 videos and 1,549 questions, covering six different types of tasks such as reasoning, event understanding and summarization.\\n\\n4https://huggingface.co/datasets/AIWinter/LVBench',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': '•  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the original website screenshots, making it multimodal. An agent needs to select an element and pick one of three elementary actions (click, type, or select) alongside a value for some actions. We report micro average over the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the predicted value, are correct.\\n\\n•  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24], and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding box.\\n\\nTable 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results. Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve high scores on all three benchmarks.',\n",
              "      'md': '•  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the original website screenshots, making it multimodal. An agent needs to select an element and pick one of three elementary actions (click, type, or select) alongside a value for some actions. We report micro average over the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the predicted value, are correct.\\n\\n•  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24], and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding box.\\n\\nTable 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results. Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve high scores on all three benchmarks.',\n",
              "      'bBox': {'x': 71, 'y': 72.04, 'w': 470, 'h': 609}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       [],\n",
              "       [],\n",
              "       [],\n",
              "       [],\n",
              "       ['', 'Model', 'Results', ''],\n",
              "       ['---', '---', '---', '---'],\n",
              "       ['VisualWebBench compositeᴰ',\n",
              "        'MM-Mind2Web step accuracy',\n",
              "        'GroundUI-1K accuracy',\n",
              "        ''],\n",
              "       ['Nova Pro', '79.7', '63.7', '81.4'],\n",
              "       ['Nova Lite', '77.7', '60.7', '80.2'],\n",
              "       ['Claude 3.5 Sonnet (Oct)', '76.7ᴹ', '61.6ᴹ', '16.3'],\n",
              "       ['GPT-4o (Nov)', '77.5ᴹ', '55.0ᴹ', '13.4C'],\n",
              "       ['GPT-4o Mini (Jul)', '71.3ᴹ', '58.6ᴹ', '7.2ᴹ'],\n",
              "       ['GPT-4 (Apr)', '64.6', '36.8ᴬ', '-'],\n",
              "       ['Gemini 1.5 Pro (002)', '76.4ᴹ', '58.4ᴹ', '35.2ᴮ'],\n",
              "       ['Gemini 1.5 Flash (002)', '76.1ᴹ', '46.2ᴹ', '59.9ᴹ'],\n",
              "       ['Gemini 1.0 Pro (001)', '48.0', '17.9ᴬ', '-'],\n",
              "       ['Llama 3.2 90B', '73.2ᴹ', '21.6ᴹ', '8.3ᴹ'],\n",
              "       ['Llama 3.2 11B', '65.1ᴹ', '22.1ᴹ', '3.7ᴹ']],\n",
              "      'md': '•  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the original website screenshots, making it multimodal. An agent needs to select an element and pick one of three elementary actions (click, type, or select) alongside a value for some actions. We report micro average over the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the predicted value, are correct.\\n•  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24], and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding box.\\n\\nTable 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results. Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve high scores on all three benchmarks.\\n\\n| |Model|Results| |\\n|---|---|---|---|\\n|VisualWebBench compositeᴰ|MM-Mind2Web step accuracy|GroundUI-1K accuracy| |\\n|Nova Pro|79.7|63.7|81.4|\\n|Nova Lite|77.7|60.7|80.2|\\n|Claude 3.5 Sonnet (Oct)|76.7ᴹ|61.6ᴹ|16.3|\\n|GPT-4o (Nov)|77.5ᴹ|55.0ᴹ|13.4C|\\n|GPT-4o Mini (Jul)|71.3ᴹ|58.6ᴹ|7.2ᴹ|\\n|GPT-4 (Apr)|64.6|36.8ᴬ|-|\\n|Gemini 1.5 Pro (002)|76.4ᴹ|58.4ᴹ|35.2ᴮ|\\n|Gemini 1.5 Flash (002)|76.1ᴹ|46.2ᴹ|59.9ᴹ|\\n|Gemini 1.0 Pro (001)|48.0|17.9ᴬ|-|\\n|Llama 3.2 90B|73.2ᴹ|21.6ᴹ|8.3ᴹ|\\n|Llama 3.2 11B|65.1ᴹ|22.1ᴹ|3.7ᴹ|',\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\\n\\n\\n\\n\"\",\"Model\",\"Results\",\"\"\\n\"---\",\"---\",\"---\",\"---\"\\n\"VisualWebBench compositeᴰ\",\"MM-Mind2Web step accuracy\",\"GroundUI-1K accuracy\",\"\"\\n\"Nova Pro\",\"79.7\",\"63.7\",\"81.4\"\\n\"Nova Lite\",\"77.7\",\"60.7\",\"80.2\"\\n\"Claude 3.5 Sonnet (Oct)\",\"76.7ᴹ\",\"61.6ᴹ\",\"16.3\"\\n\"GPT-4o (Nov)\",\"77.5ᴹ\",\"55.0ᴹ\",\"13.4C\"\\n\"GPT-4o Mini (Jul)\",\"71.3ᴹ\",\"58.6ᴹ\",\"7.2ᴹ\"\\n\"GPT-4 (Apr)\",\"64.6\",\"36.8ᴬ\",\"-\"\\n\"Gemini 1.5 Pro (002)\",\"76.4ᴹ\",\"58.4ᴹ\",\"35.2ᴮ\"\\n\"Gemini 1.5 Flash (002)\",\"76.1ᴹ\",\"46.2ᴹ\",\"59.9ᴹ\"\\n\"Gemini 1.0 Pro (001)\",\"48.0\",\"17.9ᴬ\",\"-\"\\n\"Llama 3.2 90B\",\"73.2ᴹ\",\"21.6ᴹ\",\"8.3ᴹ\"\\n\"Llama 3.2 11B\",\"65.1ᴹ\",\"22.1ᴹ\",\"3.7ᴹ\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 5: Quantitative results on three multi-modal agentic benchmarks: VisualWebBench [43], MM-Mind2Web [86] and GroundUI-1K [87]. Reference numbers are taken from the corresponding benchmark papers [43, 86, 87] and leaderboard [3]. Remarks: (A) uses in-context learning (ICL) (please note that Amazon Nova models do not need to rely on in-context examples); (B) Gemini 1.5 Pro (001); (C) GPT-4o (May); (D) Macro average over individual metrics; (M) Measured by us.',\n",
              "      'md': 'Table 5: Quantitative results on three multi-modal agentic benchmarks: VisualWebBench [43], MM-Mind2Web [86] and GroundUI-1K [87]. Reference numbers are taken from the corresponding benchmark papers [43, 86, 87] and leaderboard [3]. Remarks: (A) uses in-context learning (ICL) (please note that Amazon Nova models do not need to rely on in-context examples); (B) Gemini 1.5 Pro (001); (C) GPT-4o (May); (D) Macro average over individual metrics; (M) Measured by us.',\n",
              "      'bBox': {'x': 71, 'y': 237.04, 'w': 469, 'h': 243}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2.3  Long context',\n",
              "      'md': '# 2.3  Long context',\n",
              "      'bBox': {'x': 72, 'y': 517.04, 'w': 78, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We evaluate Amazon Nova Pro, Lite, and Micro on tasks that require the models to understand and reason over long context. These skills are crucial for tasks such as long multi-turn conversations, reasoning over long lists of retrieved documents, or understanding long videos. Amazon Nova Micro, Lite, and Pro models support context lengths of 128k, 300k, and 300k tokens, respectively. We used the following benchmarks to evaluate our models’ long context performance:\\n\\n•    Text Needle-in-a-Haystack (NIAH): Following [40], we assessed each model’s ability to locate specific information (the “needle”) within extensive contexts (the “haystack”). This “needle-in-a-haystack” test evaluates the model’s performance on context lengths starting at 32k, allowing us to measure its ability to accurately retrieve information across varying lengths of input context.\\n\\n• SQuALITY [76] (ZeroScrolls Benchmark [69]): Focused on query-based summarization of literary stories, this task evaluates the model’s capacity to generate relevant summaries from large contexts.\\n\\n•    LVBench [77]: This multimodal benchmark includes questions about YouTube videos from various domains such as TV series, sports, broadcasts, and surveillance footage. The LVBench dataset consists of 99 videos and 1,549 questions, covering six different types of tasks such as reasoning, event understanding and summarization.\\n\\n4https://huggingface.co/datasets/AIWinter/LVBench',\n",
              "      'md': 'We evaluate Amazon Nova Pro, Lite, and Micro on tasks that require the models to understand and reason over long context. These skills are crucial for tasks such as long multi-turn conversations, reasoning over long lists of retrieved documents, or understanding long videos. Amazon Nova Micro, Lite, and Pro models support context lengths of 128k, 300k, and 300k tokens, respectively. We used the following benchmarks to evaluate our models’ long context performance:\\n\\n•    Text Needle-in-a-Haystack (NIAH): Following [40], we assessed each model’s ability to locate specific information (the “needle”) within extensive contexts (the “haystack”). This “needle-in-a-haystack” test evaluates the model’s performance on context lengths starting at 32k, allowing us to measure its ability to accurately retrieve information across varying lengths of input context.\\n\\n• SQuALITY [76] (ZeroScrolls Benchmark [69]): Focused on query-based summarization of literary stories, this task evaluates the model’s capacity to generate relevant summaries from large contexts.\\n\\n•    LVBench [77]: This multimodal benchmark includes questions about YouTube videos from various domains such as TV series, sports, broadcasts, and surveillance footage. The LVBench dataset consists of 99 videos and 1,549 questions, covering six different types of tasks such as reasoning, event understanding and summarization.\\n\\n4https://huggingface.co/datasets/AIWinter/LVBench',\n",
              "      'bBox': {'x': 71, 'y': 72.04, 'w': 471, 'h': 644.98}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://huggingface.co/datasets/AIWinter/LVBench',\n",
              "      'text': 'https://huggingface.co/datasets/AIWinter/LVBench'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 11,\n",
              "    'text': '                                       The Amazon Nova Family of Models\\n                      Nova Micro               Nova Lite                       Nova Pro\\n                   10                  10                            10\\n                   20                  20                            20                          100\\n                   30                  30                            30\\n                2  40                  40                            40                          75\\n                [  50                  50                            50                          50\\n                1  60                  60                            60\\n                   70                  70                            70\\n                   80                  80                            80                          25\\n                   90                  90                            90\\n                  100                 100                           100\\n                      3   3  8            3  3 4 % 3       8             3  3 8    8   3  8\\n                     Context Length           Context Length                 Context Length\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and\\nNova Pro (up-to 300k) models.\\n                                                         SQuALITY              LVBench\\n                                                         ROUGE-L               accuracy\\n                         Nova Pro                        19.8 ±8.7            41.6 ±2.5\\n                         Nova Lite                       19.2 ±8.6            40.4 ±2.4\\n                         Nova Micro                      18.8 ±8.6                -\\n                         Claude 3.5 Sonnet (Jun)         13.4 ±7.5                -\\n                         Gemini 1.5 Pro (001)                -                33.1 ±2.3\\n                         Gemini 1.5 Pro (002)            19.1 ±8.6 M              -\\n                         Gemini 1.5 Flash (002)          18.1 ±8.4 M              -\\n                         GPT-4o                          18.8 ±8.6            30.8 ±2.3\\n                         Llama 3 - 70B                   16.4 ±8.1                -\\n                         Llama 3 - 8B                    15.3 ±7.9                -\\nTable 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For\\nSQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3\\nreport [45]. Gemini results were measured by us2                       (M). For LVBench, Gemini and GPT-4o numbers were taken from the\\ncorresponding benchmark leaderboard [77].\\nResults for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering\\ntask, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other\\nmodels. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any\\ndepth across both text and multimodal understanding use cases, delivering high accuracy and reliability.\\n2.4   Functional expertise\\nIn addition to core capabilities, foundation models must perform well in particular specialties and domains. Across\\nour many areas of performance analyses, we have selected four domains for which to present benchmarking results:\\nSoftware engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can\\nbe found in Appendix B.3.\\n                                                         11',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n|Nova Micro|Nova Lite|Nova Pro|\\n|---|---|---|\\n|10|10|10|\\n|20|20|20|\\n|30|30|30|\\n|40|40|40|\\n|50|50|50|\\n|60|60|60|\\n|70|70|70|\\n|80|80|80|\\n|90|90|90|\\n|100|100|100|\\n\\nContext Length\\n\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.\\n\\n| |SQuALITY|LVBench|\\n|---|---|---|\\n|Nova Pro|19.8 ±8.7|41.6 ±2.5|\\n|Nova Lite|19.2 ±8.6|40.4 ±2.4|\\n|Nova Micro|18.8 ±8.6|-|\\n|Claude 3.5 Sonnet (Jun)|13.4 ±7.5|-|\\n|Gemini 1.5 Pro (001)|-|33.1 ±2.3|\\n|Gemini 1.5 Pro (002)|19.1 ±8.6 M|-|\\n|Gemini 1.5 Flash (002)|18.1 ±8.4 M|-|\\n|GPT-4o|18.8 ±8.6|30.8 ±2.3|\\n|Llama 3 - 70B|16.4 ±8.1|-|\\n|Llama 3 - 8B|15.3 ±7.9|-|\\n\\nTable 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For SQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3 report [45]. Gemini results were measured by us2 (M). For LVBench, Gemini and GPT-4o numbers were taken from the corresponding benchmark leaderboard [77].\\n\\nResults for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering task, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other models. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any depth across both text and multimodal understanding use cases, delivering high accuracy and reliability.\\n\\n# 2.4 Functional expertise\\n\\nIn addition to core capabilities, foundation models must perform well in particular specialties and domains. Across our many areas of performance analyses, we have selected four domains for which to present benchmarking results: Software engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can be found in Appendix B.3.',\n",
              "    'images': [{'name': 'img_p10_1.png',\n",
              "      'height': 1028,\n",
              "      'width': 2528,\n",
              "      'x': 72,\n",
              "      'y': 72.00149999999994,\n",
              "      'original_width': 2528,\n",
              "      'original_height': 1028,\n",
              "      'ocr': [{'x': 475,\n",
              "        'y': 37,\n",
              "        'w': 208,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999322100776649',\n",
              "        'text': 'Nova Micro'},\n",
              "       {'x': 1023,\n",
              "        'y': 37,\n",
              "        'w': 176,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999627985879241',\n",
              "        'text': 'Nova Lite'},\n",
              "       {'x': 1695,\n",
              "        'y': 39,\n",
              "        'w': 168,\n",
              "        'h': 38,\n",
              "        'confidence': '0.9999550178555574',\n",
              "        'text': 'Nova Pro'},\n",
              "       {'x': 407,\n",
              "        'y': 103,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999944366205292',\n",
              "        'text': '10'},\n",
              "       {'x': 829,\n",
              "        'y': 103,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999944366205292',\n",
              "        'text': '10'},\n",
              "       {'x': 1495,\n",
              "        'y': 103,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999944366205292',\n",
              "        'text': '10'},\n",
              "       {'x': 2096,\n",
              "        'y': 148,\n",
              "        'w': 56,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9996983438622489',\n",
              "        'text': '100'},\n",
              "       {'x': 405,\n",
              "        'y': 177,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999942680334141',\n",
              "        'text': '20'},\n",
              "       {'x': 827,\n",
              "        'y': 177,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999942680334141',\n",
              "        'text': '20'},\n",
              "       {'x': 1493,\n",
              "        'y': 177,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999942680334141',\n",
              "        'text': '20'},\n",
              "       {'x': 405,\n",
              "        'y': 251,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.5168530382282581',\n",
              "        'text': '30'},\n",
              "       {'x': 827,\n",
              "        'y': 251,\n",
              "        'w': 46,\n",
              "        'h': 36,\n",
              "        'confidence': '0.5394472539629145',\n",
              "        'text': '30'},\n",
              "       {'x': 1493,\n",
              "        'y': 251,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.5168530382282581',\n",
              "        'text': '30'},\n",
              "       {'x': 2094,\n",
              "        'y': 296,\n",
              "        'w': 38,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999962910793456',\n",
              "        'text': '75'},\n",
              "       {'x': 341,\n",
              "        'y': 315,\n",
              "        'w': 36,\n",
              "        'h': 52,\n",
              "        'confidence': '0.3187985916726035',\n",
              "        'text': '2'},\n",
              "       {'x': 408,\n",
              "        'y': 326,\n",
              "        'w': 42,\n",
              "        'h': 32,\n",
              "        'confidence': '0.6243105911491494',\n",
              "        'text': '40'},\n",
              "       {'x': 830,\n",
              "        'y': 326,\n",
              "        'w': 42,\n",
              "        'h': 32,\n",
              "        'confidence': '0.6243105911491494',\n",
              "        'text': '40'},\n",
              "       {'x': 1496,\n",
              "        'y': 326,\n",
              "        'w': 42,\n",
              "        'h': 32,\n",
              "        'confidence': '0.6243105911491494',\n",
              "        'text': '40'},\n",
              "       {'x': 405,\n",
              "        'y': 399,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9997859850114889',\n",
              "        'text': '50'},\n",
              "       {'x': 827,\n",
              "        'y': 399,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9997859850114889',\n",
              "        'text': '50'},\n",
              "       {'x': 1493,\n",
              "        'y': 399,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9997859850114889',\n",
              "        'text': '50'},\n",
              "       {'x': 341,\n",
              "        'y': 369,\n",
              "        'w': 36,\n",
              "        'h': 126,\n",
              "        'confidence': '0.2668490313661245',\n",
              "        'text': '['},\n",
              "       {'x': 2094,\n",
              "        'y': 444,\n",
              "        'w': 40,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9998446502509013',\n",
              "        'text': '50'},\n",
              "       {'x': 406,\n",
              "        'y': 474,\n",
              "        'w': 42,\n",
              "        'h': 32,\n",
              "        'confidence': '0.9999579377042895',\n",
              "        'text': '60'},\n",
              "       {'x': 827,\n",
              "        'y': 473,\n",
              "        'w': 46,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999658612233858',\n",
              "        'text': '60'},\n",
              "       {'x': 1493,\n",
              "        'y': 473,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999684743028646',\n",
              "        'text': '60'},\n",
              "       {'x': 341,\n",
              "        'y': 497,\n",
              "        'w': 36,\n",
              "        'h': 116,\n",
              "        'confidence': '0.8209942657591149',\n",
              "        'text': '1'},\n",
              "       {'x': 408,\n",
              "        'y': 548,\n",
              "        'w': 40,\n",
              "        'h': 32,\n",
              "        'confidence': '0.9999754706222831',\n",
              "        'text': '70'},\n",
              "       {'x': 827,\n",
              "        'y': 547,\n",
              "        'w': 44,\n",
              "        'h': 36,\n",
              "        'confidence': '0.999979432399878',\n",
              "        'text': '70'},\n",
              "       {'x': 1495,\n",
              "        'y': 547,\n",
              "        'w': 42,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999858386882693',\n",
              "        'text': '70'},\n",
              "       {'x': 2094,\n",
              "        'y': 592,\n",
              "        'w': 40,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999998314126102',\n",
              "        'text': '25'},\n",
              "       {'x': 405,\n",
              "        'y': 621,\n",
              "        'w': 46,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999895475975691',\n",
              "        'text': '80'},\n",
              "       {'x': 827,\n",
              "        'y': 621,\n",
              "        'w': 46,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999895475975691',\n",
              "        'text': '80'},\n",
              "       {'x': 1493,\n",
              "        'y': 621,\n",
              "        'w': 46,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999895475975691',\n",
              "        'text': '80'},\n",
              "       {'x': 405,\n",
              "        'y': 695,\n",
              "        'w': 46,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9998102601606897',\n",
              "        'text': '90'},\n",
              "       {'x': 827,\n",
              "        'y': 695,\n",
              "        'w': 46,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9998102601606897',\n",
              "        'text': '90'},\n",
              "       {'x': 1493,\n",
              "        'y': 695,\n",
              "        'w': 46,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9998102601606897',\n",
              "        'text': '90'},\n",
              "       {'x': 387,\n",
              "        'y': 769,\n",
              "        'w': 64,\n",
              "        'h': 36,\n",
              "        'confidence': '0.5027778744697571',\n",
              "        'text': '100'},\n",
              "       {'x': 809,\n",
              "        'y': 769,\n",
              "        'w': 64,\n",
              "        'h': 36,\n",
              "        'confidence': '0.5027778744697571',\n",
              "        'text': '100'},\n",
              "       {'x': 1475,\n",
              "        'y': 769,\n",
              "        'w': 64,\n",
              "        'h': 36,\n",
              "        'confidence': '0.5027778744697571',\n",
              "        'text': '100'},\n",
              "       {'x': 483,\n",
              "        'y': 847,\n",
              "        'w': 36,\n",
              "        'h': 62,\n",
              "        'confidence': '0.6133456089595803',\n",
              "        'text': '3'},\n",
              "       {'x': 557,\n",
              "        'y': 847,\n",
              "        'w': 36,\n",
              "        'h': 62,\n",
              "        'confidence': '0.8439624514381059',\n",
              "        'text': '3'},\n",
              "       {'x': 633,\n",
              "        'y': 845,\n",
              "        'w': 36,\n",
              "        'h': 82,\n",
              "        'confidence': '0.46584164892928825',\n",
              "        'text': '8'},\n",
              "       {'x': 905,\n",
              "        'y': 847,\n",
              "        'w': 36,\n",
              "        'h': 62,\n",
              "        'confidence': '0.6133456089595803',\n",
              "        'text': '3'},\n",
              "       {'x': 980,\n",
              "        'y': 848,\n",
              "        'w': 32,\n",
              "        'h': 60,\n",
              "        'confidence': '0.9520704241185393',\n",
              "        'text': '3'},\n",
              "       {'x': 1046,\n",
              "        'y': 838,\n",
              "        'w': 122,\n",
              "        'h': 96,\n",
              "        'confidence': '0.18577231466770172',\n",
              "        'text': '4 %'},\n",
              "       {'x': 1201,\n",
              "        'y': 847,\n",
              "        'w': 36,\n",
              "        'h': 82,\n",
              "        'confidence': '0.6826460477094685',\n",
              "        'text': '3'},\n",
              "       {'x': 1273,\n",
              "        'y': 847,\n",
              "        'w': 38,\n",
              "        'h': 84,\n",
              "        'confidence': '0.9727422895918068',\n",
              "        'text': '8'},\n",
              "       {'x': 1571,\n",
              "        'y': 847,\n",
              "        'w': 36,\n",
              "        'h': 62,\n",
              "        'confidence': '0.6133456089595803',\n",
              "        'text': '3'},\n",
              "       {'x': 1645,\n",
              "        'y': 847,\n",
              "        'w': 36,\n",
              "        'h': 62,\n",
              "        'confidence': '0.8439624514381059',\n",
              "        'text': '3'},\n",
              "       {'x': 1719,\n",
              "        'y': 845,\n",
              "        'w': 38,\n",
              "        'h': 82,\n",
              "        'confidence': '0.3407077789243864',\n",
              "        'text': '8'},\n",
              "       {'x': 1791,\n",
              "        'y': 845,\n",
              "        'w': 38,\n",
              "        'h': 84,\n",
              "        'confidence': '0.9719812684425619',\n",
              "        'text': '8'},\n",
              "       {'x': 1868,\n",
              "        'y': 848,\n",
              "        'w': 32,\n",
              "        'h': 82,\n",
              "        'confidence': '0.7704166704233018',\n",
              "        'text': '3'},\n",
              "       {'x': 1941,\n",
              "        'y': 847,\n",
              "        'w': 36,\n",
              "        'h': 84,\n",
              "        'confidence': '0.9845581018018947',\n",
              "        'text': '8'},\n",
              "       {'x': 457,\n",
              "        'y': 949,\n",
              "        'w': 242,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999922742657932',\n",
              "        'text': 'Context Length'},\n",
              "       {'x': 989,\n",
              "        'y': 949,\n",
              "        'w': 242,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999922742657932',\n",
              "        'text': 'Context Length'},\n",
              "       {'x': 1655,\n",
              "        'y': 949,\n",
              "        'w': 242,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999922742657932',\n",
              "        'text': 'Context Length'}]}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['Nova Micro', 'Nova Lite', 'Nova Pro'],\n",
              "       ['10', '10', '10'],\n",
              "       ['20', '20', '20'],\n",
              "       ['30', '30', '30'],\n",
              "       ['40', '40', '40'],\n",
              "       ['50', '50', '50'],\n",
              "       ['60', '60', '60'],\n",
              "       ['70', '70', '70'],\n",
              "       ['80', '80', '80'],\n",
              "       ['90', '90', '90'],\n",
              "       ['100', '100', '100']],\n",
              "      'md': '|Nova Micro|Nova Lite|Nova Pro|\\n|10|10|10|\\n|20|20|20|\\n|30|30|30|\\n|40|40|40|\\n|50|50|50|\\n|60|60|60|\\n|70|70|70|\\n|80|80|80|\\n|90|90|90|\\n|100|100|100|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"Nova Micro\",\"Nova Lite\",\"Nova Pro\"\\n\"10\",\"10\",\"10\"\\n\"20\",\"20\",\"20\"\\n\"30\",\"30\",\"30\"\\n\"40\",\"40\",\"40\"\\n\"50\",\"50\",\"50\"\\n\"60\",\"60\",\"60\"\\n\"70\",\"70\",\"70\"\\n\"80\",\"80\",\"80\"\\n\"90\",\"90\",\"90\"\\n\"100\",\"100\",\"100\"',\n",
              "      'bBox': {'x': 72, 'y': 79, 'w': 468, 'h': 528.04}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Context Length\\n\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.',\n",
              "      'md': 'Context Length\\n\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.',\n",
              "      'bBox': {'x': 72, 'y': 79, 'w': 468, 'h': 426.04}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       [],\n",
              "       [],\n",
              "       ['', 'SQuALITY', 'LVBench'],\n",
              "       ['---', '---', '---'],\n",
              "       ['Nova Pro', '19.8 ±8.7', '41.6 ±2.5'],\n",
              "       ['Nova Lite', '19.2 ±8.6', '40.4 ±2.4'],\n",
              "       ['Nova Micro', '18.8 ±8.6', '-'],\n",
              "       ['Claude 3.5 Sonnet (Jun)', '13.4 ±7.5', '-'],\n",
              "       ['Gemini 1.5 Pro (001)', '-', '33.1 ±2.3'],\n",
              "       ['Gemini 1.5 Pro (002)', '19.1 ±8.6 M', '-'],\n",
              "       ['Gemini 1.5 Flash (002)', '18.1 ±8.4 M', '-'],\n",
              "       ['GPT-4o', '18.8 ±8.6', '30.8 ±2.3'],\n",
              "       ['Llama 3 - 70B', '16.4 ±8.1', '-'],\n",
              "       ['Llama 3 - 8B', '15.3 ±7.9', '-']],\n",
              "      'md': 'Context Length\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.\\n\\n| |SQuALITY|LVBench|\\n|---|---|---|\\n|Nova Pro|19.8 ±8.7|41.6 ±2.5|\\n|Nova Lite|19.2 ±8.6|40.4 ±2.4|\\n|Nova Micro|18.8 ±8.6|-|\\n|Claude 3.5 Sonnet (Jun)|13.4 ±7.5|-|\\n|Gemini 1.5 Pro (001)|-|33.1 ±2.3|\\n|Gemini 1.5 Pro (002)|19.1 ±8.6 M|-|\\n|Gemini 1.5 Flash (002)|18.1 ±8.4 M|-|\\n|GPT-4o|18.8 ±8.6|30.8 ±2.3|\\n|Llama 3 - 70B|16.4 ±8.1|-|\\n|Llama 3 - 8B|15.3 ±7.9|-|',\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\\n\\n\"\",\"SQuALITY\",\"LVBench\"\\n\"---\",\"---\",\"---\"\\n\"Nova Pro\",\"19.8 ±8.7\",\"41.6 ±2.5\"\\n\"Nova Lite\",\"19.2 ±8.6\",\"40.4 ±2.4\"\\n\"Nova Micro\",\"18.8 ±8.6\",\"-\"\\n\"Claude 3.5 Sonnet (Jun)\",\"13.4 ±7.5\",\"-\"\\n\"Gemini 1.5 Pro (001)\",\"-\",\"33.1 ±2.3\"\\n\"Gemini 1.5 Pro (002)\",\"19.1 ±8.6 M\",\"-\"\\n\"Gemini 1.5 Flash (002)\",\"18.1 ±8.4 M\",\"-\"\\n\"GPT-4o\",\"18.8 ±8.6\",\"30.8 ±2.3\"\\n\"Llama 3 - 70B\",\"16.4 ±8.1\",\"-\"\\n\"Llama 3 - 8B\",\"15.3 ±7.9\",\"-\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For SQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3 report [45]. Gemini results were measured by us2 (M). For LVBench, Gemini and GPT-4o numbers were taken from the corresponding benchmark leaderboard [77].\\n\\nResults for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering task, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other models. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any depth across both text and multimodal understanding use cases, delivering high accuracy and reliability.',\n",
              "      'md': 'Table 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For SQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3 report [45]. Gemini results were measured by us2 (M). For LVBench, Gemini and GPT-4o numbers were taken from the corresponding benchmark leaderboard [77].\\n\\nResults for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering task, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other models. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any depth across both text and multimodal understanding use cases, delivering high accuracy and reliability.',\n",
              "      'bBox': {'x': 71, 'y': 79, 'w': 469, 'h': 550.04}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2.4 Functional expertise',\n",
              "      'md': '# 2.4 Functional expertise',\n",
              "      'bBox': {'x': 72, 'y': 130, 'w': 109, 'h': 534.04}},\n",
              "     {'type': 'text',\n",
              "      'value': 'In addition to core capabilities, foundation models must perform well in particular specialties and domains. Across our many areas of performance analyses, we have selected four domains for which to present benchmarking results: Software engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can be found in Appendix B.3.',\n",
              "      'md': 'In addition to core capabilities, foundation models must perform well in particular specialties and domains. Across our many areas of performance analyses, we have selected four domains for which to present benchmarking results: Software engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can be found in Appendix B.3.',\n",
              "      'bBox': {'x': 72, 'y': 229, 'w': 469, 'h': 491.04}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 12,\n",
              "    'text': '                                      The Amazon Nova Family of Models\\n                                                    Software           Finance              RAG\\n                                                   HumanEval            FinQA               CRAG\\n                                                     Python\\n                                          tok/       0-shot             0-shot           accuracy\\n                                          sec        pass@1           accuracy\\n            Nova Pro                      100      89.0 ±4.8          77.2 ±0.9          50.3 ±1.9\\n            Nova Lite                     157      85.4 ±5.4          73.6 ±0.9          43.8 ±1.9\\n            Nova Micro                    210      81.1 ±6.0          65.2 ±1.0          43.1 ±1.9\\n            Claude 3.5 Sonnet (Oct)        57      93.7 ±3.7          77.3 ±0.9 M        52.6 ±1.8 M\\n            Claude 3.5 Haiku               64      88.1 ±5.0          73.9 ±0.9 M        31.9 ±1.8 M\\n            Gemini 1.5 Pro (002)           58      87.8 ±5.0 M        74.4 ±0.9 M        48.9 ±1.9 M\\n            Gemini 1.5 Flash (002)        190      81.1 ±6.0 M        73.5 ±1.0 M        42.4 ±1.9 M\\n            Gemini 1.5 Flash 8B (001)     283      81.1 ±6.0 M        63.7 ±1.0 M        37.7 ±1.8 M\\n            GPT-4o                        163      90.2 ±4.6          71.1 ±1.0 M        52.0 ±1.9 M\\n            GPT-4o Mini                   113      87.2 ±5.1          70.6 ±1.0 M        49.9 ±1.9 M\\n            Llama 3.2 90B                  40      80.5 ±6.1          72.8 ±1.0 M        45.2 ±1.9 M\\n            Llama 3.2 11B                 124      72.6 ±6.8          60.8 ±1.1 M        42.2 ±1.9 M\\n            Llama 3.1 8B                  157      72.6 ±6.8          61.2 ±1.0 M        42.2 ±1.8 M\\nTable 7: Performance on select functional benchmarks, including software engineering benchmarks in Python with\\nHumanEval [19], financial reasoning with FinQA [20], and retrieval augmented generation with CRAG [82]. CRAG uses\\nour scoring method described in Section 2.4.3. Where available, reference numbers are taken from the corresponding\\nbenchmark papers and technical reports [13, 11, 32, 39, 45, 58]. Additional results were measured (M) by us2. Model\\nspeed in tokens per second (Tok/Sec) is reproduced from section 2.5.\\n2.4.1  Software engineering\\nWe assessed Amazon Nova’s code generation capabilities on the Python coding task HumanEval [19]. The bench-\\nmark contains 164 original programming problems with unit tests. These problems assess language comprehension,\\nalgorithms, and simple mathematics. Some problems are comparable to simple software interview questions. Table 7\\nprovides the performance of our Nova models and select public models.\\n2.4.2  Financial analysis\\nWe use FinQA [20] to evaluate Amazon Nova’s ability to understand financial data. FinQA is an expert-annotated\\ndataset comprising 8,281 financial question-answer pairs derived from the earnings reports of S&P 500 companies. It\\nevaluates a model’s ability to extract information from both tables and unstructured text, while accurately performing\\ncalculations using relevant financial knowledge. We report the average post-rounding accuracy under the 0-shot CoT\\nsetting. Table 7 provides the performance of Amazon Nova models and select public models on FinQA.\\n2.4.3  Retrieval augmented generation\\nWe evaluate RAG capabilities on the CRAG [82] benchmark using the Task 1 setup, which considers five pre-selected\\nHTML pages as external knowledge to each input question. We extract top-20 text snippets from these pages following\\nthe standard retrieval approach used in CRAG’s official repository, whereby pages are first cleaned using BeautifulSoup\\nto remove HTML tags, after which the text is then split into sentences or chunks no longer than 1000 characters.\\nThese are then encoded using the sentence-transformers/all-MiniLM-L6-v2 model, which is also used to encode the\\nquestion. The top 20 chunks with highest similarity are passed as context in the input for model inference. We report\\nthe percentage of correct responses as judged by an LLM (gpt-4-turbo-2024-04-09), which compares each model’s\\nanswer with the expected answer using the prompt shown in Appendix B.3.2. Table 7 provides the performance of\\nAmazon Nova models and selected public models on a combined validation and test set of 2,706 examples.\\n                                                       12',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n| | |Software|Finance|RAG| |\\n|---|---|---|---|---|---|\\n| | |HumanEval|FinQA|CRAG| |\\n|tok/ sec|0-shot pass@1| | |accuracy| |\\n|Nova Pro|100|89.0 ±4.8|77.2 ±0.9|50.3 ±1.9| |\\n|Nova Lite|157|85.4 ±5.4|73.6 ±0.9|43.8 ±1.9| |\\n|Nova Micro|210|81.1 ±6.0|65.2 ±1.0|43.1 ±1.9| |\\n|Claude 3.5 Sonnet (Oct)|57|93.7 ±3.7|77.3 ±0.9 M|52.6 ±1.8 M| |\\n|Claude 3.5 Haiku|64|88.1 ±5.0|73.9 ±0.9 M|31.9 ±1.8 M| |\\n|Gemini 1.5 Pro (002)|58|87.8 ±5.0 M|74.4 ±0.9 M|48.9 ±1.9 M| |\\n|Gemini 1.5 Flash (002)|190|81.1 ±6.0 M|73.5 ±1.0 M|42.4 ±1.9 M| |\\n|Gemini 1.5 Flash 8B (001)|283|81.1 ±6.0 M|63.7 ±1.0 M|37.7 ±1.8 M| |\\n|GPT-4o|163|90.2 ±4.6|71.1 ±1.0 M|52.0 ±1.9 M| |\\n|GPT-4o Mini|113|87.2 ±5.1|70.6 ±1.0 M|49.9 ±1.9 M| |\\n|Llama 3.2 90B|40|80.5 ±6.1|72.8 ±1.0 M|45.2 ±1.9 M| |\\n|Llama 3.2 11B|124|72.6 ±6.8|60.8 ±1.1 M|42.2 ±1.9 M| |\\n|Llama 3.1 8B|157|72.6 ±6.8|61.2 ±1.0 M|42.2 ±1.8 M| |\\n\\nTable 7: Performance on select functional benchmarks, including software engineering benchmarks in Python with HumanEval [19], financial reasoning with FinQA [20], and retrieval augmented generation with CRAG [82]. CRAG uses our scoring method described in Section 2.4.3. Where available, reference numbers are taken from the corresponding benchmark papers and technical reports [13, 11, 32, 39, 45, 58]. Additional results were measured (M) by us. Model speed in tokens per second (Tok/Sec) is reproduced from section 2.5.\\n\\n# 2.4.1 Software engineering\\n\\nWe assessed Amazon Nova’s code generation capabilities on the Python coding task HumanEval [19]. The benchmark contains 164 original programming problems with unit tests. These problems assess language comprehension, algorithms, and simple mathematics. Some problems are comparable to simple software interview questions. Table 7 provides the performance of our Nova models and select public models.\\n\\n# 2.4.2 Financial analysis\\n\\nWe use FinQA [20] to evaluate Amazon Nova’s ability to understand financial data. FinQA is an expert-annotated dataset comprising 8,281 financial question-answer pairs derived from the earnings reports of S&P 500 companies. It evaluates a model’s ability to extract information from both tables and unstructured text, while accurately performing calculations using relevant financial knowledge. We report the average post-rounding accuracy under the 0-shot CoT setting. Table 7 provides the performance of Amazon Nova models and select public models on FinQA.\\n\\n# 2.4.3 Retrieval augmented generation\\n\\nWe evaluate RAG capabilities on the CRAG [82] benchmark using the Task 1 setup, which considers five pre-selected HTML pages as external knowledge to each input question. We extract top-20 text snippets from these pages following the standard retrieval approach used in CRAG’s official repository, whereby pages are first cleaned using BeautifulSoup to remove HTML tags, after which the text is then split into sentences or chunks no longer than 1000 characters. These are then encoded using the sentence-transformers/all-MiniLM-L6-v2 model, which is also used to encode the question. The top 20 chunks with highest similarity are passed as context in the input for model inference. We report the percentage of correct responses as judged by an LLM (gpt-4-turbo-2024-04-09), which compares each model’s answer with the expected answer using the prompt shown in Appendix B.3.2. Table 7 provides the performance of Amazon Nova models and selected public models on a combined validation and test set of 2,706 examples.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['', '', 'Software', 'Finance', 'RAG', ''],\n",
              "       ['', '', 'HumanEval', 'FinQA', 'CRAG', ''],\n",
              "       ['tok/ sec', '0-shot pass@1', '', '', 'accuracy', ''],\n",
              "       ['Nova Pro', '100', '89.0 ±4.8', '77.2 ±0.9', '50.3 ±1.9', ''],\n",
              "       ['Nova Lite', '157', '85.4 ±5.4', '73.6 ±0.9', '43.8 ±1.9', ''],\n",
              "       ['Nova Micro', '210', '81.1 ±6.0', '65.2 ±1.0', '43.1 ±1.9', ''],\n",
              "       ['Claude 3.5 Sonnet (Oct)',\n",
              "        '57',\n",
              "        '93.7 ±3.7',\n",
              "        '77.3 ±0.9 M',\n",
              "        '52.6 ±1.8 M',\n",
              "        ''],\n",
              "       ['Claude 3.5 Haiku',\n",
              "        '64',\n",
              "        '88.1 ±5.0',\n",
              "        '73.9 ±0.9 M',\n",
              "        '31.9 ±1.8 M',\n",
              "        ''],\n",
              "       ['Gemini 1.5 Pro (002)',\n",
              "        '58',\n",
              "        '87.8 ±5.0 M',\n",
              "        '74.4 ±0.9 M',\n",
              "        '48.9 ±1.9 M',\n",
              "        ''],\n",
              "       ['Gemini 1.5 Flash (002)',\n",
              "        '190',\n",
              "        '81.1 ±6.0 M',\n",
              "        '73.5 ±1.0 M',\n",
              "        '42.4 ±1.9 M',\n",
              "        ''],\n",
              "       ['Gemini 1.5 Flash 8B (001)',\n",
              "        '283',\n",
              "        '81.1 ±6.0 M',\n",
              "        '63.7 ±1.0 M',\n",
              "        '37.7 ±1.8 M',\n",
              "        ''],\n",
              "       ['GPT-4o', '163', '90.2 ±4.6', '71.1 ±1.0 M', '52.0 ±1.9 M', ''],\n",
              "       ['GPT-4o Mini', '113', '87.2 ±5.1', '70.6 ±1.0 M', '49.9 ±1.9 M', ''],\n",
              "       ['Llama 3.2 90B', '40', '80.5 ±6.1', '72.8 ±1.0 M', '45.2 ±1.9 M', ''],\n",
              "       ['Llama 3.2 11B', '124', '72.6 ±6.8', '60.8 ±1.1 M', '42.2 ±1.9 M', ''],\n",
              "       ['Llama 3.1 8B', '157', '72.6 ±6.8', '61.2 ±1.0 M', '42.2 ±1.8 M', '']],\n",
              "      'md': '| | |Software|Finance|RAG| |\\n| | |HumanEval|FinQA|CRAG| |\\n|tok/ sec|0-shot pass@1| | |accuracy| |\\n|Nova Pro|100|89.0 ±4.8|77.2 ±0.9|50.3 ±1.9| |\\n|Nova Lite|157|85.4 ±5.4|73.6 ±0.9|43.8 ±1.9| |\\n|Nova Micro|210|81.1 ±6.0|65.2 ±1.0|43.1 ±1.9| |\\n|Claude 3.5 Sonnet (Oct)|57|93.7 ±3.7|77.3 ±0.9 M|52.6 ±1.8 M| |\\n|Claude 3.5 Haiku|64|88.1 ±5.0|73.9 ±0.9 M|31.9 ±1.8 M| |\\n|Gemini 1.5 Pro (002)|58|87.8 ±5.0 M|74.4 ±0.9 M|48.9 ±1.9 M| |\\n|Gemini 1.5 Flash (002)|190|81.1 ±6.0 M|73.5 ±1.0 M|42.4 ±1.9 M| |\\n|Gemini 1.5 Flash 8B (001)|283|81.1 ±6.0 M|63.7 ±1.0 M|37.7 ±1.8 M| |\\n|GPT-4o|163|90.2 ±4.6|71.1 ±1.0 M|52.0 ±1.9 M| |\\n|GPT-4o Mini|113|87.2 ±5.1|70.6 ±1.0 M|49.9 ±1.9 M| |\\n|Llama 3.2 90B|40|80.5 ±6.1|72.8 ±1.0 M|45.2 ±1.9 M| |\\n|Llama 3.2 11B|124|72.6 ±6.8|60.8 ±1.1 M|42.2 ±1.9 M| |\\n|Llama 3.1 8B|157|72.6 ±6.8|61.2 ±1.0 M|42.2 ±1.8 M| |',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"\",\"\",\"Software\",\"Finance\",\"RAG\",\"\"\\n\"\",\"\",\"HumanEval\",\"FinQA\",\"CRAG\",\"\"\\n\"tok/ sec\",\"0-shot pass@1\",\"\",\"\",\"accuracy\",\"\"\\n\"Nova Pro\",\"100\",\"89.0 ±4.8\",\"77.2 ±0.9\",\"50.3 ±1.9\",\"\"\\n\"Nova Lite\",\"157\",\"85.4 ±5.4\",\"73.6 ±0.9\",\"43.8 ±1.9\",\"\"\\n\"Nova Micro\",\"210\",\"81.1 ±6.0\",\"65.2 ±1.0\",\"43.1 ±1.9\",\"\"\\n\"Claude 3.5 Sonnet (Oct)\",\"57\",\"93.7 ±3.7\",\"77.3 ±0.9 M\",\"52.6 ±1.8 M\",\"\"\\n\"Claude 3.5 Haiku\",\"64\",\"88.1 ±5.0\",\"73.9 ±0.9 M\",\"31.9 ±1.8 M\",\"\"\\n\"Gemini 1.5 Pro (002)\",\"58\",\"87.8 ±5.0 M\",\"74.4 ±0.9 M\",\"48.9 ±1.9 M\",\"\"\\n\"Gemini 1.5 Flash (002)\",\"190\",\"81.1 ±6.0 M\",\"73.5 ±1.0 M\",\"42.4 ±1.9 M\",\"\"\\n\"Gemini 1.5 Flash 8B (001)\",\"283\",\"81.1 ±6.0 M\",\"63.7 ±1.0 M\",\"37.7 ±1.8 M\",\"\"\\n\"GPT-4o\",\"163\",\"90.2 ±4.6\",\"71.1 ±1.0 M\",\"52.0 ±1.9 M\",\"\"\\n\"GPT-4o Mini\",\"113\",\"87.2 ±5.1\",\"70.6 ±1.0 M\",\"49.9 ±1.9 M\",\"\"\\n\"Llama 3.2 90B\",\"40\",\"80.5 ±6.1\",\"72.8 ±1.0 M\",\"45.2 ±1.9 M\",\"\"\\n\"Llama 3.2 11B\",\"124\",\"72.6 ±6.8\",\"60.8 ±1.1 M\",\"42.2 ±1.9 M\",\"\"\\n\"Llama 3.1 8B\",\"157\",\"72.6 ±6.8\",\"61.2 ±1.0 M\",\"42.2 ±1.8 M\",\"\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 7: Performance on select functional benchmarks, including software engineering benchmarks in Python with HumanEval [19], financial reasoning with FinQA [20], and retrieval augmented generation with CRAG [82]. CRAG uses our scoring method described in Section 2.4.3. Where available, reference numbers are taken from the corresponding benchmark papers and technical reports [13, 11, 32, 39, 45, 58]. Additional results were measured (M) by us. Model speed in tokens per second (Tok/Sec) is reproduced from section 2.5.',\n",
              "      'md': 'Table 7: Performance on select functional benchmarks, including software engineering benchmarks in Python with HumanEval [19], financial reasoning with FinQA [20], and retrieval augmented generation with CRAG [82]. CRAG uses our scoring method described in Section 2.4.3. Where available, reference numbers are taken from the corresponding benchmark papers and technical reports [13, 11, 32, 39, 45, 58]. Additional results were measured (M) by us. Model speed in tokens per second (Tok/Sec) is reproduced from section 2.5.',\n",
              "      'bBox': {'x': 71, 'y': 73.04, 'w': 469, 'h': 539}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2.4.1 Software engineering',\n",
              "      'md': '# 2.4.1 Software engineering',\n",
              "      'bBox': {'x': 72, 'y': 73.04, 'w': 249, 'h': 365}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We assessed Amazon Nova’s code generation capabilities on the Python coding task HumanEval [19]. The benchmark contains 164 original programming problems with unit tests. These problems assess language comprehension, algorithms, and simple mathematics. Some problems are comparable to simple software interview questions. Table 7 provides the performance of our Nova models and select public models.',\n",
              "      'md': 'We assessed Amazon Nova’s code generation capabilities on the Python coding task HumanEval [19]. The benchmark contains 164 original programming problems with unit tests. These problems assess language comprehension, algorithms, and simple mathematics. Some problems are comparable to simple software interview questions. Table 7 provides the performance of our Nova models and select public models.',\n",
              "      'bBox': {'x': 72, 'y': 73.04, 'w': 469, 'h': 418}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2.4.2 Financial analysis',\n",
              "      'md': '# 2.4.2 Financial analysis',\n",
              "      'bBox': {'x': 72, 'y': 510.04, 'w': 105, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We use FinQA [20] to evaluate Amazon Nova’s ability to understand financial data. FinQA is an expert-annotated dataset comprising 8,281 financial question-answer pairs derived from the earnings reports of S&P 500 companies. It evaluates a model’s ability to extract information from both tables and unstructured text, while accurately performing calculations using relevant financial knowledge. We report the average post-rounding accuracy under the 0-shot CoT setting. Table 7 provides the performance of Amazon Nova models and select public models on FinQA.',\n",
              "      'md': 'We use FinQA [20] to evaluate Amazon Nova’s ability to understand financial data. FinQA is an expert-annotated dataset comprising 8,281 financial question-answer pairs derived from the earnings reports of S&P 500 companies. It evaluates a model’s ability to extract information from both tables and unstructured text, while accurately performing calculations using relevant financial knowledge. We report the average post-rounding accuracy under the 0-shot CoT setting. Table 7 provides the performance of Amazon Nova models and select public models on FinQA.',\n",
              "      'bBox': {'x': 71, 'y': 73.04, 'w': 469, 'h': 511}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2.4.3 Retrieval augmented generation',\n",
              "      'md': '# 2.4.3 Retrieval augmented generation',\n",
              "      'bBox': {'x': 72, 'y': 602.04, 'w': 165, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We evaluate RAG capabilities on the CRAG [82] benchmark using the Task 1 setup, which considers five pre-selected HTML pages as external knowledge to each input question. We extract top-20 text snippets from these pages following the standard retrieval approach used in CRAG’s official repository, whereby pages are first cleaned using BeautifulSoup to remove HTML tags, after which the text is then split into sentences or chunks no longer than 1000 characters. These are then encoded using the sentence-transformers/all-MiniLM-L6-v2 model, which is also used to encode the question. The top 20 chunks with highest similarity are passed as context in the input for model inference. We report the percentage of correct responses as judged by an LLM (gpt-4-turbo-2024-04-09), which compares each model’s answer with the expected answer using the prompt shown in Appendix B.3.2. Table 7 provides the performance of Amazon Nova models and selected public models on a combined validation and test set of 2,706 examples.',\n",
              "      'md': 'We evaluate RAG capabilities on the CRAG [82] benchmark using the Task 1 setup, which considers five pre-selected HTML pages as external knowledge to each input question. We extract top-20 text snippets from these pages following the standard retrieval approach used in CRAG’s official repository, whereby pages are first cleaned using BeautifulSoup to remove HTML tags, after which the text is then split into sentences or chunks no longer than 1000 characters. These are then encoded using the sentence-transformers/all-MiniLM-L6-v2 model, which is also used to encode the question. The top 20 chunks with highest similarity are passed as context in the input for model inference. We report the percentage of correct responses as judged by an LLM (gpt-4-turbo-2024-04-09), which compares each model’s answer with the expected answer using the prompt shown in Appendix B.3.2. Table 7 provides the performance of Amazon Nova models and selected public models on a combined validation and test set of 2,706 examples.',\n",
              "      'bBox': {'x': 71, 'y': 73.04, 'w': 471, 'h': 647}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 13,\n",
              "    'text': '                                      The Amazon Nova Family of Models\\n2.5  Runtime performance\\nWe evaluate the runtime performance of Amazon Nova models using three metrics: Time to First Token (TTFT), Output\\nTokens per Second (OTPS) and Total Response Time. TTFT is measured as the time, in seconds, it takes to receive the\\nfirst token from the model after an API request is sent. OTPS is measured as the number of tokens generated per second\\n(tok/sec). It is the rate at which a model produces subsequent output tokens after the first token, reflecting overall\\nthroughput and efficiency during inference. Total Response Time measures the total duration in seconds from the\\nsubmission of the input prompt to the end of generation sequence for a given input-output prompt length. It represents\\nthe overall user experience for a model.\\nIn Figure 3, we show TTFT, OTPS, and Total Response Time using 1000 tokens of input and 100 tokens of output\\nfor Amazon Nova models and select public models as reported by Artificial Analysis5, an independent entity that\\nbenchmarks AI models and hosting providers. Amazon Nova Micro, Lite and Pro models are among the fastest models\\nin their respective intelligence tiers. Together, all three Amazon Nova models demonstrate state-of-the-art runtime\\nperformance, ensuring a smooth and responsive user experience in many real world use cases.\\n  5https://artificialanalysis.ai/methodology\\n                                                       13',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# 2.5 Runtime performance\\n\\nWe evaluate the runtime performance of Amazon Nova models using three metrics: Time to First Token (TTFT), Output Tokens per Second (OTPS) and Total Response Time. TTFT is measured as the time, in seconds, it takes to receive the first token from the model after an API request is sent. OTPS is measured as the number of tokens generated per second (tok/sec). It is the rate at which a model produces subsequent output tokens after the first token, reflecting overall throughput and efficiency during inference. Total Response Time measures the total duration in seconds from the submission of the input prompt to the end of generation sequence for a given input-output prompt length. It represents the overall user experience for a model.\\n\\nIn Figure 3, we show TTFT, OTPS, and Total Response Time using 1000 tokens of input and 100 tokens of output for Amazon Nova models and select public models as reported by Artificial Analysis5, an independent entity that benchmarks AI models and hosting providers. Amazon Nova Micro, Lite and Pro models are among the fastest models in their respective intelligence tiers. Together, all three Amazon Nova models demonstrate state-of-the-art runtime performance, ensuring a smooth and responsive user experience in many real world use cases.\\n\\n5',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '2.5 Runtime performance',\n",
              "      'md': '# 2.5 Runtime performance',\n",
              "      'bBox': {'x': 72, 'y': 72.04, 'w': 116, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We evaluate the runtime performance of Amazon Nova models using three metrics: Time to First Token (TTFT), Output Tokens per Second (OTPS) and Total Response Time. TTFT is measured as the time, in seconds, it takes to receive the first token from the model after an API request is sent. OTPS is measured as the number of tokens generated per second (tok/sec). It is the rate at which a model produces subsequent output tokens after the first token, reflecting overall throughput and efficiency during inference. Total Response Time measures the total duration in seconds from the submission of the input prompt to the end of generation sequence for a given input-output prompt length. It represents the overall user experience for a model.\\n\\nIn Figure 3, we show TTFT, OTPS, and Total Response Time using 1000 tokens of input and 100 tokens of output for Amazon Nova models and select public models as reported by Artificial Analysis5, an independent entity that benchmarks AI models and hosting providers. Amazon Nova Micro, Lite and Pro models are among the fastest models in their respective intelligence tiers. Together, all three Amazon Nova models demonstrate state-of-the-art runtime performance, ensuring a smooth and responsive user experience in many real world use cases.\\n\\n5',\n",
              "      'md': 'We evaluate the runtime performance of Amazon Nova models using three metrics: Time to First Token (TTFT), Output Tokens per Second (OTPS) and Total Response Time. TTFT is measured as the time, in seconds, it takes to receive the first token from the model after an API request is sent. OTPS is measured as the number of tokens generated per second (tok/sec). It is the rate at which a model produces subsequent output tokens after the first token, reflecting overall throughput and efficiency during inference. Total Response Time measures the total duration in seconds from the submission of the input prompt to the end of generation sequence for a given input-output prompt length. It represents the overall user experience for a model.\\n\\nIn Figure 3, we show TTFT, OTPS, and Total Response Time using 1000 tokens of input and 100 tokens of output for Amazon Nova models and select public models as reported by Artificial Analysis5, an independent entity that benchmarks AI models and hosting providers. Amazon Nova Micro, Lite and Pro models are among the fastest models in their respective intelligence tiers. Together, all three Amazon Nova models demonstrate state-of-the-art runtime performance, ensuring a smooth and responsive user experience in many real world use cases.\\n\\n5',\n",
              "      'bBox': {'x': 71, 'y': 72.04, 'w': 469, 'h': 156}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://artificialanalysis.ai/methodology',\n",
              "      'text': 'https://artificialanalysis.ai/methodology'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 14,\n",
              "    'text': \"                                       The Amazon Nova Family of Models\\n                                                         Model Family\\n                                Meta      Amazon      Google     Mistral Al  OpenAI     Anthropic\\n        1.0                                                                                               0.98\\n       80.8                                                                             0.72  0.72  0.87\\n       80.6                                                            0.46  0.53  0.62\\n       20.4  0.29  0.32  0.35 0.35  0.36  0.36  0.37  0.38 0.42  0.42\\n       2\\n       20.2\\n         0.0 1~  9  1 1  8      2   1 8  1ᴬ⁸    1  9  1 2 1⁵ 2     8  1~ 8  Tv 2   81   A 8   3  2  8 2   2 2\\n                                                                                       0'\\n                     J                                                                 !    3    3     A\\n                            K                                            {                             J\\n                           J                                                                    8\\n             283\\n       8²⁵⁰        210   190\\n        200                    163  157   157\\n      8 150                                     124   115   113   100\\n      2 100                                                             73    64   58    57    40    35\\n       3 50          1                9         9                  2     2           {    2     8    2      29\\n             2 8  {     2  2    8           1⁵⁸ 3    1 2   8 1   1    ~     3  1  2     8     ~           3 8\\n          J                                    1                      1    8    N?\\n                     J                                                         J     0'          {\\n                                                                                    8\\n        4.0                                                                                               4.0\\n       83.5                                                                              2.8   2.9   3.4\\n       23.0                                                                  2.4   2.7\\n       L2.5\\n        2.0                                           1.3   1.4   1.5   1.7\\n        1.5        0.8   0.9   1.0   1.0        1,.2\\n      2 1.0  0.7\\n        0.5\\n        0.0   8   {  0  2 2   1 g   58    ~ 9    8   1 2   1 2   8 1   K 2  3  2  8  2  8 {   3 8     2   3 8\\n          N?                                                               8          K\\n          J          N                                                          3    J           {\\n                     J                                                        8\\nFigure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of\\ninput and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov\\n29th, 2024).\\n                                                         14\",\n",
              "    'md': \"# The Amazon Nova Family of Models\\n\\n# Model Family\\n\\n| | |Meta|Amazon|Google|Mistral AI|OpenAI|Anthropic| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|1.0| | | |0.98| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|80.8| | |0.72|0.72|0.87| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|80.6| | |0.46|0.53|0.62| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|20.4|0.29|0.32|0.35|0.35|0.36|0.36|0.37|0.38|0.42|0.42| | | | | | | | | | | | | | | | | | | | | | | | | |\\n| | |2|20.2|0.0|1~|9|1|1|8|2|1|8|1ᴬ⁸|1|9|1|2|1⁵|2|8|1~|8|Tv|2|81|A|8|3|2|8|2|2|2| | |\\n| |0'| |J| |!|3|3|A| | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |K| |{| |J| |8| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|283|8²⁵⁰|210|190|200|163|157|157| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|8|150| |124|115|113|100| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|2|100| |73|64|58|57|40|35| | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|3|50|1|9|9|2|2|{|2|8|2|29| | | | | | | | | | | | | | | | | | | | | | | | |\\n| |2|8|{|2|2|8|1⁵⁸|3|1|2|8|1|1|~|3|1|2|8|~|3|8| | | | | | | | | | | | | | |\\n| |J|1| |1|8|N?| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |J| | |0'|{| |8| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|4.0| | | |4.0| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|83.5| | |2.8|2.9|3.4| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|23.0| |2.4|2.7| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|L2.5| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|2.0| |1.3|1.4|1.5|1.7| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|1.5|0.8|0.9|1.0|1.0|1,.2| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |2|1.0|0.7|0.5|0.0|8|{|0|2|2|1|g|58|~|9|8|1|2|1|2|8|1|K|2|3|2|8|2|8|{|3|8|2|3|8|\\n| |N?| |8|K| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |J| |N| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n\\nFigure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of input and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov 29th, 2024).\\n\\n14\",\n",
              "    'images': [{'name': 'img_p13_1.png',\n",
              "      'height': 10505,\n",
              "      'width': 8503,\n",
              "      'x': 95.401,\n",
              "      'y': 115.97222899999997,\n",
              "      'original_width': 8503,\n",
              "      'original_height': 10505,\n",
              "      'ocr': [{'x': 4088,\n",
              "        'y': 43,\n",
              "        'w': 774,\n",
              "        'h': 181,\n",
              "        'confidence': '0.9839083488780975',\n",
              "        'text': 'Model Family'},\n",
              "       {'x': 2099,\n",
              "        'y': 236,\n",
              "        'w': 323,\n",
              "        'h': 126,\n",
              "        'confidence': '0.9999979138374329',\n",
              "        'text': 'Meta'},\n",
              "       {'x': 2928,\n",
              "        'y': 236,\n",
              "        'w': 438,\n",
              "        'h': 126,\n",
              "        'confidence': '0.9999618935846967',\n",
              "        'text': 'Amazon'},\n",
              "       {'x': 3861,\n",
              "        'y': 234,\n",
              "        'w': 394,\n",
              "        'h': 147,\n",
              "        'confidence': '0.8736473632916529',\n",
              "        'text': 'Google'},\n",
              "       {'x': 4766,\n",
              "        'y': 236,\n",
              "        'w': 471,\n",
              "        'h': 126,\n",
              "        'confidence': '0.991574360549493',\n",
              "        'text': 'Mistral Al'},\n",
              "       {'x': 5749,\n",
              "        'y': 234,\n",
              "        'w': 410,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9944369465507542',\n",
              "        'text': 'OpenAI'},\n",
              "       {'x': 6660,\n",
              "        'y': 234,\n",
              "        'w': 508,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999943574190016',\n",
              "        'text': 'Anthropic'},\n",
              "       {'x': 226,\n",
              "        'y': 447,\n",
              "        'w': 204,\n",
              "        'h': 147,\n",
              "        'confidence': '0.8053494691848755',\n",
              "        'text': '1.0'},\n",
              "       {'x': 8070,\n",
              "        'y': 397,\n",
              "        'w': 314,\n",
              "        'h': 157,\n",
              "        'confidence': '0.999979555606842',\n",
              "        'text': '0.98'},\n",
              "       {'x': 7603,\n",
              "        'y': 602,\n",
              "        'w': 313,\n",
              "        'h': 157,\n",
              "        'confidence': '0.8997139530749945',\n",
              "        'text': '0.87'},\n",
              "       {'x': 71,\n",
              "        'y': 661,\n",
              "        'w': 136,\n",
              "        'h': 302,\n",
              "        'confidence': '0.688268856047646',\n",
              "        'text': '8'},\n",
              "       {'x': 210,\n",
              "        'y': 817,\n",
              "        'w': 219,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9999953198664192',\n",
              "        'text': '0.8'},\n",
              "       {'x': 6660,\n",
              "        'y': 882,\n",
              "        'w': 311,\n",
              "        'h': 147,\n",
              "        'confidence': '0.999975860118866',\n",
              "        'text': '0.72'},\n",
              "       {'x': 7136,\n",
              "        'y': 882,\n",
              "        'w': 311,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9981343976945429',\n",
              "        'text': '0.72'},\n",
              "       {'x': 53,\n",
              "        'y': 997,\n",
              "        'w': 147,\n",
              "        'h': 377,\n",
              "        'confidence': '0.8895793391360804',\n",
              "        'text': '8'},\n",
              "       {'x': 210,\n",
              "        'y': 1187,\n",
              "        'w': 219,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9999767370120424',\n",
              "        'text': '0.6'},\n",
              "       {'x': 6192,\n",
              "        'y': 1063,\n",
              "        'w': 311,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999938607215881',\n",
              "        'text': '0.62'},\n",
              "       {'x': 5724,\n",
              "        'y': 1227,\n",
              "        'w': 311,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9988323863847912',\n",
              "        'text': '0.53'},\n",
              "       {'x': 5247,\n",
              "        'y': 1349,\n",
              "        'w': 322,\n",
              "        'h': 157,\n",
              "        'confidence': '0.9999840259552002',\n",
              "        'text': '0.46'},\n",
              "       {'x': 57,\n",
              "        'y': 1370,\n",
              "        'w': 149,\n",
              "        'h': 304,\n",
              "        'confidence': '0.9471026370546013',\n",
              "        'text': '2'},\n",
              "       {'x': 210,\n",
              "        'y': 1548,\n",
              "        'w': 219,\n",
              "        'h': 137,\n",
              "        'confidence': '0.73024243558224',\n",
              "        'text': '0.4'},\n",
              "       {'x': 1489,\n",
              "        'y': 1555,\n",
              "        'w': 312,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9725168958543228',\n",
              "        'text': '0.35'},\n",
              "       {'x': 1957,\n",
              "        'y': 1555,\n",
              "        'w': 311,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999983906745911',\n",
              "        'text': '0.35'},\n",
              "       {'x': 2425,\n",
              "        'y': 1539,\n",
              "        'w': 319,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9998807311058044',\n",
              "        'text': '0.36'},\n",
              "       {'x': 2893,\n",
              "        'y': 1539,\n",
              "        'w': 319,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9166766923509',\n",
              "        'text': '0.36'},\n",
              "       {'x': 3369,\n",
              "        'y': 1522,\n",
              "        'w': 303,\n",
              "        'h': 147,\n",
              "        'confidence': '0.5533049631119245',\n",
              "        'text': '0.37'},\n",
              "       {'x': 3836,\n",
              "        'y': 1497,\n",
              "        'w': 313,\n",
              "        'h': 157,\n",
              "        'confidence': '0.9059684872627258',\n",
              "        'text': '0.38'},\n",
              "       {'x': 4303,\n",
              "        'y': 1423,\n",
              "        'w': 322,\n",
              "        'h': 157,\n",
              "        'confidence': '0.9999935626983643',\n",
              "        'text': '0.42'},\n",
              "       {'x': 4779,\n",
              "        'y': 1423,\n",
              "        'w': 314,\n",
              "        'h': 157,\n",
              "        'confidence': '0.926105747451197',\n",
              "        'text': '0.42'},\n",
              "       {'x': 62,\n",
              "        'y': 1671,\n",
              "        'w': 137,\n",
              "        'h': 161,\n",
              "        'confidence': '0.6138422000714492',\n",
              "        'text': '2'},\n",
              "       {'x': 545,\n",
              "        'y': 1661,\n",
              "        'w': 313,\n",
              "        'h': 157,\n",
              "        'confidence': '0.9449063940231073',\n",
              "        'text': '0.29'},\n",
              "       {'x': 1013,\n",
              "        'y': 1612,\n",
              "        'w': 312,\n",
              "        'h': 148,\n",
              "        'confidence': '0.9686722281280554',\n",
              "        'text': '0.32'},\n",
              "       {'x': 45,\n",
              "        'y': 1809,\n",
              "        'w': 169,\n",
              "        'h': 345,\n",
              "        'confidence': '0.8821796873711527',\n",
              "        'text': '2'},\n",
              "       {'x': 209,\n",
              "        'y': 1908,\n",
              "        'w': 221,\n",
              "        'h': 147,\n",
              "        'confidence': '0.8353034853935242',\n",
              "        'text': '0.2'},\n",
              "       {'x': 210,\n",
              "        'y': 2278,\n",
              "        'w': 211,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9999902267832098',\n",
              "        'text': '0.0'},\n",
              "       {'x': 1544,\n",
              "        'y': 2397,\n",
              "        'w': 186,\n",
              "        'h': 203,\n",
              "        'confidence': '0.9887108552995336',\n",
              "        'text': '8'},\n",
              "       {'x': 2913,\n",
              "        'y': 2569,\n",
              "        'w': 115,\n",
              "        'h': 98,\n",
              "        'confidence': '0.5001947202801524',\n",
              "        'text': 'A'},\n",
              "       {'x': 5651,\n",
              "        'y': 2697,\n",
              "        'w': 137,\n",
              "        'h': 145,\n",
              "        'confidence': '0.25478391924324834',\n",
              "        'text': 'Tv'},\n",
              "       {'x': 6606,\n",
              "        'y': 2700,\n",
              "        'w': 115,\n",
              "        'h': 98,\n",
              "        'confidence': '0.1331048338541132',\n",
              "        'text': 'A'},\n",
              "       {'x': 6547,\n",
              "        'y': 2788,\n",
              "        'w': 159,\n",
              "        'h': 127,\n",
              "        'confidence': '0.5560608740220718',\n",
              "        'text': \"0'\"},\n",
              "       {'x': 7838,\n",
              "        'y': 3021,\n",
              "        'w': 105,\n",
              "        'h': 88,\n",
              "        'confidence': '0.16249399394812158',\n",
              "        'text': 'A'},\n",
              "       {'x': 1659,\n",
              "        'y': 3119,\n",
              "        'w': 112,\n",
              "        'h': 88,\n",
              "        'confidence': '0.33426752427784834',\n",
              "        'text': 'K'},\n",
              "       {'x': 570,\n",
              "        'y': 3755,\n",
              "        'w': 270,\n",
              "        'h': 147,\n",
              "        'confidence': '1.0',\n",
              "        'text': '283'},\n",
              "       {'x': 176,\n",
              "        'y': 4075,\n",
              "        'w': 254,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999971781544015',\n",
              "        'text': '250'},\n",
              "       {'x': 1029,\n",
              "        'y': 4205,\n",
              "        'w': 280,\n",
              "        'h': 157,\n",
              "        'confidence': '0.9999995870469189',\n",
              "        'text': '210'},\n",
              "       {'x': 20,\n",
              "        'y': 4255,\n",
              "        'w': 147,\n",
              "        'h': 484,\n",
              "        'confidence': '0.4429378318724595',\n",
              "        'text': '8'},\n",
              "       {'x': 176,\n",
              "        'y': 4386,\n",
              "        'w': 254,\n",
              "        'h': 148,\n",
              "        'confidence': '0.9999741904763433',\n",
              "        'text': '200'},\n",
              "       {'x': 1513,\n",
              "        'y': 4336,\n",
              "        'w': 264,\n",
              "        'h': 158,\n",
              "        'confidence': '0.9999987611408254',\n",
              "        'text': '190'},\n",
              "       {'x': 1989,\n",
              "        'y': 4509,\n",
              "        'w': 264,\n",
              "        'h': 157,\n",
              "        'confidence': '0.9999995870469189',\n",
              "        'text': '163'},\n",
              "       {'x': 2450,\n",
              "        'y': 4551,\n",
              "        'w': 262,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999987611408254',\n",
              "        'text': '157'},\n",
              "       {'x': 2926,\n",
              "        'y': 4551,\n",
              "        'w': 253,\n",
              "        'h': 147,\n",
              "        'confidence': '0.7397971749305725',\n",
              "        'text': '157'},\n",
              "       {'x': 34,\n",
              "        'y': 4696,\n",
              "        'w': 407,\n",
              "        'h': 284,\n",
              "        'confidence': '0.4340921058134146',\n",
              "        'text': '8 150'},\n",
              "       {'x': 3402,\n",
              "        'y': 4756,\n",
              "        'w': 253,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9920586943626404',\n",
              "        'text': '124'},\n",
              "       {'x': 3878,\n",
              "        'y': 4813,\n",
              "        'w': 253,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9996430277824402',\n",
              "        'text': '115'},\n",
              "       {'x': 4345,\n",
              "        'y': 4830,\n",
              "        'w': 254,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999969716779362',\n",
              "        'text': '113'},\n",
              "       {'x': 4813,\n",
              "        'y': 4912,\n",
              "        'w': 262,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999154827455907',\n",
              "        'text': '100'},\n",
              "       {'x': 184,\n",
              "        'y': 5018,\n",
              "        'w': 246,\n",
              "        'h': 148,\n",
              "        'confidence': '0.998988151550293',\n",
              "        'text': '100'},\n",
              "       {'x': 20,\n",
              "        'y': 4969,\n",
              "        'w': 147,\n",
              "        'h': 451,\n",
              "        'confidence': '0.2324521445270875',\n",
              "        'text': '2'},\n",
              "       {'x': 5324,\n",
              "        'y': 5094,\n",
              "        'w': 176,\n",
              "        'h': 127,\n",
              "        'confidence': '0.9367439723640099',\n",
              "        'text': '73'},\n",
              "       {'x': 5782,\n",
              "        'y': 5142,\n",
              "        'w': 188,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999997471189183',\n",
              "        'text': '64'},\n",
              "       {'x': 6249,\n",
              "        'y': 5174,\n",
              "        'w': 197,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999988198884456',\n",
              "        'text': '58'},\n",
              "       {'x': 6725,\n",
              "        'y': 5183,\n",
              "        'w': 180,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999967968410157',\n",
              "        'text': '57'},\n",
              "       {'x': 251,\n",
              "        'y': 5348,\n",
              "        'w': 178,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9999833098887841',\n",
              "        'text': '50'},\n",
              "       {'x': 7193,\n",
              "        'y': 5289,\n",
              "        'w': 188,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9800780570169296',\n",
              "        'text': '40'},\n",
              "       {'x': 7661,\n",
              "        'y': 5322,\n",
              "        'w': 196,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9038006883505015',\n",
              "        'text': '35'},\n",
              "       {'x': 8137,\n",
              "        'y': 5363,\n",
              "        'w': 188,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999989884757856',\n",
              "        'text': '29'},\n",
              "       {'x': 14,\n",
              "        'y': 5423,\n",
              "        'w': 171,\n",
              "        'h': 446,\n",
              "        'confidence': '0.5015159722621689',\n",
              "        'text': '3'},\n",
              "       {'x': 2955,\n",
              "        'y': 5770,\n",
              "        'w': 186,\n",
              "        'h': 211,\n",
              "        'confidence': '0.6754507575908626',\n",
              "        'text': '8'},\n",
              "       {'x': 3273,\n",
              "        'y': 5981,\n",
              "        'w': 216,\n",
              "        'h': 249,\n",
              "        'confidence': '0.10451296868180382',\n",
              "        'text': '3'},\n",
              "       {'x': 5163,\n",
              "        'y': 6008,\n",
              "        'w': 186,\n",
              "        'h': 227,\n",
              "        'confidence': '0.1981989766605352',\n",
              "        'text': '~'},\n",
              "       {'x': 5942,\n",
              "        'y': 6295,\n",
              "        'w': 196,\n",
              "        'h': 212,\n",
              "        'confidence': '0.5486785747508187',\n",
              "        'text': 'N?'},\n",
              "       {'x': 6324,\n",
              "        'y': 6546,\n",
              "        'w': 162,\n",
              "        'h': 137,\n",
              "        'confidence': '0.10241533919824125',\n",
              "        'text': \"0'\"},\n",
              "       {'x': 8114,\n",
              "        'y': 7130,\n",
              "        'w': 226,\n",
              "        'h': 126,\n",
              "        'confidence': '0.9994204044342041',\n",
              "        'text': '4.0'},\n",
              "       {'x': 219,\n",
              "        'y': 7253,\n",
              "        'w': 201,\n",
              "        'h': 126,\n",
              "        'confidence': '0.9995023392916654',\n",
              "        'text': '4.0'},\n",
              "       {'x': 7653,\n",
              "        'y': 7390,\n",
              "        'w': 221,\n",
              "        'h': 147,\n",
              "        'confidence': '0.72256789348978',\n",
              "        'text': '3.4'},\n",
              "       {'x': 71,\n",
              "        'y': 7391,\n",
              "        'w': 136,\n",
              "        'h': 268,\n",
              "        'confidence': '0.9989661267748353',\n",
              "        'text': '8'},\n",
              "       {'x': 218,\n",
              "        'y': 7473,\n",
              "        'w': 203,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9999552635490495',\n",
              "        'text': '3.5'},\n",
              "       {'x': 218,\n",
              "        'y': 7695,\n",
              "        'w': 203,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9999300047790276',\n",
              "        'text': '3.0'},\n",
              "       {'x': 6232,\n",
              "        'y': 7701,\n",
              "        'w': 231,\n",
              "        'h': 157,\n",
              "        'confidence': '0.6016059802823547',\n",
              "        'text': '2.7'},\n",
              "       {'x': 6701,\n",
              "        'y': 7661,\n",
              "        'w': 229,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9999894499778748',\n",
              "        'text': '2.8'},\n",
              "       {'x': 7176,\n",
              "        'y': 7611,\n",
              "        'w': 231,\n",
              "        'h': 157,\n",
              "        'confidence': '0.9999029566541391',\n",
              "        'text': '2.9'},\n",
              "       {'x': 47,\n",
              "        'y': 7687,\n",
              "        'w': 163,\n",
              "        'h': 325,\n",
              "        'confidence': '0.8541750535076886',\n",
              "        'text': '2'},\n",
              "       {'x': 5765,\n",
              "        'y': 7833,\n",
              "        'w': 221,\n",
              "        'h': 148,\n",
              "        'confidence': '0.7943207621574402',\n",
              "        'text': '2.4'},\n",
              "       {'x': 209,\n",
              "        'y': 7915,\n",
              "        'w': 221,\n",
              "        'h': 148,\n",
              "        'confidence': '0.5617242455482483',\n",
              "        'text': '2.5'},\n",
              "       {'x': 218,\n",
              "        'y': 8146,\n",
              "        'w': 211,\n",
              "        'h': 137,\n",
              "        'confidence': '0.7588341236114502',\n",
              "        'text': '2.0'},\n",
              "       {'x': 5308,\n",
              "        'y': 8164,\n",
              "        'w': 208,\n",
              "        'h': 126,\n",
              "        'confidence': '0.8548241594752413',\n",
              "        'text': '1.7'},\n",
              "       {'x': 56,\n",
              "        'y': 8017,\n",
              "        'w': 175,\n",
              "        'h': 634,\n",
              "        'confidence': '0.604393879951207',\n",
              "        'text': 'L'},\n",
              "       {'x': 227,\n",
              "        'y': 8368,\n",
              "        'w': 202,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9992104754561254',\n",
              "        'text': '1.5'},\n",
              "       {'x': 3418,\n",
              "        'y': 8375,\n",
              "        'w': 221,\n",
              "        'h': 147,\n",
              "        'confidence': '0.307715336994227',\n",
              "        'text': '1,.2'},\n",
              "       {'x': 3887,\n",
              "        'y': 8335,\n",
              "        'w': 227,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9866940975189209',\n",
              "        'text': '1.3'},\n",
              "       {'x': 4354,\n",
              "        'y': 8285,\n",
              "        'w': 229,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9974104766619256',\n",
              "        'text': '1.4'},\n",
              "       {'x': 4830,\n",
              "        'y': 8244,\n",
              "        'w': 221,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9987100460591064',\n",
              "        'text': '1.5'},\n",
              "       {'x': 1523,\n",
              "        'y': 8516,\n",
              "        'w': 236,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9998371601104736',\n",
              "        'text': '0.9'},\n",
              "       {'x': 2006,\n",
              "        'y': 8465,\n",
              "        'w': 221,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9916629195213318',\n",
              "        'text': '1.0'},\n",
              "       {'x': 2482,\n",
              "        'y': 8465,\n",
              "        'w': 221,\n",
              "        'h': 147,\n",
              "        'confidence': '0.9993635975959063',\n",
              "        'text': '1.0'},\n",
              "       {'x': 226,\n",
              "        'y': 8588,\n",
              "        'w': 204,\n",
              "        'h': 148,\n",
              "        'confidence': '0.9998610425779199',\n",
              "        'text': '1.0'},\n",
              "       {'x': 588,\n",
              "        'y': 8606,\n",
              "        'w': 210,\n",
              "        'h': 137,\n",
              "        'confidence': '0.5166886317871929',\n",
              "        'text': '0.7'},\n",
              "       {'x': 1054,\n",
              "        'y': 8556,\n",
              "        'w': 230,\n",
              "        'h': 147,\n",
              "        'confidence': '0.7083911299705505',\n",
              "        'text': '0.8'},\n",
              "       {'x': 53,\n",
              "        'y': 8646,\n",
              "        'w': 147,\n",
              "        'h': 319,\n",
              "        'confidence': '0.19658218897651072',\n",
              "        'text': '2'},\n",
              "       {'x': 211,\n",
              "        'y': 8820,\n",
              "        'w': 209,\n",
              "        'h': 127,\n",
              "        'confidence': '0.9998821020126343',\n",
              "        'text': '0.5'},\n",
              "       {'x': 210,\n",
              "        'y': 9041,\n",
              "        'w': 211,\n",
              "        'h': 137,\n",
              "        'confidence': '0.9999913968155315',\n",
              "        'text': '0.0'},\n",
              "       {'x': 2478,\n",
              "        'y': 9143,\n",
              "        'w': 197,\n",
              "        'h': 212,\n",
              "        'confidence': '0.49838369517604875',\n",
              "        'text': '8'},\n",
              "       {'x': 5228,\n",
              "        'y': 9397,\n",
              "        'w': 114,\n",
              "        'h': 98,\n",
              "        'confidence': '0.17841355045820606',\n",
              "        'text': 'K'},\n",
              "       {'x': 320,\n",
              "        'y': 9635,\n",
              "        'w': 196,\n",
              "        'h': 213,\n",
              "        'confidence': '0.12168862952506394',\n",
              "        'text': 'N?'},\n",
              "       {'x': 6426,\n",
              "        'y': 9774,\n",
              "        'w': 114,\n",
              "        'h': 98,\n",
              "        'confidence': '0.3055300490850641',\n",
              "        'text': 'K'},\n",
              "       {'x': 1191,\n",
              "        'y': 9882,\n",
              "        'w': 104,\n",
              "        'h': 88,\n",
              "        'confidence': '0.10631055459378391',\n",
              "        'text': 'N'},\n",
              "       {'x': 6340,\n",
              "        'y': 2349,\n",
              "        'w': 16,\n",
              "        'h': 349,\n",
              "        'confidence': '0.7712873667529188',\n",
              "        'text': '1'},\n",
              "       {'x': 689,\n",
              "        'y': 2338,\n",
              "        'w': 17,\n",
              "        'h': 342,\n",
              "        'confidence': '0.24297760018134884',\n",
              "        'text': '9'},\n",
              "       {'x': 2560,\n",
              "        'y': 2337,\n",
              "        'w': 34,\n",
              "        'h': 425,\n",
              "        'confidence': '0.24729664785958505',\n",
              "        'text': '8'},\n",
              "       {'x': 3025,\n",
              "        'y': 2354,\n",
              "        'w': 47,\n",
              "        'h': 277,\n",
              "        'confidence': '0.9975590122924736',\n",
              "        'text': '8'},\n",
              "       {'x': 3509,\n",
              "        'y': 2335,\n",
              "        'w': 28,\n",
              "        'h': 399,\n",
              "        'confidence': '0.2454178347646243',\n",
              "        'text': '9'},\n",
              "       {'x': 4001,\n",
              "        'y': 2356,\n",
              "        'w': 44,\n",
              "        'h': 348,\n",
              "        'confidence': '0.23636700062355942',\n",
              "        'text': '2'},\n",
              "       {'x': 4446,\n",
              "        'y': 2337,\n",
              "        'w': 10,\n",
              "        'h': 368,\n",
              "        'confidence': '0.8387163422538926',\n",
              "        'text': '2'},\n",
              "       {'x': 5381,\n",
              "        'y': 2338,\n",
              "        'w': 1,\n",
              "        'h': 363,\n",
              "        'confidence': '0.6375358123601451',\n",
              "        'text': '8'},\n",
              "       {'x': 6797,\n",
              "        'y': 2340,\n",
              "        'w': 38,\n",
              "        'h': 425,\n",
              "        'confidence': '0.8758150447147841',\n",
              "        'text': '8'},\n",
              "       {'x': 1152,\n",
              "        'y': 2338,\n",
              "        'w': 87,\n",
              "        'h': 503,\n",
              "        'confidence': '0.492085690319616',\n",
              "        'text': '1'},\n",
              "       {'x': 7291,\n",
              "        'y': 2354,\n",
              "        'w': 85,\n",
              "        'h': 431,\n",
              "        'confidence': '0.5462918639807555',\n",
              "        'text': '2'},\n",
              "       {'x': 7741,\n",
              "        'y': 2334,\n",
              "        'w': 24,\n",
              "        'h': 429,\n",
              "        'confidence': '0.6988871726352954',\n",
              "        'text': '2'},\n",
              "       {'x': 4921,\n",
              "        'y': 2356,\n",
              "        'w': 111,\n",
              "        'h': 533,\n",
              "        'confidence': '0.9614069831212682',\n",
              "        'text': '8'},\n",
              "       {'x': 8211,\n",
              "        'y': 2340,\n",
              "        'w': 41,\n",
              "        'h': 452,\n",
              "        'confidence': '0.16219706043338888',\n",
              "        'text': '2'},\n",
              "       {'x': 5852,\n",
              "        'y': 2344,\n",
              "        'w': 38,\n",
              "        'h': 442,\n",
              "        'confidence': '0.31891921946466084',\n",
              "        'text': '2'},\n",
              "       {'x': 2088,\n",
              "        'y': 2373,\n",
              "        'w': 45,\n",
              "        'h': 415,\n",
              "        'confidence': '0.5449010616340253',\n",
              "        'text': '2'},\n",
              "       {'x': 555,\n",
              "        'y': 2554,\n",
              "        'w': 13,\n",
              "        'h': 332,\n",
              "        'confidence': '0.7109246713671382',\n",
              "        'text': '~'},\n",
              "       {'x': 5253,\n",
              "        'y': 2581,\n",
              "        'w': 27,\n",
              "        'h': 308,\n",
              "        'confidence': '0.48903673352641874',\n",
              "        'text': '~'},\n",
              "       {'x': 4307,\n",
              "        'y': 2589,\n",
              "        'w': 31,\n",
              "        'h': 300,\n",
              "        'confidence': '0.13600577883881115',\n",
              "        'text': '5'},\n",
              "       {'x': 6187,\n",
              "        'y': 2594,\n",
              "        'w': 109,\n",
              "        'h': 533,\n",
              "        'confidence': '0.9765466791082531',\n",
              "        'text': '8'},\n",
              "       {'x': 3835,\n",
              "        'y': 2590,\n",
              "        'w': 77,\n",
              "        'h': 483,\n",
              "        'confidence': '0.2795716485537447',\n",
              "        'text': '1'},\n",
              "       {'x': 2409,\n",
              "        'y': 2647,\n",
              "        'w': 97,\n",
              "        'h': 477,\n",
              "        'confidence': '0.33115697486482887',\n",
              "        'text': '1'},\n",
              "       {'x': 3356,\n",
              "        'y': 2626,\n",
              "        'w': 89,\n",
              "        'h': 473,\n",
              "        'confidence': '0.3041710120271546',\n",
              "        'text': '1'},\n",
              "       {'x': 7591,\n",
              "        'y': 2636,\n",
              "        'w': 108,\n",
              "        'h': 511,\n",
              "        'confidence': '0.27097032986881686',\n",
              "        'text': '8'},\n",
              "       {'x': 7088,\n",
              "        'y': 2673,\n",
              "        'w': 12,\n",
              "        'h': 306,\n",
              "        'confidence': '0.8785177079196025',\n",
              "        'text': '3'},\n",
              "       {'x': 8064,\n",
              "        'y': 2672,\n",
              "        'w': 39,\n",
              "        'h': 314,\n",
              "        'confidence': '0.6479517934494652',\n",
              "        'text': '2'},\n",
              "       {'x': 2845,\n",
              "        'y': 2727,\n",
              "        'w': 91,\n",
              "        'h': 448,\n",
              "        'confidence': '0.10543889472213497',\n",
              "        'text': '1'},\n",
              "       {'x': 929,\n",
              "        'y': 2737,\n",
              "        'w': 77,\n",
              "        'h': 475,\n",
              "        'confidence': '0.2847387905387997',\n",
              "        'text': '1'},\n",
              "       {'x': 453,\n",
              "        'y': 2780,\n",
              "        'w': 82,\n",
              "        'h': 449,\n",
              "        'confidence': '0.10974362991058584',\n",
              "        'text': '1'},\n",
              "       {'x': 4212,\n",
              "        'y': 2789,\n",
              "        'w': 82,\n",
              "        'h': 448,\n",
              "        'confidence': '0.13955380649372273',\n",
              "        'text': '1'},\n",
              "       {'x': 5156,\n",
              "        'y': 2789,\n",
              "        'w': 84,\n",
              "        'h': 452,\n",
              "        'confidence': '0.15025342471499759',\n",
              "        'text': '1'},\n",
              "       {'x': 6527,\n",
              "        'y': 2854,\n",
              "        'w': 80,\n",
              "        'h': 450,\n",
              "        'confidence': '0.1012778842840456',\n",
              "        'text': '!'},\n",
              "       {'x': 6977,\n",
              "        'y': 2871,\n",
              "        'w': 99,\n",
              "        'h': 507,\n",
              "        'confidence': '0.21164851314102862',\n",
              "        'text': '3'},\n",
              "       {'x': 1244,\n",
              "        'y': 3027,\n",
              "        'w': 108,\n",
              "        'h': 508,\n",
              "        'confidence': '0.8597333670758331',\n",
              "        'text': 'J'},\n",
              "       {'x': 7351,\n",
              "        'y': 3032,\n",
              "        'w': 19,\n",
              "        'h': 305,\n",
              "        'confidence': '0.8849601028396705',\n",
              "        'text': '3'},\n",
              "       {'x': 7811,\n",
              "        'y': 3065,\n",
              "        'w': 109,\n",
              "        'h': 510,\n",
              "        'confidence': '0.8648671976815514',\n",
              "        'text': 'J'},\n",
              "       {'x': 5451,\n",
              "        'y': 3091,\n",
              "        'w': 101,\n",
              "        'h': 477,\n",
              "        'confidence': '0.13479203859975897',\n",
              "        'text': '{'},\n",
              "       {'x': 1629,\n",
              "        'y': 3166,\n",
              "        'w': 98,\n",
              "        'h': 513,\n",
              "        'confidence': '0.8796104708736152',\n",
              "        'text': 'J'},\n",
              "       {'x': 7242,\n",
              "        'y': 3235,\n",
              "        'w': 99,\n",
              "        'h': 499,\n",
              "        'confidence': '0.543002418566175',\n",
              "        'text': '8'},\n",
              "       {'x': 4453,\n",
              "        'y': 5727,\n",
              "        'w': 10,\n",
              "        'h': 355,\n",
              "        'confidence': '0.6726373501387606',\n",
              "        'text': '1'},\n",
              "       {'x': 690,\n",
              "        'y': 5737,\n",
              "        'w': 22,\n",
              "        'h': 276,\n",
              "        'confidence': '0.8826998490445135',\n",
              "        'text': '8'},\n",
              "       {'x': 2560,\n",
              "        'y': 5717,\n",
              "        'w': 19,\n",
              "        'h': 402,\n",
              "        'confidence': '0.4157630524396758',\n",
              "        'text': '9'},\n",
              "       {'x': 3505,\n",
              "        'y': 5719,\n",
              "        'w': 0,\n",
              "        'h': 343,\n",
              "        'confidence': '0.6983019021479464',\n",
              "        'text': '9'},\n",
              "       {'x': 3977,\n",
              "        'y': 5732,\n",
              "        'w': 38,\n",
              "        'h': 406,\n",
              "        'confidence': '0.6568197627317183',\n",
              "        'text': '2'},\n",
              "       {'x': 5383,\n",
              "        'y': 5721,\n",
              "        'w': 4,\n",
              "        'h': 363,\n",
              "        'confidence': '0.9163023418543084',\n",
              "        'text': '2'},\n",
              "       {'x': 6799,\n",
              "        'y': 5712,\n",
              "        'w': 26,\n",
              "        'h': 431,\n",
              "        'confidence': '0.7325677511144022',\n",
              "        'text': '2'},\n",
              "       {'x': 7266,\n",
              "        'y': 5720,\n",
              "        'w': 1,\n",
              "        'h': 364,\n",
              "        'confidence': '0.7464041683822451',\n",
              "        'text': '8'},\n",
              "       {'x': 8203,\n",
              "        'y': 5721,\n",
              "        'w': 31,\n",
              "        'h': 425,\n",
              "        'confidence': '0.752803959460735',\n",
              "        'text': '8'},\n",
              "       {'x': 1154,\n",
              "        'y': 5722,\n",
              "        'w': 88,\n",
              "        'h': 497,\n",
              "        'confidence': '0.36279112424157134',\n",
              "        'text': '1'},\n",
              "       {'x': 4917,\n",
              "        'y': 5716,\n",
              "        'w': 14,\n",
              "        'h': 375,\n",
              "        'confidence': '0.8206169081171346',\n",
              "        'text': '2'},\n",
              "       {'x': 5880,\n",
              "        'y': 5737,\n",
              "        'w': 85,\n",
              "        'h': 428,\n",
              "        'confidence': '0.2896119153831478',\n",
              "        'text': '1'},\n",
              "       {'x': 6335,\n",
              "        'y': 5718,\n",
              "        'w': 49,\n",
              "        'h': 455,\n",
              "        'confidence': '0.3211120523353692',\n",
              "        'text': '{'},\n",
              "       {'x': 7743,\n",
              "        'y': 5719,\n",
              "        'w': 43,\n",
              "        'h': 451,\n",
              "        'confidence': '0.6244501327313543',\n",
              "        'text': '2'},\n",
              "       {'x': 2097,\n",
              "        'y': 5738,\n",
              "        'w': 112,\n",
              "        'h': 533,\n",
              "        'confidence': '0.9753512262401252',\n",
              "        'text': '8'},\n",
              "       {'x': 1619,\n",
              "        'y': 5753,\n",
              "        'w': 44,\n",
              "        'h': 416,\n",
              "        'confidence': '0.45764921581437434',\n",
              "        'text': '2'},\n",
              "       {'x': 2921,\n",
              "        'y': 5904,\n",
              "        'w': 45,\n",
              "        'h': 302,\n",
              "        'confidence': '0.313358388153663',\n",
              "        'text': '5'},\n",
              "       {'x': 591,\n",
              "        'y': 5894,\n",
              "        'w': 66,\n",
              "        'h': 442,\n",
              "        'confidence': '0.22184205355435527',\n",
              "        'text': '2'},\n",
              "       {'x': 7142,\n",
              "        'y': 5959,\n",
              "        'w': 19,\n",
              "        'h': 316,\n",
              "        'confidence': '0.43460846827020205',\n",
              "        'text': '~'},\n",
              "       {'x': 4305,\n",
              "        'y': 5973,\n",
              "        'w': 107,\n",
              "        'h': 536,\n",
              "        'confidence': '0.9448837734965743',\n",
              "        'text': '8'},\n",
              "       {'x': 4783,\n",
              "        'y': 5978,\n",
              "        'w': 90,\n",
              "        'h': 475,\n",
              "        'confidence': '0.34080415774502626',\n",
              "        'text': '1'},\n",
              "       {'x': 6657,\n",
              "        'y': 6014,\n",
              "        'w': 115,\n",
              "        'h': 512,\n",
              "        'confidence': '0.19618836920231164',\n",
              "        'text': '8'},\n",
              "       {'x': 3819,\n",
              "        'y': 6028,\n",
              "        'w': 94,\n",
              "        'h': 478,\n",
              "        'confidence': '0.49902155932421977',\n",
              "        'text': '1'},\n",
              "       {'x': 8051,\n",
              "        'y': 6038,\n",
              "        'w': 1,\n",
              "        'h': 298,\n",
              "        'confidence': '0.39238979616271763',\n",
              "        'text': '3'},\n",
              "       {'x': 5676,\n",
              "        'y': 6054,\n",
              "        'w': 13,\n",
              "        'h': 307,\n",
              "        'confidence': '0.8778884230085993',\n",
              "        'text': '3'},\n",
              "       {'x': 1455,\n",
              "        'y': 6039,\n",
              "        'w': 68,\n",
              "        'h': 438,\n",
              "        'confidence': '0.11561597225997478',\n",
              "        'text': '2'},\n",
              "       {'x': 6163,\n",
              "        'y': 6048,\n",
              "        'w': 11,\n",
              "        'h': 318,\n",
              "        'confidence': '0.9886742284800363',\n",
              "        'text': '2'},\n",
              "       {'x': 2845,\n",
              "        'y': 6108,\n",
              "        'w': 91,\n",
              "        'h': 448,\n",
              "        'confidence': '0.14013630215256345',\n",
              "        'text': '1'},\n",
              "       {'x': 935,\n",
              "        'y': 6132,\n",
              "        'w': 79,\n",
              "        'h': 454,\n",
              "        'confidence': '0.15676671819172672',\n",
              "        'text': '{'},\n",
              "       {'x': 3277,\n",
              "        'y': 6162,\n",
              "        'w': 82,\n",
              "        'h': 448,\n",
              "        'confidence': '0.11329884152377545',\n",
              "        'text': '1'},\n",
              "       {'x': 5161,\n",
              "        'y': 6171,\n",
              "        'w': 92,\n",
              "        'h': 451,\n",
              "        'confidence': '0.20488957690674514',\n",
              "        'text': '1'},\n",
              "       {'x': 5565,\n",
              "        'y': 6252,\n",
              "        'w': 98,\n",
              "        'h': 507,\n",
              "        'confidence': '0.2753797594087395',\n",
              "        'text': '8'},\n",
              "       {'x': 306,\n",
              "        'y': 6411,\n",
              "        'w': 111,\n",
              "        'h': 506,\n",
              "        'confidence': '0.8480184737341325',\n",
              "        'text': 'J'},\n",
              "       {'x': 5930,\n",
              "        'y': 6449,\n",
              "        'w': 106,\n",
              "        'h': 505,\n",
              "        'confidence': '0.7721643162033196',\n",
              "        'text': 'J'},\n",
              "       {'x': 7336,\n",
              "        'y': 6472,\n",
              "        'w': 104,\n",
              "        'h': 479,\n",
              "        'confidence': '0.22608909993781712',\n",
              "        'text': '{'},\n",
              "       {'x': 1165,\n",
              "        'y': 6548,\n",
              "        'w': 109,\n",
              "        'h': 511,\n",
              "        'confidence': '0.8339597518561845',\n",
              "        'text': 'J'},\n",
              "       {'x': 6302,\n",
              "        'y': 6612,\n",
              "        'w': 101,\n",
              "        'h': 506,\n",
              "        'confidence': '0.25724476859199186',\n",
              "        'text': '8'},\n",
              "       {'x': 4929,\n",
              "        'y': 9111,\n",
              "        'w': 16,\n",
              "        'h': 349,\n",
              "        'confidence': '0.8487730314816169',\n",
              "        'text': '1'},\n",
              "       {'x': 3031,\n",
              "        'y': 9102,\n",
              "        'w': 8,\n",
              "        'h': 341,\n",
              "        'confidence': '0.4761241798133007',\n",
              "        'text': '9'},\n",
              "       {'x': 3977,\n",
              "        'y': 9113,\n",
              "        'w': 38,\n",
              "        'h': 406,\n",
              "        'confidence': '0.6229611265155341',\n",
              "        'text': '2'},\n",
              "       {'x': 5383,\n",
              "        'y': 9102,\n",
              "        'w': 2,\n",
              "        'h': 361,\n",
              "        'confidence': '0.8835775843134606',\n",
              "        'text': '2'},\n",
              "       {'x': 5874,\n",
              "        'y': 9113,\n",
              "        'w': 73,\n",
              "        'h': 436,\n",
              "        'confidence': '0.27616171355795416',\n",
              "        'text': '2'},\n",
              "       {'x': 6326,\n",
              "        'y': 9097,\n",
              "        'w': 18,\n",
              "        'h': 423,\n",
              "        'confidence': '0.6489268059539519',\n",
              "        'text': '2'},\n",
              "       {'x': 7264,\n",
              "        'y': 9099,\n",
              "        'w': 0,\n",
              "        'h': 366,\n",
              "        'confidence': '0.4996460120341659',\n",
              "        'text': '8'},\n",
              "       {'x': 8201,\n",
              "        'y': 9100,\n",
              "        'w': 30,\n",
              "        'h': 427,\n",
              "        'confidence': '0.9644932681694627',\n",
              "        'text': '8'},\n",
              "       {'x': 681,\n",
              "        'y': 9120,\n",
              "        'w': 36,\n",
              "        'h': 272,\n",
              "        'confidence': '0.986155955974656',\n",
              "        'text': '8'},\n",
              "       {'x': 1154,\n",
              "        'y': 9103,\n",
              "        'w': 90,\n",
              "        'h': 501,\n",
              "        'confidence': '0.22169996646216905',\n",
              "        'text': '0'},\n",
              "       {'x': 2094,\n",
              "        'y': 9098,\n",
              "        'w': 23,\n",
              "        'h': 398,\n",
              "        'confidence': '0.43386847952749363',\n",
              "        'text': 'g'},\n",
              "       {'x': 4470,\n",
              "        'y': 9107,\n",
              "        'w': 50,\n",
              "        'h': 365,\n",
              "        'confidence': '0.24057624916276055',\n",
              "        'text': '2'},\n",
              "       {'x': 6807,\n",
              "        'y': 9100,\n",
              "        'w': 51,\n",
              "        'h': 450,\n",
              "        'confidence': '0.16420180079251967',\n",
              "        'text': '{'},\n",
              "       {'x': 3510,\n",
              "        'y': 9121,\n",
              "        'w': 113,\n",
              "        'h': 528,\n",
              "        'confidence': '0.9802094238780548',\n",
              "        'text': '8'},\n",
              "       {'x': 7740,\n",
              "        'y': 9107,\n",
              "        'w': 38,\n",
              "        'h': 441,\n",
              "        'confidence': '0.5758991549193446',\n",
              "        'text': '2'},\n",
              "       {'x': 1615,\n",
              "        'y': 9133,\n",
              "        'w': 39,\n",
              "        'h': 416,\n",
              "        'confidence': '0.32065367285563084',\n",
              "        'text': '2'},\n",
              "       {'x': 2462,\n",
              "        'y': 9291,\n",
              "        'w': 29,\n",
              "        'h': 295,\n",
              "        'confidence': '0.20765122716035478',\n",
              "        'text': '5'},\n",
              "       {'x': 2906,\n",
              "        'y': 9319,\n",
              "        'w': 18,\n",
              "        'h': 333,\n",
              "        'confidence': '0.5646714622819218',\n",
              "        'text': '~'},\n",
              "       {'x': 7140,\n",
              "        'y': 9344,\n",
              "        'w': 21,\n",
              "        'h': 312,\n",
              "        'confidence': '0.33463615120894374',\n",
              "        'text': '3'},\n",
              "       {'x': 4776,\n",
              "        'y': 9356,\n",
              "        'w': 110,\n",
              "        'h': 533,\n",
              "        'confidence': '0.962003312750781',\n",
              "        'text': '8'},\n",
              "       {'x': 4304,\n",
              "        'y': 9357,\n",
              "        'w': 78,\n",
              "        'h': 478,\n",
              "        'confidence': '0.3146560545851038',\n",
              "        'text': '1'},\n",
              "       {'x': 6178,\n",
              "        'y': 9398,\n",
              "        'w': 105,\n",
              "        'h': 510,\n",
              "        'confidence': '0.21285099410804165',\n",
              "        'text': '8'},\n",
              "       {'x': 1942,\n",
              "        'y': 9386,\n",
              "        'w': 85,\n",
              "        'h': 476,\n",
              "        'confidence': '0.4507907802444642',\n",
              "        'text': '1'},\n",
              "       {'x': 3817,\n",
              "        'y': 9404,\n",
              "        'w': 92,\n",
              "        'h': 484,\n",
              "        'confidence': '0.5698845067533718',\n",
              "        'text': '1'},\n",
              "       {'x': 8051,\n",
              "        'y': 9419,\n",
              "        'w': 1,\n",
              "        'h': 298,\n",
              "        'confidence': '0.38998629950219765',\n",
              "        'text': '3'},\n",
              "       {'x': 5676,\n",
              "        'y': 9436,\n",
              "        'w': 14,\n",
              "        'h': 304,\n",
              "        'confidence': '0.8627207911337251',\n",
              "        'text': '3'},\n",
              "       {'x': 1455,\n",
              "        'y': 9421,\n",
              "        'w': 68,\n",
              "        'h': 437,\n",
              "        'confidence': '0.11508754847687896',\n",
              "        'text': '2'},\n",
              "       {'x': 6658,\n",
              "        'y': 9438,\n",
              "        'w': 47,\n",
              "        'h': 311,\n",
              "        'confidence': '0.4233042690388267',\n",
              "        'text': '8'},\n",
              "       {'x': 935,\n",
              "        'y': 9514,\n",
              "        'w': 79,\n",
              "        'h': 453,\n",
              "        'confidence': '0.18732901806753777',\n",
              "        'text': '{'},\n",
              "       {'x': 5565,\n",
              "        'y': 9634,\n",
              "        'w': 97,\n",
              "        'h': 504,\n",
              "        'confidence': '0.7119009867910933',\n",
              "        'text': '8'},\n",
              "       {'x': 306,\n",
              "        'y': 9792,\n",
              "        'w': 110,\n",
              "        'h': 504,\n",
              "        'confidence': '0.8108557003475312',\n",
              "        'text': 'J'},\n",
              "       {'x': 5939,\n",
              "        'y': 9797,\n",
              "        'w': 15,\n",
              "        'h': 301,\n",
              "        'confidence': '0.7546431551077184',\n",
              "        'text': '3'},\n",
              "       {'x': 6399,\n",
              "        'y': 9828,\n",
              "        'w': 106,\n",
              "        'h': 507,\n",
              "        'confidence': '0.8658196580981894',\n",
              "        'text': 'J'},\n",
              "       {'x': 7336,\n",
              "        'y': 9853,\n",
              "        'w': 104,\n",
              "        'h': 479,\n",
              "        'confidence': '0.22608909993781712',\n",
              "        'text': '{'},\n",
              "       {'x': 1165,\n",
              "        'y': 9929,\n",
              "        'w': 108,\n",
              "        'h': 510,\n",
              "        'confidence': '0.8334826709984533',\n",
              "        'text': 'J'},\n",
              "       {'x': 5830,\n",
              "        'y': 9991,\n",
              "        'w': 98,\n",
              "        'h': 507,\n",
              "        'confidence': '0.47171733264362103',\n",
              "        'text': '8'}]}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 154, 'y': 34.04, 'w': 335, 'h': 574.96}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Model Family',\n",
              "      'md': '# Model Family',\n",
              "      'bBox': {'x': 298, 'y': 118, 'w': 191, 'h': 152}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['',\n",
              "        '',\n",
              "        'Meta',\n",
              "        'Amazon',\n",
              "        'Google',\n",
              "        'Mistral AI',\n",
              "        'OpenAI',\n",
              "        'Anthropic',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['1.0',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '0.98',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['80.8',\n",
              "        '',\n",
              "        '',\n",
              "        '0.72',\n",
              "        '0.72',\n",
              "        '0.87',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['80.6',\n",
              "        '',\n",
              "        '',\n",
              "        '0.46',\n",
              "        '0.53',\n",
              "        '0.62',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['20.4',\n",
              "        '0.29',\n",
              "        '0.32',\n",
              "        '0.35',\n",
              "        '0.35',\n",
              "        '0.36',\n",
              "        '0.36',\n",
              "        '0.37',\n",
              "        '0.38',\n",
              "        '0.42',\n",
              "        '0.42',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['',\n",
              "        '',\n",
              "        '2',\n",
              "        '20.2',\n",
              "        '0.0',\n",
              "        '1~',\n",
              "        '9',\n",
              "        '1',\n",
              "        '1',\n",
              "        '8',\n",
              "        '2',\n",
              "        '1',\n",
              "        '8',\n",
              "        '1ᴬ⁸',\n",
              "        '1',\n",
              "        '9',\n",
              "        '1',\n",
              "        '2',\n",
              "        '1⁵',\n",
              "        '2',\n",
              "        '8',\n",
              "        '1~',\n",
              "        '8',\n",
              "        'Tv',\n",
              "        '2',\n",
              "        '81',\n",
              "        'A',\n",
              "        '8',\n",
              "        '3',\n",
              "        '2',\n",
              "        '8',\n",
              "        '2',\n",
              "        '2',\n",
              "        '2',\n",
              "        '',\n",
              "        ''],\n",
              "       ['',\n",
              "        \"0'\",\n",
              "        '',\n",
              "        'J',\n",
              "        '',\n",
              "        '!',\n",
              "        '3',\n",
              "        '3',\n",
              "        'A',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['',\n",
              "        'K',\n",
              "        '',\n",
              "        '{',\n",
              "        '',\n",
              "        'J',\n",
              "        '',\n",
              "        '8',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['283',\n",
              "        '8²⁵⁰',\n",
              "        '210',\n",
              "        '190',\n",
              "        '200',\n",
              "        '163',\n",
              "        '157',\n",
              "        '157',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['8',\n",
              "        '150',\n",
              "        '',\n",
              "        '124',\n",
              "        '115',\n",
              "        '113',\n",
              "        '100',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['2',\n",
              "        '100',\n",
              "        '',\n",
              "        '73',\n",
              "        '64',\n",
              "        '58',\n",
              "        '57',\n",
              "        '40',\n",
              "        '35',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['3',\n",
              "        '50',\n",
              "        '1',\n",
              "        '9',\n",
              "        '9',\n",
              "        '2',\n",
              "        '2',\n",
              "        '{',\n",
              "        '2',\n",
              "        '8',\n",
              "        '2',\n",
              "        '29',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['',\n",
              "        '2',\n",
              "        '8',\n",
              "        '{',\n",
              "        '2',\n",
              "        '2',\n",
              "        '8',\n",
              "        '1⁵⁸',\n",
              "        '3',\n",
              "        '1',\n",
              "        '2',\n",
              "        '8',\n",
              "        '1',\n",
              "        '1',\n",
              "        '~',\n",
              "        '3',\n",
              "        '1',\n",
              "        '2',\n",
              "        '8',\n",
              "        '~',\n",
              "        '3',\n",
              "        '8',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['',\n",
              "        'J',\n",
              "        '1',\n",
              "        '',\n",
              "        '1',\n",
              "        '8',\n",
              "        'N?',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['',\n",
              "        'J',\n",
              "        '',\n",
              "        '',\n",
              "        \"0'\",\n",
              "        '{',\n",
              "        '',\n",
              "        '8',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['4.0',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '4.0',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['83.5',\n",
              "        '',\n",
              "        '',\n",
              "        '2.8',\n",
              "        '2.9',\n",
              "        '3.4',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['23.0',\n",
              "        '',\n",
              "        '2.4',\n",
              "        '2.7',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['L2.5',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['2.0',\n",
              "        '',\n",
              "        '1.3',\n",
              "        '1.4',\n",
              "        '1.5',\n",
              "        '1.7',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['1.5',\n",
              "        '0.8',\n",
              "        '0.9',\n",
              "        '1.0',\n",
              "        '1.0',\n",
              "        '1,.2',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['',\n",
              "        '2',\n",
              "        '1.0',\n",
              "        '0.7',\n",
              "        '0.5',\n",
              "        '0.0',\n",
              "        '8',\n",
              "        '{',\n",
              "        '0',\n",
              "        '2',\n",
              "        '2',\n",
              "        '1',\n",
              "        'g',\n",
              "        '58',\n",
              "        '~',\n",
              "        '9',\n",
              "        '8',\n",
              "        '1',\n",
              "        '2',\n",
              "        '1',\n",
              "        '2',\n",
              "        '8',\n",
              "        '1',\n",
              "        'K',\n",
              "        '2',\n",
              "        '3',\n",
              "        '2',\n",
              "        '8',\n",
              "        '2',\n",
              "        '8',\n",
              "        '{',\n",
              "        '3',\n",
              "        '8',\n",
              "        '2',\n",
              "        '3',\n",
              "        '8'],\n",
              "       ['',\n",
              "        'N?',\n",
              "        '',\n",
              "        '8',\n",
              "        'K',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        ''],\n",
              "       ['',\n",
              "        'J',\n",
              "        '',\n",
              "        'N',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '',\n",
              "        '']],\n",
              "      'md': \"| | |Meta|Amazon|Google|Mistral AI|OpenAI|Anthropic| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|1.0| | | |0.98| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|80.8| | |0.72|0.72|0.87| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|80.6| | |0.46|0.53|0.62| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|20.4|0.29|0.32|0.35|0.35|0.36|0.36|0.37|0.38|0.42|0.42| | | | | | | | | | | | | | | | | | | | | | | | | |\\n| | |2|20.2|0.0|1~|9|1|1|8|2|1|8|1ᴬ⁸|1|9|1|2|1⁵|2|8|1~|8|Tv|2|81|A|8|3|2|8|2|2|2| | |\\n| |0'| |J| |!|3|3|A| | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |K| |{| |J| |8| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|283|8²⁵⁰|210|190|200|163|157|157| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|8|150| |124|115|113|100| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|2|100| |73|64|58|57|40|35| | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|3|50|1|9|9|2|2|{|2|8|2|29| | | | | | | | | | | | | | | | | | | | | | | | |\\n| |2|8|{|2|2|8|1⁵⁸|3|1|2|8|1|1|~|3|1|2|8|~|3|8| | | | | | | | | | | | | | |\\n| |J|1| |1|8|N?| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |J| | |0'|{| |8| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|4.0| | | |4.0| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|83.5| | |2.8|2.9|3.4| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|23.0| |2.4|2.7| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|L2.5| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|2.0| |1.3|1.4|1.5|1.7| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|1.5|0.8|0.9|1.0|1.0|1,.2| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |2|1.0|0.7|0.5|0.0|8|{|0|2|2|1|g|58|~|9|8|1|2|1|2|8|1|K|2|3|2|8|2|8|{|3|8|2|3|8|\\n| |N?| |8|K| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |J| |N| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\",\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"\",\"\",\"Meta\",\"Amazon\",\"Google\",\"Mistral AI\",\"OpenAI\",\"Anthropic\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"1.0\",\"\",\"\",\"\",\"0.98\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"80.8\",\"\",\"\",\"0.72\",\"0.72\",\"0.87\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"80.6\",\"\",\"\",\"0.46\",\"0.53\",\"0.62\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"20.4\",\"0.29\",\"0.32\",\"0.35\",\"0.35\",\"0.36\",\"0.36\",\"0.37\",\"0.38\",\"0.42\",\"0.42\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"\",\"2\",\"20.2\",\"0.0\",\"1~\",\"9\",\"1\",\"1\",\"8\",\"2\",\"1\",\"8\",\"1ᴬ⁸\",\"1\",\"9\",\"1\",\"2\",\"1⁵\",\"2\",\"8\",\"1~\",\"8\",\"Tv\",\"2\",\"81\",\"A\",\"8\",\"3\",\"2\",\"8\",\"2\",\"2\",\"2\",\"\",\"\"\\n\"\",\"0\\'\",\"\",\"J\",\"\",\"!\",\"3\",\"3\",\"A\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"K\",\"\",\"{\",\"\",\"J\",\"\",\"8\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"283\",\"8²⁵⁰\",\"210\",\"190\",\"200\",\"163\",\"157\",\"157\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"8\",\"150\",\"\",\"124\",\"115\",\"113\",\"100\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"2\",\"100\",\"\",\"73\",\"64\",\"58\",\"57\",\"40\",\"35\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"3\",\"50\",\"1\",\"9\",\"9\",\"2\",\"2\",\"{\",\"2\",\"8\",\"2\",\"29\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"2\",\"8\",\"{\",\"2\",\"2\",\"8\",\"1⁵⁸\",\"3\",\"1\",\"2\",\"8\",\"1\",\"1\",\"~\",\"3\",\"1\",\"2\",\"8\",\"~\",\"3\",\"8\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"J\",\"1\",\"\",\"1\",\"8\",\"N?\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"J\",\"\",\"\",\"0\\'\",\"{\",\"\",\"8\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"4.0\",\"\",\"\",\"\",\"4.0\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"83.5\",\"\",\"\",\"2.8\",\"2.9\",\"3.4\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"23.0\",\"\",\"2.4\",\"2.7\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"L2.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"2.0\",\"\",\"1.3\",\"1.4\",\"1.5\",\"1.7\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"1.5\",\"0.8\",\"0.9\",\"1.0\",\"1.0\",\"1,.2\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"2\",\"1.0\",\"0.7\",\"0.5\",\"0.0\",\"8\",\"{\",\"0\",\"2\",\"2\",\"1\",\"g\",\"58\",\"~\",\"9\",\"8\",\"1\",\"2\",\"1\",\"2\",\"8\",\"1\",\"K\",\"2\",\"3\",\"2\",\"8\",\"2\",\"8\",\"{\",\"3\",\"8\",\"2\",\"3\",\"8\"\\n\"\",\"N?\",\"\",\"8\",\"K\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"J\",\"\",\"N\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"',\n",
              "      'bBox': {'x': 72, 'y': 34.04, 'w': 468, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Figure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of input and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov 29th, 2024).\\n\\n14',\n",
              "      'md': 'Figure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of input and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov 29th, 2024).\\n\\n14',\n",
              "      'bBox': {'x': 72, 'y': 128, 'w': 468, 'h': 622.04}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 15,\n",
              "    'text': '                                    The Amazon Nova Family of Models\\n3  Amazon Nova Canvas Evaluation\\nAmazon Nova Canvas is a diffusion model that takes a text prompt and an optional RGB image as input and generates\\nan image as an output conditioned on the input text and optional image. Illustrative examples of the images generated\\nby Amazon Nova Canvas can be found in our Amazon Science blog post 6. In this section, we provide details on the\\nevaluation strategy and performance of the model both in terms of automated metrics and human evaluation.\\n3.1  Automated metrics\\nWe use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n      •            ImageReward score is generated from a standardized reward model that aligns human preference with the\\n        predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-\\n        2014 [42] validation set and use this set for calculating the score.\\n      • Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated\\n        image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-\\n        selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66],\\n        PartiPrompts [84], and PaintSkill [21] datasets.\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3\\nMedium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.\\n                                                         TIFA    ImageReward\\n                            Amazon Nova Canvas          0.897       1.250\\n                            DALL.E 3                    0.863       1.052\\n                            Stable Diffusion 3.5 Large  0.891       1.082\\n                            Stable Diffusion 3 Medium   0.881       0.952\\n                            Flux Pro 1.0                0.875       1.075\\n                            Flux Schnell                0.882       0.999\\n       Table 8: Comparison of TIFA and ImageReward metrics of Amazon Nova Canvas with other models.\\n3.2  Human evaluation\\nWe conduct A/B testing to compare Amazon Nova Canvas with other third-party text-to-image models. The A/B\\ntesting prompt set is composed of approximately 1,000 prompts designed to capture customer usage of text-to-image\\nmodels. This set include prompts from datasets such as MSCOCO [42], Drawbench [66], OpenParti [84], DALL.E 3\\nEval [16], and DOCCI [54] and covers a broad set of categories such as humans, landscapes, natural scenarios, indoor\\nenvironments, creative themes, artistic themes, and so forth. A few prompts were randomly selected and repeated in\\norder to get additional data points on the quality of the model.\\nWith each prompt we generate an image from Amazon Nova Canvas as well as each other text-to-image model. We used\\nrandom seeds to generate the images from Amazon Nova Canvas and all images were generated at 1k x 1k resolution. If\\nthe prompts trigger filters such that an image is not generated, for either the Amazon Nova Canvas model or the public\\ntext-to-image model, we ignore that prompt and do not show it to the human raters. All human evaluation is done in\\na single-blind manner where the annotator is provided two sets of images, one from Amazon Nova Canvas and the\\nother from the third-party model. The order of the images are randomized for each prompt and annotator. In our blind\\ntesting, we ask human annotators to select images that they prefer based on (1) text-image alignment, which measures\\nthe instruction-following capability of the model, and (2) image quality, which quantifies the overall preference of the\\nannotators. To ensure rigorous, consistent, and unbiased evaluation, we used a third-party vendor for human evaluation.\\nWe created guidelines that were used to train the annotators so that the decision-making criteria were clear to them in\\neach dimension.\\nThe pair-wise results comparing Amazon Nova Canvas with OpenAI DALL.E 3 and Google Imagen 3 are shown in\\nTable 9, including win, tie, loss rate. The win rate reflects the percentage of samples where Amazon Nova Canvas was\\n  6 https://www.amazon.science/blog/amazon-nova-canvas-examples\\n                                                    15',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# 3  Amazon Nova Canvas Evaluation\\n\\nAmazon Nova Canvas is a diffusion model that takes a text prompt and an optional RGB image as input and generates an image as an output conditioned on the input text and optional image. Illustrative examples of the images generated by Amazon Nova Canvas can be found in our Amazon Science blog post 6. In this section, we provide details on the evaluation strategy and performance of the model both in terms of automated metrics and human evaluation.\\n\\n# 3.1  Automated metrics\\n\\nWe use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n\\n- ImageReward score is generated from a standardized reward model that aligns human preference with the predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-2014 [42] validation set and use this set for calculating the score.\\n- Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66], PartiPrompts [84], and PaintSkill [21] datasets.\\n\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3 Medium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.\\n\\n|TIFA|TIFA|ImageReward|\\n|---|---|\\n|Amazon Nova Canvas|0.897|1.250|\\n|DALL.E 3|0.863|1.052|\\n|Stable Diffusion 3.5 Large|0.891|1.082|\\n|Stable Diffusion 3 Medium|0.881|0.952|\\n|Flux Pro 1.0|0.875|1.075|\\n|Flux Schnell|0.882|0.999|\\n\\nTable 8: Comparison of TIFA and ImageReward metrics of Amazon Nova Canvas with other models.\\n\\n# 3.2  Human evaluation\\n\\nWe conduct A/B testing to compare Amazon Nova Canvas with other third-party text-to-image models. The A/B testing prompt set is composed of approximately 1,000 prompts designed to capture customer usage of text-to-image models. This set includes prompts from datasets such as MSCOCO [42], Drawbench [66], OpenParti [84], DALL.E 3 Eval [16], and DOCCI [54] and covers a broad set of categories such as humans, landscapes, natural scenarios, indoor environments, creative themes, artistic themes, and so forth. A few prompts were randomly selected and repeated in order to get additional data points on the quality of the model.\\n\\nWith each prompt we generate an image from Amazon Nova Canvas as well as each other text-to-image model. We used random seeds to generate the images from Amazon Nova Canvas and all images were generated at 1k x 1k resolution. If the prompts trigger filters such that an image is not generated, for either the Amazon Nova Canvas model or the public text-to-image model, we ignore that prompt and do not show it to the human raters. All human evaluation is done in a single-blind manner where the annotator is provided two sets of images, one from Amazon Nova Canvas and the other from the third-party model. The order of the images are randomized for each prompt and annotator. In our blind testing, we ask human annotators to select images that they prefer based on (1) text-image alignment, which measures the instruction-following capability of the model, and (2) image quality, which quantifies the overall preference of the annotators. To ensure rigorous, consistent, and unbiased evaluation, we used a third-party vendor for human evaluation. We created guidelines that were used to train the annotators so that the decision-making criteria were clear to them in each dimension.\\n\\nThe pair-wise results comparing Amazon Nova Canvas with OpenAI DALL.E 3 and Google Imagen 3 are shown in Table 9, including win, tie, loss rate. The win rate reflects the percentage of samples where Amazon Nova Canvas was...',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '3  Amazon Nova Canvas Evaluation',\n",
              "      'md': '# 3  Amazon Nova Canvas Evaluation',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 214, 'h': 272.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Amazon Nova Canvas is a diffusion model that takes a text prompt and an optional RGB image as input and generates an image as an output conditioned on the input text and optional image. Illustrative examples of the images generated by Amazon Nova Canvas can be found in our Amazon Science blog post 6. In this section, we provide details on the evaluation strategy and performance of the model both in terms of automated metrics and human evaluation.',\n",
              "      'md': 'Amazon Nova Canvas is a diffusion model that takes a text prompt and an optional RGB image as input and generates an image as an output conditioned on the input text and optional image. Illustrative examples of the images generated by Amazon Nova Canvas can be found in our Amazon Science blog post 6. In this section, we provide details on the evaluation strategy and performance of the model both in terms of automated metrics and human evaluation.',\n",
              "      'bBox': {'x': 71, 'y': 95.04, 'w': 469, 'h': 380}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '3.1  Automated metrics',\n",
              "      'md': '# 3.1  Automated metrics',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 103, 'h': 91.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n\\n- ImageReward score is generated from a standardized reward model that aligns human preference with the predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-2014 [42] validation set and use this set for calculating the score.\\n- Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66], PartiPrompts [84], and PaintSkill [21] datasets.\\n\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3 Medium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.',\n",
              "      'md': 'We use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n\\n- ImageReward score is generated from a standardized reward model that aligns human preference with the predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-2014 [42] validation set and use this set for calculating the score.\\n- Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66], PartiPrompts [84], and PaintSkill [21] datasets.\\n\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3 Medium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.',\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 470, 'h': 317.99}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       [],\n",
              "       [],\n",
              "       [],\n",
              "       [],\n",
              "       [],\n",
              "       ['TIFA', 'TIFA', 'ImageReward'],\n",
              "       ['---', '---'],\n",
              "       ['Amazon Nova Canvas', '0.897', '1.250'],\n",
              "       ['DALL.E 3', '0.863', '1.052'],\n",
              "       ['Stable Diffusion 3.5 Large', '0.891', '1.082'],\n",
              "       ['Stable Diffusion 3 Medium', '0.881', '0.952'],\n",
              "       ['Flux Pro 1.0', '0.875', '1.075'],\n",
              "       ['Flux Schnell', '0.882', '0.999']],\n",
              "      'md': 'We use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n- ImageReward score is generated from a standardized reward model that aligns human preference with the predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-2014 [42] validation set and use this set for calculating the score.\\n- Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66], PartiPrompts [84], and PaintSkill [21] datasets.\\n\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3 Medium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.\\n\\n|TIFA|TIFA|ImageReward|\\n|---|---|\\n|Amazon Nova Canvas|0.897|1.250|\\n|DALL.E 3|0.863|1.052|\\n|Stable Diffusion 3.5 Large|0.891|1.082|\\n|Stable Diffusion 3 Medium|0.881|0.952|\\n|Flux Pro 1.0|0.875|1.075|\\n|Flux Schnell|0.882|0.999|',\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\\n\\n\\n\\n\\n\"TIFA\",\"TIFA\",\"ImageReward\"\\n\"---\",\"---\"\\n\"Amazon Nova Canvas\",\"0.897\",\"1.250\"\\n\"DALL.E 3\",\"0.863\",\"1.052\"\\n\"Stable Diffusion 3.5 Large\",\"0.891\",\"1.082\"\\n\"Stable Diffusion 3 Medium\",\"0.881\",\"0.952\"\\n\"Flux Pro 1.0\",\"0.875\",\"1.075\"\\n\"Flux Schnell\",\"0.882\",\"0.999\"',\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 470, 'h': 631.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 8: Comparison of TIFA and ImageReward metrics of Amazon Nova Canvas with other models.',\n",
              "      'md': 'Table 8: Comparison of TIFA and ImageReward metrics of Amazon Nova Canvas with other models.',\n",
              "      'bBox': {'x': 103, 'y': 318.04, 'w': 404, 'h': 118}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '3.2  Human evaluation',\n",
              "      'md': '# 3.2  Human evaluation',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 101, 'h': 404.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We conduct A/B testing to compare Amazon Nova Canvas with other third-party text-to-image models. The A/B testing prompt set is composed of approximately 1,000 prompts designed to capture customer usage of text-to-image models. This set includes prompts from datasets such as MSCOCO [42], Drawbench [66], OpenParti [84], DALL.E 3 Eval [16], and DOCCI [54] and covers a broad set of categories such as humans, landscapes, natural scenarios, indoor environments, creative themes, artistic themes, and so forth. A few prompts were randomly selected and repeated in order to get additional data points on the quality of the model.\\n\\nWith each prompt we generate an image from Amazon Nova Canvas as well as each other text-to-image model. We used random seeds to generate the images from Amazon Nova Canvas and all images were generated at 1k x 1k resolution. If the prompts trigger filters such that an image is not generated, for either the Amazon Nova Canvas model or the public text-to-image model, we ignore that prompt and do not show it to the human raters. All human evaluation is done in a single-blind manner where the annotator is provided two sets of images, one from Amazon Nova Canvas and the other from the third-party model. The order of the images are randomized for each prompt and annotator. In our blind testing, we ask human annotators to select images that they prefer based on (1) text-image alignment, which measures the instruction-following capability of the model, and (2) image quality, which quantifies the overall preference of the annotators. To ensure rigorous, consistent, and unbiased evaluation, we used a third-party vendor for human evaluation. We created guidelines that were used to train the annotators so that the decision-making criteria were clear to them in each dimension.\\n\\nThe pair-wise results comparing Amazon Nova Canvas with OpenAI DALL.E 3 and Google Imagen 3 are shown in Table 9, including win, tie, loss rate. The win rate reflects the percentage of samples where Amazon Nova Canvas was...',\n",
              "      'md': 'We conduct A/B testing to compare Amazon Nova Canvas with other third-party text-to-image models. The A/B testing prompt set is composed of approximately 1,000 prompts designed to capture customer usage of text-to-image models. This set includes prompts from datasets such as MSCOCO [42], Drawbench [66], OpenParti [84], DALL.E 3 Eval [16], and DOCCI [54] and covers a broad set of categories such as humans, landscapes, natural scenarios, indoor environments, creative themes, artistic themes, and so forth. A few prompts were randomly selected and repeated in order to get additional data points on the quality of the model.\\n\\nWith each prompt we generate an image from Amazon Nova Canvas as well as each other text-to-image model. We used random seeds to generate the images from Amazon Nova Canvas and all images were generated at 1k x 1k resolution. If the prompts trigger filters such that an image is not generated, for either the Amazon Nova Canvas model or the public text-to-image model, we ignore that prompt and do not show it to the human raters. All human evaluation is done in a single-blind manner where the annotator is provided two sets of images, one from Amazon Nova Canvas and the other from the third-party model. The order of the images are randomized for each prompt and annotator. In our blind testing, we ask human annotators to select images that they prefer based on (1) text-image alignment, which measures the instruction-following capability of the model, and (2) image quality, which quantifies the overall preference of the annotators. To ensure rigorous, consistent, and unbiased evaluation, we used a third-party vendor for human evaluation. We created guidelines that were used to train the annotators so that the decision-making criteria were clear to them in each dimension.\\n\\nThe pair-wise results comparing Amazon Nova Canvas with OpenAI DALL.E 3 and Google Imagen 3 are shown in Table 9, including win, tie, loss rate. The win rate reflects the percentage of samples where Amazon Nova Canvas was...',\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 471, 'h': 631.99}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://www.amazon.science/blog/amazon-nova-canvas-examples',\n",
              "      'text': 'https://www.amazon.science/blog/amazon-nova-canvas-examples'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 16,\n",
              "    'text': '                                      The Amazon Nova Family of Models\\npreferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a\\ndifference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared\\nto the other text-to-image models.\\n   Nova Canvas versus:                                    DALL.E 3                         Imagen 3\\n                                                 win rate   tie rate loss rate   win rate   tie rate loss rate\\n   Overall preference (image quality)              54.5       6.4       39.1       48.2       5.3       46.5\\n   Instruction following (text-image alignment)    39.4       22.5      38.1       38.4       28.1      33.5\\nTable 9: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Canvas versus (a) DALL.E 3 and (b)\\nImagen 3.\\n4   Amazon Nova Reel Evaluation\\nAmazon Nova Reel is a diffusion model that takes a text prompt and an optional RGB image as input and generates\\na video as an output conditioned on the input text and optional image. Illustrative examples of the videos generated\\nby the Amazon Nova Reel can be found in our Amazon Science blog post.7   In this section, we provide details on the\\nevaluation strategy and performance of the model.\\n4.1  Human evaluation metrics\\nTo evaluate Amazon Nova Reel, we rely on human feedback to assess the generated videos across two primary axes:\\nvideo quality and video consistency. All evaluations are conducted through single-blind pairwise comparisons. Human\\nannotators are provided a set of two videos shown side-by-side and are asked to choose the better video or mark them\\nas equal if they find the videos to be equally performant across the metric on which they are evaluating. All videos were\\ngenerated in 720p resolution and different random seeds were used during generation.\\nThe video quality axis encapsulates the technical and perceptual aspects of the generated video via four primary\\ncomponents:\\n      • Image quality: The visual appeal of individual frames, including resolution, sharpness, object clarity, and\\n        overall composition, where each frame is visually pleasing and artifact-free.\\n      • Motion quality: The fluidity of movement across frames, including motion consistency and smooth transitions\\n        without flickering, distortion, or abrupt shifts, contributing to natural and realistic motion portrayal.\\n      • Image-text alignment: How closely individual frames match the prompt, considering the presence of described\\n        entities, their attributes, spatial relationships, colors, and other static visual details.\\n      •     Motion-text alignment: The accuracy of dynamic elements, including the correctness of actions performed\\n            by entities, camera movements, and temporal changes in attributes, as well as adherence to the provided\\n        description.\\nThe video quality axis additionally includes factors influencing overall appeal, such as motion degree, entity size,\\ncreative composition, and general video likability.\\nThe video consistency axis encapsulates the temporal coherence of both subjects and backgrounds throughout the\\nvideo. It includes assessments of the maintenance of entity size, shape, and appearance, as well as background stability\\nwithout unexpected morphing or changes. A high score in this dimension means believable spatial relationships between\\nforeground and background elements throughout the video duration.\\nIn combination, the video quality and video consistency metrics provide a holistic and robust evaluation framework for\\nvideo generation models by considering both technical accuracy and perceptual appeal.\\n4.2  Dataset\\nWe curated a diverse set of prompts designed to capture various aspects of video generation. The prompts are distributed\\nacross 6 broad categories: human and activities, animals, natural scenery and landscapes, indoor scenes, objects\\n   7https://www.amazon.science/blog/amazon-nova-reel-examples\\n                                                       16',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\npreferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a difference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared to the other text-to-image models.\\n\\n| | |DALL.E 3| | |Imagen 3| |\\n|---|---|---|---|---|---|---|\\n| |win rate|tie rate|loss rate|win rate|tie rate|loss rate|\\n|Overall preference (image quality)|54.5|6.4|39.1|48.2|5.3|46.5|\\n|Instruction following (text-image alignment)|39.4|22.5|38.1|38.4|28.1|33.5|\\n\\nTable 9: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Canvas versus (a) DALL.E 3 and (b) Imagen 3.\\n\\n# 4 Amazon Nova Reel Evaluation\\n\\nAmazon Nova Reel is a diffusion model that takes a text prompt and an optional RGB image as input and generates a video as an output conditioned on the input text and optional image. Illustrative examples of the videos generated by the Amazon Nova Reel can be found in our Amazon Science blog post.7 In this section, we provide details on the evaluation strategy and performance of the model.\\n\\n# 4.1 Human evaluation metrics\\n\\nTo evaluate Amazon Nova Reel, we rely on human feedback to assess the generated videos across two primary axes: video quality and video consistency. All evaluations are conducted through single-blind pairwise comparisons. Human annotators are provided a set of two videos shown side-by-side and are asked to choose the better video or mark them as equal if they find the videos to be equally performant across the metric on which they are evaluating. All videos were generated in 720p resolution and different random seeds were used during generation.\\n\\nThe video quality axis encapsulates the technical and perceptual aspects of the generated video via four primary components:\\n\\n- Image quality: The visual appeal of individual frames, including resolution, sharpness, object clarity, and overall composition, where each frame is visually pleasing and artifact-free.\\n- Motion quality: The fluidity of movement across frames, including motion consistency and smooth transitions without flickering, distortion, or abrupt shifts, contributing to natural and realistic motion portrayal.\\n- Image-text alignment: How closely individual frames match the prompt, considering the presence of described entities, their attributes, spatial relationships, colors, and other static visual details.\\n- Motion-text alignment: The accuracy of dynamic elements, including the correctness of actions performed by entities, camera movements, and temporal changes in attributes, as well as adherence to the provided description.\\n\\nThe video quality axis additionally includes factors influencing overall appeal, such as motion degree, entity size, creative composition, and general video likability.\\n\\nThe video consistency axis encapsulates the temporal coherence of both subjects and backgrounds throughout the video. It includes assessments of the maintenance of entity size, shape, and appearance, as well as background stability without unexpected morphing or changes. A high score in this dimension means believable spatial relationships between foreground and background elements throughout the video duration.\\n\\nIn combination, the video quality and video consistency metrics provide a holistic and robust evaluation framework for video generation models by considering both technical accuracy and perceptual appeal.\\n\\n# 4.2 Dataset\\n\\nWe curated a diverse set of prompts designed to capture various aspects of video generation. The prompts are distributed across 6 broad categories: human and activities, animals, natural scenery and landscapes, indoor scenes, objects.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'preferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a difference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared to the other text-to-image models.',\n",
              "      'md': 'preferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a difference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared to the other text-to-image models.',\n",
              "      'bBox': {'x': 72, 'y': 72.04, 'w': 468, 'h': 72}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       ['', '', 'DALL.E 3', '', '', 'Imagen 3', ''],\n",
              "       ['---', '---', '---', '---', '---', '---', '---'],\n",
              "       ['',\n",
              "        'win rate',\n",
              "        'tie rate',\n",
              "        'loss rate',\n",
              "        'win rate',\n",
              "        'tie rate',\n",
              "        'loss rate'],\n",
              "       ['Overall preference (image quality)',\n",
              "        '54.5',\n",
              "        '6.4',\n",
              "        '39.1',\n",
              "        '48.2',\n",
              "        '5.3',\n",
              "        '46.5'],\n",
              "       ['Instruction following (text-image alignment)',\n",
              "        '39.4',\n",
              "        '22.5',\n",
              "        '38.1',\n",
              "        '38.4',\n",
              "        '28.1',\n",
              "        '33.5']],\n",
              "      'md': 'preferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a difference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared to the other text-to-image models.\\n| | |DALL.E 3| | |Imagen 3| |\\n|---|---|---|---|---|---|---|\\n| |win rate|tie rate|loss rate|win rate|tie rate|loss rate|\\n|Overall preference (image quality)|54.5|6.4|39.1|48.2|5.3|46.5|\\n|Instruction following (text-image alignment)|39.4|22.5|38.1|38.4|28.1|33.5|',\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\"\",\"\",\"DALL.E 3\",\"\",\"\",\"Imagen 3\",\"\"\\n\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\\n\"\",\"win rate\",\"tie rate\",\"loss rate\",\"win rate\",\"tie rate\",\"loss rate\"\\n\"Overall preference (image quality)\",\"54.5\",\"6.4\",\"39.1\",\"48.2\",\"5.3\",\"46.5\"\\n\"Instruction following (text-image alignment)\",\"39.4\",\"22.5\",\"38.1\",\"38.4\",\"28.1\",\"33.5\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 9: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Canvas versus (a) DALL.E 3 and (b) Imagen 3.',\n",
              "      'md': 'Table 9: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Canvas versus (a) DALL.E 3 and (b) Imagen 3.',\n",
              "      'bBox': {'x': 71, 'y': 119.04, 'w': 469, 'h': 85}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '4 Amazon Nova Reel Evaluation',\n",
              "      'md': '# 4 Amazon Nova Reel Evaluation',\n",
              "      'bBox': {'x': 72, 'y': 235.04, 'w': 173, 'h': 12}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Amazon Nova Reel is a diffusion model that takes a text prompt and an optional RGB image as input and generates a video as an output conditioned on the input text and optional image. Illustrative examples of the videos generated by the Amazon Nova Reel can be found in our Amazon Science blog post.7 In this section, we provide details on the evaluation strategy and performance of the model.',\n",
              "      'md': 'Amazon Nova Reel is a diffusion model that takes a text prompt and an optional RGB image as input and generates a video as an output conditioned on the input text and optional image. Illustrative examples of the videos generated by the Amazon Nova Reel can be found in our Amazon Science blog post.7 In this section, we provide details on the evaluation strategy and performance of the model.',\n",
              "      'bBox': {'x': 71, 'y': 260.04, 'w': 469, 'h': 43}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '4.1 Human evaluation metrics',\n",
              "      'md': '# 4.1 Human evaluation metrics',\n",
              "      'bBox': {'x': 72, 'y': 235.04, 'w': 135, 'h': 91.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'To evaluate Amazon Nova Reel, we rely on human feedback to assess the generated videos across two primary axes: video quality and video consistency. All evaluations are conducted through single-blind pairwise comparisons. Human annotators are provided a set of two videos shown side-by-side and are asked to choose the better video or mark them as equal if they find the videos to be equally performant across the metric on which they are evaluating. All videos were generated in 720p resolution and different random seeds were used during generation.\\n\\nThe video quality axis encapsulates the technical and perceptual aspects of the generated video via four primary components:\\n\\n- Image quality: The visual appeal of individual frames, including resolution, sharpness, object clarity, and overall composition, where each frame is visually pleasing and artifact-free.\\n- Motion quality: The fluidity of movement across frames, including motion consistency and smooth transitions without flickering, distortion, or abrupt shifts, contributing to natural and realistic motion portrayal.\\n- Image-text alignment: How closely individual frames match the prompt, considering the presence of described entities, their attributes, spatial relationships, colors, and other static visual details.\\n- Motion-text alignment: The accuracy of dynamic elements, including the correctness of actions performed by entities, camera movements, and temporal changes in attributes, as well as adherence to the provided description.\\n\\nThe video quality axis additionally includes factors influencing overall appeal, such as motion degree, entity size, creative composition, and general video likability.\\n\\nThe video consistency axis encapsulates the temporal coherence of both subjects and backgrounds throughout the video. It includes assessments of the maintenance of entity size, shape, and appearance, as well as background stability without unexpected morphing or changes. A high score in this dimension means believable spatial relationships between foreground and background elements throughout the video duration.\\n\\nIn combination, the video quality and video consistency metrics provide a holistic and robust evaluation framework for video generation models by considering both technical accuracy and perceptual appeal.',\n",
              "      'md': 'To evaluate Amazon Nova Reel, we rely on human feedback to assess the generated videos across two primary axes: video quality and video consistency. All evaluations are conducted through single-blind pairwise comparisons. Human annotators are provided a set of two videos shown side-by-side and are asked to choose the better video or mark them as equal if they find the videos to be equally performant across the metric on which they are evaluating. All videos were generated in 720p resolution and different random seeds were used during generation.\\n\\nThe video quality axis encapsulates the technical and perceptual aspects of the generated video via four primary components:\\n\\n- Image quality: The visual appeal of individual frames, including resolution, sharpness, object clarity, and overall composition, where each frame is visually pleasing and artifact-free.\\n- Motion quality: The fluidity of movement across frames, including motion consistency and smooth transitions without flickering, distortion, or abrupt shifts, contributing to natural and realistic motion portrayal.\\n- Image-text alignment: How closely individual frames match the prompt, considering the presence of described entities, their attributes, spatial relationships, colors, and other static visual details.\\n- Motion-text alignment: The accuracy of dynamic elements, including the correctness of actions performed by entities, camera movements, and temporal changes in attributes, as well as adherence to the provided description.\\n\\nThe video quality axis additionally includes factors influencing overall appeal, such as motion degree, entity size, creative composition, and general video likability.\\n\\nThe video consistency axis encapsulates the temporal coherence of both subjects and backgrounds throughout the video. It includes assessments of the maintenance of entity size, shape, and appearance, as well as background stability without unexpected morphing or changes. A high score in this dimension means believable spatial relationships between foreground and background elements throughout the video duration.\\n\\nIn combination, the video quality and video consistency metrics provide a holistic and robust evaluation framework for video generation models by considering both technical accuracy and perceptual appeal.',\n",
              "      'bBox': {'x': 71, 'y': 337.04, 'w': 470, 'h': 309}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '4.2 Dataset',\n",
              "      'md': '# 4.2 Dataset',\n",
              "      'bBox': {'x': 72, 'y': 235.04, 'w': 54, 'h': 435.99}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We curated a diverse set of prompts designed to capture various aspects of video generation. The prompts are distributed across 6 broad categories: human and activities, animals, natural scenery and landscapes, indoor scenes, objects.',\n",
              "      'md': 'We curated a diverse set of prompts designed to capture various aspects of video generation. The prompts are distributed across 6 broad categories: human and activities, animals, natural scenery and landscapes, indoor scenes, objects.',\n",
              "      'bBox': {'x': 71, 'y': 681.04, 'w': 469, 'h': 20}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://www.amazon.science/blog/amazon-nova-reel-examples',\n",
              "      'text': 'https://www.amazon.science/blog/amazon-nova-reel-examples'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 17,\n",
              "    'text': '                                       The Amazon Nova Family of Models\\ninteractions, and creative scenes and activities. This broad categorization ensures that the evaluation covers a wide\\nrange of real-world scenarios. We structured the prompt set to cover various motion-related aspects, which is critical\\nfor assessing motion-text alignment in the generated videos. For example, we included prompts with a variety of\\ncamera motions to evaluate how well the models follow instructions related to camera movement. Additionally, we\\nincorporated dynamic attributes [71], in which the subject or background undergoes state or shape changes over time,\\nwhich allows us to evaluate the model’s ability to generate evolving entities. Finally, we added prompts that require\\nmotion binding [71], where specific compositions of movements and actions are requested, enabling us to assess how\\nwell models can generate complex, coordinated motions. The curated prompt set consists of approximately 700 prompts,\\nall from various open source benchmarks.\\n4.3   Implementation details & results\\nTo ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a\\nthird-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and\\nexamples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These\\nguidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside\\nthis, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to\\nspot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators.\\nBased on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high\\nstandard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus\\nvoting system. For each video comparison, annotations were collected from three different evaluators, and a majority\\nvoting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that\\nthe final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art\\nmodels including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of\\nwin, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over\\nthe other model, while the tie rate indicates cases where no perceptible difference between the two models was found\\nby the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions\\noutlined above, and report the results in Table 10.\\n                Nova Reel versus:        Runway Gen3 Alpha                     Luma 1.6\\n                                      win rate  tie rate loss rate   win rate  tie rate  loss rate\\n                Video Quality          56.4       9.9       33.7       51.1      3.4        45.5\\n                Video Consistency      67.0       9.1       23.9       74.7       5.1       20.2\\nTable 10: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Reel versus (a) Gen3-Alpha and (b)\\nLuma1.6.\\nIn video consistency, Amazon Nova Reel achieved win rates of 67.0% against Gen3 Alpha and 74.7% against Luma\\n1.6, demonstrating superior subject and background coherence. For video quality, Amazon Nova Reel secured win rates\\nof 56.4% against Gen3 Alpha and 51.1% against Luma 1.6.\\n5    Responsible AI\\nOur approach to Responsible AI (RAI) is structured around eight foundational dimensions [10] shown in Table 11.\\nThese dimensions guide our approach to RAI for the Amazon Nova family of models, which we articulate in the\\nfollowing three sections: (1) defining our RAI design objectives, (2) our actions to ensure adherence to these objectives,\\nand (3) system evaluation and red teaming. The last two components form a continuous loop of model development and\\nhuman/automated verification to ensure that our Amazon Nova models are aligned with our RAI objectives and deliver\\nan exceptional and delightful customer experience.\\n5.1   Defining our RAI objectives\\nWe operationalize our RAI dimensions into a series of detailed design objectives that guide our decision-making\\nthroughout the entire model development lifecycle, from initial data collection and pre-training to the implementation\\nof post-deployment runtime mitigations.\\n                                                       17',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\ninteractions, and creative scenes and activities. This broad categorization ensures that the evaluation covers a wide range of real-world scenarios. We structured the prompt set to cover various motion-related aspects, which is critical for assessing motion-text alignment in the generated videos. For example, we included prompts with a variety of camera motions to evaluate how well the models follow instructions related to camera movement. Additionally, we incorporated dynamic attributes [71], in which the subject or background undergoes state or shape changes over time, which allows us to evaluate the model’s ability to generate evolving entities. Finally, we added prompts that require motion binding [71], where specific compositions of movements and actions are requested, enabling us to assess how well models can generate complex, coordinated motions. The curated prompt set consists of approximately 700 prompts, all from various open source benchmarks.\\n\\n# 4.3 Implementation details & results\\n\\nTo ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a third-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and examples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These guidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside this, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to spot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators. Based on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high standard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus voting system. For each video comparison, annotations were collected from three different evaluators, and a majority voting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that the final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\n\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art models including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of win, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over the other model, while the tie rate indicates cases where no perceptible difference between the two models was found by the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions outlined above, and report the results in Table 10.\\n\\n|Nova Reel versus:|Runway Gen3 Alpha| |Luma 1.6| | | |\\n|---|---|---|---|---|---|---|\\n| |win rate|tie rate|loss rate|win rate|tie rate|loss rate|\\n|Video Quality|56.4|9.9|33.7|51.1|3.4|45.5|\\n|Video Consistency|67.0|9.1|23.9|74.7|5.1|20.2|\\n\\nTable 10: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Reel versus (a) Gen3-Alpha and (b) Luma1.6.\\n\\nIn video consistency, Amazon Nova Reel achieved win rates of 67.0% against Gen3 Alpha and 74.7% against Luma 1.6, demonstrating superior subject and background coherence. For video quality, Amazon Nova Reel secured win rates of 56.4% against Gen3 Alpha and 51.1% against Luma 1.6.\\n\\n# 5 Responsible AI\\n\\nOur approach to Responsible AI (RAI) is structured around eight foundational dimensions [10] shown in Table 11. These dimensions guide our approach to RAI for the Amazon Nova family of models, which we articulate in the following three sections: (1) defining our RAI design objectives, (2) our actions to ensure adherence to these objectives, and (3) system evaluation and red teaming. The last two components form a continuous loop of model development and human/automated verification to ensure that our Amazon Nova models are aligned with our RAI objectives and deliver an exceptional and delightful customer experience.\\n\\n# 5.1 Defining our RAI objectives\\n\\nWe operationalize our RAI dimensions into a series of detailed design objectives that guide our decision-making throughout the entire model development lifecycle, from initial data collection and pre-training to the implementation of post-deployment runtime mitigations.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'interactions, and creative scenes and activities. This broad categorization ensures that the evaluation covers a wide range of real-world scenarios. We structured the prompt set to cover various motion-related aspects, which is critical for assessing motion-text alignment in the generated videos. For example, we included prompts with a variety of camera motions to evaluate how well the models follow instructions related to camera movement. Additionally, we incorporated dynamic attributes [71], in which the subject or background undergoes state or shape changes over time, which allows us to evaluate the model’s ability to generate evolving entities. Finally, we added prompts that require motion binding [71], where specific compositions of movements and actions are requested, enabling us to assess how well models can generate complex, coordinated motions. The curated prompt set consists of approximately 700 prompts, all from various open source benchmarks.',\n",
              "      'md': 'interactions, and creative scenes and activities. This broad categorization ensures that the evaluation covers a wide range of real-world scenarios. We structured the prompt set to cover various motion-related aspects, which is critical for assessing motion-text alignment in the generated videos. For example, we included prompts with a variety of camera motions to evaluate how well the models follow instructions related to camera movement. Additionally, we incorporated dynamic attributes [71], in which the subject or background undergoes state or shape changes over time, which allows us to evaluate the model’s ability to generate evolving entities. Finally, we added prompts that require motion binding [71], where specific compositions of movements and actions are requested, enabling us to assess how well models can generate complex, coordinated motions. The curated prompt set consists of approximately 700 prompts, all from various open source benchmarks.',\n",
              "      'bBox': {'x': 71, 'y': 72.04, 'w': 470, 'h': 98}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '4.3 Implementation details & results',\n",
              "      'md': '# 4.3 Implementation details & results',\n",
              "      'bBox': {'x': 72, 'y': 185.04, 'w': 162, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'To ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a third-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and examples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These guidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside this, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to spot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators. Based on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high standard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus voting system. For each video comparison, annotations were collected from three different evaluators, and a majority voting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that the final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\n\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art models including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of win, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over the other model, while the tie rate indicates cases where no perceptible difference between the two models was found by the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions outlined above, and report the results in Table 10.',\n",
              "      'md': 'To ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a third-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and examples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These guidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside this, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to spot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators. Based on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high standard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus voting system. For each video comparison, annotations were collected from three different evaluators, and a majority voting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that the final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\n\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art models including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of win, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over the other model, while the tie rate indicates cases where no perceptible difference between the two models was found by the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions outlined above, and report the results in Table 10.',\n",
              "      'bBox': {'x': 71, 'y': 205.04, 'w': 471, 'h': 371.01}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       [],\n",
              "       [],\n",
              "       ['Nova Reel versus:', 'Runway Gen3 Alpha', '', 'Luma 1.6', '', '', ''],\n",
              "       ['---', '---', '---', '---', '---', '---', '---'],\n",
              "       ['',\n",
              "        'win rate',\n",
              "        'tie rate',\n",
              "        'loss rate',\n",
              "        'win rate',\n",
              "        'tie rate',\n",
              "        'loss rate'],\n",
              "       ['Video Quality', '56.4', '9.9', '33.7', '51.1', '3.4', '45.5'],\n",
              "       ['Video Consistency', '67.0', '9.1', '23.9', '74.7', '5.1', '20.2']],\n",
              "      'md': 'To ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a third-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and examples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These guidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside this, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to spot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators. Based on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high standard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus voting system. For each video comparison, annotations were collected from three different evaluators, and a majority voting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that the final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art models including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of win, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over the other model, while the tie rate indicates cases where no perceptible difference between the two models was found by the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions outlined above, and report the results in Table 10.\\n\\n|Nova Reel versus:|Runway Gen3 Alpha| |Luma 1.6| | | |\\n|---|---|---|---|---|---|---|\\n| |win rate|tie rate|loss rate|win rate|tie rate|loss rate|\\n|Video Quality|56.4|9.9|33.7|51.1|3.4|45.5|\\n|Video Consistency|67.0|9.1|23.9|74.7|5.1|20.2|',\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\\n\\n\"Nova Reel versus:\",\"Runway Gen3 Alpha\",\"\",\"Luma 1.6\",\"\",\"\",\"\"\\n\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\\n\"\",\"win rate\",\"tie rate\",\"loss rate\",\"win rate\",\"tie rate\",\"loss rate\"\\n\"Video Quality\",\"56.4\",\"9.9\",\"33.7\",\"51.1\",\"3.4\",\"45.5\"\\n\"Video Consistency\",\"67.0\",\"9.1\",\"23.9\",\"74.7\",\"5.1\",\"20.2\"',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 10: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Reel versus (a) Gen3-Alpha and (b) Luma1.6.\\n\\nIn video consistency, Amazon Nova Reel achieved win rates of 67.0% against Gen3 Alpha and 74.7% against Luma 1.6, demonstrating superior subject and background coherence. For video quality, Amazon Nova Reel secured win rates of 56.4% against Gen3 Alpha and 51.1% against Luma 1.6.',\n",
              "      'md': 'Table 10: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Reel versus (a) Gen3-Alpha and (b) Luma1.6.\\n\\nIn video consistency, Amazon Nova Reel achieved win rates of 67.0% against Gen3 Alpha and 74.7% against Luma 1.6, demonstrating superior subject and background coherence. For video quality, Amazon Nova Reel secured win rates of 56.4% against Gen3 Alpha and 51.1% against Luma 1.6.',\n",
              "      'bBox': {'x': 71, 'y': 409.04, 'w': 469, 'h': 167.01}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '5 Responsible AI',\n",
              "      'md': '# 5 Responsible AI',\n",
              "      'bBox': {'x': 72, 'y': 564.04, 'w': 94, 'h': 12}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Our approach to Responsible AI (RAI) is structured around eight foundational dimensions [10] shown in Table 11. These dimensions guide our approach to RAI for the Amazon Nova family of models, which we articulate in the following three sections: (1) defining our RAI design objectives, (2) our actions to ensure adherence to these objectives, and (3) system evaluation and red teaming. The last two components form a continuous loop of model development and human/automated verification to ensure that our Amazon Nova models are aligned with our RAI objectives and deliver an exceptional and delightful customer experience.',\n",
              "      'md': 'Our approach to Responsible AI (RAI) is structured around eight foundational dimensions [10] shown in Table 11. These dimensions guide our approach to RAI for the Amazon Nova family of models, which we articulate in the following three sections: (1) defining our RAI design objectives, (2) our actions to ensure adherence to these objectives, and (3) system evaluation and red teaming. The last two components form a continuous loop of model development and human/automated verification to ensure that our Amazon Nova models are aligned with our RAI objectives and deliver an exceptional and delightful customer experience.',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 620}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '5.1 Defining our RAI objectives',\n",
              "      'md': '# 5.1 Defining our RAI objectives',\n",
              "      'bBox': {'x': 72, 'y': 455.04, 'w': 349, 'h': 224}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We operationalize our RAI dimensions into a series of detailed design objectives that guide our decision-making throughout the entire model development lifecycle, from initial data collection and pre-training to the implementation of post-deployment runtime mitigations.',\n",
              "      'md': 'We operationalize our RAI dimensions into a series of detailed design objectives that guide our decision-making throughout the entire model development lifecycle, from initial data collection and pre-training to the implementation of post-deployment runtime mitigations.',\n",
              "      'bBox': {'x': 71, 'y': 689.04, 'w': 469, 'h': 31}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 18,\n",
              "    'text': '                                       The Amazon Nova Family of Models\\n  Term                          Definition\\n  Fairness                      Considering impacts on different groups of stakeholders\\n  Explainability                Understanding and evaluating system outputs\\n  Privacy and security          Appropriately obtaining, using, and protecting data and models\\n  Safety                        Preventing harmful system output and misuse\\n  Controllability               Having mechanisms to monitor and steer AI system behavior\\n  Veracity and robustness       Achieving correct system outputs, even with unexpected or adversarial inputs\\n  Governance                    Incorporating best practices into the AI supply chain, including providers and\\n                                deployers\\n  Transparency                  Enabling stakeholders to make informed choices about their engagement with an AI\\n                                system\\n                                Table 11: Our eight core Responsible AI dimensions\\n In addition to being grounded on the RAI dimensions, our objectives are informed by relevant laws and regulations,\\n voluntary frameworks, and our commitments to our customers, and they undergo an internal alignment process that\\n includes reviews from a number of stakeholders.      We will continue to iterate on these objections as we engage\\n with external experts and participate in industry and government forums, including the Frontier Model Forum [29],\\n Partnership on AI [5], and various forums organized by government agencies such as the National Institute of Standards\\n and Technology (NIST) of the U.S. Department of Commerce [7].\\n Our commitment to Responsible Scaling:      As the capabilities of AI models increase (through increased training\\n data, model size or architecture innovations), so do the potential risks that they present. We joined other technology\\n companies in signing on to the White House’s voluntary commitments on the safe, secure, and transparent development\\n and use of foundation models [6]. Since then we have actively participated in other efforts, including the AI Safety\\n Summits in the UK and Seoul, and we have committed to new standards like the G7 AI Hiroshima Process Code of\\n Conduct [30] in accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy\\n Development and Use of Artificial Intelligence. We also started a partnership with the Model Evaluation and Threat\\n Research (METR) center8 to enrich our Controllability design objectives.\\n 5.2  Ensuring adherence to RAI objectives\\nWe employed a number of methods to measure and ensure compliance for each of our core RAI dimensions depending\\n on their scope (i.e., whether they apply to model output, data management or other processes). For the dimensions\\n that govern model behavior (Safety, Fairness, Veracity and Robustness, Controllability, and Privacy and Security), we\\n curated the pre-training data and we used both Supervised Fine Tuning (SFT) and Reinforcement Learning from Human\\n Feedback (RLHF) methods to align our models. Based on the objectives for each RAI dimension, we created single-\\n and multi-turn RAI demonstrations in multiple languages and conducted helpfulness/harmfulness studies to decide on\\n SFT data mixes. We collected human preference data to be used as inputs to RLHF training where we also provided an\\n RAI-specific reward model. We also identify risk areas during our offline evaluation or red teaming exercises (Section\\n 5.4) and collect semantically similar examples to be included in future SFT and RLHF rounds.\\n In addition to the RAI model alignment, we built runtime input and output moderation models which serve as a first and\\n last line of defense and allow us to respond more quickly to newly identified threats or gaps in model alignment. The\\n main role of the input moderation model is to detect prompts that contain malicious, insecure or illegal material, or\\n attempt to bypass the core model alignment (prompt injection, jailbreaking). Similarly, the output moderation ensures\\n that the content adheres to our RAI objectives.\\n      We have a rigorous Governance methodology, developing our models in a working-backwards product process that\\n incorporates RAI at the design phase, design consultations and implementation assessments by dedicated RAI science\\n and data experts, and includes routine testing, reviews with customers, best practice development, dissemination, and\\n training.\\n   8https://metr.org/\\n                                                       18',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n|Term|Definition|\\n|---|---|\\n|Fairness|Considering impacts on different groups of stakeholders|\\n|Explainability|Understanding and evaluating system outputs|\\n|Privacy and security|Appropriately obtaining, using, and protecting data and models|\\n|Safety|Preventing harmful system output and misuse|\\n|Controllability|Having mechanisms to monitor and steer AI system behavior|\\n|Veracity and robustness|Achieving correct system outputs, even with unexpected or adversarial inputs|\\n|Governance|Incorporating best practices into the AI supply chain, including providers and deployers|\\n|Transparency|Enabling stakeholders to make informed choices about their engagement with an AI system|\\n\\nTable 11: Our eight core Responsible AI dimensions\\n\\nIn addition to being grounded on the RAI dimensions, our objectives are informed by relevant laws and regulations, voluntary frameworks, and our commitments to our customers, and they undergo an internal alignment process that includes reviews from a number of stakeholders. We will continue to iterate on these objections as we engage with external experts and participate in industry and government forums, including the Frontier Model Forum [29], Partnership on AI [5], and various forums organized by government agencies such as the National Institute of Standards and Technology (NIST) of the U.S. Department of Commerce [7].\\n\\nOur commitment to Responsible Scaling: As the capabilities of AI models increase (through increased training data, model size or architecture innovations), so do the potential risks that they present. We joined other technology companies in signing on to the White House’s voluntary commitments on the safe, secure, and transparent development and use of foundation models [6]. Since then we have actively participated in other efforts, including the AI Safety Summits in the UK and Seoul, and we have committed to new standards like the G7 AI Hiroshima Process Code of Conduct [30] in accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We also started a partnership with the Model Evaluation and Threat Research (METR) center8 to enrich our Controllability design objectives.\\n\\n# 5.2 Ensuring adherence to RAI objectives\\n\\nWe employed a number of methods to measure and ensure compliance for each of our core RAI dimensions depending on their scope (i.e., whether they apply to model output, data management or other processes). For the dimensions that govern model behavior (Safety, Fairness, Veracity and Robustness, Controllability, and Privacy and Security), we curated the pre-training data and we used both Supervised Fine Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methods to align our models. Based on the objectives for each RAI dimension, we created single- and multi-turn RAI demonstrations in multiple languages and conducted helpfulness/harmfulness studies to decide on SFT data mixes. We collected human preference data to be used as inputs to RLHF training where we also provided an RAI-specific reward model. We also identify risk areas during our offline evaluation or red teaming exercises (Section 5.4) and collect semantically similar examples to be included in future SFT and RLHF rounds.\\n\\nIn addition to the RAI model alignment, we built runtime input and output moderation models which serve as a first and last line of defense and allow us to respond more quickly to newly identified threats or gaps in model alignment. The main role of the input moderation model is to detect prompts that contain malicious, insecure or illegal material, or attempt to bypass the core model alignment (prompt injection, jailbreaking). Similarly, the output moderation ensures that the content adheres to our RAI objectives.\\n\\nWe have a rigorous Governance methodology, developing our models in a working-backwards product process that incorporates RAI at the design phase, design consultations and implementation assessments by dedicated RAI science and data experts, and includes routine testing, reviews with customers, best practice development, dissemination, and training.\\n\\n8https://metr.org/',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['Term', 'Definition'],\n",
              "       ['Fairness', 'Considering impacts on different groups of stakeholders'],\n",
              "       ['Explainability', 'Understanding and evaluating system outputs'],\n",
              "       ['Privacy and security',\n",
              "        'Appropriately obtaining, using, and protecting data and models'],\n",
              "       ['Safety', 'Preventing harmful system output and misuse'],\n",
              "       ['Controllability',\n",
              "        'Having mechanisms to monitor and steer AI system behavior'],\n",
              "       ['Veracity and robustness',\n",
              "        'Achieving correct system outputs, even with unexpected or adversarial inputs'],\n",
              "       ['Governance',\n",
              "        'Incorporating best practices into the AI supply chain, including providers and deployers'],\n",
              "       ['Transparency',\n",
              "        'Enabling stakeholders to make informed choices about their engagement with an AI system']],\n",
              "      'md': '|Term|Definition|\\n|Fairness|Considering impacts on different groups of stakeholders|\\n|Explainability|Understanding and evaluating system outputs|\\n|Privacy and security|Appropriately obtaining, using, and protecting data and models|\\n|Safety|Preventing harmful system output and misuse|\\n|Controllability|Having mechanisms to monitor and steer AI system behavior|\\n|Veracity and robustness|Achieving correct system outputs, even with unexpected or adversarial inputs|\\n|Governance|Incorporating best practices into the AI supply chain, including providers and deployers|\\n|Transparency|Enabling stakeholders to make informed choices about their engagement with an AI system|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"Term\",\"Definition\"\\n\"Fairness\",\"Considering impacts on different groups of stakeholders\"\\n\"Explainability\",\"Understanding and evaluating system outputs\"\\n\"Privacy and security\",\"Appropriately obtaining, using, and protecting data and models\"\\n\"Safety\",\"Preventing harmful system output and misuse\"\\n\"Controllability\",\"Having mechanisms to monitor and steer AI system behavior\"\\n\"Veracity and robustness\",\"Achieving correct system outputs, even with unexpected or adversarial inputs\"\\n\"Governance\",\"Incorporating best practices into the AI supply chain, including providers and deployers\"\\n\"Transparency\",\"Enabling stakeholders to make informed choices about their engagement with an AI system\"',\n",
              "      'bBox': {'x': 71, 'y': 73.04, 'w': 469, 'h': 588}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Table 11: Our eight core Responsible AI dimensions\\n\\nIn addition to being grounded on the RAI dimensions, our objectives are informed by relevant laws and regulations, voluntary frameworks, and our commitments to our customers, and they undergo an internal alignment process that includes reviews from a number of stakeholders. We will continue to iterate on these objections as we engage with external experts and participate in industry and government forums, including the Frontier Model Forum [29], Partnership on AI [5], and various forums organized by government agencies such as the National Institute of Standards and Technology (NIST) of the U.S. Department of Commerce [7].\\n\\nOur commitment to Responsible Scaling: As the capabilities of AI models increase (through increased training data, model size or architecture innovations), so do the potential risks that they present. We joined other technology companies in signing on to the White House’s voluntary commitments on the safe, secure, and transparent development and use of foundation models [6]. Since then we have actively participated in other efforts, including the AI Safety Summits in the UK and Seoul, and we have committed to new standards like the G7 AI Hiroshima Process Code of Conduct [30] in accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We also started a partnership with the Model Evaluation and Threat Research (METR) center8 to enrich our Controllability design objectives.',\n",
              "      'md': 'Table 11: Our eight core Responsible AI dimensions\\n\\nIn addition to being grounded on the RAI dimensions, our objectives are informed by relevant laws and regulations, voluntary frameworks, and our commitments to our customers, and they undergo an internal alignment process that includes reviews from a number of stakeholders. We will continue to iterate on these objections as we engage with external experts and participate in industry and government forums, including the Frontier Model Forum [29], Partnership on AI [5], and various forums organized by government agencies such as the National Institute of Standards and Technology (NIST) of the U.S. Department of Commerce [7].\\n\\nOur commitment to Responsible Scaling: As the capabilities of AI models increase (through increased training data, model size or architecture innovations), so do the potential risks that they present. We joined other technology companies in signing on to the White House’s voluntary commitments on the safe, secure, and transparent development and use of foundation models [6]. Since then we have actively participated in other efforts, including the AI Safety Summits in the UK and Seoul, and we have committed to new standards like the G7 AI Hiroshima Process Code of Conduct [30] in accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We also started a partnership with the Model Evaluation and Threat Research (METR) center8 to enrich our Controllability design objectives.',\n",
              "      'bBox': {'x': 71, 'y': 133.04, 'w': 470, 'h': 313}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '5.2 Ensuring adherence to RAI objectives',\n",
              "      'md': '# 5.2 Ensuring adherence to RAI objectives',\n",
              "      'bBox': {'x': 72, 'y': 465.04, 'w': 183, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We employed a number of methods to measure and ensure compliance for each of our core RAI dimensions depending on their scope (i.e., whether they apply to model output, data management or other processes). For the dimensions that govern model behavior (Safety, Fairness, Veracity and Robustness, Controllability, and Privacy and Security), we curated the pre-training data and we used both Supervised Fine Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methods to align our models. Based on the objectives for each RAI dimension, we created single- and multi-turn RAI demonstrations in multiple languages and conducted helpfulness/harmfulness studies to decide on SFT data mixes. We collected human preference data to be used as inputs to RLHF training where we also provided an RAI-specific reward model. We also identify risk areas during our offline evaluation or red teaming exercises (Section 5.4) and collect semantically similar examples to be included in future SFT and RLHF rounds.\\n\\nIn addition to the RAI model alignment, we built runtime input and output moderation models which serve as a first and last line of defense and allow us to respond more quickly to newly identified threats or gaps in model alignment. The main role of the input moderation model is to detect prompts that contain malicious, insecure or illegal material, or attempt to bypass the core model alignment (prompt injection, jailbreaking). Similarly, the output moderation ensures that the content adheres to our RAI objectives.\\n\\nWe have a rigorous Governance methodology, developing our models in a working-backwards product process that incorporates RAI at the design phase, design consultations and implementation assessments by dedicated RAI science and data experts, and includes routine testing, reviews with customers, best practice development, dissemination, and training.\\n\\n8https://metr.org/',\n",
              "      'md': 'We employed a number of methods to measure and ensure compliance for each of our core RAI dimensions depending on their scope (i.e., whether they apply to model output, data management or other processes). For the dimensions that govern model behavior (Safety, Fairness, Veracity and Robustness, Controllability, and Privacy and Security), we curated the pre-training data and we used both Supervised Fine Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methods to align our models. Based on the objectives for each RAI dimension, we created single- and multi-turn RAI demonstrations in multiple languages and conducted helpfulness/harmfulness studies to decide on SFT data mixes. We collected human preference data to be used as inputs to RLHF training where we also provided an RAI-specific reward model. We also identify risk areas during our offline evaluation or red teaming exercises (Section 5.4) and collect semantically similar examples to be included in future SFT and RLHF rounds.\\n\\nIn addition to the RAI model alignment, we built runtime input and output moderation models which serve as a first and last line of defense and allow us to respond more quickly to newly identified threats or gaps in model alignment. The main role of the input moderation model is to detect prompts that contain malicious, insecure or illegal material, or attempt to bypass the core model alignment (prompt injection, jailbreaking). Similarly, the output moderation ensures that the content adheres to our RAI objectives.\\n\\nWe have a rigorous Governance methodology, developing our models in a working-backwards product process that incorporates RAI at the design phase, design consultations and implementation assessments by dedicated RAI science and data experts, and includes routine testing, reviews with customers, best practice development, dissemination, and training.\\n\\n8https://metr.org/',\n",
              "      'bBox': {'x': 71, 'y': 88.04, 'w': 471, 'h': 628.98}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://metr.org/', 'text': 'https://metr.org/'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 19,\n",
              "    'text': '                                       The Amazon Nova Family of Models\\nWe work to ensure that our Privacy and Security objectives are adhered to for both the model and training data.\\nIn addition to the model output alignment described above, we take measures that include data access controls [9]\\nprotecting our model training data, resulting weights, and model versions, and watermarking model outputs (see below).\\nWe address the latter through several layers of defense, including de-identifying or removing certain types of personal\\ndata from our training data, when feasible, as well as evaluation through red teaming exercises that cover data privacy\\nassessments.\\nFor Explainability of our models’ outputs we conduct and leverage the current active research in the area of Explainable\\nAI to deeply understand our models’ current behavior, their potential future behavior, and to build capabilities to\\ncontinuously correct their behavior as and when necessary. We use various explainable AI methods throughout our\\nmodel development to guide our decisions regarding RAI alignment and other mitigations. Services like Clarify [8]\\nalso enable our downstream developers to easily explain model predictions.\\nTo work to ensure our models’ Robustness against adversarial inputs such as those that attempt to bypass alignment\\nguardrails, we focused on risks applicable to both developers building applications using our models, and users\\ninteracting with our models via those applications. We organized those risks in broad categories such as sensitive data\\nexfiltration, execution of unauthorized action, degradation of run-time model service availability, and malicious content\\ngeneration. We used this risk organization to build model resiliency against interactions that lead to the prioritized risks.\\nFinally, to maximize Transparency, we incorporate an invisible watermark during the image or video generation\\nprocess and add C2PA9 metadata in all Canvas generated content. We enhanced the robustness to alterations like\\nrotation, resizing, color inversion, and flipping. For videos, we embed our watermark in each frame and ensure that our\\nwatermarking and detection methods withstand H264 compression. To enable anyone to easily detect the watermarks in\\nAmazon Nova generated content, an API will be available soon after launch. Our watermark detection system introduces\\nseveral enhancements such as making confidence score-based predictions instead of a single binary prediction that\\nreflects the extent to which the generated content has been edited even when using external tools. The new detection\\nsystem covers both images and videos.\\n5.3   RAI Evaluation\\nThroughout model development we perform extensive RAI evaluations using publicly available benchmarks like BOLD\\n[25], RealToxicityPrompts [31], and MM-SafetyBench [44]. We also built a series of proprietary, dynamically updating\\nbenchmarks. To build them, our internal data annotation team created a diverse set of examples for each of our RAI\\ndimensions. In addition, we leveraged subject-matter experts in specific areas, such as Security and Controllability, to\\ncollect adversarial prompts. We continued updating and enhancing each dataset based on evaluation and red teaming\\nresults (see Section 5.4 for more details on red teaming). This kept the internal benchmarks evergreen, avoiding\\noverfitting during development, but also made sure the models do not regress against previously identified risks.\\nOur datasets comprise inputs in multiple languages and multiple modalities, and contain single-turn and multi-turn\\nconversation examples.\\n5.4   Red Teaming\\nStatic benchmarks give us a view of how well models perform per RAI dimension against a user’s “plain” intent (i.e.\\nthe prompts explicitly state the intent of the user to generate prohibited content). To test our models’ resilience against\\ntechniques that mask the users’ intent we rely on red teaming. We employed a multi-pronged evaluation strategy\\nconsisting of internal red teaming, red teaming with third party and subject matter experts and, automated red teaming.\\n5.4.1  Internal Red Teaming\\nWe used a team of trained data analysts and subject-matter experts to perform regular red teaming exercises to\\nevaluate the model’s robustness against adversarial prompts across all our RAI dimensions. We enhanced the diversity\\nof manually curated adversarial prompts by employing linguistic, structural, and modality based prompt mutation\\ntechniques, assessing each mutation for its effectiveness at generating a response that does not adhere to our RAI\\nobjectives, likelihood of its success, and the technique’s novelty to a model revision. In total, we identified and\\ndeveloped over 300 distinct techniques (see Figure 4), and tested techniques individually and via chaining various\\ncombinations. The attacks covered multiple languages and modalities, targeting each language/modality individually\\nand in combination. We designed cross-modality attacks, such as embedding adversarial content within seemingly\\nbenign visual inputs, to evaluate the models’ ability to handle complex scenarios involving multiple input types. Where\\nappropriate, we implemented automation to further improve the diversity, reliability, and efficiency of red teaming.\\n   9https://c2pa.org/\\n                                                       19',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\nWe work to ensure that our Privacy and Security objectives are adhered to for both the model and training data. In addition to the model output alignment described above, we take measures that include data access controls [9] protecting our model training data, resulting weights, and model versions, and watermarking model outputs (see below). We address the latter through several layers of defense, including de-identifying or removing certain types of personal data from our training data, when feasible, as well as evaluation through red teaming exercises that cover data privacy assessments.\\n\\nFor Explainability of our models’ outputs we conduct and leverage the current active research in the area of Explainable AI to deeply understand our models’ current behavior, their potential future behavior, and to build capabilities to continuously correct their behavior as and when necessary. We use various explainable AI methods throughout our model development to guide our decisions regarding RAI alignment and other mitigations. Services like Clarify [8] also enable our downstream developers to easily explain model predictions.\\n\\nTo work to ensure our models’ Robustness against adversarial inputs such as those that attempt to bypass alignment guardrails, we focused on risks applicable to both developers building applications using our models, and users interacting with our models via those applications. We organized those risks in broad categories such as sensitive data exfiltration, execution of unauthorized action, degradation of run-time model service availability, and malicious content generation. We used this risk organization to build model resiliency against interactions that lead to the prioritized risks.\\n\\nFinally, to maximize Transparency, we incorporate an invisible watermark during the image or video generation process and add C2PA9 metadata in all Canvas generated content. We enhanced the robustness to alterations like rotation, resizing, color inversion, and flipping. For videos, we embed our watermark in each frame and ensure that our watermarking and detection methods withstand H264 compression. To enable anyone to easily detect the watermarks in Amazon Nova generated content, an API will be available soon after launch. Our watermark detection system introduces several enhancements such as making confidence score-based predictions instead of a single binary prediction that reflects the extent to which the generated content has been edited even when using external tools. The new detection system covers both images and videos.\\n\\n# 5.3 RAI Evaluation\\n\\nThroughout model development we perform extensive RAI evaluations using publicly available benchmarks like BOLD [25], RealToxicityPrompts [31], and MM-SafetyBench [44]. We also built a series of proprietary, dynamically updating benchmarks. To build them, our internal data annotation team created a diverse set of examples for each of our RAI dimensions. In addition, we leveraged subject-matter experts in specific areas, such as Security and Controllability, to collect adversarial prompts. We continued updating and enhancing each dataset based on evaluation and red teaming results (see Section 5.4 for more details on red teaming). This kept the internal benchmarks evergreen, avoiding overfitting during development, but also made sure the models do not regress against previously identified risks. Our datasets comprise inputs in multiple languages and multiple modalities, and contain single-turn and multi-turn conversation examples.\\n\\n# 5.4 Red Teaming\\n\\nStatic benchmarks give us a view of how well models perform per RAI dimension against a user’s “plain” intent (i.e. the prompts explicitly state the intent of the user to generate prohibited content). To test our models’ resilience against techniques that mask the users’ intent we rely on red teaming. We employed a multi-pronged evaluation strategy consisting of internal red teaming, red teaming with third party and subject matter experts and, automated red teaming.\\n\\n# 5.4.1 Internal Red Teaming\\n\\nWe used a team of trained data analysts and subject-matter experts to perform regular red teaming exercises to evaluate the model’s robustness against adversarial prompts across all our RAI dimensions. We enhanced the diversity of manually curated adversarial prompts by employing linguistic, structural, and modality based prompt mutation techniques, assessing each mutation for its effectiveness at generating a response that does not adhere to our RAI objectives, likelihood of its success, and the technique’s novelty to a model revision. In total, we identified and developed over 300 distinct techniques (see Figure 4), and tested techniques individually and via chaining various combinations. The attacks covered multiple languages and modalities, targeting each language/modality individually and in combination. We designed cross-modality attacks, such as embedding adversarial content within seemingly benign visual inputs, to evaluate the models’ ability to handle complex scenarios involving multiple input types. Where appropriate, we implemented automation to further improve the diversity, reliability, and efficiency of red teaming.\\n\\n9 https://c2pa.org/',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We work to ensure that our Privacy and Security objectives are adhered to for both the model and training data. In addition to the model output alignment described above, we take measures that include data access controls [9] protecting our model training data, resulting weights, and model versions, and watermarking model outputs (see below). We address the latter through several layers of defense, including de-identifying or removing certain types of personal data from our training data, when feasible, as well as evaluation through red teaming exercises that cover data privacy assessments.\\n\\nFor Explainability of our models’ outputs we conduct and leverage the current active research in the area of Explainable AI to deeply understand our models’ current behavior, their potential future behavior, and to build capabilities to continuously correct their behavior as and when necessary. We use various explainable AI methods throughout our model development to guide our decisions regarding RAI alignment and other mitigations. Services like Clarify [8] also enable our downstream developers to easily explain model predictions.\\n\\nTo work to ensure our models’ Robustness against adversarial inputs such as those that attempt to bypass alignment guardrails, we focused on risks applicable to both developers building applications using our models, and users interacting with our models via those applications. We organized those risks in broad categories such as sensitive data exfiltration, execution of unauthorized action, degradation of run-time model service availability, and malicious content generation. We used this risk organization to build model resiliency against interactions that lead to the prioritized risks.\\n\\nFinally, to maximize Transparency, we incorporate an invisible watermark during the image or video generation process and add C2PA9 metadata in all Canvas generated content. We enhanced the robustness to alterations like rotation, resizing, color inversion, and flipping. For videos, we embed our watermark in each frame and ensure that our watermarking and detection methods withstand H264 compression. To enable anyone to easily detect the watermarks in Amazon Nova generated content, an API will be available soon after launch. Our watermark detection system introduces several enhancements such as making confidence score-based predictions instead of a single binary prediction that reflects the extent to which the generated content has been edited even when using external tools. The new detection system covers both images and videos.',\n",
              "      'md': 'We work to ensure that our Privacy and Security objectives are adhered to for both the model and training data. In addition to the model output alignment described above, we take measures that include data access controls [9] protecting our model training data, resulting weights, and model versions, and watermarking model outputs (see below). We address the latter through several layers of defense, including de-identifying or removing certain types of personal data from our training data, when feasible, as well as evaluation through red teaming exercises that cover data privacy assessments.\\n\\nFor Explainability of our models’ outputs we conduct and leverage the current active research in the area of Explainable AI to deeply understand our models’ current behavior, their potential future behavior, and to build capabilities to continuously correct their behavior as and when necessary. We use various explainable AI methods throughout our model development to guide our decisions regarding RAI alignment and other mitigations. Services like Clarify [8] also enable our downstream developers to easily explain model predictions.\\n\\nTo work to ensure our models’ Robustness against adversarial inputs such as those that attempt to bypass alignment guardrails, we focused on risks applicable to both developers building applications using our models, and users interacting with our models via those applications. We organized those risks in broad categories such as sensitive data exfiltration, execution of unauthorized action, degradation of run-time model service availability, and malicious content generation. We used this risk organization to build model resiliency against interactions that lead to the prioritized risks.\\n\\nFinally, to maximize Transparency, we incorporate an invisible watermark during the image or video generation process and add C2PA9 metadata in all Canvas generated content. We enhanced the robustness to alterations like rotation, resizing, color inversion, and flipping. For videos, we embed our watermark in each frame and ensure that our watermarking and detection methods withstand H264 compression. To enable anyone to easily detect the watermarks in Amazon Nova generated content, an API will be available soon after launch. Our watermark detection system introduces several enhancements such as making confidence score-based predictions instead of a single binary prediction that reflects the extent to which the generated content has been edited even when using external tools. The new detection system covers both images and videos.',\n",
              "      'bBox': {'x': 71, 'y': 72.04, 'w': 471, 'h': 435}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '5.3 RAI Evaluation',\n",
              "      'md': '# 5.3 RAI Evaluation',\n",
              "      'bBox': {'x': 72, 'y': 365.04, 'w': 89, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Throughout model development we perform extensive RAI evaluations using publicly available benchmarks like BOLD [25], RealToxicityPrompts [31], and MM-SafetyBench [44]. We also built a series of proprietary, dynamically updating benchmarks. To build them, our internal data annotation team created a diverse set of examples for each of our RAI dimensions. In addition, we leveraged subject-matter experts in specific areas, such as Security and Controllability, to collect adversarial prompts. We continued updating and enhancing each dataset based on evaluation and red teaming results (see Section 5.4 for more details on red teaming). This kept the internal benchmarks evergreen, avoiding overfitting during development, but also made sure the models do not regress against previously identified risks. Our datasets comprise inputs in multiple languages and multiple modalities, and contain single-turn and multi-turn conversation examples.',\n",
              "      'md': 'Throughout model development we perform extensive RAI evaluations using publicly available benchmarks like BOLD [25], RealToxicityPrompts [31], and MM-SafetyBench [44]. We also built a series of proprietary, dynamically updating benchmarks. To build them, our internal data annotation team created a diverse set of examples for each of our RAI dimensions. In addition, we leveraged subject-matter experts in specific areas, such as Security and Controllability, to collect adversarial prompts. We continued updating and enhancing each dataset based on evaluation and red teaming results (see Section 5.4 for more details on red teaming). This kept the internal benchmarks evergreen, avoiding overfitting during development, but also made sure the models do not regress against previously identified risks. Our datasets comprise inputs in multiple languages and multiple modalities, and contain single-turn and multi-turn conversation examples.',\n",
              "      'bBox': {'x': 71, 'y': 365.04, 'w': 471, 'h': 142}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '5.4 Red Teaming',\n",
              "      'md': '# 5.4 Red Teaming',\n",
              "      'bBox': {'x': 72, 'y': 497.04, 'w': 78, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Static benchmarks give us a view of how well models perform per RAI dimension against a user’s “plain” intent (i.e. the prompts explicitly state the intent of the user to generate prohibited content). To test our models’ resilience against techniques that mask the users’ intent we rely on red teaming. We employed a multi-pronged evaluation strategy consisting of internal red teaming, red teaming with third party and subject matter experts and, automated red teaming.',\n",
              "      'md': 'Static benchmarks give us a view of how well models perform per RAI dimension against a user’s “plain” intent (i.e. the prompts explicitly state the intent of the user to generate prohibited content). To test our models’ resilience against techniques that mask the users’ intent we rely on red teaming. We employed a multi-pronged evaluation strategy consisting of internal red teaming, red teaming with third party and subject matter experts and, automated red teaming.',\n",
              "      'bBox': {'x': 72, 'y': 497.04, 'w': 470, 'h': 87}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '5.4.1 Internal Red Teaming',\n",
              "      'md': '# 5.4.1 Internal Red Teaming',\n",
              "      'bBox': {'x': 72, 'y': 497.04, 'w': 123, 'h': 87}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We used a team of trained data analysts and subject-matter experts to perform regular red teaming exercises to evaluate the model’s robustness against adversarial prompts across all our RAI dimensions. We enhanced the diversity of manually curated adversarial prompts by employing linguistic, structural, and modality based prompt mutation techniques, assessing each mutation for its effectiveness at generating a response that does not adhere to our RAI objectives, likelihood of its success, and the technique’s novelty to a model revision. In total, we identified and developed over 300 distinct techniques (see Figure 4), and tested techniques individually and via chaining various combinations. The attacks covered multiple languages and modalities, targeting each language/modality individually and in combination. We designed cross-modality attacks, such as embedding adversarial content within seemingly benign visual inputs, to evaluate the models’ ability to handle complex scenarios involving multiple input types. Where appropriate, we implemented automation to further improve the diversity, reliability, and efficiency of red teaming.\\n\\n9 https://c2pa.org/',\n",
              "      'md': 'We used a team of trained data analysts and subject-matter experts to perform regular red teaming exercises to evaluate the model’s robustness against adversarial prompts across all our RAI dimensions. We enhanced the diversity of manually curated adversarial prompts by employing linguistic, structural, and modality based prompt mutation techniques, assessing each mutation for its effectiveness at generating a response that does not adhere to our RAI objectives, likelihood of its success, and the technique’s novelty to a model revision. In total, we identified and developed over 300 distinct techniques (see Figure 4), and tested techniques individually and via chaining various combinations. The attacks covered multiple languages and modalities, targeting each language/modality individually and in combination. We designed cross-modality attacks, such as embedding adversarial content within seemingly benign visual inputs, to evaluate the models’ ability to handle complex scenarios involving multiple input types. Where appropriate, we implemented automation to further improve the diversity, reliability, and efficiency of red teaming.\\n\\n9 https://c2pa.org/',\n",
              "      'bBox': {'x': 71, 'y': 497.04, 'w': 471, 'h': 219.98}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://c2pa.org/', 'text': 'https://c2pa.org/'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 20,\n",
              "    'text': '                                                          The Amazon Nova Family of Models\\n                                                                                                                                                Cipher-based: 3\\n                                                                                                                                      Text based obfuscation: 5\\n                                                                                                    Obfuscation: 8                            Code injection: 9\\n                                                                                                                                         Recursive injection: 1\\n                                                                                                                                              Virtualization: 9\\n                                                                                                                                   Defined dictionary attack: 1\\n                                                                                                                                           Payload splitting: 6\\n                                                                                                                                             Token smuggling: 6\\n                        Prompt injections (direct and indirect): 40                                                                                one-shot: 12\\n                                                                                                                                                  many-shot: 12\\n                                                      Jailbreak: 24                                                                  Multilingual prompting: 14\\n                                                                                                                                        Translation requests: 8\\n                                             Multiple languages: 33                                                                 Mixed language requests: 11\\n                                                                                                                                        In-Context Learning: 14\\n                                                  Context-based: 32                                                                     Syntactic separators: 8\\n                                                                                              Context switching: 18                     Semantic separators: 10\\n                                                                                                                                     Instruction repetition: 19\\n          Attacks: 307                                                                                                                Completion Compliance: 13\\n                                                                                                                                       Affirmative Suffixes: 13\\n                                                    Persuasion: 119                                                                     One-sided arguments: 12\\n                                                                                                                                        Refusal suppression: 10\\n                                                                                                                                        Chain of utterances: 10\\n                                                                                                                             Socratic Questioning Technique: 15\\n                                                                                                                                            Personification: 10\\n                                                   Obfuscations: 59                                                                        Task constraints: 17\\n                                                                                                                                         Veiled Expressions: 16\\n                                                                                                                                         Output constraints: 20\\n                                                                                                                                     Euphemisms via Ciphers: 11\\n                                                                                                                                      Decoding Manipulation: 11\\n                                                                                                                                            Macaronic prompt: 1\\n                 Figure 4: Broad taxonomy and count of attack techniques we use for our red teaming exercises\\nAfter each round of red teaming, we gathered feedback from the team regarding failure patterns which guided the next\\nstage of the model development.\\n5.4.2      External Red Teaming\\nIn accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Artificial\\nIntelligence, we partner with a variety of third parties to conduct red teaming against our AI models. These initiatives\\nare in addition to our extensive in-house efforts, which includes all aspects of Cybersecurity red teaming. Just like with\\nour internal red teaming efforts, we iterated during the model development based on feedback from these institutions to\\nimprove the RAI adherence of our models. We leverage red-teaming firms including ActiveFence to conduct testing\\nin areas such as hate speech, political misinformation, extremism and other RAI dimensions. We also work with\\nspecialized third parties to red team our models for Chemical, Biological, Radiological and Nuclear (CBRN) capabilities.\\nOur work with Deloitte Consulting, tests our AI models’ capabilities in Biological risks and harms. Our work with\\nNemesys Insights LLC tests our AI models’ capabilities in the Radiological and Nuclear domains. We also work with\\nthe Gomes Group at Carnegie Mellon University to test our models’ capabilities in Chemistry and chemical compounds.\\nEach of these partners was carefully selected based on their industry leadership, previous/parallel red teaming work\\nwith other AI model developers, and their contributions to evolving government and industry standards around CBRN\\nand overall AI safety. We provide a brief summary of expertise of each of these vendors and their testing methodology\\nbelow.\\nActiveFence: ActiveFence is a team of over 150 subject matter experts providing AI Safety and Content Moderation\\nsolutions. The team produced over 9,700 adversarial prompts, distributed over 20 categories, including content-targeted\\nred teaming (evaluating the model’s ability to generate harmful or inappropriate content), and security-targeted red\\nteaming (assessing the model’s resilience against malicious attempts to manipulate its behavior or extract sensitive\\ninformation).\\nDeloitte\\n            : The evaluation team at Deloitte Consulting LLP (formerly known as Gryphon Scientific) has unique experience\\nat the intersection of artificial intelligence and biology. The primary thrust of this effort involved evaluating the model\\nagainst a panel of 30 questions developed to test an LLM’s scientific knowledge and reasoning capabilities that could\\nfacilitate the development or use of biological weapons. The model’s responses to these questions were evaluated\\nfor their scientific accuracy and utility to someone seeking to do harm with biology. After completing the initial\\n                                                                                   20',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n|Cipher-based:|3|\\n|---|---|\\n|Text based obfuscation:|5|\\n|Obfuscation:|8|\\n|Code injection:|9|\\n|Recursive injection:|1|\\n|Virtualization:|9|\\n|Defined dictionary attack:|1|\\n|Payload splitting:|6|\\n|Token smuggling:|6|\\n|Prompt injections (direct and indirect):|40|\\n|one-shot:|12|\\n|many-shot:|12|\\n|Jailbreak:|24|\\n|Multilingual prompting:|14|\\n|Translation requests:|8|\\n|Multiple languages:|33|\\n|Mixed language requests:|11|\\n|In-Context Learning:|14|\\n|Context-based:|32|\\n|Syntactic separators:|8|\\n|Context switching:|18|\\n|Semantic separators:|10|\\n|Instruction repetition:|19|\\n|Attacks:|307|\\n|Completion Compliance:|13|\\n|Affirmative Suffixes:|13|\\n|Persuasion:|119|\\n|One-sided arguments:|12|\\n|Refusal suppression:|10|\\n|Chain of utterances:|10|\\n|Socratic Questioning Technique:|15|\\n|Personification:|10|\\n|Obfuscations:|59|\\n|Task constraints:|17|\\n|Veiled Expressions:|16|\\n|Output constraints:|20|\\n|Euphemisms via Ciphers:|11|\\n|Decoding Manipulation:|11|\\n|Macaronic prompt:|1|\\n\\nFigure 4: Broad taxonomy and count of attack techniques we use for our red teaming exercises\\n\\nAfter each round of red teaming, we gathered feedback from the team regarding failure patterns which guided the next stage of the model development.\\n\\n# 5.4.2 External Red Teaming\\n\\nIn accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Artificial Intelligence, we partner with a variety of third parties to conduct red teaming against our AI models. These initiatives are in addition to our extensive in-house efforts, which includes all aspects of Cybersecurity red teaming. Just like with our internal red teaming efforts, we iterated during the model development based on feedback from these institutions to improve the RAI adherence of our models. We leverage red-teaming firms including ActiveFence to conduct testing in areas such as hate speech, political misinformation, extremism and other RAI dimensions. We also work with specialized third parties to red team our models for Chemical, Biological, Radiological and Nuclear (CBRN) capabilities. Our work with Deloitte Consulting, tests our AI models’ capabilities in Biological risks and harms. Our work with Nemesys Insights LLC tests our AI models’ capabilities in the Radiological and Nuclear domains. We also work with the Gomes Group at Carnegie Mellon University to test our models’ capabilities in Chemistry and chemical compounds. Each of these partners was carefully selected based on their industry leadership, previous/parallel red teaming work with other AI model developers, and their contributions to evolving government and industry standards around CBRN and overall AI safety. We provide a brief summary of expertise of each of these vendors and their testing methodology below.\\n\\n# ActiveFence\\n\\nActiveFence is a team of over 150 subject matter experts providing AI Safety and Content Moderation solutions. The team produced over 9,700 adversarial prompts, distributed over 20 categories, including content-targeted red teaming (evaluating the model’s ability to generate harmful or inappropriate content), and security-targeted red teaming (assessing the model’s resilience against malicious attempts to manipulate its behavior or extract sensitive information).\\n\\n# Deloitte\\n\\nThe evaluation team at Deloitte Consulting LLP (formerly known as Gryphon Scientific) has unique experience at the intersection of artificial intelligence and biology. The primary thrust of this effort involved evaluating the model against a panel of 30 questions developed to test an LLM’s scientific knowledge and reasoning capabilities that could facilitate the development or use of biological weapons. The model’s responses to these questions were evaluated for their scientific accuracy and utility to someone seeking to do harm with biology. After completing the initial',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['Cipher-based:', '3'],\n",
              "       ['Text based obfuscation:', '5'],\n",
              "       ['Obfuscation:', '8'],\n",
              "       ['Code injection:', '9'],\n",
              "       ['Recursive injection:', '1'],\n",
              "       ['Virtualization:', '9'],\n",
              "       ['Defined dictionary attack:', '1'],\n",
              "       ['Payload splitting:', '6'],\n",
              "       ['Token smuggling:', '6'],\n",
              "       ['Prompt injections (direct and indirect):', '40'],\n",
              "       ['one-shot:', '12'],\n",
              "       ['many-shot:', '12'],\n",
              "       ['Jailbreak:', '24'],\n",
              "       ['Multilingual prompting:', '14'],\n",
              "       ['Translation requests:', '8'],\n",
              "       ['Multiple languages:', '33'],\n",
              "       ['Mixed language requests:', '11'],\n",
              "       ['In-Context Learning:', '14'],\n",
              "       ['Context-based:', '32'],\n",
              "       ['Syntactic separators:', '8'],\n",
              "       ['Context switching:', '18'],\n",
              "       ['Semantic separators:', '10'],\n",
              "       ['Instruction repetition:', '19'],\n",
              "       ['Attacks:', '307'],\n",
              "       ['Completion Compliance:', '13'],\n",
              "       ['Affirmative Suffixes:', '13'],\n",
              "       ['Persuasion:', '119'],\n",
              "       ['One-sided arguments:', '12'],\n",
              "       ['Refusal suppression:', '10'],\n",
              "       ['Chain of utterances:', '10'],\n",
              "       ['Socratic Questioning Technique:', '15'],\n",
              "       ['Personification:', '10'],\n",
              "       ['Obfuscations:', '59'],\n",
              "       ['Task constraints:', '17'],\n",
              "       ['Veiled Expressions:', '16'],\n",
              "       ['Output constraints:', '20'],\n",
              "       ['Euphemisms via Ciphers:', '11'],\n",
              "       ['Decoding Manipulation:', '11'],\n",
              "       ['Macaronic prompt:', '1']],\n",
              "      'md': '|Cipher-based:|3|\\n|Text based obfuscation:|5|\\n|Obfuscation:|8|\\n|Code injection:|9|\\n|Recursive injection:|1|\\n|Virtualization:|9|\\n|Defined dictionary attack:|1|\\n|Payload splitting:|6|\\n|Token smuggling:|6|\\n|Prompt injections (direct and indirect):|40|\\n|one-shot:|12|\\n|many-shot:|12|\\n|Jailbreak:|24|\\n|Multilingual prompting:|14|\\n|Translation requests:|8|\\n|Multiple languages:|33|\\n|Mixed language requests:|11|\\n|In-Context Learning:|14|\\n|Context-based:|32|\\n|Syntactic separators:|8|\\n|Context switching:|18|\\n|Semantic separators:|10|\\n|Instruction repetition:|19|\\n|Attacks:|307|\\n|Completion Compliance:|13|\\n|Affirmative Suffixes:|13|\\n|Persuasion:|119|\\n|One-sided arguments:|12|\\n|Refusal suppression:|10|\\n|Chain of utterances:|10|\\n|Socratic Questioning Technique:|15|\\n|Personification:|10|\\n|Obfuscations:|59|\\n|Task constraints:|17|\\n|Veiled Expressions:|16|\\n|Output constraints:|20|\\n|Euphemisms via Ciphers:|11|\\n|Decoding Manipulation:|11|\\n|Macaronic prompt:|1|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"Cipher-based:\",\"3\"\\n\"Text based obfuscation:\",\"5\"\\n\"Obfuscation:\",\"8\"\\n\"Code injection:\",\"9\"\\n\"Recursive injection:\",\"1\"\\n\"Virtualization:\",\"9\"\\n\"Defined dictionary attack:\",\"1\"\\n\"Payload splitting:\",\"6\"\\n\"Token smuggling:\",\"6\"\\n\"Prompt injections (direct and indirect):\",\"40\"\\n\"one-shot:\",\"12\"\\n\"many-shot:\",\"12\"\\n\"Jailbreak:\",\"24\"\\n\"Multilingual prompting:\",\"14\"\\n\"Translation requests:\",\"8\"\\n\"Multiple languages:\",\"33\"\\n\"Mixed language requests:\",\"11\"\\n\"In-Context Learning:\",\"14\"\\n\"Context-based:\",\"32\"\\n\"Syntactic separators:\",\"8\"\\n\"Context switching:\",\"18\"\\n\"Semantic separators:\",\"10\"\\n\"Instruction repetition:\",\"19\"\\n\"Attacks:\",\"307\"\\n\"Completion Compliance:\",\"13\"\\n\"Affirmative Suffixes:\",\"13\"\\n\"Persuasion:\",\"119\"\\n\"One-sided arguments:\",\"12\"\\n\"Refusal suppression:\",\"10\"\\n\"Chain of utterances:\",\"10\"\\n\"Socratic Questioning Technique:\",\"15\"\\n\"Personification:\",\"10\"\\n\"Obfuscations:\",\"59\"\\n\"Task constraints:\",\"17\"\\n\"Veiled Expressions:\",\"16\"\\n\"Output constraints:\",\"20\"\\n\"Euphemisms via Ciphers:\",\"11\"\\n\"Decoding Manipulation:\",\"11\"\\n\"Macaronic prompt:\",\"1\"',\n",
              "      'bBox': {'x': 71, 'y': 73.32, 'w': 469, 'h': 676.72}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Figure 4: Broad taxonomy and count of attack techniques we use for our red teaming exercises\\n\\nAfter each round of red teaming, we gathered feedback from the team regarding failure patterns which guided the next stage of the model development.',\n",
              "      'md': 'Figure 4: Broad taxonomy and count of attack techniques we use for our red teaming exercises\\n\\nAfter each round of red teaming, we gathered feedback from the team regarding failure patterns which guided the next stage of the model development.',\n",
              "      'bBox': {'x': 71, 'y': 349.04, 'w': 468, 'h': 59}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '5.4.2 External Red Teaming',\n",
              "      'md': '# 5.4.2 External Red Teaming',\n",
              "      'bBox': {'x': 72, 'y': 428.04, 'w': 125, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'In accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Artificial Intelligence, we partner with a variety of third parties to conduct red teaming against our AI models. These initiatives are in addition to our extensive in-house efforts, which includes all aspects of Cybersecurity red teaming. Just like with our internal red teaming efforts, we iterated during the model development based on feedback from these institutions to improve the RAI adherence of our models. We leverage red-teaming firms including ActiveFence to conduct testing in areas such as hate speech, political misinformation, extremism and other RAI dimensions. We also work with specialized third parties to red team our models for Chemical, Biological, Radiological and Nuclear (CBRN) capabilities. Our work with Deloitte Consulting, tests our AI models’ capabilities in Biological risks and harms. Our work with Nemesys Insights LLC tests our AI models’ capabilities in the Radiological and Nuclear domains. We also work with the Gomes Group at Carnegie Mellon University to test our models’ capabilities in Chemistry and chemical compounds. Each of these partners was carefully selected based on their industry leadership, previous/parallel red teaming work with other AI model developers, and their contributions to evolving government and industry standards around CBRN and overall AI safety. We provide a brief summary of expertise of each of these vendors and their testing methodology below.',\n",
              "      'md': 'In accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Artificial Intelligence, we partner with a variety of third parties to conduct red teaming against our AI models. These initiatives are in addition to our extensive in-house efforts, which includes all aspects of Cybersecurity red teaming. Just like with our internal red teaming efforts, we iterated during the model development based on feedback from these institutions to improve the RAI adherence of our models. We leverage red-teaming firms including ActiveFence to conduct testing in areas such as hate speech, political misinformation, extremism and other RAI dimensions. We also work with specialized third parties to red team our models for Chemical, Biological, Radiological and Nuclear (CBRN) capabilities. Our work with Deloitte Consulting, tests our AI models’ capabilities in Biological risks and harms. Our work with Nemesys Insights LLC tests our AI models’ capabilities in the Radiological and Nuclear domains. We also work with the Gomes Group at Carnegie Mellon University to test our models’ capabilities in Chemistry and chemical compounds. Each of these partners was carefully selected based on their industry leadership, previous/parallel red teaming work with other AI model developers, and their contributions to evolving government and industry standards around CBRN and overall AI safety. We provide a brief summary of expertise of each of these vendors and their testing methodology below.',\n",
              "      'bBox': {'x': 71, 'y': 449.04, 'w': 471, 'h': 228}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'ActiveFence',\n",
              "      'md': '# ActiveFence',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'ActiveFence is a team of over 150 subject matter experts providing AI Safety and Content Moderation solutions. The team produced over 9,700 adversarial prompts, distributed over 20 categories, including content-targeted red teaming (evaluating the model’s ability to generate harmful or inappropriate content), and security-targeted red teaming (assessing the model’s resilience against malicious attempts to manipulate its behavior or extract sensitive information).',\n",
              "      'md': 'ActiveFence is a team of over 150 subject matter experts providing AI Safety and Content Moderation solutions. The team produced over 9,700 adversarial prompts, distributed over 20 categories, including content-targeted red teaming (evaluating the model’s ability to generate harmful or inappropriate content), and security-targeted red teaming (assessing the model’s resilience against malicious attempts to manipulate its behavior or extract sensitive information).',\n",
              "      'bBox': {'x': 72, 'y': 618.04, 'w': 468, 'h': 132}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Deloitte',\n",
              "      'md': '# Deloitte',\n",
              "      'bBox': {'x': 72, 'y': 667.04, 'w': 33, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'The evaluation team at Deloitte Consulting LLP (formerly known as Gryphon Scientific) has unique experience at the intersection of artificial intelligence and biology. The primary thrust of this effort involved evaluating the model against a panel of 30 questions developed to test an LLM’s scientific knowledge and reasoning capabilities that could facilitate the development or use of biological weapons. The model’s responses to these questions were evaluated for their scientific accuracy and utility to someone seeking to do harm with biology. After completing the initial',\n",
              "      'md': 'The evaluation team at Deloitte Consulting LLP (formerly known as Gryphon Scientific) has unique experience at the intersection of artificial intelligence and biology. The primary thrust of this effort involved evaluating the model against a panel of 30 questions developed to test an LLM’s scientific knowledge and reasoning capabilities that could facilitate the development or use of biological weapons. The model’s responses to these questions were evaluated for their scientific accuracy and utility to someone seeking to do harm with biology. After completing the initial',\n",
              "      'bBox': {'x': 72, 'y': 667.04, 'w': 468, 'h': 53}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 21,\n",
              "    'text': '                                      The Amazon Nova Family of Models\\nevaluations, the Deloitte team probed more deeply into the questions the LLM originally replied with potentially\\nconcerning information.\\nGomes Group: The Gomes Group at Carnegie Mellon University is at the forefront of integrating advanced artificial\\nintelligence into chemical research. Their evaluation framework consisted of both automated and non-automated\\nassessments. Two non-automated evaluations explored aggregation attack vulnerabilities through purchasing and remote\\nchemical mixing scenarios. The automated evaluations utilized two distinct datasets: one containing 39 hazardous\\nchemicals (including DEA Schedule I, II, and chemical warfare agents) and another with 362 common chemicals for\\nNFPA diamond classifications. Three primary automated evaluations were conducted using the hazardous chemicals\\ndataset. The NFPA diamond evaluation comprised 1,810 prompts, testing both single-turn and multi-turn approaches\\nwith consistent accuracy across both methods.\\nNemesys: Nemesys Insights LLC run uplift studies, red teaming exercises, and risk assessments for a variety of\\ntechnology companies and third-party research entities to assess national security related risks of large language models\\nand other generative AI tools. For their testing, they started with human red teaming exercises focused on non-state\\nacquisition or use of illicit radiological/nuclear (RN) materials, followed by prompt-response evaluation and uplift\\nstudies. The exercises comprised two different scenarios (a. violent non-state actor acquisition and use of Cobalt-60; b.\\nnon-state actor acquisition and international transport of HEU [highly enriched uranium]), and utilized 8 subject matter\\nexperts with operational and technological knowledge in a 2-team x 2-scenario design to construct and refine threat\\nplans across a 6-hour planning cycle.\\n5.4.3  Automated Red Teaming\\nFinally, to augment human based red teaming, we built an automated red teaming mechanism by adapting our (Feedback\\nLoop In-context Red Teaming) FLIRT [52] framework. This approach helped us scale red teaming and repeat red\\nteaming efficiently. FLIRT uses a list of seed prompts that have been identified by human evaluators as potentially\\nviolating one or more of our RAI dimensions. For every dimension, a subset of seeds is used to generate additional\\nprompts with a dedicated language model, called red-LM, through in-context-learning (ICL) [18] and a carefully\\ncrafted set of instructions. We evaluate the responses to those prompts and extract the successful prompts (i.e., the ones\\ntriggering a prohibited response) for the next round of generation. The above steps are repeated for a chosen number of\\niterations across all RAI categories. We use our automated red teaming mechanism to evaluate both RAI adherence\\nrobustness and false refusals. We use the mechanism to generate adversarial tests across multi-turn interactions, multiple\\nlanguages, and multiple input/output modalities to uncover and correct robustness issues in our models due to potential\\nadversarial content in such interactions and inputs.\\n6   Training Infrastructure\\nThe Nova family of models were trained on Amazon’s custom Trainium1 (TRN1) chips,10 NVidia A100 (P4d instances),\\nand H100 (P5 instances) accelerators. Working with AWS SageMaker, we stood up NVidia GPU and TRN1 clusters\\nand ran parallel trainings to ensure model performance parity, while optimizing training throughput on the different\\nstacks. All clusters utilize petabit-scale non-blocking EFA network fabric which is less prone to packet loss than other\\nnetwork transport protocols11 and provides the highest network bandwidth with H100 accelerators compared to any\\nother instance type available on AWS EC212. We conducted distributed training on AWS SageMaker-managed Elastic\\nKubernetes Service (EKS) clusters, and utilized AWS File System X (FSx) and Simple Storage Solution (S3) for\\ndata and checkpoint IO. While FSx offers performant and convenient storage for large scale training jobs, S3 allowed\\ncost-efficient scaling to large multimodal datasets and model checkpoints.\\nGoodput achieved weekly average values of up to 97% in pretraining runs through optimizations targeting lower job\\nfailure rate, minimizing checkpoint overhead, and overall reduction in the Mean Time to Restart (MTTR). This time\\nis inclusive of time from the last successful checkpoint before training interruption, time taken to restart components\\nof the system and resume training at steady state from checkpoint. Techniques such as fully distributed optimizer\\nstate and weight sharding and the elimination of all blocking overhead associated with checkpoint persistence resulted\\nin a reduction of checkpointing overhead to ~1 sec on H100 clusters, and ~0.1 sec on TRN1 clusters. We exceeded\\nour MTTR target of 9 minutes and achieved an average of 6.5 minutes on our TRN1 clusters by optimizing the\\n                 10https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin\\ng-are-now-available/\\n                11https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s\\ncalable-hpc\\n                12https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor\\ne-gpus-for-accelerating-generative-ai-and-hpc-applications/\\n                                                       21',\n",
              "    'md': \"The Amazon Nova Family of Models\\n\\nevaluations, the Deloitte team probed more deeply into the questions the LLM originally replied with potentially concerning information.\\n\\nGomes Group: The Gomes Group at Carnegie Mellon University is at the forefront of integrating advanced artificial intelligence into chemical research. Their evaluation framework consisted of both automated and non-automated assessments. Two non-automated evaluations explored aggregation attack vulnerabilities through purchasing and remote chemical mixing scenarios. The automated evaluations utilized two distinct datasets: one containing 39 hazardous chemicals (including DEA Schedule I, II, and chemical warfare agents) and another with 362 common chemicals for NFPA diamond classifications. Three primary automated evaluations were conducted using the hazardous chemicals dataset. The NFPA diamond evaluation comprised 1,810 prompts, testing both single-turn and multi-turn approaches with consistent accuracy across both methods.\\n\\nNemesys: Nemesys Insights LLC run uplift studies, red teaming exercises, and risk assessments for a variety of technology companies and third-party research entities to assess national security related risks of large language models and other generative AI tools. For their testing, they started with human red teaming exercises focused on non-state acquisition or use of illicit radiological/nuclear (RN) materials, followed by prompt-response evaluation and uplift studies. The exercises comprised two different scenarios (a. violent non-state actor acquisition and use of Cobalt-60; b. non-state actor acquisition and international transport of HEU [highly enriched uranium]), and utilized 8 subject matter experts with operational and technological knowledge in a 2-team x 2-scenario design to construct and refine threat plans across a 6-hour planning cycle.\\n\\n### 5.4.3 Automated Red Teaming\\n\\nFinally, to augment human based red teaming, we built an automated red teaming mechanism by adapting our (Feedback Loop In-context Red Teaming) FLIRT [52] framework. This approach helped us scale red teaming and repeat red teaming efficiently. FLIRT uses a list of seed prompts that have been identified by human evaluators as potentially violating one or more of our RAI dimensions. For every dimension, a subset of seeds is used to generate additional prompts with a dedicated language model, called red-LM, through in-context-learning (ICL) [18] and a carefully crafted set of instructions. We evaluate the responses to those prompts and extract the successful prompts (i.e., the ones triggering a prohibited response) for the next round of generation. The above steps are repeated for a chosen number of iterations across all RAI categories. We use our automated red teaming mechanism to evaluate both RAI adherence robustness and false refusals. We use the mechanism to generate adversarial tests across multi-turn interactions, multiple languages, and multiple input/output modalities to uncover and correct robustness issues in our models due to potential adversarial content in such interactions and inputs.\\n\\n## 6 Training Infrastructure\\n\\nThe Nova family of models were trained on Amazon's custom Trainium1 (TRN1) chips, NVidia A100 (P4d instances), and H100 (P5 instances) accelerators. Working with AWS SageMaker, we stood up NVidia GPU and TRN1 clusters and ran parallel trainings to ensure model performance parity, while optimizing training throughput on the different stacks. All clusters utilize petabit-scale non-blocking EFA network fabric which is less prone to packet loss than other network transport protocols and provides the highest network bandwidth with H100 accelerators compared to any other instance type available on AWS EC2. We conducted distributed training on AWS SageMaker-managed Elastic Kubernetes Service (EKS) clusters, and utilized AWS File System X (FSx) and Simple Storage Solution (S3) for data and checkpoint IO. While FSx offers performant and convenient storage for large scale training jobs, S3 allowed cost-efficient scaling to large multimodal datasets and model checkpoints.\\n\\nGoodput achieved weekly average values of up to 97% in pretraining runs through optimizations targeting lower job failure rate, minimizing checkpoint overhead, and overall reduction in the Mean Time to Restart (MTTR). This time is inclusive of time from the last successful checkpoint before training interruption, time taken to restart components of the system and resume training at steady state from checkpoint. Techniques such as fully distributed optimizer state and weight sharding and the elimination of all blocking overhead associated with checkpoint persistence resulted in a reduction of checkpointing overhead to ~1 sec on H100 clusters, and ~0.1 sec on TRN1 clusters. We exceeded our MTTR target of 9 minutes and achieved an average of 6.5 minutes on our TRN1 clusters by optimizing the\\n\\n[Footnotes]\\n10. https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin g-are-now-available/\\n11. https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s calable-hpc\\n12. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor e-gpus-for-accelerating-generative-ai-and-hpc-applications/\",\n",
              "    'images': [{'name': 'page_21.jpg',\n",
              "      'height': 792,\n",
              "      'width': 612,\n",
              "      'x': 0,\n",
              "      'y': 0,\n",
              "      'original_width': 1236,\n",
              "      'original_height': 1600,\n",
              "      'type': 'full_page_screenshot'}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'text',\n",
              "      'value': 'The Amazon Nova Family of Models\\n\\nevaluations, the Deloitte team probed more deeply into the questions the LLM originally replied with potentially concerning information.\\n\\nGomes Group: The Gomes Group at Carnegie Mellon University is at the forefront of integrating advanced artificial intelligence into chemical research. Their evaluation framework consisted of both automated and non-automated assessments. Two non-automated evaluations explored aggregation attack vulnerabilities through purchasing and remote chemical mixing scenarios. The automated evaluations utilized two distinct datasets: one containing 39 hazardous chemicals (including DEA Schedule I, II, and chemical warfare agents) and another with 362 common chemicals for NFPA diamond classifications. Three primary automated evaluations were conducted using the hazardous chemicals dataset. The NFPA diamond evaluation comprised 1,810 prompts, testing both single-turn and multi-turn approaches with consistent accuracy across both methods.\\n\\nNemesys: Nemesys Insights LLC run uplift studies, red teaming exercises, and risk assessments for a variety of technology companies and third-party research entities to assess national security related risks of large language models and other generative AI tools. For their testing, they started with human red teaming exercises focused on non-state acquisition or use of illicit radiological/nuclear (RN) materials, followed by prompt-response evaluation and uplift studies. The exercises comprised two different scenarios (a. violent non-state actor acquisition and use of Cobalt-60; b. non-state actor acquisition and international transport of HEU [highly enriched uranium]), and utilized 8 subject matter experts with operational and technological knowledge in a 2-team x 2-scenario design to construct and refine threat plans across a 6-hour planning cycle.',\n",
              "      'md': 'The Amazon Nova Family of Models\\n\\nevaluations, the Deloitte team probed more deeply into the questions the LLM originally replied with potentially concerning information.\\n\\nGomes Group: The Gomes Group at Carnegie Mellon University is at the forefront of integrating advanced artificial intelligence into chemical research. Their evaluation framework consisted of both automated and non-automated assessments. Two non-automated evaluations explored aggregation attack vulnerabilities through purchasing and remote chemical mixing scenarios. The automated evaluations utilized two distinct datasets: one containing 39 hazardous chemicals (including DEA Schedule I, II, and chemical warfare agents) and another with 362 common chemicals for NFPA diamond classifications. Three primary automated evaluations were conducted using the hazardous chemicals dataset. The NFPA diamond evaluation comprised 1,810 prompts, testing both single-turn and multi-turn approaches with consistent accuracy across both methods.\\n\\nNemesys: Nemesys Insights LLC run uplift studies, red teaming exercises, and risk assessments for a variety of technology companies and third-party research entities to assess national security related risks of large language models and other generative AI tools. For their testing, they started with human red teaming exercises focused on non-state acquisition or use of illicit radiological/nuclear (RN) materials, followed by prompt-response evaluation and uplift studies. The exercises comprised two different scenarios (a. violent non-state actor acquisition and use of Cobalt-60; b. non-state actor acquisition and international transport of HEU [highly enriched uranium]), and utilized 8 subject matter experts with operational and technological knowledge in a 2-team x 2-scenario design to construct and refine threat plans across a 6-hour planning cycle.',\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 424.01}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 3,\n",
              "      'value': '5.4.3 Automated Red Teaming',\n",
              "      'md': '### 5.4.3 Automated Red Teaming',\n",
              "      'bBox': {'x': 72, 'y': 292.04, 'w': 135, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Finally, to augment human based red teaming, we built an automated red teaming mechanism by adapting our (Feedback Loop In-context Red Teaming) FLIRT [52] framework. This approach helped us scale red teaming and repeat red teaming efficiently. FLIRT uses a list of seed prompts that have been identified by human evaluators as potentially violating one or more of our RAI dimensions. For every dimension, a subset of seeds is used to generate additional prompts with a dedicated language model, called red-LM, through in-context-learning (ICL) [18] and a carefully crafted set of instructions. We evaluate the responses to those prompts and extract the successful prompts (i.e., the ones triggering a prohibited response) for the next round of generation. The above steps are repeated for a chosen number of iterations across all RAI categories. We use our automated red teaming mechanism to evaluate both RAI adherence robustness and false refusals. We use the mechanism to generate adversarial tests across multi-turn interactions, multiple languages, and multiple input/output modalities to uncover and correct robustness issues in our models due to potential adversarial content in such interactions and inputs.',\n",
              "      'md': 'Finally, to augment human based red teaming, we built an automated red teaming mechanism by adapting our (Feedback Loop In-context Red Teaming) FLIRT [52] framework. This approach helped us scale red teaming and repeat red teaming efficiently. FLIRT uses a list of seed prompts that have been identified by human evaluators as potentially violating one or more of our RAI dimensions. For every dimension, a subset of seeds is used to generate additional prompts with a dedicated language model, called red-LM, through in-context-learning (ICL) [18] and a carefully crafted set of instructions. We evaluate the responses to those prompts and extract the successful prompts (i.e., the ones triggering a prohibited response) for the next round of generation. The above steps are repeated for a chosen number of iterations across all RAI categories. We use our automated red teaming mechanism to evaluate both RAI adherence robustness and false refusals. We use the mechanism to generate adversarial tests across multi-turn interactions, multiple languages, and multiple input/output modalities to uncover and correct robustness issues in our models due to potential adversarial content in such interactions and inputs.',\n",
              "      'bBox': {'x': 71, 'y': 292.04, 'w': 469, 'h': 138}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 2,\n",
              "      'value': '6 Training Infrastructure',\n",
              "      'md': '## 6 Training Infrastructure',\n",
              "      'bBox': {'x': 72, 'y': 446.04, 'w': 138, 'h': 12}},\n",
              "     {'type': 'text',\n",
              "      'value': \"The Nova family of models were trained on Amazon's custom Trainium1 (TRN1) chips, NVidia A100 (P4d instances), and H100 (P5 instances) accelerators. Working with AWS SageMaker, we stood up NVidia GPU and TRN1 clusters and ran parallel trainings to ensure model performance parity, while optimizing training throughput on the different stacks. All clusters utilize petabit-scale non-blocking EFA network fabric which is less prone to packet loss than other network transport protocols and provides the highest network bandwidth with H100 accelerators compared to any other instance type available on AWS EC2. We conducted distributed training on AWS SageMaker-managed Elastic Kubernetes Service (EKS) clusters, and utilized AWS File System X (FSx) and Simple Storage Solution (S3) for data and checkpoint IO. While FSx offers performant and convenient storage for large scale training jobs, S3 allowed cost-efficient scaling to large multimodal datasets and model checkpoints.\\n\\nGoodput achieved weekly average values of up to 97% in pretraining runs through optimizations targeting lower job failure rate, minimizing checkpoint overhead, and overall reduction in the Mean Time to Restart (MTTR). This time is inclusive of time from the last successful checkpoint before training interruption, time taken to restart components of the system and resume training at steady state from checkpoint. Techniques such as fully distributed optimizer state and weight sharding and the elimination of all blocking overhead associated with checkpoint persistence resulted in a reduction of checkpointing overhead to ~1 sec on H100 clusters, and ~0.1 sec on TRN1 clusters. We exceeded our MTTR target of 9 minutes and achieved an average of 6.5 minutes on our TRN1 clusters by optimizing the\\n\\n[Footnotes]\\n10. https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin g-are-now-available/\\n11. https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s calable-hpc\\n12. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor e-gpus-for-accelerating-generative-ai-and-hpc-applications/\",\n",
              "      'md': \"The Nova family of models were trained on Amazon's custom Trainium1 (TRN1) chips, NVidia A100 (P4d instances), and H100 (P5 instances) accelerators. Working with AWS SageMaker, we stood up NVidia GPU and TRN1 clusters and ran parallel trainings to ensure model performance parity, while optimizing training throughput on the different stacks. All clusters utilize petabit-scale non-blocking EFA network fabric which is less prone to packet loss than other network transport protocols and provides the highest network bandwidth with H100 accelerators compared to any other instance type available on AWS EC2. We conducted distributed training on AWS SageMaker-managed Elastic Kubernetes Service (EKS) clusters, and utilized AWS File System X (FSx) and Simple Storage Solution (S3) for data and checkpoint IO. While FSx offers performant and convenient storage for large scale training jobs, S3 allowed cost-efficient scaling to large multimodal datasets and model checkpoints.\\n\\nGoodput achieved weekly average values of up to 97% in pretraining runs through optimizations targeting lower job failure rate, minimizing checkpoint overhead, and overall reduction in the Mean Time to Restart (MTTR). This time is inclusive of time from the last successful checkpoint before training interruption, time taken to restart components of the system and resume training at steady state from checkpoint. Techniques such as fully distributed optimizer state and weight sharding and the elimination of all blocking overhead associated with checkpoint persistence resulted in a reduction of checkpointing overhead to ~1 sec on H100 clusters, and ~0.1 sec on TRN1 clusters. We exceeded our MTTR target of 9 minutes and achieved an average of 6.5 minutes on our TRN1 clusters by optimizing the\\n\\n[Footnotes]\\n10. https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin g-are-now-available/\\n11. https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s calable-hpc\\n12. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor e-gpus-for-accelerating-generative-ai-and-hpc-applications/\",\n",
              "      'bBox': {'x': 72, 'y': 446.04, 'w': 468, 'h': 273.99}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-training-are-now-available/',\n",
              "      'text': 'https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin g-are-now-available/'},\n",
              "     {'url': 'https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-training-are-now-available/',\n",
              "      'text': 'g-are-now-available/'},\n",
              "     {'url': 'https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-scalable-hpc',\n",
              "      'text': 'https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s calable-hpc'},\n",
              "     {'url': 'https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-scalable-hpc',\n",
              "      'text': 'calable-hpc'},\n",
              "     {'url': 'https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/',\n",
              "      'text': 'https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor e-gpus-for-accelerating-generative-ai-and-hpc-applications/'},\n",
              "     {'url': 'https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/',\n",
              "      'text': 'e-gpus-for-accelerating-generative-ai-and-hpc-applications/'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': True,\n",
              "    'parsingMode': 'premium',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 22,\n",
              "    'text': '                                       The Amazon Nova Family of Models\\nnode communication initialization in the training startup process and reduced time to load checkpoints through an\\nasynchronous observer process. This process maps each latest checkpoint file to its corresponding node in the cluster.\\nWhen resuming from the checkpoint, each node only loads the checkpoint files for its corresponding rank, reducing\\nthe time taken to discover the latest checkpoint from 3 minutes to 5 seconds. We also cache and reuse data indices to\\noptimize training data loading initialization time. These improvements reduced data loading initialization to 205ms per\\nrestart.\\nTo increase training efficiency we developed a new activation checkpointing scheme called Super-Selective Activation\\nCheckpointing (SSC). SSC minimizes activation re-computation in memory-constrained environments, reducing mem-\\nory consumption by ~50% while adding ~2% re-computation overhead compared to NVidia’s Selective Checkpointing.\\nWe also found optimizations in default gradient reduction behavior and the default PyTorch memory allocator behavior.\\nThe default gradient reduction behavior leads to suboptimal communication overlap and we found the synchronous\\nnature of the default PyTorch allocation led to stragglers in collectives resulting in multiple stalled workers. We adjusted\\nthe gradient reduction order and frequency, allowing us to overlap the majority of data parallelism communication.\\n                                                        22',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\nNode communication initialization in the training startup process and reduced time to load checkpoints through an asynchronous observer process. This process maps each latest checkpoint file to its corresponding node in the cluster. When resuming from the checkpoint, each node only loads the checkpoint files for its corresponding rank, reducing the time taken to discover the latest checkpoint from 3 minutes to 5 seconds. We also cache and reuse data indices to optimize training data loading initialization time. These improvements reduced data loading initialization to 205ms per restart.\\n\\nTo increase training efficiency we developed a new activation checkpointing scheme called Super-Selective Activation Checkpointing (SSC). SSC minimizes activation re-computation in memory-constrained environments, reducing memory consumption by ~50% while adding ~2% re-computation overhead compared to NVidia’s Selective Checkpointing. We also found optimizations in default gradient reduction behavior and the default PyTorch memory allocator behavior. The default gradient reduction behavior leads to suboptimal communication overlap and we found the synchronous nature of the default PyTorch allocation led to stragglers in collectives resulting in multiple stalled workers. We adjusted the gradient reduction order and frequency, allowing us to overlap the majority of data parallelism communication.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Node communication initialization in the training startup process and reduced time to load checkpoints through an asynchronous observer process. This process maps each latest checkpoint file to its corresponding node in the cluster. When resuming from the checkpoint, each node only loads the checkpoint files for its corresponding rank, reducing the time taken to discover the latest checkpoint from 3 minutes to 5 seconds. We also cache and reuse data indices to optimize training data loading initialization time. These improvements reduced data loading initialization to 205ms per restart.\\n\\nTo increase training efficiency we developed a new activation checkpointing scheme called Super-Selective Activation Checkpointing (SSC). SSC minimizes activation re-computation in memory-constrained environments, reducing memory consumption by ~50% while adding ~2% re-computation overhead compared to NVidia’s Selective Checkpointing. We also found optimizations in default gradient reduction behavior and the default PyTorch memory allocator behavior. The default gradient reduction behavior leads to suboptimal communication overlap and we found the synchronous nature of the default PyTorch allocation led to stragglers in collectives resulting in multiple stalled workers. We adjusted the gradient reduction order and frequency, allowing us to overlap the majority of data parallelism communication.',\n",
              "      'md': 'Node communication initialization in the training startup process and reduced time to load checkpoints through an asynchronous observer process. This process maps each latest checkpoint file to its corresponding node in the cluster. When resuming from the checkpoint, each node only loads the checkpoint files for its corresponding rank, reducing the time taken to discover the latest checkpoint from 3 minutes to 5 seconds. We also cache and reuse data indices to optimize training data loading initialization time. These improvements reduced data loading initialization to 205ms per restart.\\n\\nTo increase training efficiency we developed a new activation checkpointing scheme called Super-Selective Activation Checkpointing (SSC). SSC minimizes activation re-computation in memory-constrained environments, reducing memory consumption by ~50% while adding ~2% re-computation overhead compared to NVidia’s Selective Checkpointing. We also found optimizations in default gradient reduction behavior and the default PyTorch memory allocator behavior. The default gradient reduction behavior leads to suboptimal communication overlap and we found the synchronous nature of the default PyTorch allocation led to stragglers in collectives resulting in multiple stalled workers. We adjusted the gradient reduction order and frequency, allowing us to overlap the majority of data parallelism communication.',\n",
              "      'bBox': {'x': 71, 'y': 72.04, 'w': 471, 'h': 147}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 23,\n",
              "    'text': '                                         The Amazon Nova Family of Models\\n     References\\n      [1] Efficient Batch Computing – AWS Batch - AWS, 2024. URL https://aws.amazon.com/batch/.\\n      [2] Big Data Platform - Amazon EMR - AWS, 2024. URL https://aws.amazon.com/emr/.\\n      [3]  AgentStudio. Gemini flash. https://computer-agents.github.io/agent-studio/, 2024. Accessed:\\n           2024-11-29.\\n      [4]  P. Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. D. Monicault, S. Garg,\\n           T. Gervet, S. Ghosh, A. Héliou, P. Jacob, A. Q. Jiang, K. Khandelwal, T. Lacroix, G. Lample, D. L. Casas,\\n           T. Lavril, T. L. Scao, A. Lo, W. Marshall, L. Martin, A. Mensch, P. Muddireddy, V. Nemychnikova, M. Pellat,\\n           P. V. Platen, N. Raghuraman, B. Rozière, A. Sablayrolles, L. Saulnier, R. Sauvestre, W. Shang, R. Soletskyi,\\n           L. Stewart, P. Stock, J. Studnia, S. Subramanian, S. Vaze, T. Wang, and S. Yang. Pixtral 12B, 2024. URL\\n           https://arxiv.org/abs/2410.07073.\\n      [5]  Amazon. Amazon joins Partnership on AI. https://www.aboutamazon.com/news/amazon-ai/amazon-j\\n           oins-partnership-on-ai, 2016. Accessed: 2024-11-20.\\n      [6]  Amazon. Our commitment to the responsible use of AI. https://www.aboutamazon.com/news/company-n\\n           ews/amazon-responsible-ai, 2023. Accessed: 2024-11-20.\\n      [7]  Amazon. Amazon joins US Artificial Intelligence safety institute to advance responsible AI. https://www.abou\\n           tamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-i\\n           nstitute-to-advance-responsible-ai, 2024. Accessed: 2024-11-20.\\n      [8]  Amazon. Amazon SageMaker Clarify. https://aws.amazon.com/sagemaker/clarify/, 2024. Accessed:\\n           2024-11-20.\\n      [9]  Amazon. Data protection & privacy at AWS. https://aws.amazon.com/compliance/data-protection/,\\n           2024. Accessed: 2024-11-20.\\n     [10] Amazon.   Building AI responsibly at AWS.               https://aws.amazon.com/ai/responsible-ai/, 2024.\\n           Accessed: 2024-11-20.\\n     [11] Anthropic.  The Claude 3 model family: Opus, Sonnet, Haiku.     Technical report, Anthropic, 2023.   URL\\n           https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Cla\\n           ude_3.pdf.\\n     [12] Anthropic. Claude Sonnet. https://www.anthropic.com/claude/sonnet, 2024. Accessed: 2024-11-20.\\n     [13] Anthropic AI. Claude 3.5 Sonnet model card addendum. Technical report, 2024.\\n           [14] Anthropic AI Team. Claude 3.5 Haiku and upgraded Claude 3.5 Sonnet, 2024. URL https://assets.anthr\\n           opic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf.\\n     [15] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.\\n     [16]  J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image\\n           generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\\n     [17] Black Forest Labs. Flux models. 2024. URL https://github.com/black-forest-labs/flux.\\n     [18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\\n           A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n     [19]  M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman,\\n           et al. Evaluating large language models trained on code, 2021.\\n[20] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. N. Moussa, M. I. Beane, T.-H. K. Huang, B. R.\\n           Routledge, and W. Y. Wang. FinQA: A dataset of numerical reasoning over financial data. ArXiv, abs/2109.00122,\\n           2021. URL https://api.semanticscholar.org/CorpusID:235399966.\\n     [21] J. Cho, A. Zala, and M. Bansal. DALL-eval: Probing the reasoning skills and social biases of text-to-image\\n           generation models. In    Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\\n           3043–3054, 2023.\\n                                                          23',\n",
              "    'md': 'The Amazon Nova Family of Models\\n\\n## References\\n\\n[1] Efficient Batch Computing – AWS Batch - AWS, 2024. URL https://aws.amazon.com/batch/.\\n\\n[2] Big Data Platform - Amazon EMR - AWS, 2024. URL https://aws.amazon.com/emr/.\\n\\n[3] AgentStudio. Gemini flash. https://computer-agents.github.io/agent-studio/, 2024. Accessed: 2024-11-29.\\n\\n[4] P. Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. D. Monicault, S. Garg, T. Gervet, S. Ghosh, A. Héliou, P. Jacob, A. Q. Jiang, K. Khandelwal, T. Lacroix, G. Lample, D. L. Casas, T. Lavril, T. L. Scao, A. Lo, W. Marshall, L. Martin, A. Mensch, P. Muddireddy, V. Nemychnikova, M. Pellat, P. V. Platen, N. Raghuraman, B. Rozière, A. Sablayrolles, L. Saulnier, R. Sauvestre, W. Shang, R. Soletskyi, L. Stewart, P. Stock, J. Studnia, S. Subramanian, S. Vaze, T. Wang, and S. Yang. Pixtral 12B, 2024. URL https://arxiv.org/abs/2410.07073.\\n\\n[5] Amazon. Amazon joins Partnership on AI. https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai, 2016. Accessed: 2024-11-20.\\n\\n[6] Amazon. Our commitment to the responsible use of AI. https://www.aboutamazon.com/news/company-news/amazon-responsible-ai, 2023. Accessed: 2024-11-20.\\n\\n[7] Amazon. Amazon joins US Artificial Intelligence safety institute to advance responsible AI. https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai, 2024. Accessed: 2024-11-20.\\n\\n[8] Amazon. Amazon SageMaker Clarify. https://aws.amazon.com/sagemaker/clarify/, 2024. Accessed: 2024-11-20.\\n\\n[9] Amazon. Data protection & privacy at AWS. https://aws.amazon.com/compliance/data-protection/, 2024. Accessed: 2024-11-20.\\n\\n[10] Amazon. Building AI responsibly at AWS. https://aws.amazon.com/ai/responsible-ai/, 2024. Accessed: 2024-11-20.\\n\\n[11] Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Technical report, Anthropic, 2023. URL https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n\\n[12] Anthropic. Claude Sonnet. https://www.anthropic.com/claude/sonnet, 2024. Accessed: 2024-11-20.\\n\\n[13] Anthropic AI. Claude 3.5 Sonnet model card addendum. Technical report, 2024.\\n\\n[14] Anthropic AI Team. Claude 3.5 Haiku and upgraded Claude 3.5 Sonnet, 2024. URL https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf.\\n\\n[15] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.\\n\\n[16] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\\n\\n[17] Black Forest Labs. Flux models. 2024. URL https://github.com/black-forest-labs/flux.\\n\\n[18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\n[19] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code, 2021.\\n\\n[20] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. N. Moussa, M. I. Beane, T.-H. K. Huang, B. R. Routledge, and W. Y. Wang. FinQA: A dataset of numerical reasoning over financial data. ArXiv, abs/2109.00122, 2021. URL https://api.semanticscholar.org/CorpusID:235399966.\\n\\n[21] J. Cho, A. Zala, and M. Bansal. DALL-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3043–3054, 2023.',\n",
              "    'images': [{'name': 'page_23.jpg',\n",
              "      'height': 792,\n",
              "      'width': 612,\n",
              "      'x': 0,\n",
              "      'y': 0,\n",
              "      'original_width': 1236,\n",
              "      'original_height': 1600,\n",
              "      'type': 'full_page_screenshot'}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'text',\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': 'The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 2,\n",
              "      'value': 'References',\n",
              "      'md': '## References',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 56, 'h': 12}},\n",
              "     {'type': 'text',\n",
              "      'value': '[1] Efficient Batch Computing – AWS Batch - AWS, 2024. URL https://aws.amazon.com/batch/.\\n\\n[2] Big Data Platform - Amazon EMR - AWS, 2024. URL https://aws.amazon.com/emr/.\\n\\n[3] AgentStudio. Gemini flash. https://computer-agents.github.io/agent-studio/, 2024. Accessed: 2024-11-29.\\n\\n[4] P. Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. D. Monicault, S. Garg, T. Gervet, S. Ghosh, A. Héliou, P. Jacob, A. Q. Jiang, K. Khandelwal, T. Lacroix, G. Lample, D. L. Casas, T. Lavril, T. L. Scao, A. Lo, W. Marshall, L. Martin, A. Mensch, P. Muddireddy, V. Nemychnikova, M. Pellat, P. V. Platen, N. Raghuraman, B. Rozière, A. Sablayrolles, L. Saulnier, R. Sauvestre, W. Shang, R. Soletskyi, L. Stewart, P. Stock, J. Studnia, S. Subramanian, S. Vaze, T. Wang, and S. Yang. Pixtral 12B, 2024. URL https://arxiv.org/abs/2410.07073.\\n\\n[5] Amazon. Amazon joins Partnership on AI. https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai, 2016. Accessed: 2024-11-20.\\n\\n[6] Amazon. Our commitment to the responsible use of AI. https://www.aboutamazon.com/news/company-news/amazon-responsible-ai, 2023. Accessed: 2024-11-20.\\n\\n[7] Amazon. Amazon joins US Artificial Intelligence safety institute to advance responsible AI. https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai, 2024. Accessed: 2024-11-20.\\n\\n[8] Amazon. Amazon SageMaker Clarify. https://aws.amazon.com/sagemaker/clarify/, 2024. Accessed: 2024-11-20.\\n\\n[9] Amazon. Data protection & privacy at AWS. https://aws.amazon.com/compliance/data-protection/, 2024. Accessed: 2024-11-20.\\n\\n[10] Amazon. Building AI responsibly at AWS. https://aws.amazon.com/ai/responsible-ai/, 2024. Accessed: 2024-11-20.\\n\\n[11] Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Technical report, Anthropic, 2023. URL https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n\\n[12] Anthropic. Claude Sonnet. https://www.anthropic.com/claude/sonnet, 2024. Accessed: 2024-11-20.\\n\\n[13] Anthropic AI. Claude 3.5 Sonnet model card addendum. Technical report, 2024.\\n\\n[14] Anthropic AI Team. Claude 3.5 Haiku and upgraded Claude 3.5 Sonnet, 2024. URL https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf.\\n\\n[15] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.\\n\\n[16] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\\n\\n[17] Black Forest Labs. Flux models. 2024. URL https://github.com/black-forest-labs/flux.\\n\\n[18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\n[19] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code, 2021.\\n\\n[20] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. N. Moussa, M. I. Beane, T.-H. K. Huang, B. R. Routledge, and W. Y. Wang. FinQA: A dataset of numerical reasoning over financial data. ArXiv, abs/2109.00122, 2021. URL https://api.semanticscholar.org/CorpusID:235399966.\\n\\n[21] J. Cho, A. Zala, and M. Bansal. DALL-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3043–3054, 2023.',\n",
              "      'md': '[1] Efficient Batch Computing – AWS Batch - AWS, 2024. URL https://aws.amazon.com/batch/.\\n\\n[2] Big Data Platform - Amazon EMR - AWS, 2024. URL https://aws.amazon.com/emr/.\\n\\n[3] AgentStudio. Gemini flash. https://computer-agents.github.io/agent-studio/, 2024. Accessed: 2024-11-29.\\n\\n[4] P. Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. D. Monicault, S. Garg, T. Gervet, S. Ghosh, A. Héliou, P. Jacob, A. Q. Jiang, K. Khandelwal, T. Lacroix, G. Lample, D. L. Casas, T. Lavril, T. L. Scao, A. Lo, W. Marshall, L. Martin, A. Mensch, P. Muddireddy, V. Nemychnikova, M. Pellat, P. V. Platen, N. Raghuraman, B. Rozière, A. Sablayrolles, L. Saulnier, R. Sauvestre, W. Shang, R. Soletskyi, L. Stewart, P. Stock, J. Studnia, S. Subramanian, S. Vaze, T. Wang, and S. Yang. Pixtral 12B, 2024. URL https://arxiv.org/abs/2410.07073.\\n\\n[5] Amazon. Amazon joins Partnership on AI. https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai, 2016. Accessed: 2024-11-20.\\n\\n[6] Amazon. Our commitment to the responsible use of AI. https://www.aboutamazon.com/news/company-news/amazon-responsible-ai, 2023. Accessed: 2024-11-20.\\n\\n[7] Amazon. Amazon joins US Artificial Intelligence safety institute to advance responsible AI. https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai, 2024. Accessed: 2024-11-20.\\n\\n[8] Amazon. Amazon SageMaker Clarify. https://aws.amazon.com/sagemaker/clarify/, 2024. Accessed: 2024-11-20.\\n\\n[9] Amazon. Data protection & privacy at AWS. https://aws.amazon.com/compliance/data-protection/, 2024. Accessed: 2024-11-20.\\n\\n[10] Amazon. Building AI responsibly at AWS. https://aws.amazon.com/ai/responsible-ai/, 2024. Accessed: 2024-11-20.\\n\\n[11] Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Technical report, Anthropic, 2023. URL https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n\\n[12] Anthropic. Claude Sonnet. https://www.anthropic.com/claude/sonnet, 2024. Accessed: 2024-11-20.\\n\\n[13] Anthropic AI. Claude 3.5 Sonnet model card addendum. Technical report, 2024.\\n\\n[14] Anthropic AI Team. Claude 3.5 Haiku and upgraded Claude 3.5 Sonnet, 2024. URL https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf.\\n\\n[15] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.\\n\\n[16] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\\n\\n[17] Black Forest Labs. Flux models. 2024. URL https://github.com/black-forest-labs/flux.\\n\\n[18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\n[19] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code, 2021.\\n\\n[20] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. N. Moussa, M. I. Beane, T.-H. K. Huang, B. R. Routledge, and W. Y. Wang. FinQA: A dataset of numerical reasoning over financial data. ArXiv, abs/2109.00122, 2021. URL https://api.semanticscholar.org/CorpusID:235399966.\\n\\n[21] J. Cho, A. Zala, and M. Bansal. DALL-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3043–3054, 2023.',\n",
              "      'bBox': {'x': 71, 'y': 90.04, 'w': 470, 'h': 660}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://aws.amazon.com/batch/',\n",
              "      'text': 'https://aws.amazon.com/batch/ .'},\n",
              "     {'url': 'https://aws.amazon.com/emr/',\n",
              "      'text': 'https://aws.amazon.com/emr/ .'},\n",
              "     {'url': 'https://computer-agents.github.io/agent-studio/',\n",
              "      'text': 'https://computer-agents.github.io/agent-studio/ , 2024. Accessed:'},\n",
              "     {'url': 'https://arxiv.org/abs/2410.07073',\n",
              "      'text': 'https://arxiv.org/abs/2410.07073 .'},\n",
              "     {'url': 'https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai',\n",
              "      'text': 'https://www.aboutamazon.com/news/amazon-ai/amazon-j , 2016. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai',\n",
              "      'text': 'oins-partnership-on-ai , 2016. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://www.aboutamazon.com/news/company-news/amazon-responsible-ai',\n",
              "      'text': 'https://www.aboutamazon.com/news/company-n , 2023. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://www.aboutamazon.com/news/company-news/amazon-responsible-ai',\n",
              "      'text': 'ews/amazon-responsible-ai , 2023. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai',\n",
              "      'text': 'https://www.abou tamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-i'},\n",
              "     {'url': 'https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai',\n",
              "      'text': 'tamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-i'},\n",
              "     {'url': 'https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai',\n",
              "      'text': 'nstitute-to-advance-responsible-ai , 2024. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://aws.amazon.com/sagemaker/clarify/',\n",
              "      'text': 'https://aws.amazon.com/sagemaker/clarify/ , 2024. Accessed:'},\n",
              "     {'url': 'https://aws.amazon.com/compliance/data-protection/',\n",
              "      'text': 'https://aws.amazon.com/compliance/data-protection/ ,'},\n",
              "     {'url': 'https://aws.amazon.com/ai/responsible-ai/',\n",
              "      'text': 'https://aws.amazon.com/ai/responsible-ai/ , 2024.'},\n",
              "     {'url': 'https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf',\n",
              "      'text': 'https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Cla'},\n",
              "     {'url': 'https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf',\n",
              "      'text': 'ude_3.pdf .'},\n",
              "     {'url': 'https://www.anthropic.com/claude/sonnet',\n",
              "      'text': 'https://www.anthropic.com/claude/sonnet , 2024. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf',\n",
              "      'text': 'https://assets.anthr opic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf'},\n",
              "     {'url': 'https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf',\n",
              "      'text': 'opic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf .'},\n",
              "     {'url': 'https://github.com/black-forest-labs/flux',\n",
              "      'text': 'https://github.com/black-forest-labs/flux .'},\n",
              "     {'url': 'https://api.semanticscholar.org/CorpusID:235399966',\n",
              "      'text': 'https://api.semanticscholar.org/CorpusID:235399966 .'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': True,\n",
              "    'parsingMode': 'premium',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 24,\n",
              "    'text': '                                         The Amazon Nova Family of Models\\n  [22] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved\\n          question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018.\\n    [23] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,\\n          C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,\\n          2021.\\n    [24] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a generalist\\n          agent for the web. In NeurIPS, 2023.\\n     [25] J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun, K.-W. Chang, and R. Gupta. BOLD: Dataset and\\n          metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on\\n         Fairness, Accountability, and Transparency, FAccT ’21, page 862–872, New York, NY, USA, 2021. Association\\n          for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/\\n          10.1145/3442188.3445924.\\n  [26] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark\\n          requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.\\n    [27]  P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al.\\n          Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on\\n          Machine Learning, 2024. URL https://huggingface.co/stabilityai/stable-diffusion-3-medium.\\n    [28]  P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al.\\n          Stable Diffusion 3.5. 2024. URL https://stability.ai/news/introducing-stable-diffusion-3-5.\\n    [29] Frontier Model Forum.   Amazon and Meta join the Frontier Model Forum to promote AI safety.       https:\\n          //www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-t\\n          o-promote-ai-safety/, 2024. Accessed: 2024-11-20.\\n   [30] G7 Hiroshima Summit. Hiroshima process international code of conduct for organizations developing advanced\\n          AI systems. https://www.mofa.go.jp/files/100573473.pdf, 2023. Accessed: 2024-11-20.\\n     [31] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic\\n          degeneration in language models. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for\\n          Computational Linguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational\\n          Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings\\n        -emnlp.301.\\n    [32]  Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL\\n          https://arxiv.org/abs/2403.05530.\\n    [33]  Google Deepmind. Gemini Flash. https://deepmind.google/technologies/gemini/flash/, 2024.\\n          Accessed: 2024-11-20.\\n    [34] N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzmán, and A. Fan.\\n          The FLORES-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.\\n[35] F. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. Two new evaluation\\n          datasets for low-resource machine translation: Nepali-english and sinhala-english. 2019.\\n    [36] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt.      Measuring massive\\n          multitask language understanding. In International Conference on Learning Representations, 2021.   URL\\n          https://openreview.net/forum?id=d7KBjmI3GmQ.\\n    [37]  D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring\\n          mathematical problem solving with the MATH dataset. NeurIPS, 2021.\\n    [38]  Y. Hu, B. Liu, J. Kasai, Y. Wang, M. Ostendorf, R. Krishna, and N. A. Smith. TIFA: Accurate and interpretable\\n          text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International\\n          Conference on Computer Vision, pages 20406–20417, 2023.\\n    [39] R. Islam and O. M. Moushi. GPT-4o: The cutting-edge advancement in multimodal LLM. Technical report, 2024.\\n                                                         24',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n1. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018.\\n2. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n3. X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a generalist agent for the web. In NeurIPS, 2023.\\n4. J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun, K.-W. Chang, and R. Gupta. BOLD: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 862–872, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/10.1145/3442188.3445924.\\n5. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.\\n6. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. URL https://huggingface.co/stabilityai/stable-diffusion-3-medium.\\n7. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Stable Diffusion 3.5. 2024. URL https://stability.ai/news/introducing-stable-diffusion-3-5.\\n8. Frontier Model Forum. Amazon and Meta join the Frontier Model Forum to promote AI safety. https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/, 2024. Accessed: 2024-11-20.\\n9. G7 Hiroshima Summit. Hiroshima process international code of conduct for organizations developing advanced AI systems. https://www.mofa.go.jp/files/100573473.pdf, 2023. Accessed: 2024-11-20.\\n10. S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.\\n11. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530.\\n12. Google Deepmind. Gemini Flash. https://deepmind.google/technologies/gemini/flash/, 2024. Accessed: 2024-11-20.\\n13. N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzmán, and A. Fan. The FLORES-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.\\n14. F. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. Two new evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english. 2019.\\n15. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\\n16. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. NeurIPS, 2021.\\n17. Y. Hu, B. Liu, J. Kasai, Y. Wang, M. Ostendorf, R. Krishna, and N. A. Smith. TIFA: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20406–20417, 2023.\\n18. R. Islam and O. M. Moushi. GPT-4o: The cutting-edge advancement in multimodal LLM. Technical report, 2024.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': '1. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018.\\n2. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n3. X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a generalist agent for the web. In NeurIPS, 2023.\\n4. J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun, K.-W. Chang, and R. Gupta. BOLD: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 862–872, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/10.1145/3442188.3445924.\\n5. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.\\n6. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. URL https://huggingface.co/stabilityai/stable-diffusion-3-medium.\\n7. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Stable Diffusion 3.5. 2024. URL https://stability.ai/news/introducing-stable-diffusion-3-5.\\n8. Frontier Model Forum. Amazon and Meta join the Frontier Model Forum to promote AI safety. https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/, 2024. Accessed: 2024-11-20.\\n9. G7 Hiroshima Summit. Hiroshima process international code of conduct for organizations developing advanced AI systems. https://www.mofa.go.jp/files/100573473.pdf, 2023. Accessed: 2024-11-20.\\n10. S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.\\n11. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530.\\n12. Google Deepmind. Gemini Flash. https://deepmind.google/technologies/gemini/flash/, 2024. Accessed: 2024-11-20.\\n13. N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzmán, and A. Fan. The FLORES-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.\\n14. F. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. Two new evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english. 2019.\\n15. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\\n16. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. NeurIPS, 2021.\\n17. Y. Hu, B. Liu, J. Kasai, Y. Wang, M. Ostendorf, R. Krishna, and N. A. Smith. TIFA: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20406–20417, 2023.\\n18. R. Islam and O. M. Moushi. GPT-4o: The cutting-edge advancement in multimodal LLM. Technical report, 2024.',\n",
              "      'md': '1. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018.\\n2. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n3. X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a generalist agent for the web. In NeurIPS, 2023.\\n4. J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun, K.-W. Chang, and R. Gupta. BOLD: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 862–872, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/10.1145/3442188.3445924.\\n5. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.\\n6. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. URL https://huggingface.co/stabilityai/stable-diffusion-3-medium.\\n7. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Stable Diffusion 3.5. 2024. URL https://stability.ai/news/introducing-stable-diffusion-3-5.\\n8. Frontier Model Forum. Amazon and Meta join the Frontier Model Forum to promote AI safety. https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/, 2024. Accessed: 2024-11-20.\\n9. G7 Hiroshima Summit. Hiroshima process international code of conduct for organizations developing advanced AI systems. https://www.mofa.go.jp/files/100573473.pdf, 2023. Accessed: 2024-11-20.\\n10. S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.\\n11. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530.\\n12. Google Deepmind. Gemini Flash. https://deepmind.google/technologies/gemini/flash/, 2024. Accessed: 2024-11-20.\\n13. N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzmán, and A. Fan. The FLORES-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.\\n14. F. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. Two new evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english. 2019.\\n15. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\\n16. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. NeurIPS, 2021.\\n17. Y. Hu, B. Liu, J. Kasai, Y. Wang, M. Ostendorf, R. Krishna, and N. A. Smith. TIFA: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20406–20417, 2023.\\n18. R. Islam and O. M. Moushi. GPT-4o: The cutting-edge advancement in multimodal LLM. Technical report, 2024.',\n",
              "      'bBox': {'x': 90, 'y': 83.04, 'w': 452, 'h': 667}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://doi.org/10.1145/3442188.3445924',\n",
              "      'text': 'https://doi.org/'},\n",
              "     {'url': 'https://doi.org/10.1145/3442188.3445924',\n",
              "      'text': '10.1145/3442188.3445924 .'},\n",
              "     {'url': 'https://huggingface.co/stabilityai/stable-diffusion-3-medium',\n",
              "      'text': 'https://huggingface.co/stabilityai/stable-diffusion-3-medium .'},\n",
              "     {'url': 'https://stability.ai/news/introducing-stable-diffusion-3-5',\n",
              "      'text': 'https://stability.ai/news/introducing-stable-diffusion-3-5 .'},\n",
              "     {'url': 'https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/',\n",
              "      'text': 'https: //www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-t'},\n",
              "     {'url': 'https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/',\n",
              "      'text': '//www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-t'},\n",
              "     {'url': 'https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/',\n",
              "      'text': 'o-promote-ai-safety/ , 2024. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://www.mofa.go.jp/files/100573473.pdf',\n",
              "      'text': 'https://www.mofa.go.jp/files/100573473.pdf , 2023. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://aclanthology.org/2020.findings-emnlp.301',\n",
              "      'text': 'https://aclanthology.org/2020.findings'},\n",
              "     {'url': 'https://aclanthology.org/2020.findings-emnlp.301', 'text': '.'},\n",
              "     {'url': 'https://arxiv.org/abs/2403.05530',\n",
              "      'text': 'https://arxiv.org/abs/2403.05530 .'},\n",
              "     {'url': 'https://deepmind.google/technologies/gemini/flash/',\n",
              "      'text': 'https://deepmind.google/technologies/gemini/flash/ , 2024.'},\n",
              "     {'url': 'https://openreview.net/forum?id=d7KBjmI3GmQ',\n",
              "      'text': 'https://openreview.net/forum?id=d7KBjmI3GmQ .'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 25,\n",
              "    'text': '                                     The Amazon Nova Family of Models\\n[40] G. Kamradt. LLMTest NeedleInAHaystack, 2023. URL https://github.com/gkamradt/LLMTestNeedleI\\n      nAHaystack/blob/main/README.md.\\n[41] D. P. Kingma. Auto-encoding variational Bayes.  2nd International Conference on Learning Representations,\\n      ICLR, 2014.\\n[42] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO:\\n      Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,\\n      September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\\n[43] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. VisualWebBench: How far have multimodal\\n      llms evolved in web page understanding and grounding?, 2024.\\n[44] X. Liu, Y. Zhu, J. Gu, Y. Lan, C. Yang, and Y. Qiao. MM-SafetyBench: A benchmark for safety evaluation of\\n      multimodal large language models. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol,\\n      editors, Computer Vision – ECCV 2024, pages 386–403, Cham, 2025. Springer Nature Switzerland.       ISBN\\n      978-3-031-72992-8.\\n[45] Llama Team, AI Meta. The Llama 3 herd of models, 2024. URL https://github.com/meta-llama/llama\\n    -models/blob/main/models/llama3_1/MODEL_CARD.md.\\n[46] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play\\n      compositional reasoning with large language models. In The 37th Conference on Neural Information Processing\\n      Systems (NeurIPS), 2023.\\n[47] Luma Labs, 2024. URL https://lumalabs.ai/dream-machine.\\n[48]  L. Madaan, A. K. Singh, R. Schaeffer, A. Poulton, S. Koyejo, P. Stenetorp, S. Narang, and D. Hupkes. Quantifying\\n      variance in evaluation benchmarks, 2024. URL https://arxiv.org/abs/2406.10229.\\n[49] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: A diagnostic benchmark for very long-form video\\n      language understanding. In NeurIPS, 2023.\\n[50] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question answering about\\n      charts with visual and logical reasoning. In ACL Findings, 2022.\\n[51] M. Mathew, D. Karatzas, and C. Jawahar. DocVQA: A dataset for VQA on document images. In WACV, 2021.\\n[52]  N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta. FLIRT:\\n      Feedback loop in-context red teaming. In EMNLP 2024, 2024. URL https://www.amazon.science/publi\\n      cations/flirt-feedback-loop-in-context-red-teaming.\\n[53]  Meta. Llama 3.2 Github model card vision.                https://github.com/meta-llama/llama-models/blob\\n      /main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models, 2024.                     Accessed:\\n      2024-11-20.\\n[54]  Y. Onoe, S. Rane, Z. Berger, Y. Bitton, J. Cho, R. Garg, A. Ku, Z. Parekh, J. Pont-Tuset, G. Tanzer, et al. DOCCI:\\n      Descriptions of connected and contrasting images. URL https://arxiv. org/abs/2404.19753.\\n[55] OpenAI. GPT 4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-int\\n      elligence, 2024. Accessed: 2024-11-20.\\n[56] OpenAI. Hello GPT 4o. https://openai.com/index/hello-gpt-4o, 2024. Accessed: 2024-11-20.\\n[57] OpenAI Team. simple evals GPT4, 2024. URL https://github.com/openai/simple-evals.\\n[58] OpenAI Team. o1 mini system card, 2024. URL https://cdn.openai.com/o1-system-card-20240917.\\n      pdf.\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\\n      J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike,\\n      and R. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural\\n      Information Processing Systems, volume 35, pages 27730–27744, 2022.\\n                                                     25',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n[40] G. Kamradt. LLMTest NeedleInAHaystack, 2023. URL https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md.\\n\\n[41] D. P. Kingma. Auto-encoding variational Bayes. 2nd International Conference on Learning Representations, ICLR, 2014.\\n\\n[42] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\\n\\n[43] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. VisualWebBench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.\\n\\n[44] X. Liu, Y. Zhu, J. Gu, Y. Lan, C. Yang, and Y. Qiao. MM-SafetyBench: A benchmark for safety evaluation of multimodal large language models. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol, editors, Computer Vision – ECCV 2024, pages 386–403, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-72992-8.\\n\\n[45] Llama Team, AI Meta. The Llama 3 herd of models, 2024. URL https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md.\\n\\n[46] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In The 37th Conference on Neural Information Processing Systems (NeurIPS), 2023.\\n\\n[47] Luma Labs, 2024. URL https://lumalabs.ai/dream-machine.\\n\\n[48] L. Madaan, A. K. Singh, R. Schaeffer, A. Poulton, S. Koyejo, P. Stenetorp, S. Narang, and D. Hupkes. Quantifying variance in evaluation benchmarks, 2024. URL https://arxiv.org/abs/2406.10229.\\n\\n[49] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: A diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023.\\n\\n[50] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In ACL Findings, 2022.\\n\\n[51] M. Mathew, D. Karatzas, and C. Jawahar. DocVQA: A dataset for VQA on document images. In WACV, 2021.\\n\\n[52] N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta. FLIRT: Feedback loop in-context red teaming. In EMNLP 2024, 2024. URL https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming.\\n\\n[53] Meta. Llama 3.2 Github model card vision. https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models, 2024. Accessed: 2024-11-20.\\n\\n[54] Y. Onoe, S. Rane, Z. Berger, Y. Bitton, J. Cho, R. Garg, A. Ku, Z. Parekh, J. Pont-Tuset, G. Tanzer, et al. DOCCI: Descriptions of connected and contrasting images. URL https://arxiv.org/abs/2404.19753.\\n\\n[55] OpenAI. GPT 4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence, 2024. Accessed: 2024-11-20.\\n\\n[56] OpenAI. Hello GPT 4o. https://openai.com/index/hello-gpt-4o, 2024. Accessed: 2024-11-20.\\n\\n[57] OpenAI Team. simple evals GPT4, 2024. URL https://github.com/openai/simple-evals.\\n\\n[58] OpenAI Team. o1 mini system card, 2024. URL https://cdn.openai.com/o1-system-card-20240917.pdf.\\n\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744, 2022.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': '[40] G. Kamradt. LLMTest NeedleInAHaystack, 2023. URL https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md.\\n\\n[41] D. P. Kingma. Auto-encoding variational Bayes. 2nd International Conference on Learning Representations, ICLR, 2014.\\n\\n[42] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\\n\\n[43] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. VisualWebBench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.\\n\\n[44] X. Liu, Y. Zhu, J. Gu, Y. Lan, C. Yang, and Y. Qiao. MM-SafetyBench: A benchmark for safety evaluation of multimodal large language models. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol, editors, Computer Vision – ECCV 2024, pages 386–403, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-72992-8.\\n\\n[45] Llama Team, AI Meta. The Llama 3 herd of models, 2024. URL https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md.\\n\\n[46] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In The 37th Conference on Neural Information Processing Systems (NeurIPS), 2023.\\n\\n[47] Luma Labs, 2024. URL https://lumalabs.ai/dream-machine.\\n\\n[48] L. Madaan, A. K. Singh, R. Schaeffer, A. Poulton, S. Koyejo, P. Stenetorp, S. Narang, and D. Hupkes. Quantifying variance in evaluation benchmarks, 2024. URL https://arxiv.org/abs/2406.10229.\\n\\n[49] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: A diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023.\\n\\n[50] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In ACL Findings, 2022.\\n\\n[51] M. Mathew, D. Karatzas, and C. Jawahar. DocVQA: A dataset for VQA on document images. In WACV, 2021.\\n\\n[52] N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta. FLIRT: Feedback loop in-context red teaming. In EMNLP 2024, 2024. URL https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming.\\n\\n[53] Meta. Llama 3.2 Github model card vision. https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models, 2024. Accessed: 2024-11-20.\\n\\n[54] Y. Onoe, S. Rane, Z. Berger, Y. Bitton, J. Cho, R. Garg, A. Ku, Z. Parekh, J. Pont-Tuset, G. Tanzer, et al. DOCCI: Descriptions of connected and contrasting images. URL https://arxiv.org/abs/2404.19753.\\n\\n[55] OpenAI. GPT 4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence, 2024. Accessed: 2024-11-20.\\n\\n[56] OpenAI. Hello GPT 4o. https://openai.com/index/hello-gpt-4o, 2024. Accessed: 2024-11-20.\\n\\n[57] OpenAI Team. simple evals GPT4, 2024. URL https://github.com/openai/simple-evals.\\n\\n[58] OpenAI Team. o1 mini system card, 2024. URL https://cdn.openai.com/o1-system-card-20240917.pdf.\\n\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744, 2022.',\n",
              "      'md': '[40] G. Kamradt. LLMTest NeedleInAHaystack, 2023. URL https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md.\\n\\n[41] D. P. Kingma. Auto-encoding variational Bayes. 2nd International Conference on Learning Representations, ICLR, 2014.\\n\\n[42] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\\n\\n[43] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. VisualWebBench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.\\n\\n[44] X. Liu, Y. Zhu, J. Gu, Y. Lan, C. Yang, and Y. Qiao. MM-SafetyBench: A benchmark for safety evaluation of multimodal large language models. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol, editors, Computer Vision – ECCV 2024, pages 386–403, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-72992-8.\\n\\n[45] Llama Team, AI Meta. The Llama 3 herd of models, 2024. URL https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md.\\n\\n[46] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In The 37th Conference on Neural Information Processing Systems (NeurIPS), 2023.\\n\\n[47] Luma Labs, 2024. URL https://lumalabs.ai/dream-machine.\\n\\n[48] L. Madaan, A. K. Singh, R. Schaeffer, A. Poulton, S. Koyejo, P. Stenetorp, S. Narang, and D. Hupkes. Quantifying variance in evaluation benchmarks, 2024. URL https://arxiv.org/abs/2406.10229.\\n\\n[49] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: A diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023.\\n\\n[50] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In ACL Findings, 2022.\\n\\n[51] M. Mathew, D. Karatzas, and C. Jawahar. DocVQA: A dataset for VQA on document images. In WACV, 2021.\\n\\n[52] N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta. FLIRT: Feedback loop in-context red teaming. In EMNLP 2024, 2024. URL https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming.\\n\\n[53] Meta. Llama 3.2 Github model card vision. https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models, 2024. Accessed: 2024-11-20.\\n\\n[54] Y. Onoe, S. Rane, Z. Berger, Y. Bitton, J. Cho, R. Garg, A. Ku, Z. Parekh, J. Pont-Tuset, G. Tanzer, et al. DOCCI: Descriptions of connected and contrasting images. URL https://arxiv.org/abs/2404.19753.\\n\\n[55] OpenAI. GPT 4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence, 2024. Accessed: 2024-11-20.\\n\\n[56] OpenAI. Hello GPT 4o. https://openai.com/index/hello-gpt-4o, 2024. Accessed: 2024-11-20.\\n\\n[57] OpenAI Team. simple evals GPT4, 2024. URL https://github.com/openai/simple-evals.\\n\\n[58] OpenAI Team. o1 mini system card, 2024. URL https://cdn.openai.com/o1-system-card-20240917.pdf.\\n\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744, 2022.',\n",
              "      'bBox': {'x': 71, 'y': 72.04, 'w': 472, 'h': 678}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md',\n",
              "      'text': 'https://github.com/gkamradt/LLMTestNeedleI'},\n",
              "     {'url': 'https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md',\n",
              "      'text': 'nAHaystack/blob/main/README.md .'},\n",
              "     {'url': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md',\n",
              "      'text': 'https://github.com/meta-llama/llama'},\n",
              "     {'url': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md',\n",
              "      'text': '.'},\n",
              "     {'url': 'https://lumalabs.ai/dream-machine',\n",
              "      'text': 'https://lumalabs.ai/dream-machine .'},\n",
              "     {'url': 'https://arxiv.org/abs/2406.10229',\n",
              "      'text': 'https://arxiv.org/abs/2406.10229 .'},\n",
              "     {'url': 'https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming',\n",
              "      'text': 'https://www.amazon.science/publi'},\n",
              "     {'url': 'https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming',\n",
              "      'text': 'cations/flirt-feedback-loop-in-context-red-teaming .'},\n",
              "     {'url': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models',\n",
              "      'text': 'https://github.com/meta-llama/llama-models/blob /main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models'},\n",
              "     {'url': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models',\n",
              "      'text': '/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models , 2024.'},\n",
              "     {'url': 'https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence',\n",
              "      'text': 'https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-int , 2024. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence',\n",
              "      'text': 'elligence , 2024. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://openai.com/index/hello-gpt-4o',\n",
              "      'text': 'https://openai.com/index/hello-gpt-4o , 2024. Accessed: 2024-11-20.'},\n",
              "     {'url': 'https://github.com/openai/simple-evals',\n",
              "      'text': 'https://github.com/openai/simple-evals .'},\n",
              "     {'url': 'https://cdn.openai.com/o1-system-card-20240917.pdf',\n",
              "      'text': 'https://cdn.openai.com/o1-system-card-20240917.'},\n",
              "     {'url': 'https://cdn.openai.com/o1-system-card-20240917.pdf',\n",
              "      'text': 'pdf .'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 26,\n",
              "    'text': '                                     The Amazon Nova Family of Models\\n [60] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive APIs,\\n      2023. URL https://arxiv.org/abs/2305.15334.\\n[61] W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, 2023.\\n[62]  R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization:\\n      Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing\\n      Systems, 2023.\\n  [63] R. Rei, J. G. C. de Souza, D. Alves, C. Zerva, A. C. Farinha, T. Glushkova, A. Lavie, L. Coheur, and A. F. T.\\n      Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task.          In P. Koehn, L. Barrault,\\n      O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussà, C. Federmann, M. Fishel, A. Fraser, M. Freitag,\\n      Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno Yepes, T. Kocmi, A. Martins,\\n      M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Névéol, M. Neves, M. Popel, M. Turchi, and\\n      M. Zampieri, editors, Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578–585,\\n      Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics.            URL\\n      https://aclanthology.org/2022.wmt-1.52.\\n    [64] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A\\n      graduate-level google-proof Q&A benchmark, 2023. URL https://arxiv.org/abs/2311.12022.\\n[65] Runway Research, 2024. URL https://runwayml.com/research/introducing-gen-3-alpha.\\n[66] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,\\n      B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language un-\\n      derstanding. Advances in neural information processing systems, 35:36479–36494, 2022.\\n[67] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and\\n      T. Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on\\n      Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.\\n [68] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.\\n[69] U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark for long text\\n      understanding. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational\\n      Linguistics: EMNLP 2023, pages 7977–7989, Singapore, Dec. 2023. Association for Computational Linguistics.\\n      doi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.findings-emnlp.5\\n      36.\\n[70] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models\\n      that can read. In CVPR, 2019.\\n        [71] K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu. T2V-CompBench: A comprehensive benchmark for\\n      compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024.\\n         [72] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi,\\n      D. Zhou, , and J. Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint\\n      arXiv:2210.09261, 2022.\\n   [73] N. Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam,\\n      D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez,\\n      P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan,\\n      S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem,\\n      H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. 2022.\\n[74] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention\\n      is all you need, 2023. URL https://arxiv.org/abs/1706.03762.\\n       [75] R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In CVPR,\\n      2015.\\n[76] A. Wang, R. Y. Pang, A. Chen, J. Phang, and S. R. Bowman. SQuALITY: Building a long-document summarization\\n      dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\n      Processing, pages 1139–1156, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational\\n      Linguistics. URL https://aclanthology.org/2022.emnlp-main.75.\\n                                                     26',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n1. S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive APIs, 2023. URL https://arxiv.org/abs/2305.15334.\\n2. W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, 2023.\\n3. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n4. R. Rei, J. G. C. de Souza, D. Alves, C. Zerva, A. C. Farinha, T. Glushkova, A. Lavie, L. Coheur, and A. F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussà, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno Yepes, T. Kocmi, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Névéol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, editors, Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.52.\\n5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof Q&A benchmark, 2023. URL https://arxiv.org/abs/2311.12022.\\n6. Runway Research, 2024. URL https://runwayml.com/research/introducing-gen-3-alpha.\\n7. C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479–36494, 2022.\\n8. T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.\\n9. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.\\n10. U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977–7989, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.findings-emnlp.536.\\n11. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In CVPR, 2019.\\n12. K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu. T2V-CompBench: A comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024.\\n13. M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n14. N. Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. 2022.\\n15. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762.\\n16. R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In CVPR, 2015.\\n17. A. Wang, R. Y. Pang, A. Chen, J. Phang, and S. R. Bowman. SQuALITY: Building a long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139–1156, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.75.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': '1. S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive APIs, 2023. URL https://arxiv.org/abs/2305.15334.\\n2. W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, 2023.\\n3. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n4. R. Rei, J. G. C. de Souza, D. Alves, C. Zerva, A. C. Farinha, T. Glushkova, A. Lavie, L. Coheur, and A. F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussà, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno Yepes, T. Kocmi, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Névéol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, editors, Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.52.\\n5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof Q&A benchmark, 2023. URL https://arxiv.org/abs/2311.12022.\\n6. Runway Research, 2024. URL https://runwayml.com/research/introducing-gen-3-alpha.\\n7. C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479–36494, 2022.\\n8. T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.\\n9. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.\\n10. U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977–7989, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.findings-emnlp.536.\\n11. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In CVPR, 2019.\\n12. K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu. T2V-CompBench: A comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024.\\n13. M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n14. N. Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. 2022.\\n15. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762.\\n16. R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In CVPR, 2015.\\n17. A. Wang, R. Y. Pang, A. Chen, J. Phang, and S. R. Bowman. SQuALITY: Building a long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139–1156, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.75.',\n",
              "      'md': '1. S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive APIs, 2023. URL https://arxiv.org/abs/2305.15334.\\n2. W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, 2023.\\n3. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n4. R. Rei, J. G. C. de Souza, D. Alves, C. Zerva, A. C. Farinha, T. Glushkova, A. Lavie, L. Coheur, and A. F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussà, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno Yepes, T. Kocmi, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Névéol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, editors, Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.52.\\n5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof Q&A benchmark, 2023. URL https://arxiv.org/abs/2311.12022.\\n6. Runway Research, 2024. URL https://runwayml.com/research/introducing-gen-3-alpha.\\n7. C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479–36494, 2022.\\n8. T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.\\n9. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.\\n10. U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977–7989, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.findings-emnlp.536.\\n11. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In CVPR, 2019.\\n12. K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu. T2V-CompBench: A comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024.\\n13. M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n14. N. Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. 2022.\\n15. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762.\\n16. R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In CVPR, 2015.\\n17. A. Wang, R. Y. Pang, A. Chen, J. Phang, and S. R. Bowman. SQuALITY: Building a long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139–1156, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.75.',\n",
              "      'bBox': {'x': 93, 'y': 83.04, 'w': 448, 'h': 667}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://arxiv.org/abs/2305.15334',\n",
              "      'text': 'https://arxiv.org/abs/2305.15334 .'},\n",
              "     {'url': 'https://aclanthology.org/2022.wmt-1.52',\n",
              "      'text': 'https://aclanthology.org/2022.wmt-1.52 .'},\n",
              "     {'url': 'https://arxiv.org/abs/2311.12022',\n",
              "      'text': 'https://arxiv.org/abs/2311.12022 .'},\n",
              "     {'url': 'https://runwayml.com/research/introducing-gen-3-alpha',\n",
              "      'text': 'https://runwayml.com/research/introducing-gen-3-alpha .'},\n",
              "     {'url': 'https://openreview.net/forum?id=Yacmpz84TH',\n",
              "      'text': 'https://openreview.net/forum?id=Yacmpz84TH .'},\n",
              "     {'url': 'https://aclanthology.org/2023.findings-emnlp.536',\n",
              "      'text': 'https://aclanthology.org/2023.findings-emnlp.5'},\n",
              "     {'url': 'https://aclanthology.org/2023.findings-emnlp.536',\n",
              "      'text': '36 .'},\n",
              "     {'url': 'https://arxiv.org/abs/1706.03762',\n",
              "      'text': 'https://arxiv.org/abs/1706.03762 .'},\n",
              "     {'url': 'https://aclanthology.org/2022.emnlp-main.75',\n",
              "      'text': 'https://aclanthology.org/2022.emnlp-main.75 .'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 27,\n",
              "    'text': '                                     The Amazon Nova Family of Models\\n[77] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. LVBench: An\\n      extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024.\\n[78] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. VATEX: A large-scale, high-quality multilingual\\n      dataset for video-and-language research. In ICCV, 2019.\\n[79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought\\n      prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on\\n      Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN\\n      9781713871088.\\n[80] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. ImageReward: Learning and evaluating\\n      human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.\\n[81] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling\\n      leaderboard. 2024.\\n[82] X. Yang, K. Sun, H. Xin, Y. Sun, N. Bhalla, X. Chen, S. Choudhary, R. D. Gui, Z. W. Jiang, Z. Jiang, L. Kong,\\n      B. Moran, J. Wang, Y. E. Xu, A. Yan, C. Yang, E. Yuan, H. Zha, N. Tang, L. Chen, N. Scheffer, Y. Liu, N. Shah,\\n      R. Wanga, A. Kumar, W. tau Yih, and X. L. Dong.    Crag – comprehensive rag benchmark.       arXiv preprint\\n      arXiv:2406.04744, 2024. URL https://arxiv.org/abs/2406.04744.\\n[83] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting\\n      in language models. In International Conference on Learning Representations (ICLR), 2023.\\n[84] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling\\n      autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\\n[85] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu,\\n      R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive\\n      multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024.\\n[86]  B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded. In ICML, 2024.\\n[87] L. Zheng, Z. Huang, Z. Xue, X. Wang, B. An, and S. Yan. AgentStudio: A toolkit for building general virtual\\n      agents. arXiv preprint arXiv:2403.17918, 2024.\\n[88]  M. Zhong, A. Zhang, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang,\\n      M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten. Law of the weakest link: Cross capabilities\\n      of large language models. arXiv preprint arXiv:2409.19951, 2024.\\n[89]  J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for\\n      large language models, 2023. URL https://arxiv.org/abs/2311.07911.\\n                                                     27',\n",
              "    'md': \"The Amazon Nova Family of Models\\n\\n[77] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. LVBench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024.\\n\\n[78] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In ICCV, 2019.\\n\\n[79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.\\n\\n[80] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. ImageReward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[81] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling leaderboard. 2024.\\n\\n[82] X. Yang, K. Sun, H. Xin, Y. Sun, N. Bhalla, X. Chen, S. Choudhary, R. D. Gui, Z. W. Jiang, Z. Jiang, L. Kong, B. Moran, J. Wang, Y. E. Xu, A. Yan, C. Yang, E. Yuan, H. Zha, N. Tang, L. Chen, N. Scheffer, Y. Liu, N. Shah, R. Wanga, A. Kumar, W. tau Yih, and X. L. Dong. Crag – comprehensive rag benchmark. arXiv preprint arXiv:2406.04744, 2024. URL https://arxiv.org/abs/2406.04744.\\n\\n[83] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[84] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\\n\\n[85] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024.\\n\\n[86] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded. In ICML, 2024.\\n\\n[87] L. Zheng, Z. Huang, Z. Xue, X. Wang, B. An, and S. Yan. AgentStudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024.\\n\\n[88] M. Zhong, A. Zhang, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang, M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten. Law of the weakest link: Cross capabilities of large language models. arXiv preprint arXiv:2409.19951, 2024.\\n\\n[89] J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911.\",\n",
              "    'images': [{'name': 'page_27.jpg',\n",
              "      'height': 792,\n",
              "      'width': 612,\n",
              "      'x': 0,\n",
              "      'y': 0,\n",
              "      'original_width': 1236,\n",
              "      'original_height': 1600,\n",
              "      'type': 'full_page_screenshot'}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'text',\n",
              "      'value': \"The Amazon Nova Family of Models\\n\\n[77] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. LVBench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024.\\n\\n[78] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In ICCV, 2019.\\n\\n[79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.\\n\\n[80] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. ImageReward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[81] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling leaderboard. 2024.\\n\\n[82] X. Yang, K. Sun, H. Xin, Y. Sun, N. Bhalla, X. Chen, S. Choudhary, R. D. Gui, Z. W. Jiang, Z. Jiang, L. Kong, B. Moran, J. Wang, Y. E. Xu, A. Yan, C. Yang, E. Yuan, H. Zha, N. Tang, L. Chen, N. Scheffer, Y. Liu, N. Shah, R. Wanga, A. Kumar, W. tau Yih, and X. L. Dong. Crag – comprehensive rag benchmark. arXiv preprint arXiv:2406.04744, 2024. URL https://arxiv.org/abs/2406.04744.\\n\\n[83] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[84] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\\n\\n[85] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024.\\n\\n[86] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded. In ICML, 2024.\\n\\n[87] L. Zheng, Z. Huang, Z. Xue, X. Wang, B. An, and S. Yan. AgentStudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024.\\n\\n[88] M. Zhong, A. Zhang, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang, M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten. Law of the weakest link: Cross capabilities of large language models. arXiv preprint arXiv:2409.19951, 2024.\\n\\n[89] J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911.\",\n",
              "      'md': \"The Amazon Nova Family of Models\\n\\n[77] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. LVBench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024.\\n\\n[78] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In ICCV, 2019.\\n\\n[79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.\\n\\n[80] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. ImageReward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[81] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling leaderboard. 2024.\\n\\n[82] X. Yang, K. Sun, H. Xin, Y. Sun, N. Bhalla, X. Chen, S. Choudhary, R. D. Gui, Z. W. Jiang, Z. Jiang, L. Kong, B. Moran, J. Wang, Y. E. Xu, A. Yan, C. Yang, E. Yuan, H. Zha, N. Tang, L. Chen, N. Scheffer, Y. Liu, N. Shah, R. Wanga, A. Kumar, W. tau Yih, and X. L. Dong. Crag – comprehensive rag benchmark. arXiv preprint arXiv:2406.04744, 2024. URL https://arxiv.org/abs/2406.04744.\\n\\n[83] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[84] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\\n\\n[85] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024.\\n\\n[86] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded. In ICML, 2024.\\n\\n[87] L. Zheng, Z. Huang, Z. Xue, X. Wang, B. An, and S. Yan. AgentStudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024.\\n\\n[88] M. Zhong, A. Zhang, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang, M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten. Law of the weakest link: Cross capabilities of large language models. arXiv preprint arXiv:2409.19951, 2024.\\n\\n[89] J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911.\",\n",
              "      'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 471}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://arxiv.org/abs/2406.04744',\n",
              "      'text': 'https://arxiv.org/abs/2406.04744 .'},\n",
              "     {'url': 'https://arxiv.org/abs/2311.07911',\n",
              "      'text': 'https://arxiv.org/abs/2311.07911 .'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': True,\n",
              "    'parsingMode': 'premium',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 28,\n",
              "    'text': '                                      The Amazon Nova Family of Models\\nA    Amazon Nova Canvas Capabilities\\nOur Nova Canvas model offers the following functionalities, with examples given in Figure 5.\\n       •  Text-to-image generation allows customers to create images with various resolutions (from 512×512 up to\\n          2K×2K resolution).\\n       •  Editing allows developers to edit images using a combination of text prompt or mask image. Amazon Nova\\n          Canvas supports text-to-image editing and image-to-image editing, including inpainting, outpainting and object\\n          removal.\\n       •  Image variation allows customers to output images with similar contents but with variations from the user\\n          provided ones.\\n       •  Image conditioning provide a reference image along with a text prompt, resulting in outputs that follow the\\n          layout and structure of the user-supplied reference.\\n       •  Image guidance with color palette allows customers to precisely control the color palette of generated images\\n          by providing a list of hex codes along with the text prompt.\\n       •  Background removal automatically removes background from images containing multiple objects.\\n                                                      28',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# Amazon Nova Canvas Capabilities\\n\\nOur Nova Canvas model offers the following functionalities, with examples given in Figure 5.\\n\\n- Text-to-image generation allows customers to create images with various resolutions (from 512×512 up to 2K×2K resolution).\\n- Editing allows developers to edit images using a combination of text prompt or mask image. Amazon Nova Canvas supports text-to-image editing and image-to-image editing, including inpainting, outpainting and object removal.\\n- Image variation allows customers to output images with similar contents but with variations from the user provided ones.\\n- Image conditioning provide a reference image along with a text prompt, resulting in outputs that follow the layout and structure of the user-supplied reference.\\n- Image guidance with color palette allows customers to precisely control the color palette of generated images by providing a list of hex codes along with the text prompt.\\n- Background removal automatically removes background from images containing multiple objects.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 72, 'y': 34.04, 'w': 306, 'h': 48.01}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Amazon Nova Canvas Capabilities',\n",
              "      'md': '# Amazon Nova Canvas Capabilities',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 196, 'h': 12}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Our Nova Canvas model offers the following functionalities, with examples given in Figure 5.\\n\\n- Text-to-image generation allows customers to create images with various resolutions (from 512×512 up to 2K×2K resolution).\\n- Editing allows developers to edit images using a combination of text prompt or mask image. Amazon Nova Canvas supports text-to-image editing and image-to-image editing, including inpainting, outpainting and object removal.\\n- Image variation allows customers to output images with similar contents but with variations from the user provided ones.\\n- Image conditioning provide a reference image along with a text prompt, resulting in outputs that follow the layout and structure of the user-supplied reference.\\n- Image guidance with color palette allows customers to precisely control the color palette of generated images by providing a list of hex codes along with the text prompt.\\n- Background removal automatically removes background from images containing multiple objects.',\n",
              "      'md': 'Our Nova Canvas model offers the following functionalities, with examples given in Figure 5.\\n\\n- Text-to-image generation allows customers to create images with various resolutions (from 512×512 up to 2K×2K resolution).\\n- Editing allows developers to edit images using a combination of text prompt or mask image. Amazon Nova Canvas supports text-to-image editing and image-to-image editing, including inpainting, outpainting and object removal.\\n- Image variation allows customers to output images with similar contents but with variations from the user provided ones.\\n- Image conditioning provide a reference image along with a text prompt, resulting in outputs that follow the layout and structure of the user-supplied reference.\\n- Image guidance with color palette allows customers to precisely control the color palette of generated images by providing a list of hex codes along with the text prompt.\\n- Background removal automatically removes background from images containing multiple objects.',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 468, 'h': 195.99}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 29,\n",
              "    'text': '                                   The Amazon Nova Family of Models\\n A dinosaur sitting\\n    in a tea cup\\n       (a) Image generation from a text prompt                     (b) Inpainting the image with swans\\n                                                                  amazon                    amazon\\nchange flowers to orange color\\n                   (c) Image editing                                (d) Outpainting a new background\\na hamster eats apple slice                               A wooden boat in summer\\n                   (e) Style transfer                                     (f) Guided generation\\n      A jar of salad dressing\\n   in a rustic kitchen\\n   surrounded by fresh vegetables\\n   with studio lighting\\n            (g) Controlling the color palette                            (h) Background Removal\\n   Figure 5: Example capabilities of Amazon Nova Canvas, our content generation model for images.\\n                                                      29',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\nA dinosaur sitting in a tea cup\\n\\n|(a) Image generation from a text prompt|(b) Inpainting the image with swans|\\n|---|---|\\n|amazon|amazon|\\n|change flowers to orange color| |\\n|(c) Image editing|(d) Outpainting a new background|\\n|a hamster eats apple slice|A wooden boat in summer|\\n|(e) Style transfer|(f) Guided generation|\\n|A jar of salad dressing in a rustic kitchen surrounded by fresh vegetables with studio lighting| |\\n|(g) Controlling the color palette|(h) Background Removal|\\n\\nFigure 5: Example capabilities of Amazon Nova Canvas, our content generation model for images.\\n\\n29',\n",
              "    'images': [{'name': 'img_p28_1.png',\n",
              "      'height': 324,\n",
              "      'width': 324,\n",
              "      'x': 191.415,\n",
              "      'y': 126.615,\n",
              "      'original_width': 324,\n",
              "      'original_height': 324},\n",
              "     {'name': 'img_p28_2.png',\n",
              "      'height': 222,\n",
              "      'width': 394,\n",
              "      'x': 307.245,\n",
              "      'y': 167.99055999999996,\n",
              "      'original_width': 394,\n",
              "      'original_height': 222},\n",
              "     {'name': 'img_p28_3.png',\n",
              "      'height': 221,\n",
              "      'width': 392,\n",
              "      'x': 404.505,\n",
              "      'y': 167.96145999999996,\n",
              "      'original_width': 392,\n",
              "      'original_height': 221},\n",
              "     {'name': 'img_p28_4.png',\n",
              "      'height': 1365,\n",
              "      'width': 1365,\n",
              "      'x': 94.156,\n",
              "      'y': 250.27924876000003,\n",
              "      'original_width': 1365,\n",
              "      'original_height': 1365},\n",
              "     {'name': 'img_p28_5.png',\n",
              "      'height': 1365,\n",
              "      'width': 1365,\n",
              "      'x': 191.41500000000002,\n",
              "      'y': 260.56224876000005,\n",
              "      'original_width': 1365,\n",
              "      'original_height': 1365},\n",
              "     {'name': 'img_p28_6.png',\n",
              "      'height': 316,\n",
              "      'width': 315,\n",
              "      'x': 307.245,\n",
              "      'y': 269.22856,\n",
              "      'original_width': 315,\n",
              "      'original_height': 316,\n",
              "      'ocr': [{'x': 116,\n",
              "        'y': 140,\n",
              "        'w': 74,\n",
              "        'h': 28,\n",
              "        'confidence': '0.9992311411385467',\n",
              "        'text': 'amazon'}]},\n",
              "     {'name': 'img_p28_7.png',\n",
              "      'height': 315,\n",
              "      'width': 316,\n",
              "      'x': 404.505,\n",
              "      'y': 269.8255,\n",
              "      'original_width': 316,\n",
              "      'original_height': 315,\n",
              "      'ocr': [{'x': 116,\n",
              "        'y': 140,\n",
              "        'w': 74,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9997271158260564',\n",
              "        'text': 'amazon'}]},\n",
              "     {'name': 'img_p28_8.png',\n",
              "      'height': 262,\n",
              "      'width': 262,\n",
              "      'x': 94.156,\n",
              "      'y': 393.37298000000004,\n",
              "      'original_width': 262,\n",
              "      'original_height': 262},\n",
              "     {'name': 'img_p28_9.png',\n",
              "      'height': 262,\n",
              "      'width': 263,\n",
              "      'x': 191.41500000000002,\n",
              "      'y': 404.52492,\n",
              "      'original_width': 263,\n",
              "      'original_height': 262},\n",
              "     {'name': 'img_p28_10.png',\n",
              "      'height': 306,\n",
              "      'width': 307,\n",
              "      'x': 307.245,\n",
              "      'y': 393.18386,\n",
              "      'original_width': 307,\n",
              "      'original_height': 306},\n",
              "     {'name': 'img_p28_11.png',\n",
              "      'height': 306,\n",
              "      'width': 307,\n",
              "      'x': 404.505,\n",
              "      'y': 404.47486,\n",
              "      'original_width': 307,\n",
              "      'original_height': 306},\n",
              "     {'name': 'img_p28_12.png',\n",
              "      'height': 36,\n",
              "      'width': 126,\n",
              "      'x': 94.156,\n",
              "      'y': 562.7536,\n",
              "      'original_width': 126,\n",
              "      'original_height': 36},\n",
              "     {'name': 'img_p28_13.png',\n",
              "      'height': 283,\n",
              "      'width': 284,\n",
              "      'x': 191.41500000000002,\n",
              "      'y': 537.1229000000001,\n",
              "      'original_width': 284,\n",
              "      'original_height': 283},\n",
              "     {'name': 'img_p28_14.png',\n",
              "      'height': 322,\n",
              "      'width': 322,\n",
              "      'x': 307.245,\n",
              "      'y': 536.7921799999999,\n",
              "      'original_width': 322,\n",
              "      'original_height': 322},\n",
              "     {'name': 'img_p28_15.png',\n",
              "      'height': 320,\n",
              "      'width': 320,\n",
              "      'x': 404.505,\n",
              "      'y': 536.7951999999999,\n",
              "      'original_width': 320,\n",
              "      'original_height': 320}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 232, 'h': 285.96}},\n",
              "     {'type': 'text',\n",
              "      'value': 'A dinosaur sitting in a tea cup',\n",
              "      'md': 'A dinosaur sitting in a tea cup',\n",
              "      'bBox': {'x': 103, 'y': 168.98, 'w': 76, 'h': 17}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       ['(a) Image generation from a text prompt',\n",
              "        '(b) Inpainting the image with swans'],\n",
              "       ['---', '---'],\n",
              "       ['amazon', 'amazon'],\n",
              "       ['change flowers to orange color', ''],\n",
              "       ['(c) Image editing', '(d) Outpainting a new background'],\n",
              "       ['a hamster eats apple slice', 'A wooden boat in summer'],\n",
              "       ['(e) Style transfer', '(f) Guided generation'],\n",
              "       ['A jar of salad dressing in a rustic kitchen surrounded by fresh vegetables with studio lighting',\n",
              "        ''],\n",
              "       ['(g) Controlling the color palette', '(h) Background Removal']],\n",
              "      'md': 'A dinosaur sitting in a tea cup\\n|(a) Image generation from a text prompt|(b) Inpainting the image with swans|\\n|---|---|\\n|amazon|amazon|\\n|change flowers to orange color| |\\n|(c) Image editing|(d) Outpainting a new background|\\n|a hamster eats apple slice|A wooden boat in summer|\\n|(e) Style transfer|(f) Guided generation|\\n|A jar of salad dressing in a rustic kitchen surrounded by fresh vegetables with studio lighting| |\\n|(g) Controlling the color palette|(h) Background Removal|',\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\"(a) Image generation from a text prompt\",\"(b) Inpainting the image with swans\"\\n\"---\",\"---\"\\n\"amazon\",\"amazon\"\\n\"change flowers to orange color\",\"\"\\n\"(c) Image editing\",\"(d) Outpainting a new background\"\\n\"a hamster eats apple slice\",\"A wooden boat in summer\"\\n\"(e) Style transfer\",\"(f) Guided generation\"\\n\"A jar of salad dressing in a rustic kitchen surrounded by fresh vegetables with studio lighting\",\"\"\\n\"(g) Controlling the color palette\",\"(h) Background Removal\"',\n",
              "      'bBox': {'x': 94, 'y': 34.04, 'w': 409, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Figure 5: Example capabilities of Amazon Nova Canvas, our content generation model for images.\\n\\n29',\n",
              "      'md': 'Figure 5: Example capabilities of Amazon Nova Canvas, our content generation model for images.\\n\\n29',\n",
              "      'bBox': {'x': 109, 'y': 311, 'w': 394, 'h': 439.04}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 30,\n",
              "    'text': '                              The Amazon Nova Family of Models\\nB  Prompts and Scoring\\nPrompt templates used for Amazon Nova evaluations are given below, along with those used for select other public\\nmodels where noted. Additional materials and evaluation results from this report can be found at:\\n                            https://huggingface.co/amazon-agi\\nB.1 Text evaluation\\nB.1.1 Language Understanding\\nFor MMLU:\\nWhat is the correct answer to this question: <question>\\nChoices: <choices>. Let’s think step by step:\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"\\nThe correct answer is (insert answer here).\"\\nFor ARC-C:\\nGiven the following question and four candidate answers (A, B, C and D), choose the best\\nanswer.\\nQuestion: <question>\\nYour response should end with \"The best answer is [the_answer_letter]\" where the [\\nthe_answer_letter] is one of A, B, C or D.\\nFor DROP:\\nWe use the following 6 shots:\\n - answer: >-\\n     According to the passage, the European Coal and Steel Community was\\n     established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the\\n     answer is 7\\n   passage: >-\\n     Since the 1970s, U.S. governments have negotiated managed-trade\\n     agreements, such as the North American Free Trade Agreement in the 1990s,\\n     the Dominican Republic-Central America Free Trade Agreement in 2006, and a\\n     number of bilateral agreements. In Europe, six countries formed the\\n     European Coal and Steel Community in 1951 which became the European\\n     Economic Community in 1958. Two core objectives of the EEC were the\\n     development of a common market, subsequently renamed the single market,\\n     and establishing a customs union between its member states.\\n   question: How many years did the European Coal and Steel Community exist?\\n - answer: >-\\n     According to the passage, 23.5% are under the age of 18, and 8.7% are from\\n     ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8\\n   passage: >-\\n     In the county, the population was spread out with 23.50% under the age of\\n     18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and\\n     13.30% who were 65 years of age or older.\\n   question: >-\\n     How many more percent are under the age of 18 compared to the 18 to 24\\n     group?\\n - answer: >-\\n     According to the passage, Stafford threw 5 TD passes, 3 of which were to\\n     Johnson. 5 - 3 = 2. So the answer is 2\\n   passage: >-\\n     Playing in their second straight Thanksgiving game, the Eagles struggled\\n     especially on defense, where they were unable to stop the much-hyped Lions\\n     offense. The worst of it all was how unproven rookie Eric Rowe was tasked\\n                                           30',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# Prompts and Scoring\\n\\nPrompt templates used for Amazon Nova evaluations are given below, along with those used for select other public models where noted. Additional materials and evaluation results from this report can be found at: https://huggingface.co/amazon-agi\\n\\n# B.1 Text evaluation\\n\\n# B.1.1 Language Understanding\\n\\n# For MMLU:\\n\\nWhat is the correct answer to this question: &lt;question&gt;\\n\\nChoices: &lt;choices&gt;. Let’s think step by step:\\n\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"The correct answer is (insert answer here).\"\\n\\n# For ARC-C:\\n\\nGiven the following question and four candidate answers (A, B, C and D), choose the best answer.\\n\\nQuestion: &lt;question&gt;\\n\\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.\\n\\n# For DROP:\\n\\nWe use the following 6 shots:\\n\\n- answer: &gt;-\\n\\nAccording to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the answer is 7\\n\\npassage: &gt;-\\n\\nSince the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states.\\n\\nquestion: How many years did the European Coal and Steel Community exist?\\n- answer: &gt;-\\n\\nAccording to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8\\n\\npassage: &gt;-\\n\\nIn the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older.\\n\\nquestion: &gt;-\\n\\nHow many more percent are under the age of 18 compared to the 18 to 24 group?\\n- answer: &gt;-\\n\\nAccording to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - 3 = 2. So the answer is 2\\n\\npassage: &gt;-\\n\\nPlaying in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Prompts and Scoring',\n",
              "      'md': '# Prompts and Scoring',\n",
              "      'bBox': {'x': 91, 'y': 70.04, 'w': 108, 'h': 12}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Prompt templates used for Amazon Nova evaluations are given below, along with those used for select other public models where noted. Additional materials and evaluation results from this report can be found at: https://huggingface.co/amazon-agi',\n",
              "      'md': 'Prompt templates used for Amazon Nova evaluations are given below, along with those used for select other public models where noted. Additional materials and evaluation results from this report can be found at: https://huggingface.co/amazon-agi',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 468, 'h': 67.99}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.1 Text evaluation',\n",
              "      'md': '# B.1 Text evaluation',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 89, 'h': 91.99}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.1.1 Language Understanding',\n",
              "      'md': '# B.1.1 Language Understanding',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 139, 'h': 111.99}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'For MMLU:',\n",
              "      'md': '# For MMLU:',\n",
              "      'bBox': {'x': 72, 'y': 191.04, 'w': 50, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'What is the correct answer to this question: &lt;question&gt;\\n\\nChoices: &lt;choices&gt;. Let’s think step by step:\\n\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"The correct answer is (insert answer here).\"',\n",
              "      'md': 'What is the correct answer to this question: &lt;question&gt;\\n\\nChoices: &lt;choices&gt;. Let’s think step by step:\\n\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"The correct answer is (insert answer here).\"',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 465, 'h': 181.99}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'For ARC-C:',\n",
              "      'md': '# For ARC-C:',\n",
              "      'bBox': {'x': 72, 'y': 267.04, 'w': 49, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Given the following question and four candidate answers (A, B, C and D), choose the best answer.\\n\\nQuestion: &lt;question&gt;\\n\\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.',\n",
              "      'md': 'Given the following question and four candidate answers (A, B, C and D), choose the best answer.\\n\\nQuestion: &lt;question&gt;\\n\\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 460, 'h': 268.99}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'For DROP:',\n",
              "      'md': '# For DROP:',\n",
              "      'bBox': {'x': 72, 'y': 354.04, 'w': 45, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We use the following 6 shots:\\n\\n- answer: &gt;-\\n\\nAccording to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the answer is 7\\n\\npassage: &gt;-\\n\\nSince the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states.\\n\\nquestion: How many years did the European Coal and Steel Community exist?\\n- answer: &gt;-\\n\\nAccording to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8\\n\\npassage: &gt;-\\n\\nIn the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older.\\n\\nquestion: &gt;-\\n\\nHow many more percent are under the age of 18 compared to the 18 to 24 group?\\n- answer: &gt;-\\n\\nAccording to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - 3 = 2. So the answer is 2\\n\\npassage: &gt;-\\n\\nPlaying in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked',\n",
              "      'md': 'We use the following 6 shots:\\n\\n- answer: &gt;-\\n\\nAccording to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the answer is 7\\n\\npassage: &gt;-\\n\\nSince the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states.\\n\\nquestion: How many years did the European Coal and Steel Community exist?\\n- answer: &gt;-\\n\\nAccording to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8\\n\\npassage: &gt;-\\n\\nIn the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older.\\n\\nquestion: &gt;-\\n\\nHow many more percent are under the age of 18 compared to the 18 to 24 group?\\n- answer: &gt;-\\n\\nAccording to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - 3 = 2. So the answer is 2\\n\\npassage: &gt;-\\n\\nPlaying in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked',\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 416, 'h': 679.99}}],\n",
              "    'status': 'OK',\n",
              "    'links': [{'url': 'https://huggingface.co/amazon-agi',\n",
              "      'text': 'https://huggingface.co/amazon-agi'}],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 31,\n",
              "    'text': '                              The Amazon Nova Family of Models\\n     with covering wide receiver Calvin Johnson, leading to Johnson catching 3\\n     touchdowns. Staffords five passing touchdowns, including three of them to\\n     Johnson was too much for the Eagles to overcome and for the second\\n     consecutive time this season, the Eagles gave up 45 points in a game. With\\n     the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on\\n     Thanksgiving.\\n   question: How many TD passes did Stafford throw other than to Johnson?\\n - answer: >-\\n     All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown\\n     run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So\\n     the shortest touchdown run was 9 yards. All the touchdown passes are: a\\n     12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the\\n     shortest touchdown run and the longest touchdown pass combine for 9 + 12 =\\n     21 yards. So the answer is 21\\n   passage: >-\\n     The Seahawks played the San Francisco 49ers. In the first quarter, the\\n     Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill\\n     returning a fumble 9 yards for a touchdown. In the third quarter, the\\n     49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas\\n     Nelly, along with Mare kicking a 32-yard field goal. In the final quarter,\\n     Julius Jones got another 11-yard TD.\\n   question: >-\\n     How many yards do the shortest touchdown run and the longest touchdown\\n     pass combine for?\\n - answer: >-\\n     The Ravens kicker Billy Cundiff got a 45-yard field goal in the second\\n     quarter, concluding the first half with a 10-7 lead. So the Ravens had 10\\n     points at halftime. So the answer is 10\\n   passage: >-\\n     The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh\\n     would deliver the opening punch in the first quarter with a 1-yard\\n     touchdown from running back Rashard Mendenhall. The Ravens would make it\\n     even as running back Willis McGahee got a 9-yard TD. The Ravens kicker\\n     Billy Cundiff got a 45-yard field goal in the second quarter, concluding\\n     the first half with a 10-7 lead. The Steelers brought the game into\\n     overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy\\n     Cundiff pulled off a winning 33-yard field goal in overtime.\\n   question: How many points did the Ravens have at halftime?\\n - answer: >-\\n     The first and third quarters were the scoreless quarters. So there are 2\\n     scoreless quarters. So the answer is 2\\n   passage: >-\\n     The Vikings flew to Bank of America Stadium to face the Carolina Panthers.\\n     After a scoreless first quarter, Carolina got on the board with\\n     quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass.\\n     After yet another scoreless quarter, Carolina sealed the game as Matt\\n     Moore completed a 42-yard touchdown pass to wide receiver Steve Smith.\\n   question: How many scoreless quarters were there?\\nFor each shot we provide the following instruction:\\nConclude your answer with: \"So the answer is {final answer}\". Make sure the final answer\\nis in plain text format\\nAnd we create each user prompt as follows:\\n<passage>\\n<question>\\n<instruction>\\n                                           31',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\nwith covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Staffords five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving.\\n\\n# Question\\n\\nHow many TD passes did Stafford throw other than to Johnson?\\n\\n# Answer\\n\\nAll the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21.\\n\\n# Passage\\n\\nThe Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD.\\n\\n# Question\\n\\nHow many yards do the shortest touchdown run and the longest touchdown pass combine for?\\n\\n# Answer\\n\\nThe Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10.\\n\\n# Passage\\n\\nThe Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime.\\n\\n# Question\\n\\nHow many points did the Ravens have at halftime?\\n\\n# Answer\\n\\nThe first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2.\\n\\n# Passage\\n\\nThe Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith.\\n\\n# Question\\n\\nHow many scoreless quarters were there?',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'with covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Staffords five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving.',\n",
              "      'md': 'with covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Staffords five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving.',\n",
              "      'bBox': {'x': 100, 'y': 72.04, 'w': 387, 'h': 65}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Question',\n",
              "      'md': '# Question',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'How many TD passes did Stafford throw other than to Johnson?',\n",
              "      'md': 'How many TD passes did Stafford throw other than to Johnson?',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Answer',\n",
              "      'md': '# Answer',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21.',\n",
              "      'md': 'All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21.',\n",
              "      'bBox': {'x': 100, 'y': 160.04, 'w': 387, 'h': 64}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Passage',\n",
              "      'md': '# Passage',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD.',\n",
              "      'md': 'The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD.',\n",
              "      'bBox': {'x': 100, 'y': 236.04, 'w': 387, 'h': 65}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Question',\n",
              "      'md': '# Question',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'How many yards do the shortest touchdown run and the longest touchdown pass combine for?',\n",
              "      'md': 'How many yards do the shortest touchdown run and the longest touchdown pass combine for?',\n",
              "      'bBox': {'x': 100, 'y': 312.04, 'w': 366, 'h': 21}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Answer',\n",
              "      'md': '# Answer',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10.',\n",
              "      'md': 'The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10.',\n",
              "      'bBox': {'x': 100, 'y': 345.04, 'w': 382, 'h': 97}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Passage',\n",
              "      'md': '# Passage',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime.',\n",
              "      'md': 'The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime.',\n",
              "      'bBox': {'x': 100, 'y': 345.04, 'w': 376, 'h': 130}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Question',\n",
              "      'md': '# Question',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'How many points did the Ravens have at halftime?',\n",
              "      'md': 'How many points did the Ravens have at halftime?',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Answer',\n",
              "      'md': '# Answer',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2.',\n",
              "      'md': 'The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2.',\n",
              "      'bBox': {'x': 100, 'y': 498.04, 'w': 376, 'h': 21}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Passage',\n",
              "      'md': '# Passage',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith.',\n",
              "      'md': 'The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith.',\n",
              "      'bBox': {'x': 100, 'y': 531.04, 'w': 387, 'h': 53}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Question',\n",
              "      'md': '# Question',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'How many scoreless quarters were there?',\n",
              "      'md': 'How many scoreless quarters were there?',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 32,\n",
              "    'text': '                              The Amazon Nova Family of Models\\nFor IFEval:\\nNo particular prompt was added (query was inputted to the model).\\nFor BBH:\\nWe use a preamble that describes the task, for example:\\nEvaluate the result of a random Boolean expression.\\nWe then provide few shot examples in the following format:\\n<preamble>\\nQuestion: <question>\\n<instruction>\\nLet’s think step by step.\\n<ground truth chain of thought>. So the answer is <answer>\\nAnd we follow this by the query:\\n<preamble>\\nQuestion: <question>\\n<instruction>\\nLet’s think step by step.\\nFor each subject, We provide the subject-specific instructions as below:\\n- subject: boolean_expressions\\n  instruction: Conclude your answer with: \"So the answer is True or False.\".\\n- subject: causal_judgement\\n  instruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: date_understanding\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: disambiguation_qa\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: dyck_languages\\n  instruction: Correctly close a Dyck-n word. Conclude your answer with: \"So the answer\\n  is {final answer}.\". Make sure the final answer is in plain text format\\n- subject: formal_fallacies\\n  instruction: Conclude your answer with: \"So the answer is valid or invalid.\".\\n- subject: geometric_shapes\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: hyperbaton\\n  instruction: Conclude your answer with: \\\\\"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: logical_deduction_five_objects\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: logical_deduction_seven_objects\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: logical_deduction_three_objects\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: movie_recommendation\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: multistep_arithmetic_two\\n  instruction: Conclude your answer with: \"So the answer is {final answer}.\". Make sure\\n  the final answer is in plain text format\\n                                           32',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# For IFEval:\\n\\nNo particular prompt was added (query was inputted to the model).\\n\\n# For BBH:\\n\\nWe use a preamble that describes the task, for example:\\n\\nEvaluate the result of a random Boolean expression.\\n\\nWe then provide few shot examples in the following format:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.\\n\\n<ground truth chain of thought>. So the answer is <answer>\\n\\nAnd we follow this by the query:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.\\n\\n# For each subject, We provide the subject-specific instructions as below:\\n\\n- subject: boolean_expressions\\n\\ninstruction: Conclude your answer with: \"So the answer is True or False.\".\\n- subject: causal_judgement\\n\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: date_understanding\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: disambiguation_qa\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: dyck_languages\\n\\ninstruction: Correctly close a Dyck-n word. Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.\\n- subject: formal_fallacies\\n\\ninstruction: Conclude your answer with: \"So the answer is valid or invalid.\".\\n- subject: geometric_shapes\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: hyperbaton\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_five_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_seven_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_three_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: movie_recommendation\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: multistep_arithmetic_two\\n\\ninstruction: Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'For IFEval:',\n",
              "      'md': '# For IFEval:',\n",
              "      'bBox': {'x': 72, 'y': 72.04, 'w': 46, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'No particular prompt was added (query was inputted to the model).',\n",
              "      'md': 'No particular prompt was added (query was inputted to the model).',\n",
              "      'bBox': {'x': 72, 'y': 83.04, 'w': 268, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'For BBH:',\n",
              "      'md': '# For BBH:',\n",
              "      'bBox': {'x': 72, 'y': 100.04, 'w': 39, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'We use a preamble that describes the task, for example:\\n\\nEvaluate the result of a random Boolean expression.\\n\\nWe then provide few shot examples in the following format:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.\\n\\n<ground truth chain of thought>. So the answer is <answer>\\n\\nAnd we follow this by the query:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.',\n",
              "      'md': 'We use a preamble that describes the task, for example:\\n\\nEvaluate the result of a random Boolean expression.\\n\\nWe then provide few shot examples in the following format:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.\\n\\n<ground truth chain of thought>. So the answer is <answer>\\n\\nAnd we follow this by the query:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.',\n",
              "      'bBox': {'x': 71, 'y': 111.04, 'w': 304, 'h': 186}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'For each subject, We provide the subject-specific instructions as below:',\n",
              "      'md': '# For each subject, We provide the subject-specific instructions as below:',\n",
              "      'bBox': {'x': 72, 'y': 312.04, 'w': 284, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': '- subject: boolean_expressions\\n\\ninstruction: Conclude your answer with: \"So the answer is True or False.\".\\n- subject: causal_judgement\\n\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: date_understanding\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: disambiguation_qa\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: dyck_languages\\n\\ninstruction: Correctly close a Dyck-n word. Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.\\n- subject: formal_fallacies\\n\\ninstruction: Conclude your answer with: \"So the answer is valid or invalid.\".\\n- subject: geometric_shapes\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: hyperbaton\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_five_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_seven_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_three_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: movie_recommendation\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: multistep_arithmetic_two\\n\\ninstruction: Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.',\n",
              "      'md': '- subject: boolean_expressions\\n\\ninstruction: Conclude your answer with: \"So the answer is True or False.\".\\n- subject: causal_judgement\\n\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: date_understanding\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: disambiguation_qa\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: dyck_languages\\n\\ninstruction: Correctly close a Dyck-n word. Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.\\n- subject: formal_fallacies\\n\\ninstruction: Conclude your answer with: \"So the answer is valid or invalid.\".\\n- subject: geometric_shapes\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: hyperbaton\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_five_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_seven_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_three_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: movie_recommendation\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: multistep_arithmetic_two\\n\\ninstruction: Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.',\n",
              "      'bBox': {'x': 72, 'y': 329.04, 'w': 453, 'h': 391}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 33,\n",
              "    'text': '                              The Amazon Nova Family of Models\\n- subject: navigate\\n instruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: object_counting\\n instruction: Conclude your answer with: \"So the answer is <ANSWER>.\". Where <ANSWER> is\\n  an integer\\n- subject: penguins_in_a_table\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: reasoning_about_colored_objects\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: ruin_names\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: salient_translation_error_detection\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: snarks\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: sports_understanding\\n instruction: Conclude your answer with: \"So the answer is yes or no.\".\\n- subject: temporal_sequences\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_five_objects\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_seven_objects\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_three_objects\\n instruction: \"Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: web_of_lies\\n instruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: word_sorting\\n instruction: Conclude your answer with: \"So the answer is word_1 word_2 ... word_n.\".\"\\nFor GPQA:\\nWhat is the correct answer to this question: <question>\\nChoices: <choices>. Let’s think step by step:\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"\\nThe correct answer is (insert answer here).\"\\nB.1.2 Mathematical Reasoning\\nFor MATH, GSM8K:\\nSolve the following math problem step by step.\\n<problem>\\nRemember to put your answer inside \\\\boxed{}\\n                                           33',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n- subject: navigate\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: object_counting\\ninstruction: Conclude your answer with: \"So the answer is <ANSWER>.\". Where <ANSWER> is an integer\\n- subject: penguins_in_a_table\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: reasoning_about_colored_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: ruin_names\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: salient_translation_error_detection\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: snarks\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: sports_understanding\\ninstruction: Conclude your answer with: \"So the answer is yes or no.\"\\n- subject: temporal_sequences\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_five_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_seven_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_three_objects\\ninstruction: \"Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: web_of_lies\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: word_sorting\\ninstruction: Conclude your answer with: \"So the answer is word_1 word_2 ... word_n.\"\\n\\n# B.1.2 Mathematical Reasoning\\n\\nFor MATH, GSM8K:\\n\\nSolve the following math problem step by step.\\n\\n&lt;problem&gt;\\n\\nRemember to put your answer inside \\\\boxed{}\\n\\n33',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': '- subject: navigate\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: object_counting\\ninstruction: Conclude your answer with: \"So the answer is <ANSWER>.\". Where <ANSWER> is an integer\\n- subject: penguins_in_a_table\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: reasoning_about_colored_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: ruin_names\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: salient_translation_error_detection\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: snarks\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: sports_understanding\\ninstruction: Conclude your answer with: \"So the answer is yes or no.\"\\n- subject: temporal_sequences\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_five_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_seven_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_three_objects\\ninstruction: \"Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: web_of_lies\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: word_sorting\\ninstruction: Conclude your answer with: \"So the answer is word_1 word_2 ... word_n.\"',\n",
              "      'md': '- subject: navigate\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: object_counting\\ninstruction: Conclude your answer with: \"So the answer is <ANSWER>.\". Where <ANSWER> is an integer\\n- subject: penguins_in_a_table\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: reasoning_about_colored_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: ruin_names\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: salient_translation_error_detection\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: snarks\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: sports_understanding\\ninstruction: Conclude your answer with: \"So the answer is yes or no.\"\\n- subject: temporal_sequences\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_five_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_seven_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_three_objects\\ninstruction: \"Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: web_of_lies\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: word_sorting\\ninstruction: Conclude your answer with: \"So the answer is word_1 word_2 ... word_n.\"',\n",
              "      'bBox': {'x': 72, 'y': 72.04, 'w': 464, 'h': 403}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.1.2 Mathematical Reasoning',\n",
              "      'md': '# B.1.2 Mathematical Reasoning',\n",
              "      'bBox': {'x': 72, 'y': 605.04, 'w': 137, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'For MATH, GSM8K:\\n\\nSolve the following math problem step by step.\\n\\n&lt;problem&gt;\\n\\nRemember to put your answer inside \\\\boxed{}\\n\\n33',\n",
              "      'md': 'For MATH, GSM8K:\\n\\nSolve the following math problem step by step.\\n\\n&lt;problem&gt;\\n\\nRemember to put your answer inside \\\\boxed{}\\n\\n33',\n",
              "      'bBox': {'x': 72, 'y': 626.04, 'w': 241, 'h': 124}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 34,\n",
              "    'text': '                              The Amazon Nova Family of Models\\nB.1.3  Translation\\nFor Flores:\\nNova and LLama:\\nTranslate the following text into {tgt_lang}. Please output only the translated text with\\n no prefix or introduction: {src}\\nGemini and GPT:\\nYour job is to translate a sentence from {src_lang} into {tgt_lang}. Please output ONLY\\nthe translation and nothing else: {src}\\nB.1.4  Long Context\\nFor SQuALITY (ZeroScrolls Benchmark), we use the standard prompt template for Amazon Nova and Gemini models\\nas in [69]:\\nYou are given a story and a question. Answer the question in a paragraph.\\nStory:\\n<story>\\nQuestion:\\n<question>\\nAnswer:\\nB.2 Multimodal evaluation\\nB.2.1 MMMU\\nFor multiple-choice questions:\\nWith the image, the following question, and the four possible answers (A, B, C and D),\\nselect the correct answer.\\n<question>\\n(A) <answer-a>\\n(B) <answer-b>\\n...\\n(X) <answer-x>\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n- For complex questions: Adopt this step-by-step method:\\n## Step 1: [Concise description]\\n[Brief explanation]\\n## Step 2: [Concise description]\\n[Brief explanation]\\nIn every scenario, conclude with: The best answer is [the_answer_letter]. where [\\nthe_answer_letter] is one of A, B, C or D. Let’s proceed with a systematic approach\\nFor open-ended questions:\\nWith the image and the following question, provide a correct answer.\\n<question>\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n- For complex questions: Adopt this step-by-step method:\\n                                           34',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# B.1.3 Translation\\n\\nFor Flores:\\n\\nNova and LLama:\\n\\nTranslate the following text into {tgt_lang}. Please output only the translated text with no prefix or introduction: {src}\\n\\nGemini and GPT:\\n\\nYour job is to translate a sentence from {src_lang} into {tgt_lang}. Please output ONLY the translation and nothing else: {src}\\n\\n# B.1.4 Long Context\\n\\nFor SQuALITY (ZeroScrolls Benchmark), we use the standard prompt template for Amazon Nova and Gemini models as in [69]:\\n\\nYou are given a story and a question. Answer the question in a paragraph.\\n\\nStory:\\n\\n&lt;story&gt;\\n\\nQuestion:\\n\\n&lt;question&gt;\\n\\nAnswer:\\n\\n# B.2 Multimodal evaluation\\n\\n# B.2.1 MMMU\\n\\nFor multiple-choice questions:\\n\\nWith the image, the following question, and the four possible answers (A, B, C and D), select the correct answer.\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n...\\n\\n(X) &lt;answer-x&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:\\n\\n## Step 1: [Concise description]\\n\\n[Brief explanation]\\n\\n## Step 2: [Concise description]\\n\\n[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_letter]. where [the_answer_letter] is one of A, B, C or D. Let’s proceed with a systematic approach.\\n\\nFor open-ended questions:\\n\\nWith the image and the following question, provide a correct answer.\\n\\n&lt;question&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.1.3 Translation',\n",
              "      'md': '# B.1.3 Translation',\n",
              "      'bBox': {'x': 72, 'y': 72.04, 'w': 80, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'For Flores:\\n\\nNova and LLama:\\n\\nTranslate the following text into {tgt_lang}. Please output only the translated text with no prefix or introduction: {src}\\n\\nGemini and GPT:\\n\\nYour job is to translate a sentence from {src_lang} into {tgt_lang}. Please output ONLY the translation and nothing else: {src}',\n",
              "      'md': 'For Flores:\\n\\nNova and LLama:\\n\\nTranslate the following text into {tgt_lang}. Please output only the translated text with no prefix or introduction: {src}\\n\\nGemini and GPT:\\n\\nYour job is to translate a sentence from {src_lang} into {tgt_lang}. Please output ONLY the translation and nothing else: {src}',\n",
              "      'bBox': {'x': 72, 'y': 72.04, 'w': 465, 'h': 125}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.1.4 Long Context',\n",
              "      'md': '# B.1.4 Long Context',\n",
              "      'bBox': {'x': 72, 'y': 220.04, 'w': 89, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'For SQuALITY (ZeroScrolls Benchmark), we use the standard prompt template for Amazon Nova and Gemini models as in [69]:\\n\\nYou are given a story and a question. Answer the question in a paragraph.\\n\\nStory:\\n\\n&lt;story&gt;\\n\\nQuestion:\\n\\n&lt;question&gt;\\n\\nAnswer:',\n",
              "      'md': 'For SQuALITY (ZeroScrolls Benchmark), we use the standard prompt template for Amazon Nova and Gemini models as in [69]:\\n\\nYou are given a story and a question. Answer the question in a paragraph.\\n\\nStory:\\n\\n&lt;story&gt;\\n\\nQuestion:\\n\\n&lt;question&gt;\\n\\nAnswer:',\n",
              "      'bBox': {'x': 72, 'y': 239.04, 'w': 468, 'h': 127}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.2 Multimodal evaluation',\n",
              "      'md': '# B.2 Multimodal evaluation',\n",
              "      'bBox': {'x': 72, 'y': 390.04, 'w': 121, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.2.1 MMMU',\n",
              "      'md': '# B.2.1 MMMU',\n",
              "      'bBox': {'x': 72, 'y': 390.04, 'w': 66, 'h': 31}},\n",
              "     {'type': 'text',\n",
              "      'value': 'For multiple-choice questions:\\n\\nWith the image, the following question, and the four possible answers (A, B, C and D), select the correct answer.\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n...\\n\\n(X) &lt;answer-x&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "      'md': 'For multiple-choice questions:\\n\\nWith the image, the following question, and the four possible answers (A, B, C and D), select the correct answer.\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n...\\n\\n(X) &lt;answer-x&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "      'bBox': {'x': 72, 'y': 429.04, 'w': 450, 'h': 291}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 2,\n",
              "      'value': 'Step 1: [Concise description]',\n",
              "      'md': '## Step 1: [Concise description]',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': '[Brief explanation]',\n",
              "      'md': '[Brief explanation]',\n",
              "      'bBox': {'x': 72, 'y': 568.04, 'w': 99, 'h': 32}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 2,\n",
              "      'value': 'Step 2: [Concise description]',\n",
              "      'md': '## Step 2: [Concise description]',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': '[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_letter]. where [the_answer_letter] is one of A, B, C or D. Let’s proceed with a systematic approach.\\n\\nFor open-ended questions:\\n\\nWith the image and the following question, provide a correct answer.\\n\\n&lt;question&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "      'md': '[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_letter]. where [the_answer_letter] is one of A, B, C or D. Let’s proceed with a systematic approach.\\n\\nFor open-ended questions:\\n\\nWith the image and the following question, provide a correct answer.\\n\\n&lt;question&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "      'bBox': {'x': 72, 'y': 535.04, 'w': 434, 'h': 185}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 35,\n",
              "    'text': '                              The Amazon Nova Family of Models\\n## Step 1: [Concise description]\\n[Brief explanation]\\n## Step 2: [Concise description]\\n[Brief explanation]\\nIn every scenario, conclude with: The best answer is [the_answer_phrase]. where [\\nthe_answer_phrase] is a concise and direct answer to the question Let’s proceed with a\\nsystematic approach.\\nB.2.2  ChartQA, DocVQA, and TextVQA\\n<question>\\nAnswer the question using a single word or phrase.\\nB.2.3  VATEX\\nRender a clear and concise one-sentence summary of the video. The summary should be at\\nleast 10 words but no more than 20 words. Analyze the video first before summarizing it.\\nDo not hallucinate objects.\\nB.2.4  EgoSchema\\nYou will be given a question about a video and three possible answer options. You will be\\n provided frames from the video, sampled evenly across the video\\n<question>\\n(A) <answer-a>\\n(B) <answer-b>\\n(C) <answer-c>\\nAnswer with the option’s letter from the given choices directly.\\nAnswer with the option letter from the given choices directly.\\nB.2.5  VisualWebBench\\nFor the web captioning task:\\n\"You are given a screenshot of a webpage. Please generate the meta web description\\ninformation of this webpage, i.e., content attribute in <meta name=\"description\" content\\n=\"\"> HTML element.\\nYou should use this format, and do not output any explanation or any other contents:\\n<meta name=\"description\" content=\"YOUR ANSWER\">\\nFor the heading OCR task:\\nYou are given a screenshot of a webpage. Please generate the main text within the\\nscreenshot, which can be regarded as the heading of the webpage.\\nYou should directly tell me the first sentence of the main content, and do not output any\\n explanation or any other contents.\\nFor the web QA task:\\n<question>\\nYou should directly tell me your answer in the fewest words possible, and do not output\\nany explanation or any other contents.\\nFor the element OCR task:\\n                                           35',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# Step 1: [Concise description]\\n\\n[Brief explanation]\\n\\n# Step 2: [Concise description]\\n\\n[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_phrase]. where [the_answer_phrase] is a concise and direct answer to the question Let’s proceed with a systematic approach.\\n\\n# B.2.2  ChartQA, DocVQA, and TextVQA\\n\\n&lt;question&gt;\\n\\nAnswer the question using a single word or phrase.\\n\\n# B.2.3  VATEX\\n\\nRender a clear and concise one-sentence summary of the video. The summary should be at least 10 words but no more than 20 words. Analyze the video first before summarizing it. Do not hallucinate objects.\\n\\n# B.2.4  EgoSchema\\n\\nYou will be given a question about a video and three possible answer options. You will be provided frames from the video, sampled evenly across the video\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n(C) &lt;answer-c&gt;\\n\\nAnswer with the option’s letter from the given choices directly.\\n\\nAnswer with the option letter from the given choices directly.\\n\\n# B.2.5  VisualWebBench\\n\\nFor the web captioning task:\\n\\n\"You are given a screenshot of a webpage. Please generate the meta web description information of this webpage, i.e., content attribute in &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt; HTML element.\\n\\nYou should use this format, and do not output any explanation or any other contents:\\n\\n&lt;meta name=&quot;description&quot; content=&quot;YOUR ANSWER&quot;&gt;\\n\\nFor the heading OCR task:\\n\\nYou are given a screenshot of a webpage. Please generate the main text within the screenshot, which can be regarded as the heading of the webpage.\\n\\nYou should directly tell me the first sentence of the main content, and do not output any explanation or any other contents.\\n\\nFor the web QA task:\\n\\n&lt;question&gt;\\n\\nYou should directly tell me your answer in the fewest words possible, and do not output any explanation or any other contents.\\n\\nFor the element OCR task:',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Step 1: [Concise description]',\n",
              "      'md': '# Step 1: [Concise description]',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': '[Brief explanation]',\n",
              "      'md': '[Brief explanation]',\n",
              "      'bBox': {'x': 72, 'y': 83.04, 'w': 99, 'h': 32}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Step 2: [Concise description]',\n",
              "      'md': '# Step 2: [Concise description]',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': '[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_phrase]. where [the_answer_phrase] is a concise and direct answer to the question Let’s proceed with a systematic approach.',\n",
              "      'md': '[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_phrase]. where [the_answer_phrase] is a concise and direct answer to the question Let’s proceed with a systematic approach.',\n",
              "      'bBox': {'x': 72, 'y': 83.04, 'w': 450, 'h': 76}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.2.2  ChartQA, DocVQA, and TextVQA',\n",
              "      'md': '# B.2.2  ChartQA, DocVQA, and TextVQA',\n",
              "      'bBox': {'x': 72, 'y': 180.04, 'w': 179, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': '&lt;question&gt;\\n\\nAnswer the question using a single word or phrase.',\n",
              "      'md': '&lt;question&gt;\\n\\nAnswer the question using a single word or phrase.',\n",
              "      'bBox': {'x': 72, 'y': 211.04, 'w': 261, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.2.3  VATEX',\n",
              "      'md': '# B.2.3  VATEX',\n",
              "      'bBox': {'x': 72, 'y': 243.04, 'w': 64, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Render a clear and concise one-sentence summary of the video. The summary should be at least 10 words but no more than 20 words. Analyze the video first before summarizing it. Do not hallucinate objects.',\n",
              "      'md': 'Render a clear and concise one-sentence summary of the video. The summary should be at least 10 words but no more than 20 words. Analyze the video first before summarizing it. Do not hallucinate objects.',\n",
              "      'bBox': {'x': 72, 'y': 263.04, 'w': 460, 'h': 31}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.2.4  EgoSchema',\n",
              "      'md': '# B.2.4  EgoSchema',\n",
              "      'bBox': {'x': 72, 'y': 316.04, 'w': 81, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'You will be given a question about a video and three possible answer options. You will be provided frames from the video, sampled evenly across the video\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n(C) &lt;answer-c&gt;\\n\\nAnswer with the option’s letter from the given choices directly.\\n\\nAnswer with the option letter from the given choices directly.',\n",
              "      'md': 'You will be given a question about a video and three possible answer options. You will be provided frames from the video, sampled evenly across the video\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n(C) &lt;answer-c&gt;\\n\\nAnswer with the option’s letter from the given choices directly.\\n\\nAnswer with the option letter from the given choices directly.',\n",
              "      'bBox': {'x': 72, 'y': 336.04, 'w': 465, 'h': 86}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.2.5  VisualWebBench',\n",
              "      'md': '# B.2.5  VisualWebBench',\n",
              "      'bBox': {'x': 72, 'y': 444.04, 'w': 104, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'For the web captioning task:\\n\\n\"You are given a screenshot of a webpage. Please generate the meta web description information of this webpage, i.e., content attribute in &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt; HTML element.\\n\\nYou should use this format, and do not output any explanation or any other contents:\\n\\n&lt;meta name=&quot;description&quot; content=&quot;YOUR ANSWER&quot;&gt;\\n\\nFor the heading OCR task:\\n\\nYou are given a screenshot of a webpage. Please generate the main text within the screenshot, which can be regarded as the heading of the webpage.\\n\\nYou should directly tell me the first sentence of the main content, and do not output any explanation or any other contents.\\n\\nFor the web QA task:\\n\\n&lt;question&gt;\\n\\nYou should directly tell me your answer in the fewest words possible, and do not output any explanation or any other contents.\\n\\nFor the element OCR task:',\n",
              "      'md': 'For the web captioning task:\\n\\n\"You are given a screenshot of a webpage. Please generate the meta web description information of this webpage, i.e., content attribute in &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt; HTML element.\\n\\nYou should use this format, and do not output any explanation or any other contents:\\n\\n&lt;meta name=&quot;description&quot; content=&quot;YOUR ANSWER&quot;&gt;\\n\\nFor the heading OCR task:\\n\\nYou are given a screenshot of a webpage. Please generate the main text within the screenshot, which can be regarded as the heading of the webpage.\\n\\nYou should directly tell me the first sentence of the main content, and do not output any explanation or any other contents.\\n\\nFor the web QA task:\\n\\n&lt;question&gt;\\n\\nYou should directly tell me your answer in the fewest words possible, and do not output any explanation or any other contents.\\n\\nFor the element OCR task:',\n",
              "      'bBox': {'x': 72, 'y': 462.04, 'w': 465, 'h': 258}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 36,\n",
              "    'text': '                              The Amazon Nova Family of Models\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease perform OCR in the bounding box and recognize the text content within the red\\nbounding box.\\nFor the action prediction task:\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease select the best webpage description that matches the new webpage after clicking\\nthe selected element in the bounding box:\\n<choices_text>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the element grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one best matches the description: <element_desc>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the action grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one I should click to complete the following task: <instruction>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nB.2.6 MM-Mind2Web\\nImagine that you are imitating humans doing web navigation for a task step by step. At\\neach stage, you can see the webpage like humans by a screenshot and know the previous\\nactions before the current step decided by yourself through recorded history. You need to\\n decide on the first following action to take. You can click on an element with the mouse\\n, select an option, type text or press Enter with the keyboard. (For your understanding,\\nthey are like the click(), select_option() type() functions in playwright respectively).\\nOne next step means one operation within the three.\\nYou are asked to complete the following task: <question>\\nPrevious Actions:\\n<previous_actions>\\nThe screenshot below shows the webpage you see.\\nFollow the following guidance to think step by step before outlining the next action step\\n at the current stage:\\n(Current Webpage Identification)\\nFirstly, think about what the current webpage is.\\n(Previous Action Analysis)\\n                                           36',\n",
              "    'md': '                              The Amazon Nova Family of Models\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease perform OCR in the bounding box and recognize the text content within the red\\nbounding box.\\nFor the action prediction task:\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease select the best webpage description that matches the new webpage after clicking\\nthe selected element in the bounding box:\\n<choices_text>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the element grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one best matches the description: <element_desc>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the action grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one I should click to complete the following task: <instruction>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nB.2.6 MM-Mind2Web\\nImagine that you are imitating humans doing web navigation for a task step by step. At\\neach stage, you can see the webpage like humans by a screenshot and know the previous\\nactions before the current step decided by yourself through recorded history. You need to\\n decide on the first following action to take. You can click on an element with the mouse\\n, select an option, type text or press Enter with the keyboard. (For your understanding,\\nthey are like the click(), select_option() type() functions in playwright respectively).\\nOne next step means one operation within the three.\\nYou are asked to complete the following task: <question>\\nPrevious Actions:\\n<previous_actions>\\nThe screenshot below shows the webpage you see.\\nFollow the following guidance to think step by step before outlining the next action step\\n at the current stage:\\n(Current Webpage Identification)\\nFirstly, think about what the current webpage is.\\n(Previous Action Analysis)\\n                                           36',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'text',\n",
              "      'value': 'The Amazon Nova Family of Models\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease perform OCR in the bounding box and recognize the text content within the red\\nbounding box.\\nFor the action prediction task:\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease select the best webpage description that matches the new webpage after clicking\\nthe selected element in the bounding box:\\n<choices_text>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the element grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one best matches the description: <element_desc>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the action grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one I should click to complete the following task: <instruction>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nB.2.6 MM-Mind2Web\\nImagine that you are imitating humans doing web navigation for a task step by step. At\\neach stage, you can see the webpage like humans by a screenshot and know the previous\\nactions before the current step decided by yourself through recorded history. You need to\\n decide on the first following action to take. You can click on an element with the mouse\\n, select an option, type text or press Enter with the keyboard. (For your understanding,\\nthey are like the click(), select_option() type() functions in playwright respectively).\\nOne next step means one operation within the three.\\nYou are asked to complete the following task: <question>\\nPrevious Actions:\\n<previous_actions>\\nThe screenshot below shows the webpage you see.\\nFollow the following guidance to think step by step before outlining the next action step\\n at the current stage:\\n(Current Webpage Identification)\\nFirstly, think about what the current webpage is.\\n(Previous Action Analysis)\\n                                           36',\n",
              "      'md': 'The Amazon Nova Family of Models\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease perform OCR in the bounding box and recognize the text content within the red\\nbounding box.\\nFor the action prediction task:\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease select the best webpage description that matches the new webpage after clicking\\nthe selected element in the bounding box:\\n<choices_text>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the element grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one best matches the description: <element_desc>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the action grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one I should click to complete the following task: <instruction>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nB.2.6 MM-Mind2Web\\nImagine that you are imitating humans doing web navigation for a task step by step. At\\neach stage, you can see the webpage like humans by a screenshot and know the previous\\nactions before the current step decided by yourself through recorded history. You need to\\n decide on the first following action to take. You can click on an element with the mouse\\n, select an option, type text or press Enter with the keyboard. (For your understanding,\\nthey are like the click(), select_option() type() functions in playwright respectively).\\nOne next step means one operation within the three.\\nYou are asked to complete the following task: <question>\\nPrevious Actions:\\n<previous_actions>\\nThe screenshot below shows the webpage you see.\\nFollow the following guidance to think step by step before outlining the next action step\\n at the current stage:\\n(Current Webpage Identification)\\nFirstly, think about what the current webpage is.\\n(Previous Action Analysis)\\n                                           36',\n",
              "      'bBox': {'x': 72, 'y': 34.04, 'w': 465, 'h': 716}}],\n",
              "    'status': 'WARNING',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': True,\n",
              "    'noTextContent': False},\n",
              "   {'page': 37,\n",
              "    'text': '                              The Amazon Nova Family of Models\\nSecondly, combined with the screenshot, analyze each step of the previous action history\\nand their intention one by one. Particularly, pay more attention to the last step, which\\nmay be more related to what you should do now as the next step.\\n(Screenshot Details Analysis)\\nClosely examine the screenshot to check the status of every part of the webpage to\\nunderstand what you can operate with and what has been set or completed. You should\\nclosely examine the screenshot details to see what steps have been completed by previous\\nactions even though you are given the textual previous actions. Because the textual\\nhistory may not clearly and sufficiently record some effects of previous actions, you\\nshould closely evaluate the status of every part of the webpage to understand what you\\nhave done.\\n(Next Action Based on Webpage and Analysis)\\nThen, based on your analysis, in conjunction with human web browsing habits and the logic\\n of web design, decide on the following action. And clearly outline which element in the\\nwebpage users will operate with as the first next target element, its detailed location,\\nand the corresponding operation.\\nTo be successful, it is important to follow the following rules:\\n1. You should only issue a valid action given the current observation.\\n2. You should only issue one action at a time.\\n(Reiteration)\\nFirst, reiterate your next target element, its detailed location, and the corresponding\\noperation.\\n(Multichoice Question)\\nBelow is a multi-choice question, where the choices are elements in the webpage. From the\\n screenshot, find out where and what each one is on the webpage. Then, determine whether\\none matches your target element. Please examine the choices one by one. Choose the\\nmatching one. If multiple options match your answer, choose the most likely one by re-\\nexamining the screenshot, the choices, and your further reasoning.\\nIf none of these elements match your target element, please select, select <none_choice>.\\n None of the other options match the correct element.\\n<choices><none_choice>. None of the other options match the correct element.\\n(Final Answer)Finally, conclude your answer using the format below. Ensure your answer is\\n strictly adhering to the format provided below. Please do not leave any explanation in\\nyour answers of the final standardized format part, and this final part should be clear\\nand certain. The element choice, action, and value should be in three separate lines.\\nFormat:\\nELEMENT: The uppercase letter of your choice.\\nACTION: Choose an action from {CLICK, TYPE, SELECT, NONE}. Use NONE only if you choose\\noption F for the ELEMENT\\nVALUE: Provide additional input based on ACTION.\\nThe VALUE means:\\nIf ACTION == TYPE, specify the text to be typed.\\nIf ACTION == SELECT, specify the option to be chosen.\\nIf ACTION == CLICK, write \"None\".\\nB.2.7 GroundUI-1K\\n                                           37',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\nSecondly, combined with the screenshot, analyze each step of the previous action history and their intention one by one. Particularly, pay more attention to the last step, which may be more related to what you should do now as the next step.\\n\\n# (Screenshot Details Analysis)\\n\\nClosely examine the screenshot to check the status of every part of the webpage to understand what you can operate with and what has been set or completed. You should closely examine the screenshot details to see what steps have been completed by previous actions even though you are given the textual previous actions. Because the textual history may not clearly and sufficiently record some effects of previous actions, you should closely evaluate the status of every part of the webpage to understand what you have done.\\n\\n# (Next Action Based on Webpage and Analysis)\\n\\nThen, based on your analysis, in conjunction with human web browsing habits and the logic of web design, decide on the following action. And clearly outline which element in the webpage users will operate with as the first next target element, its detailed location, and the corresponding operation.\\n\\nTo be successful, it is important to follow the following rules:\\n\\n1. You should only issue a valid action given the current observation.\\n2. You should only issue one action at a time.\\n\\n# (Reiteration)\\n\\nFirst, reiterate your next target element, its detailed location, and the corresponding operation.\\n\\n# (Multichoice Question)\\n\\nBelow is a multi-choice question, where the choices are elements in the webpage. From the screenshot, find out where and what each one is on the webpage. Then, determine whether one matches your target element. Please examine the choices one by one. Choose the matching one. If multiple options match your answer, choose the most likely one by re-examining the screenshot, the choices, and your further reasoning.\\n\\nIf none of these elements match your target element, please select, select <none_choice>. None of the other options match the correct element.\\n\\n# (Final Answer)\\n\\nFinally, conclude your answer using the format below. Ensure your answer is strictly adhering to the format provided below. Please do not leave any explanation in your answers of the final standardized format part, and this final part should be clear and certain. The element choice, action, and value should be in three separate lines.\\n\\nFormat:\\n\\nELEMENT: The uppercase letter of your choice.\\n\\nACTION: Choose an action from {CLICK, TYPE, SELECT, NONE}. Use NONE only if you choose option F for the ELEMENT\\n\\nVALUE: Provide additional input based on ACTION.\\n\\nThe VALUE means:\\n\\n- If ACTION == TYPE, specify the text to be typed.\\n- If ACTION == SELECT, specify the option to be chosen.\\n- If ACTION == CLICK, write \"None\".\\n\\nB.2.7 GroundUI-1K',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Secondly, combined with the screenshot, analyze each step of the previous action history and their intention one by one. Particularly, pay more attention to the last step, which may be more related to what you should do now as the next step.',\n",
              "      'md': 'Secondly, combined with the screenshot, analyze each step of the previous action history and their intention one by one. Particularly, pay more attention to the last step, which may be more related to what you should do now as the next step.',\n",
              "      'bBox': {'x': 72, 'y': 72.04, 'w': 460, 'h': 32}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '(Screenshot Details Analysis)',\n",
              "      'md': '# (Screenshot Details Analysis)',\n",
              "      'bBox': {'x': 72, 'y': 116.04, 'w': 152, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Closely examine the screenshot to check the status of every part of the webpage to understand what you can operate with and what has been set or completed. You should closely examine the screenshot details to see what steps have been completed by previous actions even though you are given the textual previous actions. Because the textual history may not clearly and sufficiently record some effects of previous actions, you should closely evaluate the status of every part of the webpage to understand what you have done.',\n",
              "      'md': 'Closely examine the screenshot to check the status of every part of the webpage to understand what you can operate with and what has been set or completed. You should closely examine the screenshot details to see what steps have been completed by previous actions even though you are given the textual previous actions. Because the textual history may not clearly and sufficiently record some effects of previous actions, you should closely evaluate the status of every part of the webpage to understand what you have done.',\n",
              "      'bBox': {'x': 72, 'y': 127.04, 'w': 460, 'h': 75}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '(Next Action Based on Webpage and Analysis)',\n",
              "      'md': '# (Next Action Based on Webpage and Analysis)',\n",
              "      'bBox': {'x': 72, 'y': 214.04, 'w': 225, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Then, based on your analysis, in conjunction with human web browsing habits and the logic of web design, decide on the following action. And clearly outline which element in the webpage users will operate with as the first next target element, its detailed location, and the corresponding operation.\\n\\nTo be successful, it is important to follow the following rules:\\n\\n1. You should only issue a valid action given the current observation.\\n2. You should only issue one action at a time.',\n",
              "      'md': 'Then, based on your analysis, in conjunction with human web browsing habits and the logic of web design, decide on the following action. And clearly outline which element in the webpage users will operate with as the first next target element, its detailed location, and the corresponding operation.\\n\\nTo be successful, it is important to follow the following rules:\\n\\n1. You should only issue a valid action given the current observation.\\n2. You should only issue one action at a time.',\n",
              "      'bBox': {'x': 72, 'y': 225.04, 'w': 465, 'h': 130}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '(Reiteration)',\n",
              "      'md': '# (Reiteration)',\n",
              "      'bBox': {'x': 72, 'y': 323.04, 'w': 68, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'First, reiterate your next target element, its detailed location, and the corresponding operation.',\n",
              "      'md': 'First, reiterate your next target element, its detailed location, and the corresponding operation.',\n",
              "      'bBox': {'x': 72, 'y': 258.04, 'w': 455, 'h': 97}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '(Multichoice Question)',\n",
              "      'md': '# (Multichoice Question)',\n",
              "      'bBox': {'x': 72, 'y': 367.04, 'w': 115, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Below is a multi-choice question, where the choices are elements in the webpage. From the screenshot, find out where and what each one is on the webpage. Then, determine whether one matches your target element. Please examine the choices one by one. Choose the matching one. If multiple options match your answer, choose the most likely one by re-examining the screenshot, the choices, and your further reasoning.\\n\\nIf none of these elements match your target element, please select, select <none_choice>. None of the other options match the correct element.',\n",
              "      'md': 'Below is a multi-choice question, where the choices are elements in the webpage. From the screenshot, find out where and what each one is on the webpage. Then, determine whether one matches your target element. Please examine the choices one by one. Choose the matching one. If multiple options match your answer, choose the most likely one by re-examining the screenshot, the choices, and your further reasoning.\\n\\nIf none of these elements match your target element, please select, select <none_choice>. None of the other options match the correct element.',\n",
              "      'bBox': {'x': 72, 'y': 378.04, 'w': 465, 'h': 86}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': '(Final Answer)',\n",
              "      'md': '# (Final Answer)',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Finally, conclude your answer using the format below. Ensure your answer is strictly adhering to the format provided below. Please do not leave any explanation in your answers of the final standardized format part, and this final part should be clear and certain. The element choice, action, and value should be in three separate lines.\\n\\nFormat:\\n\\nELEMENT: The uppercase letter of your choice.\\n\\nACTION: Choose an action from {CLICK, TYPE, SELECT, NONE}. Use NONE only if you choose option F for the ELEMENT\\n\\nVALUE: Provide additional input based on ACTION.\\n\\nThe VALUE means:\\n\\n- If ACTION == TYPE, specify the text to be typed.\\n- If ACTION == SELECT, specify the option to be chosen.\\n- If ACTION == CLICK, write \"None\".\\n\\nB.2.7 GroundUI-1K',\n",
              "      'md': 'Finally, conclude your answer using the format below. Ensure your answer is strictly adhering to the format provided below. Please do not leave any explanation in your answers of the final standardized format part, and this final part should be clear and certain. The element choice, action, and value should be in three separate lines.\\n\\nFormat:\\n\\nELEMENT: The uppercase letter of your choice.\\n\\nACTION: Choose an action from {CLICK, TYPE, SELECT, NONE}. Use NONE only if you choose option F for the ELEMENT\\n\\nVALUE: Provide additional input based on ACTION.\\n\\nThe VALUE means:\\n\\n- If ACTION == TYPE, specify the text to be typed.\\n- If ACTION == SELECT, specify the option to be chosen.\\n- If ACTION == CLICK, write \"None\".\\n\\nB.2.7 GroundUI-1K',\n",
              "      'bBox': {'x': 72, 'y': 498.04, 'w': 455, 'h': 220}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 38,\n",
              "    'text': '                              The Amazon Nova Family of Models\\nWhich action should I do if I want to Click on <element> and where is the action? Express\\n the location coordinates using the (x1, y1, x2, y2) format, scaled between 0 and 1000.\\nB.3 Functional Capabilities\\nB.3.1 FinQA\\nGiven the following finance question, analyze the question in details step-by-step before\\n giving the final answer. Your answer should begin with \"Lets think step-by-step\". Your\\nresponse should end with \"The answer is [the_final_answer]\", where [the_final_answer]\\nshould be the most concise answer without any explanation.\\n### Input\\nSupporting Facts:\\n<pre-text>\\n<table>\\n<post-text>\\nQuestion:\\n<question>\\nWe use regex “The answer is (.*)” to extract the answer. We convert answers with percent signs and magnitude terms to\\ndecimal numerical representation (e.g. convert “1.3%” to 0.013 and “5.2 millions” to 5,200,000). An answer is correct\\nif it is identical to the ground truth when rounded to the same decimal places.\\nB.3.2 RAG\\nYou are a teacher grading a quiz.\\nYou are given a question, the student’s answer, and the true answer, and are asked to\\nscore the student answer as either Correct or Incorrect.\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student’s answer here\\nTRUE ANSWER: true answer here\\nGRADE: Correct or Incorrect here\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in\\npunctuation and phrasing between the student answer and true answer. It is OK if the\\nstudent answer contains more information than the true answer, as long as it does not\\ncontain any conflicting statements. Begin!\\nQUESTION: {query}\\nSTUDENT ANSWER: {answer}\\nTRUE ANSWER: {expected_answer}\\nGRADE:\\nYour response should be in json format as follows:\\n{{\\n \"justification\": (Without mentioning the student/teacher framing of this prompt, explain\\n  why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep\\n the answer as concise as possible.),\\n \"grade\": (correct or incorrect)\\n}}\\n                                           38',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\nWhich action should I do if I want to Click on &lt;element&gt; and where is the action? Express the location coordinates using the (x1, y1, x2, y2) format, scaled between 0 and 1000.\\n\\n# B.3 Functional Capabilities\\n\\n# B.3.1 FinQA\\n\\nGiven the following finance question, analyze the question in details step-by-step before giving the final answer. Your answer should begin with \"Lets think step-by-step\". Your response should end with \"The answer is [the_final_answer]\", where [the_final_answer] should be the most concise answer without any explanation.\\n\\n# Input\\n\\nSupporting Facts:\\n\\nQuestion:\\n\\n&lt;question&gt;\\nWe use regex “The answer is (.*)” to extract the answer. We convert answers with percent signs and magnitude terms to decimal numerical representation (e.g. convert “1.3%” to 0.013 and “5.2 millions” to 5,200,000). An answer is correct if it is identical to the ground truth when rounded to the same decimal places.\\n\\n# B.3.2 RAG\\n\\nYou are a teacher grading a quiz. You are given a question, the student’s answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\\n\\n# Example Format:\\n\\nQUESTION: question here\\n\\nSTUDENT ANSWER: student’s answer here\\n\\nTRUE ANSWER: true answer here\\n\\nGRADE: Correct or Incorrect here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\n\\nSTUDENT ANSWER: {answer}\\n\\nTRUE ANSWER: {expected_answer}\\n\\nGRADE:\\n\\nYour response should be in json format as follows:\\n\\n{\\n\"justification\": (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.),\\n\"grade\": (correct or incorrect)\\n}',\n",
              "    'images': [],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Which action should I do if I want to Click on &lt;element&gt; and where is the action? Express the location coordinates using the (x1, y1, x2, y2) format, scaled between 0 and 1000.',\n",
              "      'md': 'Which action should I do if I want to Click on &lt;element&gt; and where is the action? Express the location coordinates using the (x1, y1, x2, y2) format, scaled between 0 and 1000.',\n",
              "      'bBox': {'x': 77, 'y': 91.04, 'w': 450, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.3 Functional Capabilities',\n",
              "      'md': '# B.3 Functional Capabilities',\n",
              "      'bBox': {'x': 72, 'y': 125.04, 'w': 123, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.3.1 FinQA',\n",
              "      'md': '# B.3.1 FinQA',\n",
              "      'bBox': {'x': 72, 'y': 125.04, 'w': 60, 'h': 30}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Given the following finance question, analyze the question in details step-by-step before giving the final answer. Your answer should begin with \"Lets think step-by-step\". Your response should end with \"The answer is [the_final_answer]\", where [the_final_answer] should be the most concise answer without any explanation.',\n",
              "      'md': 'Given the following finance question, analyze the question in details step-by-step before giving the final answer. Your answer should begin with \"Lets think step-by-step\". Your response should end with \"The answer is [the_final_answer]\", where [the_final_answer] should be the most concise answer without any explanation.',\n",
              "      'bBox': {'x': 72, 'y': 165.04, 'w': 465, 'h': 43}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Input',\n",
              "      'md': '# Input',\n",
              "      'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Supporting Facts:\\n\\nQuestion:\\n\\n&lt;question&gt;\\nWe use regex “The answer is (.*)” to extract the answer. We convert answers with percent signs and magnitude terms to decimal numerical representation (e.g. convert “1.3%” to 0.013 and “5.2 millions” to 5,200,000). An answer is correct if it is identical to the ground truth when rounded to the same decimal places.',\n",
              "      'md': 'Supporting Facts:\\n\\nQuestion:\\n\\n&lt;question&gt;\\nWe use regex “The answer is (.*)” to extract the answer. We convert answers with percent signs and magnitude terms to decimal numerical representation (e.g. convert “1.3%” to 0.013 and “5.2 millions” to 5,200,000). An answer is correct if it is identical to the ground truth when rounded to the same decimal places.',\n",
              "      'bBox': {'x': 71, 'y': 230.04, 'w': 469, 'h': 123}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'B.3.2 RAG',\n",
              "      'md': '# B.3.2 RAG',\n",
              "      'bBox': {'x': 72, 'y': 125.04, 'w': 53, 'h': 251}},\n",
              "     {'type': 'text',\n",
              "      'value': 'You are a teacher grading a quiz. You are given a question, the student’s answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.',\n",
              "      'md': 'You are a teacher grading a quiz. You are given a question, the student’s answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.',\n",
              "      'bBox': {'x': 72, 'y': 387.04, 'w': 444, 'h': 31}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Example Format:',\n",
              "      'md': '# Example Format:',\n",
              "      'bBox': {'x': 72, 'y': 419.04, 'w': 78, 'h': 10}},\n",
              "     {'type': 'text',\n",
              "      'value': 'QUESTION: question here\\n\\nSTUDENT ANSWER: student’s answer here\\n\\nTRUE ANSWER: true answer here\\n\\nGRADE: Correct or Incorrect here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\n\\nSTUDENT ANSWER: {answer}\\n\\nTRUE ANSWER: {expected_answer}\\n\\nGRADE:\\n\\nYour response should be in json format as follows:\\n\\n{\\n\"justification\": (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.),\\n\"grade\": (correct or incorrect)\\n}',\n",
              "      'md': 'QUESTION: question here\\n\\nSTUDENT ANSWER: student’s answer here\\n\\nTRUE ANSWER: true answer here\\n\\nGRADE: Correct or Incorrect here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\n\\nSTUDENT ANSWER: {answer}\\n\\nTRUE ANSWER: {expected_answer}\\n\\nGRADE:\\n\\nYour response should be in json format as follows:\\n\\n{\\n\"justification\": (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.),\\n\"grade\": (correct or incorrect)\\n}',\n",
              "      'bBox': {'x': 72, 'y': 285.04, 'w': 464, 'h': 341}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 39,\n",
              "    'text': \"                                      The Amazon Nova Family of Models\\nC    Qualitative examples of multimodal intelligence\\n       User input                                                Response\\n  This looks like an amazing site. What is it and where      This is the Space Needle, a landmark in Seattle,\\n  is it located? Are there other fun things around this      Washington, USA. It is a well-known observation\\n  site to explore?                                           tower and asymbol of Seattle: It was built for the\\n                                                             1962 World's Fair and has since become a popular\\n                                                             tourist attraction. The Space Needle offers\\n                                                             panoramic views of thecity,the surrounding\\n                                                             mountains, and the Puget Sound.\\n                                                             The Space Needle is located in the Seattle Center;\\n                                                             which is a culturaland entertainment hub. The\\n                                                             Seattle Center is home to many other attractions,\\n                                                             including the Chihuly Garden and Glass, the\\n                                                             Museum of Pop Culture, and the Pacific Science\\n                                                             Center: The Seattle Center also hosts events like\\n                                                             concerts, festivals, and art exhibitions.\\n                                                             The Seattle Center is also close to other popular\\n                                                             attractions in Seattle, such as the Pike Place Market,\\n                                                             the Seattle Aquarium, and the Seattle Art Museum:\\n                                                             The city is known for its vibrant arts and culture\\n                                                             scene, outdoor activities, and beautiful natural\\n                                                             surroundings.\\n                         Figure 6: Created with Nova Pro. Photo taken by a team member.\\n                                                        39\",\n",
              "    'md': \"# The Amazon Nova Family of Models\\n\\n# Qualitative examples of multimodal intelligence\\n\\n|User input|Response|\\n|---|---|\\n|This looks like an amazing site. What is it and where is it located? Are there other fun things around this site to explore?|This is the Space Needle, a landmark in Seattle, Washington, USA. It is a well-known observation tower and a symbol of Seattle: It was built for the 1962 World's Fair and has since become a popular tourist attraction. The Space Needle offers panoramic views of the city, the surrounding mountains, and the Puget Sound. The Space Needle is located in the Seattle Center; which is a cultural and entertainment hub. The Seattle Center is home to many other attractions, including the Chihuly Garden and Glass, the Museum of Pop Culture, and the Pacific Science Center: The Seattle Center also hosts events like concerts, festivals, and art exhibitions. The Seattle Center is also close to other popular attractions in Seattle, such as the Pike Place Market, the Seattle Aquarium, and the Seattle Art Museum: The city is known for its vibrant arts and culture scene, outdoor activities, and beautiful natural surroundings.|\\n\\nFigure 6: Created with Nova Pro. Photo taken by a team member.\\n\\n39\",\n",
              "    'images': [{'name': 'img_p38_1.png',\n",
              "      'height': 1837,\n",
              "      'width': 3112,\n",
              "      'x': 72,\n",
              "      'y': 103.05993999999998,\n",
              "      'original_width': 3112,\n",
              "      'original_height': 1837,\n",
              "      'ocr': [{'x': 205,\n",
              "        'y': 62,\n",
              "        'w': 366,\n",
              "        'h': 90,\n",
              "        'confidence': '0.6628547286717683',\n",
              "        'text': 'User input'},\n",
              "       {'x': 1757,\n",
              "        'y': 61,\n",
              "        'w': 340,\n",
              "        'h': 101,\n",
              "        'confidence': '0.9999955324299241',\n",
              "        'text': 'Response'},\n",
              "       {'x': 68,\n",
              "        'y': 255,\n",
              "        'w': 1366,\n",
              "        'h': 86,\n",
              "        'confidence': '0.7810586773935181',\n",
              "        'text': 'This looks like an amazing site. What is it and where'},\n",
              "       {'x': 1651,\n",
              "        'y': 262,\n",
              "        'w': 1244,\n",
              "        'h': 85,\n",
              "        'confidence': '0.79727348576406',\n",
              "        'text': 'This is the Space Needle, a landmark in Seattle,'},\n",
              "       {'x': 68,\n",
              "        'y': 335,\n",
              "        'w': 1353,\n",
              "        'h': 86,\n",
              "        'confidence': '0.7041544994948097',\n",
              "        'text': 'is it located? Are there other fun things around this'},\n",
              "       {'x': 1654,\n",
              "        'y': 336,\n",
              "        'w': 1272,\n",
              "        'h': 86,\n",
              "        'confidence': '0.8550619505172774',\n",
              "        'text': 'Washington, USA. It is a well-known observation'},\n",
              "       {'x': 69,\n",
              "        'y': 417,\n",
              "        'w': 420,\n",
              "        'h': 80,\n",
              "        'confidence': '0.6981948604601093',\n",
              "        'text': 'site to explore?'},\n",
              "       {'x': 1650,\n",
              "        'y': 420,\n",
              "        'w': 324,\n",
              "        'h': 61,\n",
              "        'confidence': '0.8698726592883513',\n",
              "        'text': 'tower and a'},\n",
              "       {'x': 1973,\n",
              "        'y': 412,\n",
              "        'w': 980,\n",
              "        'h': 87,\n",
              "        'confidence': '0.7442992882417068',\n",
              "        'text': 'symbol of Seattle: It was built for the'},\n",
              "       {'x': 1653,\n",
              "        'y': 493,\n",
              "        'w': 1324,\n",
              "        'h': 86,\n",
              "        'confidence': '0.8500821333386501',\n",
              "        'text': \"1962 World's Fair and has since become a popular\"},\n",
              "       {'x': 1649,\n",
              "        'y': 572,\n",
              "        'w': 1118,\n",
              "        'h': 75,\n",
              "        'confidence': '0.9542317425274454',\n",
              "        'text': 'tourist attraction. The Space Needle offers'},\n",
              "       {'x': 1650,\n",
              "        'y': 649,\n",
              "        'w': 624,\n",
              "        'h': 77,\n",
              "        'confidence': '0.7315620401156177',\n",
              "        'text': 'panoramic views of the'},\n",
              "       {'x': 2388,\n",
              "        'y': 643,\n",
              "        'w': 439,\n",
              "        'h': 81,\n",
              "        'confidence': '0.983363656053505',\n",
              "        'text': 'the surrounding'},\n",
              "       {'x': 1654,\n",
              "        'y': 727,\n",
              "        'w': 880,\n",
              "        'h': 78,\n",
              "        'confidence': '0.4865689817306662',\n",
              "        'text': 'mountains, and the Puget Sound.'},\n",
              "       {'x': 1652,\n",
              "        'y': 802,\n",
              "        'w': 1302,\n",
              "        'h': 79,\n",
              "        'confidence': '0.8627259401501235',\n",
              "        'text': 'The Space Needle is located in the Seattle Center;'},\n",
              "       {'x': 1655,\n",
              "        'y': 882,\n",
              "        'w': 1228,\n",
              "        'h': 66,\n",
              "        'confidence': '0.8050970984365342',\n",
              "        'text': 'which is a culturaland entertainment hub. The'},\n",
              "       {'x': 1652,\n",
              "        'y': 954,\n",
              "        'w': 1301,\n",
              "        'h': 76,\n",
              "        'confidence': '0.8879288367708251',\n",
              "        'text': 'Seattle Center is home to many other attractions,'},\n",
              "       {'x': 1647,\n",
              "        'y': 1026,\n",
              "        'w': 1158,\n",
              "        'h': 85,\n",
              "        'confidence': '0.7586642731645438',\n",
              "        'text': 'including the Chihuly Garden and Glass, the'},\n",
              "       {'x': 1656,\n",
              "        'y': 1111,\n",
              "        'w': 1250,\n",
              "        'h': 74,\n",
              "        'confidence': '0.7087349628305936',\n",
              "        'text': 'Museum of Pop Culture, and the Pacific Science'},\n",
              "       {'x': 1653,\n",
              "        'y': 1188,\n",
              "        'w': 1254,\n",
              "        'h': 66,\n",
              "        'confidence': '0.7633825588016168',\n",
              "        'text': 'Center: The Seattle Center also hosts events like'},\n",
              "       {'x': 1652,\n",
              "        'y': 1263,\n",
              "        'w': 1008,\n",
              "        'h': 75,\n",
              "        'confidence': '0.6185691724891468',\n",
              "        'text': 'concerts, festivals, and art exhibitions.'},\n",
              "       {'x': 1652,\n",
              "        'y': 1340,\n",
              "        'w': 1266,\n",
              "        'h': 80,\n",
              "        'confidence': '0.9050881059450384',\n",
              "        'text': 'The Seattle Center is also close to other popular'},\n",
              "       {'x': 1649,\n",
              "        'y': 1416,\n",
              "        'w': 1375,\n",
              "        'h': 76,\n",
              "        'confidence': '0.7419498307335177',\n",
              "        'text': 'attractions in Seattle, such as the Pike Place Market,'},\n",
              "       {'x': 1649,\n",
              "        'y': 1495,\n",
              "        'w': 1350,\n",
              "        'h': 77,\n",
              "        'confidence': '0.8692300865639507',\n",
              "        'text': 'the Seattle Aquarium, and the Seattle Art Museum:'},\n",
              "       {'x': 1652,\n",
              "        'y': 1574,\n",
              "        'w': 1252,\n",
              "        'h': 73,\n",
              "        'confidence': '0.9698072830299177',\n",
              "        'text': 'The city is known for its vibrant arts and culture'},\n",
              "       {'x': 1649,\n",
              "        'y': 1649,\n",
              "        'w': 1235,\n",
              "        'h': 73,\n",
              "        'confidence': '0.6691461561841235',\n",
              "        'text': 'scene, outdoor activities, and beautiful natural'},\n",
              "       {'x': 1648,\n",
              "        'y': 1722,\n",
              "        'w': 372,\n",
              "        'h': 79,\n",
              "        'confidence': '0.6824997127742795',\n",
              "        'text': 'surroundings.'},\n",
              "       {'x': 2278,\n",
              "        'y': 646,\n",
              "        'w': 106,\n",
              "        'h': 80,\n",
              "        'confidence': '0.9200347122722271',\n",
              "        'text': 'city,'}]}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Qualitative examples of multimodal intelligence',\n",
              "      'md': '# Qualitative examples of multimodal intelligence',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 263, 'h': 12}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['User input', 'Response'],\n",
              "       ['This looks like an amazing site. What is it and where is it located? Are there other fun things around this site to explore?',\n",
              "        \"This is the Space Needle, a landmark in Seattle, Washington, USA. It is a well-known observation tower and a symbol of Seattle: It was built for the 1962 World's Fair and has since become a popular tourist attraction. The Space Needle offers panoramic views of the city, the surrounding mountains, and the Puget Sound. The Space Needle is located in the Seattle Center; which is a cultural and entertainment hub. The Seattle Center is home to many other attractions, including the Chihuly Garden and Glass, the Museum of Pop Culture, and the Pacific Science Center: The Seattle Center also hosts events like concerts, festivals, and art exhibitions. The Seattle Center is also close to other popular attractions in Seattle, such as the Pike Place Market, the Seattle Aquarium, and the Seattle Art Museum: The city is known for its vibrant arts and culture scene, outdoor activities, and beautiful natural surroundings.\"]],\n",
              "      'md': \"|User input|Response|\\n|This looks like an amazing site. What is it and where is it located? Are there other fun things around this site to explore?|This is the Space Needle, a landmark in Seattle, Washington, USA. It is a well-known observation tower and a symbol of Seattle: It was built for the 1962 World's Fair and has since become a popular tourist attraction. The Space Needle offers panoramic views of the city, the surrounding mountains, and the Puget Sound. The Space Needle is located in the Seattle Center; which is a cultural and entertainment hub. The Seattle Center is home to many other attractions, including the Chihuly Garden and Glass, the Museum of Pop Culture, and the Pacific Science Center: The Seattle Center also hosts events like concerts, festivals, and art exhibitions. The Seattle Center is also close to other popular attractions in Seattle, such as the Pike Place Market, the Seattle Aquarium, and the Seattle Art Museum: The city is known for its vibrant arts and culture scene, outdoor activities, and beautiful natural surroundings.|\",\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"User input\",\"Response\"\\n\"This looks like an amazing site. What is it and where is it located? Are there other fun things around this site to explore?\",\"This is the Space Needle, a landmark in Seattle, Washington, USA. It is a well-known observation tower and a symbol of Seattle: It was built for the 1962 World\\'s Fair and has since become a popular tourist attraction. The Space Needle offers panoramic views of the city, the surrounding mountains, and the Puget Sound. The Space Needle is located in the Seattle Center; which is a cultural and entertainment hub. The Seattle Center is home to many other attractions, including the Chihuly Garden and Glass, the Museum of Pop Culture, and the Pacific Science Center: The Seattle Center also hosts events like concerts, festivals, and art exhibitions. The Seattle Center is also close to other popular attractions in Seattle, such as the Pike Place Market, the Seattle Aquarium, and the Seattle Art Museum: The city is known for its vibrant arts and culture scene, outdoor activities, and beautiful natural surroundings.\"',\n",
              "      'bBox': {'x': 103, 'y': 112, 'w': 284, 'h': 15}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Figure 6: Created with Nova Pro. Photo taken by a team member.\\n\\n39',\n",
              "      'md': 'Figure 6: Created with Nova Pro. Photo taken by a team member.\\n\\n39',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 365, 'h': 679.99}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 40,\n",
              "    'text': '                                                                                                        The Amazon Nova Family of Models\\n                    User input:                                                           \"Search for Psych and buythe first episode\"\\n                    Agent step 1                                                                                                                                        Agent step 2\\n      amazon  Prime Video    Search Amazon                                                                         EN   Hello, sign in  Retums   10             amazon        Prime Video Psych                                                                                   EN -  Hello, sign in   Retums 10\\n                                                                                                                        Account & Lists   Orders                                            psych             Action: Click              \\\\\\'Psych\"                                       Account & Lists  & Orders\\n                                                                                                                                Black Friday Week=\\n   All Black Friday Deals          Action: Type               \\\\\\'Psych\"                ases  Music Customer ServiceAmazon Home                 shopnow             All  Black Friday Dcals                                                                                        Amazon Home   Black Friday Weekshopnow\\n                                                            1)                                                                                                                              psycho\\n    prime video     Home                                                                                                                Join Prime                  prime video    Home     psychological thrillers                                                                                      Join Prime\\n                                                                                                                                                                                            psychic\\n                                                                                                                                                                                            psychic kids\\n       prime                                                                                                                                                                                psych the movie\\n                                                                                                                                                                                            psychology\\n      amazonwish list                                                                                                                                                                       psychic investigators\\n    GAMES                                                                                                                                                           YELLO                   psycho-pass\\n                                                                                                                                                                    GOLDEN GLOBE@ winner    psychopath\\n            Episode                                                                                                                                                   BuyEpisode\\n            Watch now                                                                                                                                                 HD $2.99\\n       Freevee (with ads)                                                                                                                                             Available to buy                                                                                        TV-MA\\n    Popular movies     See more                                                                                                                                     Popular movies    See more\\n                                           IN     Tₚᵣₘₜₑ                BROTHERS                                                                                         APCCALTO                     NFANIT                            BROTHERS|\\n                                                                                  prime                        prime                                                                                                                              prime                       prime\\n    Top-~rated movies        See more                                                                                                                               Top-~rated movies       See more\\n      R                                    4                    Ihe                                                                                                 Re                                                         Iie\\n                                                                                                                            F78 E                                                                                              Godeler                                                     at8e\\n                                                      prime                       prime                        prime                                                                                                  prime                       ~prime                      prime\\n    Featured Originals and Exclusives                                                                                                                               Featured Originals and Exclusives\\n                                                                         Neeson                                                                                                                                                          Neeson\\n                                                              M E M & R Y                                                                                                                                                     M EM & R Y\\n       PIMPINERD                           FRoM                                                  RINGS                                        PFACHEP                 PIMPINERD                     FRoM                                                         RINGS                                  PFACHEP\\n          Agent step 3                                                                                                                                                   Agent step 4\\namazon         Prime Video   psych                                                                                 EN -   Hello, sign in  Retums           10,  amazon        Prime Video      psych                                                                                    Hello, sign in Retums   10\\n                                                                                                                          Accdunt Lists        Orders                                                                                                                             EN -  Account Lists    Orders\\n ZAl    Black Friday Dcals   Medical Care  Best SellersPrime  Amazon BasicsGroceries  New ReleasesMusicCustomer ServiceAmazon Home  Black Friday Week shop now  ZAUL  Black Friday Dcals    Medical Care  Best SellersPrime  Amazon BasicsGroceries  New ReleasesMusicCustomer ServiceAmazon Home  Black Friday Weekshopnow\\n   Home   Store Free with ads  Categories  My Stuff                                                                         Settings  Getting Started  Help         prime video    Home     Movies    TV shows    Sports    Live TV      88 Subscriptions                                                Join Prime\\n                  1-16 of 423 results for \"psych\\\\\\'                                                                   Soit Dy:Fejined\\n                  Department                   Results\\n                   Any Department\\n                     MoviesTV                                         psych  Psych Season\\n                  Prime Video                                                 Dna\\n                  Customer Reviews                                                        Action: Click \\\\\\'Psych Season 1\"\\n                               Up\\n                  Actor                                                      For 534.99to buy season         et al\\n                  James Roday                                                Or $0.00 withPrime membership\\n                  Dule Hill\\n                  Timothy Omundson\\n                  Maggie Lawson                                              Psych Season 3\\n                  Price                                               psych  2008   CC                                                                             prime\\n                  Frce\\n                  S0 to $1                                            Ua                  894\\n                  S1to $3                                                    Prime Video                     Starring: James Roday , Dule Hill                      Psych\\n                  53 to $5                                                       299                         and Timothy Omundson\\n                  S5 to $10                                                  For     [0buyepisode            Directed by: Mel DamskiSteve\\n                  510 to $15                                                 For 534.99 to buy season        Franks  Roday Rodriguez\\n                  515 to S20                                                 Or $0.00 withPrime membership   et al James                                            PRIMETIME EMMY? nominee\\n                  Condition                                                                                                                                                Season 1\\n                  New\\n                  Format                               peych                 Psych Season                                                                           James Roday plays the somewhat eccentric Shawn Spencer;     police\\n                    Prime Video                                              2013   CC                                                                              consultant who solves crimes with powers of observation so acute that\\n                  Genre                                                      Prime Video  1,326              Starring: James RodayShawn                             the police think he\\'$ psychic or S0 he lets them think:\\n                  ActionAdventure                                            For *299 to buy episode         Spencer,Dule Hill and Corbin                                       739 IMDb 8.42007        Action: Click BuyEpisode 1 HD $2.99\"\\n                  Animation                                                                                  Bernsen                                                Comedy    Suspense\\n                  Arts, CultureEntertainment                                 For 534.99 to buy season        Directed by: Mel DamskiSteve\\n                  Comedy                                                     Or $0.00 withPrime membership   FranksJames Roday Rodriguez\\n                  Documentary                                                                                et al.                                                   Watch with Prime                BuyEpisode          BuySeason          More purchase\\n                  Drama                                                                                                                                              Start your 30-day free trial     HD $2.99            HD $34.99          options                                     08\\n                  Fantasy\\n                  Horror                               amizonoRiginal        Bosch   Season 3\\n                  International                            BOScH             2017   TV-MA   CC                                                                        Join Prime\\n                  Music Videos  Concerts                                                  \\'782\\n                  Romance                                                    Prime Video                     Starring: Titus WelliverJamie                          Terms apply\\n                  Science Fiction                                                                            HectorAmy Aquino and\\n                    See more                                                     Watch now: Free with ads    Madison Lintz                                          Episodes   Related    Details\\n                  Video Length                                               Or $0.00 withPrime membership   Directed by: Adam Davidson ,\\n                     Up toMinutes                                                                            Alex Zakrzewski ,Sarah Pia\\n                  11t0 20 Minutes                                                                            Andersonet al.\\n                 21 t0 30 Minutes                                                                                                                                                                                                                                                                        Sort\\n                 31 to 60 Minutes                                            Hollywood Homicide\\n                 61 to 90 Minutes                                            2003   PG-13  CC                                                                                                  51 E1 - Psych\\n                    More Than 90 Minutes                                                  609\\n                  Decade                                                     Prime Video                     Starring: Harrison FordJosh                                                        July 6, 2006 1h 6 min13+\\n                      2020   Newer                                               399                         Hartnett .Isaiah Washington ,et                                                    When the Santa Barbara police department hit a wall on a high profile kidnapping case;\\n                    20102019                                                 For     t0 rent                 al:                                                                                thev reluctantlv hire Shawn Spencer the town\\'s newest Dsvchic detectiveto held solve the\\n                                                                                                   Figure 7: Example of a multimodal agent.\\n                                                                                                                                                      40',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\nUser input: \"Search for Psych and buy the first episode\"\\n\\n|Agent step 1|Agent step 2|\\n|---|---|\\n|amazon Prime Video Search Amazon|EN Hello, sign in Returns 10|\\n|amazon Prime Video Psych|EN - Hello, sign in Returns 10|\\n|Account & Lists Orders|psych Action: Click \\'Psych\\'|\\n|Black Friday Week= All Black Friday Deals|Action: Type \\'Psych\\'|\\n|amazon Home shop now|All Black Friday Deals|\\n|prime video Home|Join Prime|\\n|prime|psychological thrillers|\\n|psychic|psychic kids|\\n|psych the movie|psychology|\\n|psychic investigators|YELLO|\\n|psycho|GOLDEN GLOBE@ winner|\\n|Buy Episode|Watch now HD $2.99|\\n|Freevee (with ads)|Available to buy TV-MA|\\n|Popular movies See more|Top-rated movies See more|\\n|Featured Originals and Exclusives|Neeson|\\n|Agent step 3|Agent step 4|\\n|amazon Prime Video psych|EN - Hello, sign in Returns 10|\\n|Account Lists Orders|EN - Account Lists Orders|\\n|Black Friday Deals Medical Care Best Sellers Prime|Amazon Basics Groceries New Releases Music Customer Service Amazon Home Black Friday Week shop now|\\n|Home Store Free with ads Categories My Stuff|Settings Getting Started Help|\\n|prime video Home Movies TV shows Sports Live TV 88 Subscriptions|Join Prime|\\n|1-16 of 423 results for \"psych\"|Sort By: Featured|\\n|Department Results|Any Department Movies TV|\\n|psych Psych Season|Prime Video|\\n|Customer Reviews|Action: Click \\'Psych Season 1\\'|\\n|Actor|For $34.99 to buy season et al|\\n|James Roday|Or $0.00 with Prime membership|\\n|Dule Hill|Psych Season 3|\\n|Price|psych 2008 CC|\\n|Free|$0 to $1|\\n|$1 to $3|Prime Video|\\n|$3 to $5|Starring: James Roday, Dule Hill|\\n|$5 to $10|and Timothy Omundson|\\n|$10 to $15|For $34.99 to buy season|\\n|$15 to $20|Or $0.00 with Prime membership|\\n|Condition|New|\\n|Format|peych|\\n|Psych Season|2013 CC|\\n|Genre|Prime Video 1,326|\\n|Action Adventure|For $2.99 to buy episode|\\n|Animation|Comedy|\\n|Documentary|Drama|\\n|Fantasy|Horror|\\n|International|Music Videos Concerts|\\n|Romance|Science Fiction|\\n|See more|Watch now: Free with ads|\\n|Or $0.00 with Prime membership|Directed by: Adam Davidson, Alex Zakrzewski, Sarah Pia Anderson et al.|\\n|Video Length|Up to Minutes|\\n|11 to 20 Minutes|21 to 30 Minutes|\\n|31 to 60 Minutes|61 to 90 Minutes|\\n|More Than 90 Minutes|Decade|\\n|2020 Newer|2010-2019|\\n|Figure 7: Example of a multimodal agent.| |',\n",
              "    'images': [{'name': 'img_p39_1.png',\n",
              "      'height': 5536,\n",
              "      'width': 4817,\n",
              "      'x': 72,\n",
              "      'y': 118.15459999999996,\n",
              "      'original_width': 4817,\n",
              "      'original_height': 5536,\n",
              "      'ocr': [{'x': 211,\n",
              "        'y': 67,\n",
              "        'w': 400,\n",
              "        'h': 110,\n",
              "        'confidence': '0.917916340319179',\n",
              "        'text': 'User input:'},\n",
              "       {'x': 658,\n",
              "        'y': 53,\n",
              "        'w': 1013,\n",
              "        'h': 135,\n",
              "        'confidence': '0.7173904635202686',\n",
              "        'text': '\"Search for Psych and'},\n",
              "       {'x': 1865,\n",
              "        'y': 44,\n",
              "        'w': 824,\n",
              "        'h': 146,\n",
              "        'confidence': '0.9960798557273275',\n",
              "        'text': 'the first episode\"'},\n",
              "       {'x': 144,\n",
              "        'y': 360,\n",
              "        'w': 438,\n",
              "        'h': 92,\n",
              "        'confidence': '0.9254418084415431',\n",
              "        'text': 'Agent step 1'},\n",
              "       {'x': 2609,\n",
              "        'y': 360,\n",
              "        'w': 438,\n",
              "        'h': 92,\n",
              "        'confidence': '0.7972097171610071',\n",
              "        'text': 'Agent step 2'},\n",
              "       {'x': 15,\n",
              "        'y': 508,\n",
              "        'w': 198,\n",
              "        'h': 77,\n",
              "        'confidence': '0.9997983194279729',\n",
              "        'text': 'amazon'},\n",
              "       {'x': 235,\n",
              "        'y': 534,\n",
              "        'w': 134,\n",
              "        'h': 25,\n",
              "        'confidence': '0.9981976125871117',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 413,\n",
              "        'y': 529,\n",
              "        'w': 203,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8427519742649034',\n",
              "        'text': 'Search Amazon'},\n",
              "       {'x': 1870,\n",
              "        'y': 517,\n",
              "        'w': 138,\n",
              "        'h': 29,\n",
              "        'confidence': '0.8995546589959356',\n",
              "        'text': 'Hello, sign in'},\n",
              "       {'x': 2113,\n",
              "        'y': 522,\n",
              "        'w': 80,\n",
              "        'h': 19,\n",
              "        'confidence': '0.967562371594528',\n",
              "        'text': 'Retums'},\n",
              "       {'x': 2252,\n",
              "        'y': 509,\n",
              "        'w': 84,\n",
              "        'h': 62,\n",
              "        'confidence': '0.7054501210846282',\n",
              "        'text': '10'},\n",
              "       {'x': 2477,\n",
              "        'y': 509,\n",
              "        'w': 196,\n",
              "        'h': 66,\n",
              "        'confidence': '0.9998917146325323',\n",
              "        'text': 'amazon'},\n",
              "       {'x': 2701,\n",
              "        'y': 534,\n",
              "        'w': 129,\n",
              "        'h': 25,\n",
              "        'confidence': '0.9951375603568849',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 2878,\n",
              "        'y': 529,\n",
              "        'w': 77,\n",
              "        'h': 30,\n",
              "        'confidence': '0.6727589206489252',\n",
              "        'text': 'Psych'},\n",
              "       {'x': 4331,\n",
              "        'y': 517,\n",
              "        'w': 143,\n",
              "        'h': 29,\n",
              "        'confidence': '0.8490529989198822',\n",
              "        'text': 'Hello, sign in'},\n",
              "       {'x': 4574,\n",
              "        'y': 522,\n",
              "        'w': 80,\n",
              "        'h': 19,\n",
              "        'confidence': '0.6403322268615618',\n",
              "        'text': 'Retums'},\n",
              "       {'x': 4714,\n",
              "        'y': 506,\n",
              "        'w': 82,\n",
              "        'h': 55,\n",
              "        'confidence': '0.37215836309626016',\n",
              "        'text': '10'},\n",
              "       {'x': 1787,\n",
              "        'y': 537,\n",
              "        'w': 45,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9998894084607064',\n",
              "        'text': 'EN'},\n",
              "       {'x': 1870,\n",
              "        'y': 542,\n",
              "        'w': 199,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7941518116105512',\n",
              "        'text': 'Account & Lists'},\n",
              "       {'x': 2138,\n",
              "        'y': 542,\n",
              "        'w': 91,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9972698197387989',\n",
              "        'text': 'Orders'},\n",
              "       {'x': 4248,\n",
              "        'y': 537,\n",
              "        'w': 62,\n",
              "        'h': 36,\n",
              "        'confidence': '0.8833368416373905',\n",
              "        'text': 'EN -'},\n",
              "       {'x': 4335,\n",
              "        'y': 542,\n",
              "        'w': 195,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9237044086334374',\n",
              "        'text': 'Account & Lists'},\n",
              "       {'x': 4573,\n",
              "        'y': 542,\n",
              "        'w': 121,\n",
              "        'h': 30,\n",
              "        'confidence': '0.729798099310388',\n",
              "        'text': '& Orders'},\n",
              "       {'x': 62,\n",
              "        'y': 625,\n",
              "        'w': 43,\n",
              "        'h': 25,\n",
              "        'confidence': '0.8894435063933288',\n",
              "        'text': 'All'},\n",
              "       {'x': 127,\n",
              "        'y': 625,\n",
              "        'w': 233,\n",
              "        'h': 30,\n",
              "        'confidence': '0.5409348803293581',\n",
              "        'text': 'Black Friday Deals'},\n",
              "       {'x': 1351,\n",
              "        'y': 625,\n",
              "        'w': 60,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9976315498352051',\n",
              "        'text': 'ases'},\n",
              "       {'x': 1433,\n",
              "        'y': 625,\n",
              "        'w': 78,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999880581002243',\n",
              "        'text': 'Music'},\n",
              "       {'x': 1533,\n",
              "        'y': 625,\n",
              "        'w': 216,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9998624531978386',\n",
              "        'text': 'Customer Service'},\n",
              "       {'x': 1771,\n",
              "        'y': 625,\n",
              "        'w': 181,\n",
              "        'h': 30,\n",
              "        'confidence': '0.735600427666857',\n",
              "        'text': 'Amazon Home'},\n",
              "       {'x': 1986,\n",
              "        'y': 615,\n",
              "        'w': 235,\n",
              "        'h': 42,\n",
              "        'confidence': '0.9882184565070454',\n",
              "        'text': 'Black Friday Week='},\n",
              "       {'x': 2281,\n",
              "        'y': 625,\n",
              "        'w': 60,\n",
              "        'h': 25,\n",
              "        'confidence': '0.9801376369645807',\n",
              "        'text': 'now'},\n",
              "       {'x': 2523,\n",
              "        'y': 625,\n",
              "        'w': 48,\n",
              "        'h': 25,\n",
              "        'confidence': '0.6126816408990812',\n",
              "        'text': 'All'},\n",
              "       {'x': 2593,\n",
              "        'y': 625,\n",
              "        'w': 228,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7774366098608428',\n",
              "        'text': 'Black Friday Dcals'},\n",
              "       {'x': 2923,\n",
              "        'y': 596,\n",
              "        'w': 91,\n",
              "        'h': 42,\n",
              "        'confidence': '0.9995584583844068',\n",
              "        'text': 'psych'},\n",
              "       {'x': 3198,\n",
              "        'y': 573,\n",
              "        'w': 384,\n",
              "        'h': 77,\n",
              "        'confidence': '0.7123096345604704',\n",
              "        'text': 'Action: Click'},\n",
              "       {'x': 3609,\n",
              "        'y': 573,\n",
              "        'w': 211,\n",
              "        'h': 81,\n",
              "        'confidence': '0.9887209128477316',\n",
              "        'text': '\\\\\\'Psych\"'},\n",
              "       {'x': 4232,\n",
              "        'y': 625,\n",
              "        'w': 181,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9813658198515635',\n",
              "        'text': 'Amazon Home'},\n",
              "       {'x': 4451,\n",
              "        'y': 616,\n",
              "        'w': 231,\n",
              "        'h': 41,\n",
              "        'confidence': '0.9516638402020058',\n",
              "        'text': 'Black Friday Week'},\n",
              "       {'x': 4742,\n",
              "        'y': 625,\n",
              "        'w': 60,\n",
              "        'h': 25,\n",
              "        'confidence': '0.9153761731179411',\n",
              "        'text': 'now'},\n",
              "       {'x': 949,\n",
              "        'y': 642,\n",
              "        'w': 30,\n",
              "        'h': 30,\n",
              "        'confidence': '0.21406213480730973',\n",
              "        'text': '1)'},\n",
              "       {'x': 555,\n",
              "        'y': 637,\n",
              "        'w': 386,\n",
              "        'h': 87,\n",
              "        'confidence': '0.9997844124868857',\n",
              "        'text': 'Action: Type'},\n",
              "       {'x': 968,\n",
              "        'y': 631,\n",
              "        'w': 217,\n",
              "        'h': 93,\n",
              "        'confidence': '0.8367244481629066',\n",
              "        'text': '\\\\\\'Psych\"'},\n",
              "       {'x': 2925,\n",
              "        'y': 663,\n",
              "        'w': 105,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9998207558702209',\n",
              "        'text': 'psycho'},\n",
              "       {'x': 83,\n",
              "        'y': 710,\n",
              "        'w': 157,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9917668229322705',\n",
              "        'text': 'prime video'},\n",
              "       {'x': 318,\n",
              "        'y': 711,\n",
              "        'w': 81,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9615869522094727',\n",
              "        'text': 'Home'},\n",
              "       {'x': 2112,\n",
              "        'y': 711,\n",
              "        'w': 143,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9978273477166106',\n",
              "        'text': 'Join Prime'},\n",
              "       {'x': 2544,\n",
              "        'y': 710,\n",
              "        'w': 162,\n",
              "        'h': 36,\n",
              "        'confidence': '0.7017326399948782',\n",
              "        'text': 'prime video'},\n",
              "       {'x': 2778,\n",
              "        'y': 711,\n",
              "        'w': 87,\n",
              "        'h': 30,\n",
              "        'confidence': '0.4211942568797071',\n",
              "        'text': 'Home '},\n",
              "       {'x': 4578,\n",
              "        'y': 711,\n",
              "        'w': 142,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9930809543215755',\n",
              "        'text': 'Join Prime'},\n",
              "       {'x': 2925,\n",
              "        'y': 723,\n",
              "        'w': 313,\n",
              "        'h': 41,\n",
              "        'confidence': '0.9951432366839401',\n",
              "        'text': 'psychological thrillers'},\n",
              "       {'x': 2922,\n",
              "        'y': 784,\n",
              "        'w': 115,\n",
              "        'h': 49,\n",
              "        'confidence': '0.9996173533669184',\n",
              "        'text': 'psychic'},\n",
              "       {'x': 2922,\n",
              "        'y': 849,\n",
              "        'w': 178,\n",
              "        'h': 49,\n",
              "        'confidence': '0.7994014658019796',\n",
              "        'text': 'psychic kids'},\n",
              "       {'x': 75,\n",
              "        'y': 921,\n",
              "        'w': 130,\n",
              "        'h': 62,\n",
              "        'confidence': '0.9999660401783838',\n",
              "        'text': 'prime'},\n",
              "       {'x': 2923,\n",
              "        'y': 915,\n",
              "        'w': 234,\n",
              "        'h': 45,\n",
              "        'confidence': '0.8201774631623797',\n",
              "        'text': 'psych the movie'},\n",
              "       {'x': 2923,\n",
              "        'y': 977,\n",
              "        'w': 169,\n",
              "        'h': 48,\n",
              "        'confidence': '0.8603640747466503',\n",
              "        'text': 'psychology'},\n",
              "       {'x': 116,\n",
              "        'y': 1041,\n",
              "        'w': 390,\n",
              "        'h': 66,\n",
              "        'confidence': '0.8467535318909727',\n",
              "        'text': 'amazonwish list'},\n",
              "       {'x': 2925,\n",
              "        'y': 1047,\n",
              "        'w': 295,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9672519231614006',\n",
              "        'text': 'psychic investigators'},\n",
              "       {'x': 2925,\n",
              "        'y': 1112,\n",
              "        'w': 174,\n",
              "        'h': 37,\n",
              "        'confidence': '0.999543995190507',\n",
              "        'text': 'psycho-pass'},\n",
              "       {'x': 79,\n",
              "        'y': 1096,\n",
              "        'w': 463,\n",
              "        'h': 134,\n",
              "        'confidence': '0.9271827762124761',\n",
              "        'text': 'GAMES'},\n",
              "       {'x': 2540,\n",
              "        'y': 1104,\n",
              "        'w': 308,\n",
              "        'h': 88,\n",
              "        'confidence': '0.9982188715022721',\n",
              "        'text': 'YELLO'},\n",
              "       {'x': 2925,\n",
              "        'y': 1177,\n",
              "        'w': 166,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9999072998734861',\n",
              "        'text': 'psychopath'},\n",
              "       {'x': 2548,\n",
              "        'y': 1199,\n",
              "        'w': 305,\n",
              "        'h': 36,\n",
              "        'confidence': '0.4939987933599963',\n",
              "        'text': 'GOLDEN GLOBE@ winner'},\n",
              "       {'x': 195,\n",
              "        'y': 1290,\n",
              "        'w': 128,\n",
              "        'h': 40,\n",
              "        'confidence': '0.8988524286352164',\n",
              "        'text': 'Episode'},\n",
              "       {'x': 2648,\n",
              "        'y': 1290,\n",
              "        'w': 127,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999432278155246',\n",
              "        'text': 'Episode'},\n",
              "       {'x': 191,\n",
              "        'y': 1333,\n",
              "        'w': 183,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9899806313257733',\n",
              "        'text': 'Watch now'},\n",
              "       {'x': 2574,\n",
              "        'y': 1324,\n",
              "        'w': 158,\n",
              "        'h': 45,\n",
              "        'confidence': '0.7819594813342599',\n",
              "        'text': 'HD $2.99'},\n",
              "       {'x': 126,\n",
              "        'y': 1404,\n",
              "        'w': 223,\n",
              "        'h': 40,\n",
              "        'confidence': '0.998806670609146',\n",
              "        'text': 'Freevee (with ads)'},\n",
              "       {'x': 2587,\n",
              "        'y': 1406,\n",
              "        'w': 201,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9891956004758735',\n",
              "        'text': 'Available to buy'},\n",
              "       {'x': 4171,\n",
              "        'y': 1407,\n",
              "        'w': 91,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8352621589437896',\n",
              "        'text': 'TV-MA'},\n",
              "       {'x': 83,\n",
              "        'y': 1545,\n",
              "        'w': 253,\n",
              "        'h': 40,\n",
              "        'confidence': '0.8385811474590389',\n",
              "        'text': 'Popular movies'},\n",
              "       {'x': 368,\n",
              "        'y': 1541,\n",
              "        'w': 158,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9986442499686763',\n",
              "        'text': 'See more'},\n",
              "       {'x': 2548,\n",
              "        'y': 1545,\n",
              "        'w': 248,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999680439525179',\n",
              "        'text': 'Popular movies'},\n",
              "       {'x': 2829,\n",
              "        'y': 1545,\n",
              "        'w': 158,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9150156555142158',\n",
              "        'text': 'See more'},\n",
              "       {'x': 585,\n",
              "        'y': 1744,\n",
              "        'w': 71,\n",
              "        'h': 40,\n",
              "        'confidence': '0.38223549336090634',\n",
              "        'text': 'IN'},\n",
              "       {'x': 792,\n",
              "        'y': 1739,\n",
              "        'w': 41,\n",
              "        'h': 41,\n",
              "        'confidence': '0.8880757040141276',\n",
              "        'text': 'T'},\n",
              "       {'x': 1132,\n",
              "        'y': 1682,\n",
              "        'w': 234,\n",
              "        'h': 91,\n",
              "        'confidence': '0.48674205939742043',\n",
              "        'text': 'BROTHERS '},\n",
              "       {'x': 2629,\n",
              "        'y': 1708,\n",
              "        'w': 164,\n",
              "        'h': 51,\n",
              "        'confidence': '0.17719991165716037',\n",
              "        'text': 'APCCALTO'},\n",
              "       {'x': 3066,\n",
              "        'y': 1734,\n",
              "        'w': 233,\n",
              "        'h': 51,\n",
              "        'confidence': '0.19991481737953512',\n",
              "        'text': 'NFANIT'},\n",
              "       {'x': 3593,\n",
              "        'y': 1682,\n",
              "        'w': 234,\n",
              "        'h': 92,\n",
              "        'confidence': '0.7475934680822576',\n",
              "        'text': 'BROTHERS|'},\n",
              "       {'x': 846,\n",
              "        'y': 1780,\n",
              "        'w': 63,\n",
              "        'h': 20,\n",
              "        'confidence': '0.1731092374075883',\n",
              "        'text': 'prmte'},\n",
              "       {'x': 1286,\n",
              "        'y': 1779,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9970902099118429',\n",
              "        'text': 'prime'},\n",
              "       {'x': 1728,\n",
              "        'y': 1779,\n",
              "        'w': 64,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9985151302025602',\n",
              "        'text': 'prime'},\n",
              "       {'x': 3747,\n",
              "        'y': 1779,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9867869357255199',\n",
              "        'text': 'prime'},\n",
              "       {'x': 4188,\n",
              "        'y': 1779,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9871968984726133',\n",
              "        'text': 'prime'},\n",
              "       {'x': 144,\n",
              "        'y': 1904,\n",
              "        'w': 222,\n",
              "        'h': 36,\n",
              "        'confidence': '0.6950236292087872',\n",
              "        'text': '~rated movies'},\n",
              "       {'x': 399,\n",
              "        'y': 1900,\n",
              "        'w': 157,\n",
              "        'h': 40,\n",
              "        'confidence': '0.882151743133577',\n",
              "        'text': 'See more'},\n",
              "       {'x': 2604,\n",
              "        'y': 1904,\n",
              "        'w': 227,\n",
              "        'h': 36,\n",
              "        'confidence': '0.7445332049797466',\n",
              "        'text': '~rated movies'},\n",
              "       {'x': 2860,\n",
              "        'y': 1900,\n",
              "        'w': 157,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9997985261523906',\n",
              "        'text': 'See more'},\n",
              "       {'x': 998,\n",
              "        'y': 1971,\n",
              "        'w': 83,\n",
              "        'h': 66,\n",
              "        'confidence': '0.3739930204056462',\n",
              "        'text': 'Ihe'},\n",
              "       {'x': 3460,\n",
              "        'y': 1976,\n",
              "        'w': 77,\n",
              "        'h': 56,\n",
              "        'confidence': '0.6595128084053345',\n",
              "        'text': 'Iie'},\n",
              "       {'x': 101,\n",
              "        'y': 2022,\n",
              "        'w': 26,\n",
              "        'h': 38,\n",
              "        'confidence': '0.9747805501358222',\n",
              "        'text': 'R'},\n",
              "       {'x': 681,\n",
              "        'y': 1996,\n",
              "        'w': 116,\n",
              "        'h': 77,\n",
              "        'confidence': '0.12459114791059633',\n",
              "        'text': '4'},\n",
              "       {'x': 2557,\n",
              "        'y': 2016,\n",
              "        'w': 53,\n",
              "        'h': 45,\n",
              "        'confidence': '0.9261868900162286',\n",
              "        'text': 'Re'},\n",
              "       {'x': 3459,\n",
              "        'y': 2027,\n",
              "        'w': 179,\n",
              "        'h': 66,\n",
              "        'confidence': '0.1627831894621377',\n",
              "        'text': 'Godeler'},\n",
              "       {'x': 1923,\n",
              "        'y': 2083,\n",
              "        'w': 54,\n",
              "        'h': 19,\n",
              "        'confidence': '0.10930342113813432',\n",
              "        'text': 'F78 E'},\n",
              "       {'x': 4383,\n",
              "        'y': 2082,\n",
              "        'w': 60,\n",
              "        'h': 26,\n",
              "        'confidence': '0.15148858726024628',\n",
              "        'text': 'at8e'},\n",
              "       {'x': 844,\n",
              "        'y': 2131,\n",
              "        'w': 67,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9977921682164943',\n",
              "        'text': 'prime'},\n",
              "       {'x': 1286,\n",
              "        'y': 2134,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9934718228293573',\n",
              "        'text': 'prime'},\n",
              "       {'x': 1728,\n",
              "        'y': 2134,\n",
              "        'w': 64,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9989846118089108',\n",
              "        'text': 'prime'},\n",
              "       {'x': 3310,\n",
              "        'y': 2134,\n",
              "        'w': 65,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8773624812166075',\n",
              "        'text': 'prime'},\n",
              "       {'x': 3746,\n",
              "        'y': 2130,\n",
              "        'w': 67,\n",
              "        'h': 31,\n",
              "        'confidence': '0.2620499518470425',\n",
              "        'text': '~prime'},\n",
              "       {'x': 4188,\n",
              "        'y': 2134,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.949982069103508',\n",
              "        'text': 'prime'},\n",
              "       {'x': 83,\n",
              "        'y': 2254,\n",
              "        'w': 534,\n",
              "        'h': 45,\n",
              "        'confidence': '0.6732333477203712',\n",
              "        'text': 'Featured Originals and Exclusives'},\n",
              "       {'x': 2544,\n",
              "        'y': 2254,\n",
              "        'w': 534,\n",
              "        'h': 45,\n",
              "        'confidence': '0.848370909667181',\n",
              "        'text': 'Featured Originals and Exclusives'},\n",
              "       {'x': 1144,\n",
              "        'y': 2428,\n",
              "        'w': 138,\n",
              "        'h': 30,\n",
              "        'confidence': '0.2673903358681503',\n",
              "        'text': 'Neeson'},\n",
              "       {'x': 3609,\n",
              "        'y': 2428,\n",
              "        'w': 134,\n",
              "        'h': 30,\n",
              "        'confidence': '0.17985100582766964',\n",
              "        'text': 'Neeson'},\n",
              "       {'x': 981,\n",
              "        'y': 2451,\n",
              "        'w': 369,\n",
              "        'h': 62,\n",
              "        'confidence': '0.4454750748562714',\n",
              "        'text': 'M E M & R Y'},\n",
              "       {'x': 3442,\n",
              "        'y': 2451,\n",
              "        'w': 369,\n",
              "        'h': 62,\n",
              "        'confidence': '0.4195348774355594',\n",
              "        'text': 'M EM & R Y'},\n",
              "       {'x': 122,\n",
              "        'y': 2790,\n",
              "        'w': 326,\n",
              "        'h': 45,\n",
              "        'confidence': '0.27078450149156',\n",
              "        'text': 'PIMPINERD'},\n",
              "       {'x': 575,\n",
              "        'y': 2738,\n",
              "        'w': 302,\n",
              "        'h': 98,\n",
              "        'confidence': '0.8603183031082153',\n",
              "        'text': 'FRoM'},\n",
              "       {'x': 1517,\n",
              "        'y': 2745,\n",
              "        'w': 179,\n",
              "        'h': 66,\n",
              "        'confidence': '0.9997064056458465',\n",
              "        'text': 'RINGS'},\n",
              "       {'x': 1858,\n",
              "        'y': 2732,\n",
              "        'w': 379,\n",
              "        'h': 123,\n",
              "        'confidence': '0.7889320393138717',\n",
              "        'text': 'PFACHEP'},\n",
              "       {'x': 2583,\n",
              "        'y': 2790,\n",
              "        'w': 326,\n",
              "        'h': 45,\n",
              "        'confidence': '0.31063726699233973',\n",
              "        'text': 'PIMPINERD'},\n",
              "       {'x': 3036,\n",
              "        'y': 2738,\n",
              "        'w': 302,\n",
              "        'h': 98,\n",
              "        'confidence': '0.9141307473182678',\n",
              "        'text': 'FRoM'},\n",
              "       {'x': 3980,\n",
              "        'y': 2743,\n",
              "        'w': 179,\n",
              "        'h': 88,\n",
              "        'confidence': '0.9990941804283194',\n",
              "        'text': 'RINGS'},\n",
              "       {'x': 4324,\n",
              "        'y': 2737,\n",
              "        'w': 373,\n",
              "        'h': 117,\n",
              "        'confidence': '0.5193644770889223',\n",
              "        'text': 'PFACHEP'},\n",
              "       {'x': 160,\n",
              "        'y': 3054,\n",
              "        'w': 436,\n",
              "        'h': 98,\n",
              "        'confidence': '0.9995856884278198',\n",
              "        'text': 'Agent step 3'},\n",
              "       {'x': 2621,\n",
              "        'y': 3054,\n",
              "        'w': 440,\n",
              "        'h': 98,\n",
              "        'confidence': '0.9653420440883207',\n",
              "        'text': 'Agent step 4'},\n",
              "       {'x': 24,\n",
              "        'y': 3207,\n",
              "        'w': 198,\n",
              "        'h': 73,\n",
              "        'confidence': '0.9996660345894655',\n",
              "        'text': 'amazon'},\n",
              "       {'x': 248,\n",
              "        'y': 3233,\n",
              "        'w': 130,\n",
              "        'h': 25,\n",
              "        'confidence': '0.9992945627826011',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 426,\n",
              "        'y': 3228,\n",
              "        'w': 77,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9995702409457151',\n",
              "        'text': 'psych'},\n",
              "       {'x': 1879,\n",
              "        'y': 3215,\n",
              "        'w': 142,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8063767119203064',\n",
              "        'text': 'Hello, sign in'},\n",
              "       {'x': 2122,\n",
              "        'y': 3221,\n",
              "        'w': 80,\n",
              "        'h': 19,\n",
              "        'confidence': '0.8818010373665427',\n",
              "        'text': 'Retums'},\n",
              "       {'x': 2261,\n",
              "        'y': 3204,\n",
              "        'w': 83,\n",
              "        'h': 62,\n",
              "        'confidence': '0.45654509350812367',\n",
              "        'text': '10,'},\n",
              "       {'x': 2477,\n",
              "        'y': 3208,\n",
              "        'w': 196,\n",
              "        'h': 66,\n",
              "        'confidence': '0.9998943913812494',\n",
              "        'text': 'amazon'},\n",
              "       {'x': 2701,\n",
              "        'y': 3233,\n",
              "        'w': 129,\n",
              "        'h': 25,\n",
              "        'confidence': '0.9904803909534744',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 2878,\n",
              "        'y': 3228,\n",
              "        'w': 77,\n",
              "        'h': 30,\n",
              "        'confidence': '0.6786084458704948',\n",
              "        'text': 'psych'},\n",
              "       {'x': 4331,\n",
              "        'y': 3215,\n",
              "        'w': 143,\n",
              "        'h': 30,\n",
              "        'confidence': '0.765797291472659',\n",
              "        'text': 'Hello, sign in'},\n",
              "       {'x': 4573,\n",
              "        'y': 3220,\n",
              "        'w': 87,\n",
              "        'h': 25,\n",
              "        'confidence': '0.7068215433704742',\n",
              "        'text': 'Retums'},\n",
              "       {'x': 4713,\n",
              "        'y': 3204,\n",
              "        'w': 79,\n",
              "        'h': 62,\n",
              "        'confidence': '0.9968554407069007',\n",
              "        'text': '10'},\n",
              "       {'x': 1796,\n",
              "        'y': 3236,\n",
              "        'w': 62,\n",
              "        'h': 36,\n",
              "        'confidence': '0.7061376999481564',\n",
              "        'text': 'EN -'},\n",
              "       {'x': 1884,\n",
              "        'y': 3247,\n",
              "        'w': 102,\n",
              "        'h': 19,\n",
              "        'confidence': '0.5740531352993434',\n",
              "        'text': 'Accdunt'},\n",
              "       {'x': 2013,\n",
              "        'y': 3241,\n",
              "        'w': 65,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9997830173201773',\n",
              "        'text': 'Lists'},\n",
              "       {'x': 2147,\n",
              "        'y': 3241,\n",
              "        'w': 95,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9960772184229003',\n",
              "        'text': 'Orders'},\n",
              "       {'x': 4248,\n",
              "        'y': 3236,\n",
              "        'w': 62,\n",
              "        'h': 36,\n",
              "        'confidence': '0.8023887011671593',\n",
              "        'text': 'EN -'},\n",
              "       {'x': 4335,\n",
              "        'y': 3246,\n",
              "        'w': 108,\n",
              "        'h': 25,\n",
              "        'confidence': '0.9998392328926677',\n",
              "        'text': 'Account'},\n",
              "       {'x': 4465,\n",
              "        'y': 3241,\n",
              "        'w': 65,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9963450179021607',\n",
              "        'text': 'Lists'},\n",
              "       {'x': 4599,\n",
              "        'y': 3241,\n",
              "        'w': 95,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9978304804360096',\n",
              "        'text': 'Orders'},\n",
              "       {'x': 31,\n",
              "        'y': 3309,\n",
              "        'w': 88,\n",
              "        'h': 45,\n",
              "        'confidence': '0.26610803320015247',\n",
              "        'text': 'ZAl'},\n",
              "       {'x': 139,\n",
              "        'y': 3318,\n",
              "        'w': 231,\n",
              "        'h': 36,\n",
              "        'confidence': '0.8676475366326386',\n",
              "        'text': 'Black Friday Dcals'},\n",
              "       {'x': 390,\n",
              "        'y': 3318,\n",
              "        'w': 162,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9993112787009898',\n",
              "        'text': 'Medical Care'},\n",
              "       {'x': 593,\n",
              "        'y': 3318,\n",
              "        'w': 149,\n",
              "        'h': 36,\n",
              "        'confidence': '0.8836507467321031',\n",
              "        'text': 'Best Sellers'},\n",
              "       {'x': 763,\n",
              "        'y': 3323,\n",
              "        'w': 78,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9980259991648822',\n",
              "        'text': 'Prime'},\n",
              "       {'x': 886,\n",
              "        'y': 3322,\n",
              "        'w': 185,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9978911920596507',\n",
              "        'text': 'Amazon Basics'},\n",
              "       {'x': 1092,\n",
              "        'y': 3323,\n",
              "        'w': 116,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9603345200832023',\n",
              "        'text': 'Groceries'},\n",
              "       {'x': 1252,\n",
              "        'y': 3323,\n",
              "        'w': 172,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9987198815089615',\n",
              "        'text': 'New Releases'},\n",
              "       {'x': 1442,\n",
              "        'y': 3323,\n",
              "        'w': 78,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999940823642687',\n",
              "        'text': 'Music'},\n",
              "       {'x': 1542,\n",
              "        'y': 3323,\n",
              "        'w': 215,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9934419094029704',\n",
              "        'text': 'Customer Service'},\n",
              "       {'x': 1779,\n",
              "        'y': 3323,\n",
              "        'w': 182,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9963578755774053',\n",
              "        'text': 'Amazon Home'},\n",
              "       {'x': 1999,\n",
              "        'y': 3314,\n",
              "        'w': 352,\n",
              "        'h': 42,\n",
              "        'confidence': '0.7340397826886083',\n",
              "        'text': 'Black Friday Week shop now'},\n",
              "       {'x': 2483,\n",
              "        'y': 3309,\n",
              "        'w': 89,\n",
              "        'h': 45,\n",
              "        'confidence': '0.1351856142282486',\n",
              "        'text': 'ZAUL'},\n",
              "       {'x': 2593,\n",
              "        'y': 3323,\n",
              "        'w': 228,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7175983205750327',\n",
              "        'text': 'Black Friday Dcals'},\n",
              "       {'x': 2843,\n",
              "        'y': 3323,\n",
              "        'w': 160,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7168807469486497',\n",
              "        'text': 'Medical Care'},\n",
              "       {'x': 3047,\n",
              "        'y': 3323,\n",
              "        'w': 146,\n",
              "        'h': 30,\n",
              "        'confidence': '0.998392015251538',\n",
              "        'text': 'Best Sellers'},\n",
              "       {'x': 3213,\n",
              "        'y': 3319,\n",
              "        'w': 83,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9963950201220982',\n",
              "        'text': 'Prime'},\n",
              "       {'x': 3338,\n",
              "        'y': 3321,\n",
              "        'w': 185,\n",
              "        'h': 37,\n",
              "        'confidence': '0.8912872582901439',\n",
              "        'text': 'Amazon Basics'},\n",
              "       {'x': 3544,\n",
              "        'y': 3323,\n",
              "        'w': 116,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9998148991439115',\n",
              "        'text': 'Groceries'},\n",
              "       {'x': 3704,\n",
              "        'y': 3323,\n",
              "        'w': 173,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9471368496395136',\n",
              "        'text': 'New Releases'},\n",
              "       {'x': 3899,\n",
              "        'y': 3323,\n",
              "        'w': 77,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9737345790254467',\n",
              "        'text': 'Music'},\n",
              "       {'x': 3998,\n",
              "        'y': 3323,\n",
              "        'w': 212,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9956111847127106',\n",
              "        'text': 'Customer Service'},\n",
              "       {'x': 4232,\n",
              "        'y': 3323,\n",
              "        'w': 181,\n",
              "        'h': 30,\n",
              "        'confidence': '0.5957980788385012',\n",
              "        'text': 'Amazon Home'},\n",
              "       {'x': 4451,\n",
              "        'y': 3314,\n",
              "        'w': 231,\n",
              "        'h': 42,\n",
              "        'confidence': '0.9959123470892208',\n",
              "        'text': 'Black Friday Week'},\n",
              "       {'x': 4742,\n",
              "        'y': 3323,\n",
              "        'w': 60,\n",
              "        'h': 26,\n",
              "        'confidence': '0.5967769405863255',\n",
              "        'text': 'now'},\n",
              "       {'x': 62,\n",
              "        'y': 3388,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.999903678894043',\n",
              "        'text': 'Home'},\n",
              "       {'x': 163,\n",
              "        'y': 3389,\n",
              "        'w': 58,\n",
              "        'h': 20,\n",
              "        'confidence': '0.9994680360107827',\n",
              "        'text': 'Store'},\n",
              "       {'x': 257,\n",
              "        'y': 3388,\n",
              "        'w': 142,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999247470031682',\n",
              "        'text': 'Free with ads'},\n",
              "       {'x': 430,\n",
              "        'y': 3388,\n",
              "        'w': 116,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9094991597863141',\n",
              "        'text': 'Categories'},\n",
              "       {'x': 573,\n",
              "        'y': 3388,\n",
              "        'w': 95,\n",
              "        'h': 30,\n",
              "        'confidence': '0.805773094284618',\n",
              "        'text': 'My Stuff'},\n",
              "       {'x': 1957,\n",
              "        'y': 3388,\n",
              "        'w': 95,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9992413217937008',\n",
              "        'text': 'Settings'},\n",
              "       {'x': 2082,\n",
              "        'y': 3388,\n",
              "        'w': 164,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9843695935267467',\n",
              "        'text': 'Getting Started'},\n",
              "       {'x': 2277,\n",
              "        'y': 3388,\n",
              "        'w': 56,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9996105432510376',\n",
              "        'text': 'Help'},\n",
              "       {'x': 2544,\n",
              "        'y': 3409,\n",
              "        'w': 162,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9967215546035639',\n",
              "        'text': 'prime video'},\n",
              "       {'x': 2778,\n",
              "        'y': 3414,\n",
              "        'w': 87,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9994767308235168',\n",
              "        'text': 'Home'},\n",
              "       {'x': 2921,\n",
              "        'y': 3414,\n",
              "        'w': 95,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9811283838337265',\n",
              "        'text': 'Movies'},\n",
              "       {'x': 3077,\n",
              "        'y': 3410,\n",
              "        'w': 125,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9246114292590658',\n",
              "        'text': 'TV shows'},\n",
              "       {'x': 3258,\n",
              "        'y': 3409,\n",
              "        'w': 92,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9997655162696993',\n",
              "        'text': 'Sports'},\n",
              "       {'x': 3406,\n",
              "        'y': 3410,\n",
              "        'w': 103,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8474373564599108',\n",
              "        'text': 'Live TV'},\n",
              "       {'x': 3617,\n",
              "        'y': 3400,\n",
              "        'w': 226,\n",
              "        'h': 45,\n",
              "        'confidence': '0.8174373558377389',\n",
              "        'text': '88 Subscriptions'},\n",
              "       {'x': 4578,\n",
              "        'y': 3410,\n",
              "        'w': 142,\n",
              "        'h': 30,\n",
              "        'confidence': '0.99553294392977',\n",
              "        'text': 'Join Prime'},\n",
              "       {'x': 278,\n",
              "        'y': 3452,\n",
              "        'w': 360,\n",
              "        'h': 36,\n",
              "        'confidence': '0.7047169073709755',\n",
              "        'text': '1-16 of 423 results for \"psych\\\\\\''},\n",
              "       {'x': 1889,\n",
              "        'y': 3459,\n",
              "        'w': 75,\n",
              "        'h': 19,\n",
              "        'confidence': '0.11005944172241902',\n",
              "        'text': 'Soit Dy:'},\n",
              "       {'x': 1971,\n",
              "        'y': 3459,\n",
              "        'w': 84,\n",
              "        'h': 19,\n",
              "        'confidence': '0.29499002159361254',\n",
              "        'text': 'Fejined'},\n",
              "       {'x': 287,\n",
              "        'y': 3548,\n",
              "        'w': 156,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9990852999023038',\n",
              "        'text': 'Department'},\n",
              "       {'x': 740,\n",
              "        'y': 3547,\n",
              "        'w': 136,\n",
              "        'h': 45,\n",
              "        'confidence': '0.9999943228283826',\n",
              "        'text': 'Results'},\n",
              "       {'x': 305,\n",
              "        'y': 3592,\n",
              "        'w': 203,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9998931275118514',\n",
              "        'text': 'Any Department'},\n",
              "       {'x': 305,\n",
              "        'y': 3626,\n",
              "        'w': 90,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9900162228444945',\n",
              "        'text': 'Movies'},\n",
              "       {'x': 417,\n",
              "        'y': 3626,\n",
              "        'w': 39,\n",
              "        'h': 30,\n",
              "        'confidence': '0.692302817065619',\n",
              "        'text': 'TV'},\n",
              "       {'x': 1208,\n",
              "        'y': 3625,\n",
              "        'w': 209,\n",
              "        'h': 41,\n",
              "        'confidence': '0.9998672130865801',\n",
              "        'text': 'Psych Season'},\n",
              "       {'x': 308,\n",
              "        'y': 3660,\n",
              "        'w': 157,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9967419744129417',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 963,\n",
              "        'y': 3640,\n",
              "        'w': 210,\n",
              "        'h': 89,\n",
              "        'confidence': '0.9924588504985821',\n",
              "        'text': 'psych'},\n",
              "       {'x': 1210,\n",
              "        'y': 3670,\n",
              "        'w': 45,\n",
              "        'h': 15,\n",
              "        'confidence': '0.31488314995478933',\n",
              "        'text': 'Dna'},\n",
              "       {'x': 287,\n",
              "        'y': 3721,\n",
              "        'w': 229,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9653939816018824',\n",
              "        'text': 'Customer Reviews'},\n",
              "       {'x': 491,\n",
              "        'y': 3769,\n",
              "        'w': 38,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9531486091757126',\n",
              "        'text': 'Up'},\n",
              "       {'x': 1243,\n",
              "        'y': 3730,\n",
              "        'w': 380,\n",
              "        'h': 77,\n",
              "        'confidence': '0.9748831519376201',\n",
              "        'text': 'Action: Click'},\n",
              "       {'x': 1649,\n",
              "        'y': 3729,\n",
              "        'w': 494,\n",
              "        'h': 88,\n",
              "        'confidence': '0.9910871912900234',\n",
              "        'text': '\\\\\\'Psych Season 1\"'},\n",
              "       {'x': 287,\n",
              "        'y': 3838,\n",
              "        'w': 73,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9568974753211537',\n",
              "        'text': 'Actor'},\n",
              "       {'x': 1204,\n",
              "        'y': 3864,\n",
              "        'w': 307,\n",
              "        'h': 30,\n",
              "        'confidence': '0.6405649787830849',\n",
              "        'text': 'For 534.99to buy season'},\n",
              "       {'x': 1710,\n",
              "        'y': 3868,\n",
              "        'w': 65,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9726776926547237',\n",
              "        'text': 'et al'},\n",
              "       {'x': 283,\n",
              "        'y': 3881,\n",
              "        'w': 164,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9998532018858871',\n",
              "        'text': 'James Roday'},\n",
              "       {'x': 287,\n",
              "        'y': 3916,\n",
              "        'w': 108,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9997481346479044',\n",
              "        'text': 'Dule Hill'},\n",
              "       {'x': 1208,\n",
              "        'y': 3906,\n",
              "        'w': 170,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9976254385110922',\n",
              "        'text': 'Or $0.00 with'},\n",
              "       {'x': 1394,\n",
              "        'y': 3912,\n",
              "        'w': 229,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9971502541214071',\n",
              "        'text': 'Prime membership'},\n",
              "       {'x': 286,\n",
              "        'y': 3950,\n",
              "        'w': 240,\n",
              "        'h': 36,\n",
              "        'confidence': '0.7538909452511954',\n",
              "        'text': 'Timothy Omundson'},\n",
              "       {'x': 286,\n",
              "        'y': 3988,\n",
              "        'w': 188,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9679767153315857',\n",
              "        'text': 'Maggie Lawson'},\n",
              "       {'x': 287,\n",
              "        'y': 4050,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9998992396924596',\n",
              "        'text': 'Price'},\n",
              "       {'x': 1203,\n",
              "        'y': 4036,\n",
              "        'w': 240,\n",
              "        'h': 45,\n",
              "        'confidence': '0.785766285786742',\n",
              "        'text': 'Psych Season 3'},\n",
              "       {'x': 287,\n",
              "        'y': 4093,\n",
              "        'w': 56,\n",
              "        'h': 30,\n",
              "        'confidence': '0.832883894443512',\n",
              "        'text': 'Frce'},\n",
              "       {'x': 1005,\n",
              "        'y': 4066,\n",
              "        'w': 160,\n",
              "        'h': 62,\n",
              "        'confidence': '0.9861756384847814',\n",
              "        'text': 'psych'},\n",
              "       {'x': 1209,\n",
              "        'y': 4085,\n",
              "        'w': 68,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999942779541016',\n",
              "        'text': '2008'},\n",
              "       {'x': 1308,\n",
              "        'y': 4085,\n",
              "        'w': 39,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9281070729500017',\n",
              "        'text': 'CC'},\n",
              "       {'x': 2536,\n",
              "        'y': 4074,\n",
              "        'w': 134,\n",
              "        'h': 62,\n",
              "        'confidence': '0.9999823003925323',\n",
              "        'text': 'prime'},\n",
              "       {'x': 286,\n",
              "        'y': 4127,\n",
              "        'w': 106,\n",
              "        'h': 36,\n",
              "        'confidence': '0.8334278497918738',\n",
              "        'text': 'S0 to $1'},\n",
              "       {'x': 1100,\n",
              "        'y': 4132,\n",
              "        'w': 56,\n",
              "        'h': 30,\n",
              "        'confidence': '0.2602988776779021',\n",
              "        'text': 'Ua'},\n",
              "       {'x': 1390,\n",
              "        'y': 4128,\n",
              "        'w': 52,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999765993614418',\n",
              "        'text': '894'},\n",
              "       {'x': 287,\n",
              "        'y': 4167,\n",
              "        'w': 104,\n",
              "        'h': 30,\n",
              "        'confidence': '0.5475368389681508',\n",
              "        'text': 'S1to $3'},\n",
              "       {'x': 1209,\n",
              "        'y': 4175,\n",
              "        'w': 155,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8170518009386879',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 1696,\n",
              "        'y': 4170,\n",
              "        'w': 387,\n",
              "        'h': 36,\n",
              "        'confidence': '0.8668839692776761',\n",
              "        'text': 'Starring: James Roday , Dule Hill'},\n",
              "       {'x': 287,\n",
              "        'y': 4201,\n",
              "        'w': 104,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8026197172479016',\n",
              "        'text': '53 to $5'},\n",
              "       {'x': 1693,\n",
              "        'y': 4210,\n",
              "        'w': 294,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8163355052910958',\n",
              "        'text': 'and Timothy Omundson'},\n",
              "       {'x': 2545,\n",
              "        'y': 4189,\n",
              "        'w': 190,\n",
              "        'h': 81,\n",
              "        'confidence': '0.999968545852762',\n",
              "        'text': 'Psych'},\n",
              "       {'x': 286,\n",
              "        'y': 4235,\n",
              "        'w': 123,\n",
              "        'h': 36,\n",
              "        'confidence': '0.7592226570413584',\n",
              "        'text': 'S5 to $10'},\n",
              "       {'x': 1209,\n",
              "        'y': 4240,\n",
              "        'w': 47,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9983256631214406',\n",
              "        'text': 'For'},\n",
              "       {'x': 1263,\n",
              "        'y': 4217,\n",
              "        'w': 68,\n",
              "        'h': 55,\n",
              "        'confidence': '0.9996564997575981',\n",
              "        'text': '299'},\n",
              "       {'x': 1331,\n",
              "        'y': 4246,\n",
              "        'w': 28,\n",
              "        'h': 19,\n",
              "        'confidence': '0.15495014034007515',\n",
              "        'text': '[0'},\n",
              "       {'x': 1412,\n",
              "        'y': 4245,\n",
              "        'w': 99,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9581236128935542',\n",
              "        'text': 'episode'},\n",
              "       {'x': 1697,\n",
              "        'y': 4249,\n",
              "        'w': 290,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8343972966348543',\n",
              "        'text': 'Directed by: Mel Damski'},\n",
              "       {'x': 1996,\n",
              "        'y': 4249,\n",
              "        'w': 73,\n",
              "        'h': 30,\n",
              "        'confidence': '0.6928710396108171',\n",
              "        'text': 'Steve'},\n",
              "       {'x': 286,\n",
              "        'y': 4274,\n",
              "        'w': 136,\n",
              "        'h': 36,\n",
              "        'confidence': '0.4999700013841887',\n",
              "        'text': '510 to $15'},\n",
              "       {'x': 1208,\n",
              "        'y': 4278,\n",
              "        'w': 300,\n",
              "        'h': 37,\n",
              "        'confidence': '0.6527953011558812',\n",
              "        'text': 'For 534.99 to buy season'},\n",
              "       {'x': 1697,\n",
              "        'y': 4284,\n",
              "        'w': 82,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7117880086550257',\n",
              "        'text': 'Franks'},\n",
              "       {'x': 1792,\n",
              "        'y': 4288,\n",
              "        'w': 82,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9980833227547308',\n",
              "        'text': 'James'},\n",
              "       {'x': 1868,\n",
              "        'y': 4281,\n",
              "        'w': 211,\n",
              "        'h': 39,\n",
              "        'confidence': '0.8117847887744913',\n",
              "        'text': 'Roday Rodriguez'},\n",
              "       {'x': 2545,\n",
              "        'y': 4292,\n",
              "        'w': 359,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8326778433084102',\n",
              "        'text': 'PRIMETIME EMMY? nominee'},\n",
              "       {'x': 286,\n",
              "        'y': 4309,\n",
              "        'w': 136,\n",
              "        'h': 36,\n",
              "        'confidence': '0.4918433719422026',\n",
              "        'text': '515 to S20'},\n",
              "       {'x': 1208,\n",
              "        'y': 4322,\n",
              "        'w': 170,\n",
              "        'h': 36,\n",
              "        'confidence': '0.7693419679834789',\n",
              "        'text': 'Or $0.00 with'},\n",
              "       {'x': 1393,\n",
              "        'y': 4322,\n",
              "        'w': 231,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9812259560094939',\n",
              "        'text': 'Prime membership'},\n",
              "       {'x': 1710,\n",
              "        'y': 4323,\n",
              "        'w': 65,\n",
              "        'h': 29,\n",
              "        'confidence': '0.9912953690929662',\n",
              "        'text': 'et al'},\n",
              "       {'x': 287,\n",
              "        'y': 4370,\n",
              "        'w': 125,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999849795731132',\n",
              "        'text': 'Condition'},\n",
              "       {'x': 2635,\n",
              "        'y': 4369,\n",
              "        'w': 144,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9972974868276134',\n",
              "        'text': 'Season 1'},\n",
              "       {'x': 287,\n",
              "        'y': 4418,\n",
              "        'w': 61,\n",
              "        'h': 25,\n",
              "        'confidence': '0.999096691608429',\n",
              "        'text': 'New'},\n",
              "       {'x': 287,\n",
              "        'y': 4474,\n",
              "        'w': 95,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999573675136857',\n",
              "        'text': 'Format'},\n",
              "       {'x': 1203,\n",
              "        'y': 4456,\n",
              "        'w': 214,\n",
              "        'h': 45,\n",
              "        'confidence': '0.9999662406675327',\n",
              "        'text': 'Psych Season'},\n",
              "       {'x': 2544,\n",
              "        'y': 4460,\n",
              "        'w': 906,\n",
              "        'h': 41,\n",
              "        'confidence': '0.6211726488937785',\n",
              "        'text': 'James Roday plays the somewhat eccentric Shawn Spencer;'},\n",
              "       {'x': 3478,\n",
              "        'y': 4460,\n",
              "        'w': 97,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9998614429258561',\n",
              "        'text': 'police'},\n",
              "       {'x': 326,\n",
              "        'y': 4517,\n",
              "        'w': 147,\n",
              "        'h': 30,\n",
              "        'confidence': '0.984611954249115',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 869,\n",
              "        'y': 4472,\n",
              "        'w': 186,\n",
              "        'h': 76,\n",
              "        'confidence': '0.8063558468174334',\n",
              "        'text': 'peych'},\n",
              "       {'x': 1209,\n",
              "        'y': 4500,\n",
              "        'w': 68,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9495017332787029',\n",
              "        'text': '2013'},\n",
              "       {'x': 1308,\n",
              "        'y': 4500,\n",
              "        'w': 39,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9165904917167833',\n",
              "        'text': 'CC'},\n",
              "       {'x': 2544,\n",
              "        'y': 4503,\n",
              "        'w': 1087,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9154273680241156',\n",
              "        'text': 'consultant who solves crimes with powers of observation so acute that'},\n",
              "       {'x': 1390,\n",
              "        'y': 4547,\n",
              "        'w': 73,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999718512093189',\n",
              "        'text': '1,326'},\n",
              "       {'x': 2544,\n",
              "        'y': 4538,\n",
              "        'w': 438,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9230218630956153',\n",
              "        'text': \"the police think he'$ psychic\"},\n",
              "       {'x': 3007,\n",
              "        'y': 4538,\n",
              "        'w': 378,\n",
              "        'h': 36,\n",
              "        'confidence': '0.5419409648766186',\n",
              "        'text': 'or S0 he lets them think:'},\n",
              "       {'x': 287,\n",
              "        'y': 4578,\n",
              "        'w': 78,\n",
              "        'h': 25,\n",
              "        'confidence': '0.9939055735363',\n",
              "        'text': 'Genre'},\n",
              "       {'x': 1209,\n",
              "        'y': 4591,\n",
              "        'w': 155,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7177894969122179',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 1695,\n",
              "        'y': 4589,\n",
              "        'w': 267,\n",
              "        'h': 38,\n",
              "        'confidence': '0.9362746908554749',\n",
              "        'text': 'Starring: James Roday'},\n",
              "       {'x': 1974,\n",
              "        'y': 4591,\n",
              "        'w': 86,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9826079371534815',\n",
              "        'text': 'Shawn'},\n",
              "       {'x': 2727,\n",
              "        'y': 4608,\n",
              "        'w': 51,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999151386219184',\n",
              "        'text': '739'},\n",
              "       {'x': 2796,\n",
              "        'y': 4608,\n",
              "        'w': 121,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9604323541602485',\n",
              "        'text': 'IMDb 8.4'},\n",
              "       {'x': 2930,\n",
              "        'y': 4608,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999858140945435',\n",
              "        'text': '2007'},\n",
              "       {'x': 287,\n",
              "        'y': 4621,\n",
              "        'w': 82,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9981046880712884',\n",
              "        'text': 'Action'},\n",
              "       {'x': 391,\n",
              "        'y': 4621,\n",
              "        'w': 130,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9949315883031272',\n",
              "        'text': 'Adventure'},\n",
              "       {'x': 1697,\n",
              "        'y': 4630,\n",
              "        'w': 104,\n",
              "        'h': 30,\n",
              "        'confidence': '0.36887581954279874',\n",
              "        'text': 'Spencer,'},\n",
              "       {'x': 1810,\n",
              "        'y': 4630,\n",
              "        'w': 242,\n",
              "        'h': 30,\n",
              "        'confidence': '0.4473675958028089',\n",
              "        'text': 'Dule Hill and Corbin'},\n",
              "       {'x': 287,\n",
              "        'y': 4656,\n",
              "        'w': 130,\n",
              "        'h': 29,\n",
              "        'confidence': '0.9999192936825748',\n",
              "        'text': 'Animation'},\n",
              "       {'x': 1202,\n",
              "        'y': 4636,\n",
              "        'w': 311,\n",
              "        'h': 56,\n",
              "        'confidence': '0.6855254578212047',\n",
              "        'text': 'For *299 to buy episode'},\n",
              "       {'x': 1697,\n",
              "        'y': 4664,\n",
              "        'w': 104,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999467873719561',\n",
              "        'text': 'Bernsen'},\n",
              "       {'x': 2547,\n",
              "        'y': 4663,\n",
              "        'w': 112,\n",
              "        'h': 39,\n",
              "        'confidence': '0.9939527722190847',\n",
              "        'text': 'Comedy'},\n",
              "       {'x': 2695,\n",
              "        'y': 4663,\n",
              "        'w': 127,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9954190078821036',\n",
              "        'text': 'Suspense'},\n",
              "       {'x': 3095,\n",
              "        'y': 4643,\n",
              "        'w': 383,\n",
              "        'h': 77,\n",
              "        'confidence': '0.9952330939086883',\n",
              "        'text': 'Action: Click'},\n",
              "       {'x': 3631,\n",
              "        'y': 4637,\n",
              "        'w': 632,\n",
              "        'h': 96,\n",
              "        'confidence': '0.5855708824536582',\n",
              "        'text': 'Episode 1 HD $2.99\"'},\n",
              "       {'x': 287,\n",
              "        'y': 4694,\n",
              "        'w': 151,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7662581681295304',\n",
              "        'text': 'Arts, Culture'},\n",
              "       {'x': 465,\n",
              "        'y': 4694,\n",
              "        'w': 177,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9989231041551517',\n",
              "        'text': 'Entertainment'},\n",
              "       {'x': 1208,\n",
              "        'y': 4693,\n",
              "        'w': 300,\n",
              "        'h': 37,\n",
              "        'confidence': '0.530635886841011',\n",
              "        'text': 'For 534.99 to buy season'},\n",
              "       {'x': 1697,\n",
              "        'y': 4703,\n",
              "        'w': 290,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7805488218482368',\n",
              "        'text': 'Directed by: Mel Damski'},\n",
              "       {'x': 1996,\n",
              "        'y': 4703,\n",
              "        'w': 73,\n",
              "        'h': 30,\n",
              "        'confidence': '0.786025840214382',\n",
              "        'text': 'Steve'},\n",
              "       {'x': 287,\n",
              "        'y': 4729,\n",
              "        'w': 104,\n",
              "        'h': 30,\n",
              "        'confidence': '0.99974410161451',\n",
              "        'text': 'Comedy'},\n",
              "       {'x': 1208,\n",
              "        'y': 4737,\n",
              "        'w': 170,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9330388542176832',\n",
              "        'text': 'Or $0.00 with'},\n",
              "       {'x': 1394,\n",
              "        'y': 4742,\n",
              "        'w': 229,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9812735244723495',\n",
              "        'text': 'Prime membership'},\n",
              "       {'x': 1697,\n",
              "        'y': 4738,\n",
              "        'w': 82,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9928392791083546',\n",
              "        'text': 'Franks'},\n",
              "       {'x': 1791,\n",
              "        'y': 4737,\n",
              "        'w': 288,\n",
              "        'h': 36,\n",
              "        'confidence': '0.8558148069885567',\n",
              "        'text': 'James Roday Rodriguez'},\n",
              "       {'x': 287,\n",
              "        'y': 4768,\n",
              "        'w': 164,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8949498504423591',\n",
              "        'text': 'Documentary'},\n",
              "       {'x': 1710,\n",
              "        'y': 4777,\n",
              "        'w': 65,\n",
              "        'h': 30,\n",
              "        'confidence': '0.504750896239558',\n",
              "        'text': 'et al.'},\n",
              "       {'x': 287,\n",
              "        'y': 4803,\n",
              "        'w': 82,\n",
              "        'h': 30,\n",
              "        'confidence': '0.999692224146474',\n",
              "        'text': 'Drama'},\n",
              "       {'x': 2579,\n",
              "        'y': 4797,\n",
              "        'w': 278,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9935780648519309',\n",
              "        'text': 'Watch with Prime'},\n",
              "       {'x': 3145,\n",
              "        'y': 4797,\n",
              "        'w': 127,\n",
              "        'h': 41,\n",
              "        'confidence': '0.9999625575094643',\n",
              "        'text': 'Episode'},\n",
              "       {'x': 3448,\n",
              "        'y': 4793,\n",
              "        'w': 118,\n",
              "        'h': 41,\n",
              "        'confidence': '0.999979511150598',\n",
              "        'text': 'Season'},\n",
              "       {'x': 3677,\n",
              "        'y': 4794,\n",
              "        'w': 232,\n",
              "        'h': 44,\n",
              "        'confidence': '0.96254729144712',\n",
              "        'text': 'More purchase'},\n",
              "       {'x': 287,\n",
              "        'y': 4842,\n",
              "        'w': 95,\n",
              "        'h': 29,\n",
              "        'confidence': '0.9958418326927505',\n",
              "        'text': 'Fantasy'},\n",
              "       {'x': 2578,\n",
              "        'y': 4831,\n",
              "        'w': 423,\n",
              "        'h': 51,\n",
              "        'confidence': '0.7225171684800967',\n",
              "        'text': 'Start your 30-day free trial'},\n",
              "       {'x': 3072,\n",
              "        'y': 4832,\n",
              "        'w': 157,\n",
              "        'h': 45,\n",
              "        'confidence': '0.7316211805694955',\n",
              "        'text': 'HD $2.99'},\n",
              "       {'x': 3379,\n",
              "        'y': 4832,\n",
              "        'w': 174,\n",
              "        'h': 45,\n",
              "        'confidence': '0.6774419478289473',\n",
              "        'text': 'HD $34.99'},\n",
              "       {'x': 3673,\n",
              "        'y': 4836,\n",
              "        'w': 127,\n",
              "        'h': 41,\n",
              "        'confidence': '0.9999857169372777',\n",
              "        'text': 'options'},\n",
              "       {'x': 4359,\n",
              "        'y': 4805,\n",
              "        'w': 56,\n",
              "        'h': 60,\n",
              "        'confidence': '0.24962467663013274',\n",
              "        'text': '08'},\n",
              "       {'x': 287,\n",
              "        'y': 4876,\n",
              "        'w': 86,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9996397523650223',\n",
              "        'text': 'Horror'},\n",
              "       {'x': 1208,\n",
              "        'y': 4875,\n",
              "        'w': 96,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999976542710154',\n",
              "        'text': 'Bosch'},\n",
              "       {'x': 1329,\n",
              "        'y': 4875,\n",
              "        'w': 144,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9667137893626345',\n",
              "        'text': 'Season 3'},\n",
              "       {'x': 283,\n",
              "        'y': 4915,\n",
              "        'w': 164,\n",
              "        'h': 26,\n",
              "        'confidence': '0.8148624472963694',\n",
              "        'text': 'International'},\n",
              "       {'x': 833,\n",
              "        'y': 4912,\n",
              "        'w': 119,\n",
              "        'h': 19,\n",
              "        'confidence': '0.23265289232885192',\n",
              "        'text': 'amizon'},\n",
              "       {'x': 962,\n",
              "        'y': 4911,\n",
              "        'w': 138,\n",
              "        'h': 25,\n",
              "        'confidence': '0.5942612661573402',\n",
              "        'text': 'oRiginal'},\n",
              "       {'x': 1203,\n",
              "        'y': 4914,\n",
              "        'w': 71,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999985694885254',\n",
              "        'text': '2017'},\n",
              "       {'x': 1307,\n",
              "        'y': 4914,\n",
              "        'w': 88,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9957530902424929',\n",
              "        'text': 'TV-MA'},\n",
              "       {'x': 1429,\n",
              "        'y': 4919,\n",
              "        'w': 39,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9248411484234061',\n",
              "        'text': 'CC'},\n",
              "       {'x': 2593,\n",
              "        'y': 4919,\n",
              "        'w': 129,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9995481298657944',\n",
              "        'text': 'Join Prime'},\n",
              "       {'x': 287,\n",
              "        'y': 4950,\n",
              "        'w': 156,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9990344497015597',\n",
              "        'text': 'Music Videos'},\n",
              "       {'x': 469,\n",
              "        'y': 4950,\n",
              "        'w': 112,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9960566367023705',\n",
              "        'text': 'Concerts'},\n",
              "       {'x': 817,\n",
              "        'y': 4921,\n",
              "        'w': 299,\n",
              "        'h': 109,\n",
              "        'confidence': '0.49661450941371776',\n",
              "        'text': 'BOScH'},\n",
              "       {'x': 1407,\n",
              "        'y': 4963,\n",
              "        'w': 56,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9810856580734253',\n",
              "        'text': \"'782\"},\n",
              "       {'x': 287,\n",
              "        'y': 4984,\n",
              "        'w': 117,\n",
              "        'h': 30,\n",
              "        'confidence': '0.999962332222351',\n",
              "        'text': 'Romance'},\n",
              "       {'x': 2544,\n",
              "        'y': 4979,\n",
              "        'w': 183,\n",
              "        'h': 41,\n",
              "        'confidence': '0.9940388788432443',\n",
              "        'text': 'Terms apply'},\n",
              "       {'x': 287,\n",
              "        'y': 5023,\n",
              "        'w': 177,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9518605897530277',\n",
              "        'text': 'Science Fiction'},\n",
              "       {'x': 1209,\n",
              "        'y': 5010,\n",
              "        'w': 155,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7665930388756772',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 1696,\n",
              "        'y': 5006,\n",
              "        'w': 274,\n",
              "        'h': 37,\n",
              "        'confidence': '0.7134498429984544',\n",
              "        'text': 'Starring: Titus Welliver'},\n",
              "       {'x': 1978,\n",
              "        'y': 5010,\n",
              "        'w': 78,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7341447794222655',\n",
              "        'text': 'Jamie'},\n",
              "       {'x': 313,\n",
              "        'y': 5062,\n",
              "        'w': 117,\n",
              "        'h': 26,\n",
              "        'confidence': '0.649215776200284',\n",
              "        'text': 'See more'},\n",
              "       {'x': 1697,\n",
              "        'y': 5045,\n",
              "        'w': 86,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9904024909282815',\n",
              "        'text': 'Hector'},\n",
              "       {'x': 1791,\n",
              "        'y': 5044,\n",
              "        'w': 205,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9954534218674623',\n",
              "        'text': 'Amy Aquino and'},\n",
              "       {'x': 1265,\n",
              "        'y': 5075,\n",
              "        'w': 263,\n",
              "        'h': 26,\n",
              "        'confidence': '0.7838216814208566',\n",
              "        'text': 'Watch now: Free with ads'},\n",
              "       {'x': 1697,\n",
              "        'y': 5084,\n",
              "        'w': 169,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9763734471595076',\n",
              "        'text': 'Madison Lintz'},\n",
              "       {'x': 2548,\n",
              "        'y': 5100,\n",
              "        'w': 140,\n",
              "        'h': 36,\n",
              "        'confidence': '0.926190924083751',\n",
              "        'text': 'Episodes'},\n",
              "       {'x': 2721,\n",
              "        'y': 5096,\n",
              "        'w': 127,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999979273829965',\n",
              "        'text': 'Related'},\n",
              "       {'x': 2881,\n",
              "        'y': 5096,\n",
              "        'w': 114,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9999908534406047',\n",
              "        'text': 'Details'},\n",
              "       {'x': 286,\n",
              "        'y': 5117,\n",
              "        'w': 171,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9996432307628514',\n",
              "        'text': 'Video Length'},\n",
              "       {'x': 1696,\n",
              "        'y': 5117,\n",
              "        'w': 339,\n",
              "        'h': 37,\n",
              "        'confidence': '0.7543648527777784',\n",
              "        'text': 'Directed by: Adam Davidson ,'},\n",
              "       {'x': 1208,\n",
              "        'y': 5135,\n",
              "        'w': 170,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9468598820017944',\n",
              "        'text': 'Or $0.00 with'},\n",
              "       {'x': 1393,\n",
              "        'y': 5135,\n",
              "        'w': 231,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9890128560644751',\n",
              "        'text': 'Prime membership'},\n",
              "       {'x': 325,\n",
              "        'y': 5161,\n",
              "        'w': 71,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9924531240420943',\n",
              "        'text': 'Up to'},\n",
              "       {'x': 417,\n",
              "        'y': 5162,\n",
              "        'w': 104,\n",
              "        'h': 30,\n",
              "        'confidence': '0.999867800303433',\n",
              "        'text': 'Minutes'},\n",
              "       {'x': 1697,\n",
              "        'y': 5157,\n",
              "        'w': 195,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7861779961983838',\n",
              "        'text': 'Alex Zakrzewski ,'},\n",
              "       {'x': 1900,\n",
              "        'y': 5152,\n",
              "        'w': 118,\n",
              "        'h': 36,\n",
              "        'confidence': '0.8732828567329605',\n",
              "        'text': 'Sarah Pia'},\n",
              "       {'x': 326,\n",
              "        'y': 5200,\n",
              "        'w': 203,\n",
              "        'h': 30,\n",
              "        'confidence': '0.5888104485972826',\n",
              "        'text': '11t0 20 Minutes'},\n",
              "       {'x': 1697,\n",
              "        'y': 5192,\n",
              "        'w': 121,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9998939983488226',\n",
              "        'text': 'Anderson'},\n",
              "       {'x': 1827,\n",
              "        'y': 5192,\n",
              "        'w': 65,\n",
              "        'h': 30,\n",
              "        'confidence': '0.6075091678316357',\n",
              "        'text': 'et al.'},\n",
              "       {'x': 326,\n",
              "        'y': 5235,\n",
              "        'w': 203,\n",
              "        'h': 30,\n",
              "        'confidence': '0.70088241925712',\n",
              "        'text': '21 t0 30 Minutes'},\n",
              "       {'x': 4577,\n",
              "        'y': 5238,\n",
              "        'w': 79,\n",
              "        'h': 41,\n",
              "        'confidence': '0.9999500513076782',\n",
              "        'text': 'Sort'},\n",
              "       {'x': 322,\n",
              "        'y': 5274,\n",
              "        'w': 207,\n",
              "        'h': 30,\n",
              "        'confidence': '0.49642419118112946',\n",
              "        'text': '31 to 60 Minutes'},\n",
              "       {'x': 326,\n",
              "        'y': 5309,\n",
              "        'w': 203,\n",
              "        'h': 30,\n",
              "        'confidence': '0.6566842875856942',\n",
              "        'text': '61 to 90 Minutes'},\n",
              "       {'x': 1208,\n",
              "        'y': 5290,\n",
              "        'w': 321,\n",
              "        'h': 41,\n",
              "        'confidence': '0.8773204839451406',\n",
              "        'text': 'Hollywood Homicide'},\n",
              "       {'x': 326,\n",
              "        'y': 5348,\n",
              "        'w': 268,\n",
              "        'h': 29,\n",
              "        'confidence': '0.6416611893769193',\n",
              "        'text': 'More Than 90 Minutes'},\n",
              "       {'x': 1204,\n",
              "        'y': 5335,\n",
              "        'w': 73,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999058842658997',\n",
              "        'text': '2003'},\n",
              "       {'x': 1308,\n",
              "        'y': 5335,\n",
              "        'w': 82,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7845349973247815',\n",
              "        'text': 'PG-13'},\n",
              "       {'x': 1425,\n",
              "        'y': 5335,\n",
              "        'w': 34,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9695600535432867',\n",
              "        'text': 'CC'},\n",
              "       {'x': 2967,\n",
              "        'y': 5341,\n",
              "        'w': 228,\n",
              "        'h': 51,\n",
              "        'confidence': '0.5825681578172452',\n",
              "        'text': '51 E1 - Psych'},\n",
              "       {'x': 1390,\n",
              "        'y': 5378,\n",
              "        'w': 52,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999905709103443',\n",
              "        'text': '609'},\n",
              "       {'x': 287,\n",
              "        'y': 5404,\n",
              "        'w': 95,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9992110868276357',\n",
              "        'text': 'Decade'},\n",
              "       {'x': 2972,\n",
              "        'y': 5403,\n",
              "        'w': 326,\n",
              "        'h': 40,\n",
              "        'confidence': '0.8056074388372636',\n",
              "        'text': 'July 6, 2006 1h 6 min'},\n",
              "       {'x': 3315,\n",
              "        'y': 5404,\n",
              "        'w': 51,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9733342524526458',\n",
              "        'text': '13+'},\n",
              "       {'x': 1208,\n",
              "        'y': 5420,\n",
              "        'w': 157,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9993080250573056',\n",
              "        'text': 'Prime Video'},\n",
              "       {'x': 1697,\n",
              "        'y': 5425,\n",
              "        'w': 272,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7593454392633849',\n",
              "        'text': 'Starring: Harrison Ford'},\n",
              "       {'x': 1978,\n",
              "        'y': 5425,\n",
              "        'w': 65,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8185861189822426',\n",
              "        'text': 'Josh'},\n",
              "       {'x': 326,\n",
              "        'y': 5447,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.992163659623242',\n",
              "        'text': '2020'},\n",
              "       {'x': 417,\n",
              "        'y': 5451,\n",
              "        'w': 82,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999231237255782',\n",
              "        'text': 'Newer'},\n",
              "       {'x': 1697,\n",
              "        'y': 5460,\n",
              "        'w': 112,\n",
              "        'h': 30,\n",
              "        'confidence': '0.5196377349789545',\n",
              "        'text': 'Hartnett .'},\n",
              "       {'x': 1817,\n",
              "        'y': 5455,\n",
              "        'w': 224,\n",
              "        'h': 43,\n",
              "        'confidence': '0.9485102473799144',\n",
              "        'text': 'Isaiah Washington ,'},\n",
              "       {'x': 2052,\n",
              "        'y': 5464,\n",
              "        'w': 30,\n",
              "        'h': 26,\n",
              "        'confidence': '0.7537987281514168',\n",
              "        'text': 'et'},\n",
              "       {'x': 2971,\n",
              "        'y': 5450,\n",
              "        'w': 1336,\n",
              "        'h': 55,\n",
              "        'confidence': '0.7962302505386596',\n",
              "        'text': 'When the Santa Barbara police department hit a wall on a high profile kidnapping case;'},\n",
              "       {'x': 326,\n",
              "        'y': 5486,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8388739736371865',\n",
              "        'text': '2010'},\n",
              "       {'x': 408,\n",
              "        'y': 5486,\n",
              "        'w': 69,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999998807907104',\n",
              "        'text': '2019'},\n",
              "       {'x': 1204,\n",
              "        'y': 5490,\n",
              "        'w': 52,\n",
              "        'h': 30,\n",
              "        'confidence': '0.9999075679051405',\n",
              "        'text': 'For'},\n",
              "       {'x': 1263,\n",
              "        'y': 5471,\n",
              "        'w': 68,\n",
              "        'h': 51,\n",
              "        'confidence': '0.9999871296397141',\n",
              "        'text': '399'},\n",
              "       {'x': 1330,\n",
              "        'y': 5495,\n",
              "        'w': 90,\n",
              "        'h': 25,\n",
              "        'confidence': '0.35750720858567797',\n",
              "        'text': 't0 rent'},\n",
              "       {'x': 1693,\n",
              "        'y': 5499,\n",
              "        'w': 39,\n",
              "        'h': 30,\n",
              "        'confidence': '0.8796454602748326',\n",
              "        'text': 'al:'},\n",
              "       {'x': 2973,\n",
              "        'y': 5503,\n",
              "        'w': 1111,\n",
              "        'h': 30,\n",
              "        'confidence': '0.6758566999967923',\n",
              "        'text': \"thev reluctantlv hire Shawn Spencer the town's newest Dsvchic detective\"},\n",
              "       {'x': 4093,\n",
              "        'y': 5503,\n",
              "        'w': 264,\n",
              "        'h': 30,\n",
              "        'confidence': '0.528841751484451',\n",
              "        'text': 'to held solve the'},\n",
              "       {'x': 1688,\n",
              "        'y': 50,\n",
              "        'w': 168,\n",
              "        'h': 140,\n",
              "        'confidence': '0.9998497554120749',\n",
              "        'text': 'buy'},\n",
              "       {'x': 2217,\n",
              "        'y': 615,\n",
              "        'w': 62,\n",
              "        'h': 42,\n",
              "        'confidence': '0.9920787076168529',\n",
              "        'text': 'shop'},\n",
              "       {'x': 4678,\n",
              "        'y': 615,\n",
              "        'w': 66,\n",
              "        'h': 43,\n",
              "        'confidence': '0.6087168228752607',\n",
              "        'text': 'shop'},\n",
              "       {'x': 2581,\n",
              "        'y': 1286,\n",
              "        'w': 60,\n",
              "        'h': 47,\n",
              "        'confidence': '0.99998946970357',\n",
              "        'text': 'Buy'},\n",
              "       {'x': 86,\n",
              "        'y': 1901,\n",
              "        'w': 64,\n",
              "        'h': 46,\n",
              "        'confidence': '0.9953294396400452',\n",
              "        'text': 'Top-'},\n",
              "       {'x': 2547,\n",
              "        'y': 1900,\n",
              "        'w': 64,\n",
              "        'h': 47,\n",
              "        'confidence': '0.8335041999816895',\n",
              "        'text': 'Top-'},\n",
              "       {'x': 4678,\n",
              "        'y': 3314,\n",
              "        'w': 66,\n",
              "        'h': 42,\n",
              "        'confidence': '0.9968127012252808',\n",
              "        'text': 'shop'},\n",
              "       {'x': 1361,\n",
              "        'y': 4236,\n",
              "        'w': 48,\n",
              "        'h': 42,\n",
              "        'confidence': '0.9998781798318682',\n",
              "        'text': 'buy'},\n",
              "       {'x': 3515,\n",
              "        'y': 4639,\n",
              "        'w': 112,\n",
              "        'h': 95,\n",
              "        'confidence': '0.9999821053874435',\n",
              "        'text': 'Buy'},\n",
              "       {'x': 3082,\n",
              "        'y': 4794,\n",
              "        'w': 60,\n",
              "        'h': 47,\n",
              "        'confidence': '0.9999765305361424',\n",
              "        'text': 'Buy'},\n",
              "       {'x': 3385,\n",
              "        'y': 4794,\n",
              "        'w': 60,\n",
              "        'h': 46,\n",
              "        'confidence': '0.9999933927534448',\n",
              "        'text': 'Buy'}]}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 73, 'y': 34.04, 'w': 305, 'h': 402.96}},\n",
              "     {'type': 'text',\n",
              "      'value': 'User input: \"Search for Psych and buy the first episode\"',\n",
              "      'md': 'User input: \"Search for Psych and buy the first episode\"',\n",
              "      'bBox': {'x': 82, 'y': 123, 'w': 283, 'h': 532}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       ['Agent step 1', 'Agent step 2'],\n",
              "       ['---', '---'],\n",
              "       ['amazon Prime Video Search Amazon', 'EN Hello, sign in Returns 10'],\n",
              "       ['amazon Prime Video Psych', 'EN - Hello, sign in Returns 10'],\n",
              "       ['Account & Lists Orders', \"psych Action: Click 'Psych'\"],\n",
              "       ['Black Friday Week= All Black Friday Deals', \"Action: Type 'Psych'\"],\n",
              "       ['amazon Home shop now', 'All Black Friday Deals'],\n",
              "       ['prime video Home', 'Join Prime'],\n",
              "       ['prime', 'psychological thrillers'],\n",
              "       ['psychic', 'psychic kids'],\n",
              "       ['psych the movie', 'psychology'],\n",
              "       ['psychic investigators', 'YELLO'],\n",
              "       ['psycho', 'GOLDEN GLOBE@ winner'],\n",
              "       ['Buy Episode', 'Watch now HD $2.99'],\n",
              "       ['Freevee (with ads)', 'Available to buy TV-MA'],\n",
              "       ['Popular movies See more', 'Top-rated movies See more'],\n",
              "       ['Featured Originals and Exclusives', 'Neeson'],\n",
              "       ['Agent step 3', 'Agent step 4'],\n",
              "       ['amazon Prime Video psych', 'EN - Hello, sign in Returns 10'],\n",
              "       ['Account Lists Orders', 'EN - Account Lists Orders'],\n",
              "       ['Black Friday Deals Medical Care Best Sellers Prime',\n",
              "        'Amazon Basics Groceries New Releases Music Customer Service Amazon Home Black Friday Week shop now'],\n",
              "       ['Home Store Free with ads Categories My Stuff',\n",
              "        'Settings Getting Started Help'],\n",
              "       ['prime video Home Movies TV shows Sports Live TV 88 Subscriptions',\n",
              "        'Join Prime'],\n",
              "       ['1-16 of 423 results for \"psych\"', 'Sort By: Featured'],\n",
              "       ['Department Results', 'Any Department Movies TV'],\n",
              "       ['psych Psych Season', 'Prime Video'],\n",
              "       ['Customer Reviews', \"Action: Click 'Psych Season 1'\"],\n",
              "       ['Actor', 'For $34.99 to buy season et al'],\n",
              "       ['James Roday', 'Or $0.00 with Prime membership'],\n",
              "       ['Dule Hill', 'Psych Season 3'],\n",
              "       ['Price', 'psych 2008 CC'],\n",
              "       ['Free', '$0 to $1'],\n",
              "       ['$1 to $3', 'Prime Video'],\n",
              "       ['$3 to $5', 'Starring: James Roday, Dule Hill'],\n",
              "       ['$5 to $10', 'and Timothy Omundson'],\n",
              "       ['$10 to $15', 'For $34.99 to buy season'],\n",
              "       ['$15 to $20', 'Or $0.00 with Prime membership'],\n",
              "       ['Condition', 'New'],\n",
              "       ['Format', 'peych'],\n",
              "       ['Psych Season', '2013 CC'],\n",
              "       ['Genre', 'Prime Video 1,326'],\n",
              "       ['Action Adventure', 'For $2.99 to buy episode'],\n",
              "       ['Animation', 'Comedy'],\n",
              "       ['Documentary', 'Drama'],\n",
              "       ['Fantasy', 'Horror'],\n",
              "       ['International', 'Music Videos Concerts'],\n",
              "       ['Romance', 'Science Fiction'],\n",
              "       ['See more', 'Watch now: Free with ads'],\n",
              "       ['Or $0.00 with Prime membership',\n",
              "        'Directed by: Adam Davidson, Alex Zakrzewski, Sarah Pia Anderson et al.'],\n",
              "       ['Video Length', 'Up to Minutes'],\n",
              "       ['11 to 20 Minutes', '21 to 30 Minutes'],\n",
              "       ['31 to 60 Minutes', '61 to 90 Minutes'],\n",
              "       ['More Than 90 Minutes', 'Decade'],\n",
              "       ['2020 Newer', '2010-2019'],\n",
              "       ['Figure 7: Example of a multimodal agent.', '']],\n",
              "      'md': 'User input: \"Search for Psych and buy the first episode\"\\n|Agent step 1|Agent step 2|\\n|---|---|\\n|amazon Prime Video Search Amazon|EN Hello, sign in Returns 10|\\n|amazon Prime Video Psych|EN - Hello, sign in Returns 10|\\n|Account & Lists Orders|psych Action: Click \\'Psych\\'|\\n|Black Friday Week= All Black Friday Deals|Action: Type \\'Psych\\'|\\n|amazon Home shop now|All Black Friday Deals|\\n|prime video Home|Join Prime|\\n|prime|psychological thrillers|\\n|psychic|psychic kids|\\n|psych the movie|psychology|\\n|psychic investigators|YELLO|\\n|psycho|GOLDEN GLOBE@ winner|\\n|Buy Episode|Watch now HD $2.99|\\n|Freevee (with ads)|Available to buy TV-MA|\\n|Popular movies See more|Top-rated movies See more|\\n|Featured Originals and Exclusives|Neeson|\\n|Agent step 3|Agent step 4|\\n|amazon Prime Video psych|EN - Hello, sign in Returns 10|\\n|Account Lists Orders|EN - Account Lists Orders|\\n|Black Friday Deals Medical Care Best Sellers Prime|Amazon Basics Groceries New Releases Music Customer Service Amazon Home Black Friday Week shop now|\\n|Home Store Free with ads Categories My Stuff|Settings Getting Started Help|\\n|prime video Home Movies TV shows Sports Live TV 88 Subscriptions|Join Prime|\\n|1-16 of 423 results for \"psych\"|Sort By: Featured|\\n|Department Results|Any Department Movies TV|\\n|psych Psych Season|Prime Video|\\n|Customer Reviews|Action: Click \\'Psych Season 1\\'|\\n|Actor|For $34.99 to buy season et al|\\n|James Roday|Or $0.00 with Prime membership|\\n|Dule Hill|Psych Season 3|\\n|Price|psych 2008 CC|\\n|Free|$0 to $1|\\n|$1 to $3|Prime Video|\\n|$3 to $5|Starring: James Roday, Dule Hill|\\n|$5 to $10|and Timothy Omundson|\\n|$10 to $15|For $34.99 to buy season|\\n|$15 to $20|Or $0.00 with Prime membership|\\n|Condition|New|\\n|Format|peych|\\n|Psych Season|2013 CC|\\n|Genre|Prime Video 1,326|\\n|Action Adventure|For $2.99 to buy episode|\\n|Animation|Comedy|\\n|Documentary|Drama|\\n|Fantasy|Horror|\\n|International|Music Videos Concerts|\\n|Romance|Science Fiction|\\n|See more|Watch now: Free with ads|\\n|Or $0.00 with Prime membership|Directed by: Adam Davidson, Alex Zakrzewski, Sarah Pia Anderson et al.|\\n|Video Length|Up to Minutes|\\n|11 to 20 Minutes|21 to 30 Minutes|\\n|31 to 60 Minutes|61 to 90 Minutes|\\n|More Than 90 Minutes|Decade|\\n|2020 Newer|2010-2019|\\n|Figure 7: Example of a multimodal agent.| |',\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\"Agent step 1\",\"Agent step 2\"\\n\"---\",\"---\"\\n\"amazon Prime Video Search Amazon\",\"EN Hello, sign in Returns 10\"\\n\"amazon Prime Video Psych\",\"EN - Hello, sign in Returns 10\"\\n\"Account & Lists Orders\",\"psych Action: Click \\'Psych\\'\"\\n\"Black Friday Week= All Black Friday Deals\",\"Action: Type \\'Psych\\'\"\\n\"amazon Home shop now\",\"All Black Friday Deals\"\\n\"prime video Home\",\"Join Prime\"\\n\"prime\",\"psychological thrillers\"\\n\"psychic\",\"psychic kids\"\\n\"psych the movie\",\"psychology\"\\n\"psychic investigators\",\"YELLO\"\\n\"psycho\",\"GOLDEN GLOBE@ winner\"\\n\"Buy Episode\",\"Watch now HD $2.99\"\\n\"Freevee (with ads)\",\"Available to buy TV-MA\"\\n\"Popular movies See more\",\"Top-rated movies See more\"\\n\"Featured Originals and Exclusives\",\"Neeson\"\\n\"Agent step 3\",\"Agent step 4\"\\n\"amazon Prime Video psych\",\"EN - Hello, sign in Returns 10\"\\n\"Account Lists Orders\",\"EN - Account Lists Orders\"\\n\"Black Friday Deals Medical Care Best Sellers Prime\",\"Amazon Basics Groceries New Releases Music Customer Service Amazon Home Black Friday Week shop now\"\\n\"Home Store Free with ads Categories My Stuff\",\"Settings Getting Started Help\"\\n\"prime video Home Movies TV shows Sports Live TV 88 Subscriptions\",\"Join Prime\"\\n\"1-16 of 423 results for \"\"psych\"\"\",\"Sort By: Featured\"\\n\"Department Results\",\"Any Department Movies TV\"\\n\"psych Psych Season\",\"Prime Video\"\\n\"Customer Reviews\",\"Action: Click \\'Psych Season 1\\'\"\\n\"Actor\",\"For $34.99 to buy season et al\"\\n\"James Roday\",\"Or $0.00 with Prime membership\"\\n\"Dule Hill\",\"Psych Season 3\"\\n\"Price\",\"psych 2008 CC\"\\n\"Free\",\"$0 to $1\"\\n\"$1 to $3\",\"Prime Video\"\\n\"$3 to $5\",\"Starring: James Roday, Dule Hill\"\\n\"$5 to $10\",\"and Timothy Omundson\"\\n\"$10 to $15\",\"For $34.99 to buy season\"\\n\"$15 to $20\",\"Or $0.00 with Prime membership\"\\n\"Condition\",\"New\"\\n\"Format\",\"peych\"\\n\"Psych Season\",\"2013 CC\"\\n\"Genre\",\"Prime Video 1,326\"\\n\"Action Adventure\",\"For $2.99 to buy episode\"\\n\"Animation\",\"Comedy\"\\n\"Documentary\",\"Drama\"\\n\"Fantasy\",\"Horror\"\\n\"International\",\"Music Videos Concerts\"\\n\"Romance\",\"Science Fiction\"\\n\"See more\",\"Watch now: Free with ads\"\\n\"Or $0.00 with Prime membership\",\"Directed by: Adam Davidson, Alex Zakrzewski, Sarah Pia Anderson et al.\"\\n\"Video Length\",\"Up to Minutes\"\\n\"11 to 20 Minutes\",\"21 to 30 Minutes\"\\n\"31 to 60 Minutes\",\"61 to 90 Minutes\"\\n\"More Than 90 Minutes\",\"Decade\"\\n\"2020 Newer\",\"2010-2019\"\\n\"Figure 7: Example of a multimodal agent.\",\"\"',\n",
              "      'bBox': {'x': 73, 'y': 34.04, 'w': 466, 'h': 716}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 41,\n",
              "    'text': \"                                    The Amazon Nova Family of Models\\n     User input                                       Response\\nWhat happened to my tire?                        It appears that your tire has a significant puncture, as\\nWhat should | do?                                 evidenced by the deflated state and visible damage. Here's\\n                                                  what you should do:\\n                                                  1. Assess the Damage: Confirm the puncture by checking for\\n                                                      any visible holes or debris embedded in the tire:\\n                                                  2. Check Tire Pressure: Ensure the tire is completely deflated.\\n                                                  3. Remove the Tire: If    re comfortable, you can remove the\\n                                                                       you'\\n                                                      wheel to inspect the puncture more closely:\\n                                                  4. Temporary Fix: Use a tire repair kit to patch the hole if it's\\n                                                      small: This is a temporary solution:\\n                                                  5.  Professional Repair: Visit a tire shop for a proper repair or\\n                                                      replacement: They can assess if the tire can be patched or if\\n                                                      a new tire is needed.\\n                                                  6.  Inspect Other Tires: Check the other tires for any signs of\\n                                                      damage or wear:\\n                                                  Would you like more detailed instructions on any of these\\n                                                 steps?\\n                             Figure 8: Created with Nova Pro. Image source: [88].\\n                                                      41\",\n",
              "    'md': \"# The Amazon Nova Family of Models\\n\\n|User input|Response|\\n|---|---|\\n|What happened to my tire?|It appears that your tire has a significant puncture, as evidenced by the deflated state and visible damage. Here's what you should do:|\\n| |1. Assess the Damage: Confirm the puncture by checking for any visible holes or debris embedded in the tire.\\n2. Check Tire Pressure: Ensure the tire is completely deflated.\\n3. Remove the Tire: If you're comfortable, you can remove the wheel to inspect the puncture more closely.\\n4. Temporary Fix: Use a tire repair kit to patch the hole if it's small. This is a temporary solution.\\n5. Professional Repair: Visit a tire shop for a proper repair or replacement. They can assess if the tire can be patched or if a new tire is needed.\\n6. Inspect Other Tires: Check the other tires for any signs of damage or wear.\\n|\\n| |Would you like more detailed instructions on any of these steps?|\\n\\nFigure 8: Created with Nova Pro. Image source: [88].\",\n",
              "    'images': [{'name': 'img_p40_1.png',\n",
              "      'height': 1778,\n",
              "      'width': 3190,\n",
              "      'x': 72,\n",
              "      'y': 256.64039999999994,\n",
              "      'original_width': 3190,\n",
              "      'original_height': 1778,\n",
              "      'ocr': [{'x': 207,\n",
              "        'y': 62,\n",
              "        'w': 365,\n",
              "        'h': 92,\n",
              "        'confidence': '0.9998985655994233',\n",
              "        'text': 'User input'},\n",
              "       {'x': 1538,\n",
              "        'y': 63,\n",
              "        'w': 337,\n",
              "        'h': 98,\n",
              "        'confidence': '0.9999960381929892',\n",
              "        'text': 'Response'},\n",
              "       {'x': 76,\n",
              "        'y': 265,\n",
              "        'w': 727,\n",
              "        'h': 77,\n",
              "        'confidence': '0.74907757966919',\n",
              "        'text': 'What happened to my tire?'},\n",
              "       {'x': 1428,\n",
              "        'y': 257,\n",
              "        'w': 1405,\n",
              "        'h': 86,\n",
              "        'confidence': '0.8678158402406111',\n",
              "        'text': 'It appears that your tire has a significant puncture, as'},\n",
              "       {'x': 77,\n",
              "        'y': 343,\n",
              "        'w': 481,\n",
              "        'h': 65,\n",
              "        'confidence': '0.7793777700308451',\n",
              "        'text': 'What should | do?'},\n",
              "       {'x': 1429,\n",
              "        'y': 340,\n",
              "        'w': 1547,\n",
              "        'h': 79,\n",
              "        'confidence': '0.8191668091659111',\n",
              "        'text': \"evidenced by the deflated state and visible damage. Here's\"},\n",
              "       {'x': 1431,\n",
              "        'y': 417,\n",
              "        'w': 543,\n",
              "        'h': 72,\n",
              "        'confidence': '0.9317608190189094',\n",
              "        'text': 'what you should do:'},\n",
              "       {'x': 1433,\n",
              "        'y': 501,\n",
              "        'w': 56,\n",
              "        'h': 56,\n",
              "        'confidence': '0.994519250024855',\n",
              "        'text': '1.'},\n",
              "       {'x': 1515,\n",
              "        'y': 488,\n",
              "        'w': 1524,\n",
              "        'h': 85,\n",
              "        'confidence': '0.839888696734239',\n",
              "        'text': 'Assess the Damage: Confirm the puncture by checking for'},\n",
              "       {'x': 1508,\n",
              "        'y': 570,\n",
              "        'w': 1269,\n",
              "        'h': 78,\n",
              "        'confidence': '0.5460592944053829',\n",
              "        'text': 'any visible holes or debris embedded in the tire:'},\n",
              "       {'x': 1431,\n",
              "        'y': 653,\n",
              "        'w': 58,\n",
              "        'h': 56,\n",
              "        'confidence': '0.9984289586003544',\n",
              "        'text': '2.'},\n",
              "       {'x': 1514,\n",
              "        'y': 649,\n",
              "        'w': 1552,\n",
              "        'h': 79,\n",
              "        'confidence': '0.656593935369633',\n",
              "        'text': 'Check Tire Pressure: Ensure the tire is completely deflated.'},\n",
              "       {'x': 1431,\n",
              "        'y': 731,\n",
              "        'w': 58,\n",
              "        'h': 55,\n",
              "        'confidence': '0.9997909580314662',\n",
              "        'text': '3.'},\n",
              "       {'x': 1515,\n",
              "        'y': 727,\n",
              "        'w': 513,\n",
              "        'h': 63,\n",
              "        'confidence': '0.9988775322297723',\n",
              "        'text': 'Remove the Tire: If'},\n",
              "       {'x': 2139,\n",
              "        'y': 726,\n",
              "        'w': 947,\n",
              "        'h': 76,\n",
              "        'confidence': '0.7266379182344462',\n",
              "        'text': 're comfortable, you can remove the'},\n",
              "       {'x': 1512,\n",
              "        'y': 797,\n",
              "        'w': 1156,\n",
              "        'h': 84,\n",
              "        'confidence': '0.8820809333588429',\n",
              "        'text': 'wheel to inspect the puncture more closely:'},\n",
              "       {'x': 1433,\n",
              "        'y': 885,\n",
              "        'w': 53,\n",
              "        'h': 53,\n",
              "        'confidence': '0.9972287063703513',\n",
              "        'text': '4.'},\n",
              "       {'x': 1509,\n",
              "        'y': 875,\n",
              "        'w': 1523,\n",
              "        'h': 87,\n",
              "        'confidence': '0.564719939784979',\n",
              "        'text': \"Temporary Fix: Use a tire repair kit to patch the hole if it's\"},\n",
              "       {'x': 1510,\n",
              "        'y': 954,\n",
              "        'w': 919,\n",
              "        'h': 84,\n",
              "        'confidence': '0.7388713418379286',\n",
              "        'text': 'small: This is a temporary solution:'},\n",
              "       {'x': 1433,\n",
              "        'y': 1040,\n",
              "        'w': 56,\n",
              "        'h': 55,\n",
              "        'confidence': '0.9999289411528949',\n",
              "        'text': '5.'},\n",
              "       {'x': 1514,\n",
              "        'y': 1031,\n",
              "        'w': 1524,\n",
              "        'h': 81,\n",
              "        'confidence': '0.8374793536717562',\n",
              "        'text': 'Professional Repair: Visit a tire shop for a proper repair or'},\n",
              "       {'x': 1509,\n",
              "        'y': 1103,\n",
              "        'w': 1578,\n",
              "        'h': 87,\n",
              "        'confidence': '0.5969811519263037',\n",
              "        'text': 'replacement: They can assess if the tire can be patched or if'},\n",
              "       {'x': 1515,\n",
              "        'y': 1188,\n",
              "        'w': 551,\n",
              "        'h': 63,\n",
              "        'confidence': '0.6936305143138947',\n",
              "        'text': 'a new tire is needed.'},\n",
              "       {'x': 1433,\n",
              "        'y': 1269,\n",
              "        'w': 56,\n",
              "        'h': 55,\n",
              "        'confidence': '0.999904665159737',\n",
              "        'text': '6.'},\n",
              "       {'x': 1511,\n",
              "        'y': 1265,\n",
              "        'w': 1508,\n",
              "        'h': 76,\n",
              "        'confidence': '0.8212998889018067',\n",
              "        'text': 'Inspect Other Tires: Check the other tires for any signs of'},\n",
              "       {'x': 1509,\n",
              "        'y': 1340,\n",
              "        'w': 448,\n",
              "        'h': 80,\n",
              "        'confidence': '0.8497017117035138',\n",
              "        'text': 'damage or wear:'},\n",
              "       {'x': 1434,\n",
              "        'y': 1417,\n",
              "        'w': 1512,\n",
              "        'h': 76,\n",
              "        'confidence': '0.843726429546946',\n",
              "        'text': 'Would you like more detailed instructions on any of these'},\n",
              "       {'x': 1428,\n",
              "        'y': 1495,\n",
              "        'w': 184,\n",
              "        'h': 81,\n",
              "        'confidence': '0.7785940797531822',\n",
              "        'text': 'steps?'},\n",
              "       {'x': 2027,\n",
              "        'y': 734,\n",
              "        'w': 129,\n",
              "        'h': 54,\n",
              "        'confidence': '0.771219254519134',\n",
              "        'text': \"you' \"}]}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['User input', 'Response'],\n",
              "       ['What happened to my tire?',\n",
              "        \"It appears that your tire has a significant puncture, as evidenced by the deflated state and visible damage. Here's what you should do:\"],\n",
              "       ['']],\n",
              "      'md': \"|User input|Response|\\n|What happened to my tire?|It appears that your tire has a significant puncture, as evidenced by the deflated state and visible damage. Here's what you should do:|\\n| |1. Assess the Damage: Confirm the puncture by checking for any visible holes or debris embedded in the tire.\",\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\"User input\",\"Response\"\\n\"What happened to my tire?\",\"It appears that your tire has a significant puncture, as evidenced by the deflated state and visible damage. Here\\'s what you should do:\"\\n\"\"',\n",
              "      'bBox': {'x': 83, 'y': 34.04, 'w': 442, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': \"2. Check Tire Pressure: Ensure the tire is completely deflated.\\n3. Remove the Tire: If you're comfortable, you can remove the wheel to inspect the puncture more closely.\\n4. Temporary Fix: Use a tire repair kit to patch the hole if it's small. This is a temporary solution.\\n5. Professional Repair: Visit a tire shop for a proper repair or replacement. They can assess if the tire can be patched or if a new tire is needed.\\n6. Inspect Other Tires: Check the other tires for any signs of damage or wear.\",\n",
              "      'md': \"2. Check Tire Pressure: Ensure the tire is completely deflated.\\n3. Remove the Tire: If you're comfortable, you can remove the wheel to inspect the puncture more closely.\\n4. Temporary Fix: Use a tire repair kit to patch the hole if it's small. This is a temporary solution.\\n5. Professional Repair: Visit a tire shop for a proper repair or replacement. They can assess if the tire can be patched or if a new tire is needed.\\n6. Inspect Other Tires: Check the other tires for any signs of damage or wear.\",\n",
              "      'bBox': {'x': 282, 'y': 352, 'w': 243, 'h': 101}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       [],\n",
              "       [],\n",
              "       [],\n",
              "       [],\n",
              "       ['',\n",
              "        'Would you like more detailed instructions on any of these steps?']],\n",
              "      'md': \"2. Check Tire Pressure: Ensure the tire is completely deflated.\\n4. Temporary Fix: Use a tire repair kit to patch the hole if it's small. This is a temporary solution.\\n5. Professional Repair: Visit a tire shop for a proper repair or replacement. They can assess if the tire can be patched or if a new tire is needed.\\n6. Inspect Other Tires: Check the other tires for any signs of damage or wear.\\n|\\n| |Would you like more detailed instructions on any of these steps?|\",\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\\n\\n\\n\\n\"\",\"Would you like more detailed instructions on any of these steps?\"',\n",
              "      'bBox': {'x': 83, 'y': 34.04, 'w': 442, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Figure 8: Created with Nova Pro. Image source: [88].',\n",
              "      'md': 'Figure 8: Created with Nova Pro. Image source: [88].',\n",
              "      'bBox': {'x': 198, 'y': 523.04, 'w': 214, 'h': 10}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 42,\n",
              "    'text': '                                             The Amazon Nova Family of Models\\n       User input                                                             Response\\nGroup this sales data by month using pandas and                          import pandas as pd\\nthen create a bar chart of the sales per month:                          import matplotlib.pyplot as plt\\n                                                                           Data\\n                 Month           Sales                                   data =                                    May\\n                 Jan                     1523.13                              Month   [\\'Jan  \\'Jan    Feb  March          June             July\\n                                                                             \\'Sales    July   Aug \"  Dec \\' ] ,\\n                 Jan                         957                                     [1523.13, 957, 133 , 452 , 521, 901, 1334 , 131, 2601, 3005]\\n                 Feb                         133                           Create DataFrame\\n                 March                       452                         df= pd. DataFrame(data)\\n                 May                        521/                         # Group bymonth and sum sales\\n                 June                        901                         monthly_sales = df.groupby Month  [ \\'Sales\\' ] . sum(  . reset_index\\n                 July                      1334]                           Sort months based oncalendar\\n                 July                        131                         monthly_sales [ Month1 = pd. Categorical(\\n                 Aug                        2601                             monthly_sales [ \\'Month\\' ]\\n                                                                             categories= [ \\' Jan     Feb  March  April\\'  May \"            June  July\\n                 Dec                      30051                              ordered-TrueAug \" \\'Sept      \\'Oct  \\'Nov  Dec\\' ] ,\\n                                                                         monthly_sales = monthly_sales. sort_values Month\\'\\n       Generated plot:                                                     Plot barchart\\n                             Total Sales per Month                       plt.figure( figsize=( 10 ,  6) )\\n     3000                                                                plt.bar(monthly_sales [ \\'Month\\' ] , monthly_sales [ \\'Sales\\' ] ,  color=\\'blue\\'\\n                                                                         plt.xlabel  Month\\n     2500                                                                plt.ylabel( \\'Total Sales\\'\\n                                                                         plt.title  Total Sales per Month\\'\\n     2000                                                                plt. show\\n     3\\n    2¹⁵⁰⁰\\n     1000\\n      500\\n             Jan   Feb  March MayMonthJune  July  Aug  Dec\\n                                                 Figure 9: Created with Nova Pro.\\n       User input                                                             Response\\n How many people say the European Union                                  The chart shows that 57% of people say the\\nresponded well to the pandemic?                                          European Union responded well to the pandemic\\n                                                                         This is indicated by the green bar labeled \"Good\" for\\n         Poor marks for America\\'$ response to                            the EU, which extends to a height of 57%. This\\n         coronavirus outbreak                                            suggests that a majority of the people surveyed had\\n         %6 who say each has done a job dealing with the                  apositive view of the EU\\'s response to the\\n         coronavirus outbreak                                            pandemic\\n                                  Bad       Good\\n        Our country               2520            7496\\n                WHO              34              64\\n                 EU             39             57\\n              China        60              37\\n               US     84               15\\n         Note: Percentages are medians hased on 13 countries Those who\\n         did not answer are not shcren\\n         Source: Summer 2020 Global Attitudes Survey. Q10a-e\\n         US. Image Plummets Internationally as Most Say Country Has\\n         Handled Coronavirus Badly\"\\n         PEW RESEARCH CENTER\\n                                  Figure 10: Created with Nova Lite. Image source: ChartQA dataset [50].\\n                                                                   42',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# User input\\n\\nGroup this sales data by month using pandas and then create a bar chart of the sales per month:\\n\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nData\\nMonth           Sales\\nJan                     1523.13\\nJan                         957\\nFeb                         133\\nMarch                       452\\nMay                        521\\nJune                        901\\nJuly                      1334\\nJuly                        131\\nAug                        2601\\nDec                      3005\\nCreate DataFrame\\ndf= pd.DataFrame(data)\\n# Group by month and sum sales\\nmonthly_sales = df.groupby(\\'Month\\')[\\'Sales\\'].sum().reset_index()\\n# Sort months based on calendar\\nmonthly_sales[\\'Month\\'] = pd.Categorical(monthly_sales[\\'Month\\'],\\ncategories=[\\'Jan\\', \\'Feb\\', \\'March\\', \\'April\\', \\'May\\', \\'June\\', \\'July\\', \\'Aug\\', \\'Sept\\', \\'Oct\\', \\'Nov\\', \\'Dec\\'],\\nordered=True)\\nmonthly_sales = monthly_sales.sort_values(\\'Month\\')\\nPlot barchart\\nplt.figure(figsize=(10, 6))\\nplt.bar(monthly_sales[\\'Month\\'], monthly_sales[\\'Sales\\'], color=\\'blue\\')\\nplt.xlabel(\\'Month\\')\\nplt.ylabel(\\'Total Sales\\')\\nplt.title(\\'Total Sales per Month\\')\\nplt.show()\\n# Generated plot:\\n\\n# Total Sales per Month\\n\\n|Month|Sales|\\n|---|---|\\n|Jan|1523.13|\\n|Jan|957|\\n|Feb|133|\\n|March|452|\\n|May|521|\\n|June|901|\\n|July|1334|\\n|July|131|\\n|Aug|2601|\\n|Dec|3005|\\n\\n# User input\\n\\nHow many people say the European Union responded well to the pandemic?\\n\\n# Response\\n\\nThe chart shows that 57% of people say the European Union responded well to the pandemic. This is indicated by the green bar labeled \"Good\" for the EU, which extends to a height of 57%. This suggests that a majority of the people surveyed had a positive view of the EU\\'s response to the pandemic.\\n\\n| |Poor|Good|\\n|---|---|---|\\n|Our country|2520|7496|\\n|WHO|34|64|\\n|EU|39|57|\\n|China|60|37|\\n|US|84|15|\\n\\nNote: Percentages are medians based on 13 countries. Those who did not answer are not shown.\\n\\nSource: Summer 2020 Global Attitudes Survey. Q10a-e\\n\\nUS. Image Plummets Internationally as Most Say Country Has Handled Coronavirus Badly\\n\\nPEW RESEARCH CENTER\\n\\nFigure 10: Created with Nova Lite. Image source: ChartQA dataset [50].',\n",
              "    'images': [{'name': 'img_p41_1.png',\n",
              "      'height': 2328,\n",
              "      'width': 3112,\n",
              "      'x': 72,\n",
              "      'y': 72.49135999999999,\n",
              "      'original_width': 3112,\n",
              "      'original_height': 2328,\n",
              "      'ocr': [{'x': 205,\n",
              "        'y': 62,\n",
              "        'w': 366,\n",
              "        'h': 90,\n",
              "        'confidence': '0.6628547286717683',\n",
              "        'text': 'User input'},\n",
              "       {'x': 1757,\n",
              "        'y': 61,\n",
              "        'w': 340,\n",
              "        'h': 101,\n",
              "        'confidence': '0.9999955324299241',\n",
              "        'text': 'Response'},\n",
              "       {'x': 1651,\n",
              "        'y': 245,\n",
              "        'w': 267,\n",
              "        'h': 51,\n",
              "        'confidence': '0.7061460522444595',\n",
              "        'text': 'import pandas'},\n",
              "       {'x': 1928,\n",
              "        'y': 255,\n",
              "        'w': 43,\n",
              "        'h': 31,\n",
              "        'confidence': '0.9997889351072115',\n",
              "        'text': 'as'},\n",
              "       {'x': 1988,\n",
              "        'y': 249,\n",
              "        'w': 47,\n",
              "        'h': 45,\n",
              "        'confidence': '0.9997774718924124',\n",
              "        'text': 'pd'},\n",
              "       {'x': 69,\n",
              "        'y': 264,\n",
              "        'w': 559,\n",
              "        'h': 78,\n",
              "        'confidence': '0.7450867758499787',\n",
              "        'text': 'Group this sales data'},\n",
              "       {'x': 709,\n",
              "        'y': 260,\n",
              "        'w': 654,\n",
              "        'h': 84,\n",
              "        'confidence': '0.8887413937513079',\n",
              "        'text': 'month using pandas and'},\n",
              "       {'x': 1652,\n",
              "        'y': 300,\n",
              "        'w': 483,\n",
              "        'h': 45,\n",
              "        'confidence': '0.6243432686179393',\n",
              "        'text': 'import matplotlib.pyplot'},\n",
              "       {'x': 2147,\n",
              "        'y': 306,\n",
              "        'w': 43,\n",
              "        'h': 29,\n",
              "        'confidence': '0.999669921840324',\n",
              "        'text': 'as'},\n",
              "       {'x': 2204,\n",
              "        'y': 300,\n",
              "        'w': 69,\n",
              "        'h': 43,\n",
              "        'confidence': '0.9955437103578304',\n",
              "        'text': 'plt'},\n",
              "       {'x': 69,\n",
              "        'y': 341,\n",
              "        'w': 1229,\n",
              "        'h': 73,\n",
              "        'confidence': '0.9485636305676513',\n",
              "        'text': 'then create a bar chart of the sales per month:'},\n",
              "       {'x': 1689,\n",
              "        'y': 403,\n",
              "        'w': 88,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9999737739562988',\n",
              "        'text': 'Data'},\n",
              "       {'x': 1650,\n",
              "        'y': 454,\n",
              "        'w': 88,\n",
              "        'h': 37,\n",
              "        'confidence': '1.0',\n",
              "        'text': 'data'},\n",
              "       {'x': 1750,\n",
              "        'y': 459,\n",
              "        'w': 26,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9959056649123887',\n",
              "        'text': '='},\n",
              "       {'x': 438,\n",
              "        'y': 457,\n",
              "        'w': 164,\n",
              "        'h': 57,\n",
              "        'confidence': '0.9999955751015998',\n",
              "        'text': 'Month'},\n",
              "       {'x': 770,\n",
              "        'y': 456,\n",
              "        'w': 149,\n",
              "        'h': 61,\n",
              "        'confidence': '0.8629570765447315',\n",
              "        'text': 'Sales'},\n",
              "       {'x': 1749,\n",
              "        'y': 502,\n",
              "        'w': 109,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999982940153483',\n",
              "        'text': 'Month'},\n",
              "       {'x': 1912,\n",
              "        'y': 502,\n",
              "        'w': 106,\n",
              "        'h': 40,\n",
              "        'confidence': '0.8769650686514936',\n",
              "        'text': \"['Jan\"},\n",
              "       {'x': 2069,\n",
              "        'y': 500,\n",
              "        'w': 86,\n",
              "        'h': 42,\n",
              "        'confidence': '0.634786133086383',\n",
              "        'text': \"'Jan\"},\n",
              "       {'x': 2224,\n",
              "        'y': 502,\n",
              "        'w': 71,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9970781207084656',\n",
              "        'text': 'Feb'},\n",
              "       {'x': 2362,\n",
              "        'y': 502,\n",
              "        'w': 111,\n",
              "        'h': 40,\n",
              "        'confidence': '0.999998933759633',\n",
              "        'text': 'March'},\n",
              "       {'x': 2681,\n",
              "        'y': 502,\n",
              "        'w': 91,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999806880950928',\n",
              "        'text': 'June'},\n",
              "       {'x': 2839,\n",
              "        'y': 502,\n",
              "        'w': 93,\n",
              "        'h': 45,\n",
              "        'confidence': '0.999989926815033',\n",
              "        'text': 'July'},\n",
              "       {'x': 431,\n",
              "        'y': 543,\n",
              "        'w': 93,\n",
              "        'h': 56,\n",
              "        'confidence': '0.7449458241462708',\n",
              "        'text': 'Jan'},\n",
              "       {'x': 885,\n",
              "        'y': 543,\n",
              "        'w': 213,\n",
              "        'h': 56,\n",
              "        'confidence': '0.797107626599632',\n",
              "        'text': '1523.13'},\n",
              "       {'x': 1946,\n",
              "        'y': 553,\n",
              "        'w': 91,\n",
              "        'h': 43,\n",
              "        'confidence': '0.9999898076057434',\n",
              "        'text': 'July'},\n",
              "       {'x': 2102,\n",
              "        'y': 551,\n",
              "        'w': 79,\n",
              "        'h': 50,\n",
              "        'confidence': '0.5753179469122375',\n",
              "        'text': 'Aug \"'},\n",
              "       {'x': 2243,\n",
              "        'y': 551,\n",
              "        'w': 125,\n",
              "        'h': 45,\n",
              "        'confidence': '0.5684229842662811',\n",
              "        'text': \"Dec ' ] ,\"},\n",
              "       {'x': 1735,\n",
              "        'y': 602,\n",
              "        'w': 123,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999573188462305',\n",
              "        'text': \"'Sales\"},\n",
              "       {'x': 1911,\n",
              "        'y': 601,\n",
              "        'w': 380,\n",
              "        'h': 48,\n",
              "        'confidence': '0.4379503665877404',\n",
              "        'text': '[1523.13, 957, 133 ,'},\n",
              "       {'x': 2304,\n",
              "        'y': 604,\n",
              "        'w': 84,\n",
              "        'h': 43,\n",
              "        'confidence': '0.7427031030109713',\n",
              "        'text': '452 ,'},\n",
              "       {'x': 2403,\n",
              "        'y': 604,\n",
              "        'w': 84,\n",
              "        'h': 43,\n",
              "        'confidence': '0.9999959468841553',\n",
              "        'text': '521,'},\n",
              "       {'x': 2503,\n",
              "        'y': 604,\n",
              "        'w': 84,\n",
              "        'h': 43,\n",
              "        'confidence': '0.8833313777020365',\n",
              "        'text': '901,'},\n",
              "       {'x': 2603,\n",
              "        'y': 604,\n",
              "        'w': 103,\n",
              "        'h': 43,\n",
              "        'confidence': '0.7605743090268744',\n",
              "        'text': '1334 ,'},\n",
              "       {'x': 2721,\n",
              "        'y': 599,\n",
              "        'w': 323,\n",
              "        'h': 50,\n",
              "        'confidence': '0.5750727258563677',\n",
              "        'text': '131, 2601, 3005]'},\n",
              "       {'x': 431,\n",
              "        'y': 628,\n",
              "        'w': 95,\n",
              "        'h': 56,\n",
              "        'confidence': '0.5567623376846313',\n",
              "        'text': 'Jan'},\n",
              "       {'x': 992,\n",
              "        'y': 628,\n",
              "        'w': 103,\n",
              "        'h': 54,\n",
              "        'confidence': '0.999997040503424',\n",
              "        'text': '957'},\n",
              "       {'x': 435,\n",
              "        'y': 707,\n",
              "        'w': 102,\n",
              "        'h': 61,\n",
              "        'confidence': '0.9999993117448777',\n",
              "        'text': 'Feb'},\n",
              "       {'x': 997,\n",
              "        'y': 710,\n",
              "        'w': 101,\n",
              "        'h': 57,\n",
              "        'confidence': '0.7333797812461853',\n",
              "        'text': '133'},\n",
              "       {'x': 1689,\n",
              "        'y': 753,\n",
              "        'w': 327,\n",
              "        'h': 44,\n",
              "        'confidence': '0.7361677703204764',\n",
              "        'text': 'Create DataFrame'},\n",
              "       {'x': 437,\n",
              "        'y': 792,\n",
              "        'w': 163,\n",
              "        'h': 61,\n",
              "        'confidence': '0.7693878741181885',\n",
              "        'text': 'March'},\n",
              "       {'x': 995,\n",
              "        'y': 795,\n",
              "        'w': 103,\n",
              "        'h': 55,\n",
              "        'confidence': '0.9999997246979434',\n",
              "        'text': '452'},\n",
              "       {'x': 1650,\n",
              "        'y': 804,\n",
              "        'w': 50,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999828041291119',\n",
              "        'text': 'df'},\n",
              "       {'x': 1709,\n",
              "        'y': 812,\n",
              "        'w': 26,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9023674810898719',\n",
              "        'text': '='},\n",
              "       {'x': 1748,\n",
              "        'y': 803,\n",
              "        'w': 363,\n",
              "        'h': 49,\n",
              "        'confidence': '0.7882276701622939',\n",
              "        'text': 'pd. DataFrame(data)'},\n",
              "       {'x': 433,\n",
              "        'y': 873,\n",
              "        'w': 123,\n",
              "        'h': 78,\n",
              "        'confidence': '0.9999021307587381',\n",
              "        'text': 'May'},\n",
              "       {'x': 995,\n",
              "        'y': 878,\n",
              "        'w': 103,\n",
              "        'h': 57,\n",
              "        'confidence': '0.6750254034996033',\n",
              "        'text': '521/'},\n",
              "       {'x': 1651,\n",
              "        'y': 909,\n",
              "        'w': 26,\n",
              "        'h': 31,\n",
              "        'confidence': '0.9999995231628986',\n",
              "        'text': '#'},\n",
              "       {'x': 1686,\n",
              "        'y': 903,\n",
              "        'w': 173,\n",
              "        'h': 50,\n",
              "        'confidence': '0.9671685634693481',\n",
              "        'text': 'Group by'},\n",
              "       {'x': 1869,\n",
              "        'y': 906,\n",
              "        'w': 108,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9913080837331406',\n",
              "        'text': 'month'},\n",
              "       {'x': 1988,\n",
              "        'y': 906,\n",
              "        'w': 66,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9899277687072754',\n",
              "        'text': 'and'},\n",
              "       {'x': 2069,\n",
              "        'y': 912,\n",
              "        'w': 62,\n",
              "        'h': 30,\n",
              "        'confidence': '0.7601930499076843',\n",
              "        'text': 'sum'},\n",
              "       {'x': 2146,\n",
              "        'y': 906,\n",
              "        'w': 108,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999279751615774',\n",
              "        'text': 'sales'},\n",
              "       {'x': 431,\n",
              "        'y': 961,\n",
              "        'w': 127,\n",
              "        'h': 56,\n",
              "        'confidence': '0.6494529987323368',\n",
              "        'text': 'June'},\n",
              "       {'x': 992,\n",
              "        'y': 961,\n",
              "        'w': 103,\n",
              "        'h': 56,\n",
              "        'confidence': '0.9999996558724309',\n",
              "        'text': '901'},\n",
              "       {'x': 1650,\n",
              "        'y': 957,\n",
              "        'w': 266,\n",
              "        'h': 45,\n",
              "        'confidence': '0.7842421263353325',\n",
              "        'text': 'monthly_sales'},\n",
              "       {'x': 1928,\n",
              "        'y': 963,\n",
              "        'w': 26,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9978067992508386',\n",
              "        'text': '='},\n",
              "       {'x': 1967,\n",
              "        'y': 953,\n",
              "        'w': 210,\n",
              "        'h': 50,\n",
              "        'confidence': '0.8308851746798424',\n",
              "        'text': 'df.groupby'},\n",
              "       {'x': 2204,\n",
              "        'y': 957,\n",
              "        'w': 111,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9999969339786576',\n",
              "        'text': 'Month'},\n",
              "       {'x': 2325,\n",
              "        'y': 953,\n",
              "        'w': 564,\n",
              "        'h': 50,\n",
              "        'confidence': '0.6548727867452127',\n",
              "        'text': \" [ 'Sales' ] . sum(  . reset_index\"},\n",
              "       {'x': 431,\n",
              "        'y': 1044,\n",
              "        'w': 109,\n",
              "        'h': 73,\n",
              "        'confidence': '0.8996173838283181',\n",
              "        'text': 'July'},\n",
              "       {'x': 964,\n",
              "        'y': 1040,\n",
              "        'w': 136,\n",
              "        'h': 64,\n",
              "        'confidence': '0.456758245268208',\n",
              "        'text': '1334]'},\n",
              "       {'x': 1689,\n",
              "        'y': 1057,\n",
              "        'w': 88,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999857544898987',\n",
              "        'text': 'Sort'},\n",
              "       {'x': 1788,\n",
              "        'y': 1057,\n",
              "        'w': 128,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999234461682571',\n",
              "        'text': 'months'},\n",
              "       {'x': 1927,\n",
              "        'y': 1057,\n",
              "        'w': 108,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999974943349246',\n",
              "        'text': 'based'},\n",
              "       {'x': 2046,\n",
              "        'y': 1061,\n",
              "        'w': 50,\n",
              "        'h': 36,\n",
              "        'confidence': '0.9996966404632243',\n",
              "        'text': 'on'},\n",
              "       {'x': 2107,\n",
              "        'y': 1057,\n",
              "        'w': 166,\n",
              "        'h': 40,\n",
              "        'confidence': '0.8868142003959861',\n",
              "        'text': 'calendar'},\n",
              "       {'x': 1649,\n",
              "        'y': 1107,\n",
              "        'w': 285,\n",
              "        'h': 49,\n",
              "        'confidence': '0.7247811960764724',\n",
              "        'text': 'monthly_sales ['},\n",
              "       {'x': 1946,\n",
              "        'y': 1108,\n",
              "        'w': 111,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9719580316647767',\n",
              "        'text': 'Month'},\n",
              "       {'x': 2070,\n",
              "        'y': 1112,\n",
              "        'w': 17,\n",
              "        'h': 31,\n",
              "        'confidence': '0.9254704002889866',\n",
              "        'text': '1'},\n",
              "       {'x': 2105,\n",
              "        'y': 1116,\n",
              "        'w': 26,\n",
              "        'h': 26,\n",
              "        'confidence': '0.7093282172900786',\n",
              "        'text': '='},\n",
              "       {'x': 2146,\n",
              "        'y': 1108,\n",
              "        'w': 305,\n",
              "        'h': 44,\n",
              "        'confidence': '0.8592975165104563',\n",
              "        'text': 'pd. Categorical('},\n",
              "       {'x': 427,\n",
              "        'y': 1123,\n",
              "        'w': 116,\n",
              "        'h': 78,\n",
              "        'confidence': '0.9537892101519285',\n",
              "        'text': 'July'},\n",
              "       {'x': 995,\n",
              "        'y': 1128,\n",
              "        'w': 100,\n",
              "        'h': 57,\n",
              "        'confidence': '0.9999981417113152',\n",
              "        'text': '131'},\n",
              "       {'x': 1730,\n",
              "        'y': 1159,\n",
              "        'w': 446,\n",
              "        'h': 45,\n",
              "        'confidence': '0.7078350743052692',\n",
              "        'text': \"monthly_sales [ 'Month' ]\"},\n",
              "       {'x': 962,\n",
              "        'y': 1210,\n",
              "        'w': 134,\n",
              "        'h': 61,\n",
              "        'confidence': '1.0',\n",
              "        'text': '2601'},\n",
              "       {'x': 1729,\n",
              "        'y': 1206,\n",
              "        'w': 329,\n",
              "        'h': 49,\n",
              "        'confidence': '0.7535657726891194',\n",
              "        'text': \"categories= [ ' Jan\"},\n",
              "       {'x': 2126,\n",
              "        'y': 1210,\n",
              "        'w': 72,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9999991052683532',\n",
              "        'text': 'Feb'},\n",
              "       {'x': 2265,\n",
              "        'y': 1210,\n",
              "        'w': 108,\n",
              "        'h': 40,\n",
              "        'confidence': '0.813309352413769',\n",
              "        'text': 'March'},\n",
              "       {'x': 2440,\n",
              "        'y': 1204,\n",
              "        'w': 130,\n",
              "        'h': 51,\n",
              "        'confidence': '0.9994682813739889',\n",
              "        'text': \"April'\"},\n",
              "       {'x': 2617,\n",
              "        'y': 1207,\n",
              "        'w': 79,\n",
              "        'h': 48,\n",
              "        'confidence': '0.7584798871236232',\n",
              "        'text': 'May \"'},\n",
              "       {'x': 2761,\n",
              "        'y': 1210,\n",
              "        'w': 88,\n",
              "        'h': 37,\n",
              "        'confidence': '0.8992589358566507',\n",
              "        'text': 'June'},\n",
              "       {'x': 2919,\n",
              "        'y': 1210,\n",
              "        'w': 91,\n",
              "        'h': 42,\n",
              "        'confidence': '0.9171257832017555',\n",
              "        'text': 'July'},\n",
              "       {'x': 1983,\n",
              "        'y': 1258,\n",
              "        'w': 77,\n",
              "        'h': 50,\n",
              "        'confidence': '0.8104181340016374',\n",
              "        'text': 'Aug \"'},\n",
              "       {'x': 2107,\n",
              "        'y': 1256,\n",
              "        'w': 110,\n",
              "        'h': 50,\n",
              "        'confidence': '0.993541185690036',\n",
              "        'text': \"'Sept\"},\n",
              "       {'x': 2270,\n",
              "        'y': 1258,\n",
              "        'w': 86,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9995055198669434',\n",
              "        'text': \"'Oct\"},\n",
              "       {'x': 2409,\n",
              "        'y': 1262,\n",
              "        'w': 80,\n",
              "        'h': 33,\n",
              "        'confidence': '0.9946573551765109',\n",
              "        'text': \"'Nov\"},\n",
              "       {'x': 2561,\n",
              "        'y': 1261,\n",
              "        'w': 126,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9128742919836034',\n",
              "        'text': \"Dec' ] ,\"},\n",
              "       {'x': 437,\n",
              "        'y': 1293,\n",
              "        'w': 107,\n",
              "        'h': 61,\n",
              "        'confidence': '0.9999785264700803',\n",
              "        'text': 'Dec'},\n",
              "       {'x': 962,\n",
              "        'y': 1293,\n",
              "        'w': 137,\n",
              "        'h': 61,\n",
              "        'confidence': '0.5938760741975789',\n",
              "        'text': '30051'},\n",
              "       {'x': 1730,\n",
              "        'y': 1309,\n",
              "        'w': 247,\n",
              "        'h': 40,\n",
              "        'confidence': '0.966733671458014',\n",
              "        'text': 'ordered-True'},\n",
              "       {'x': 1650,\n",
              "        'y': 1411,\n",
              "        'w': 266,\n",
              "        'h': 45,\n",
              "        'confidence': '0.9042375438574491',\n",
              "        'text': 'monthly_sales'},\n",
              "       {'x': 1931,\n",
              "        'y': 1423,\n",
              "        'w': 22,\n",
              "        'h': 19,\n",
              "        'confidence': '0.9945084355656633',\n",
              "        'text': '='},\n",
              "       {'x': 1964,\n",
              "        'y': 1406,\n",
              "        'w': 510,\n",
              "        'h': 54,\n",
              "        'confidence': '0.9758296089470004',\n",
              "        'text': 'monthly_sales. sort_values'},\n",
              "       {'x': 2501,\n",
              "        'y': 1409,\n",
              "        'w': 144,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9378642242023233',\n",
              "        'text': \"Month' \"},\n",
              "       {'x': 222,\n",
              "        'y': 1479,\n",
              "        'w': 435,\n",
              "        'h': 75,\n",
              "        'confidence': '0.878943156828182',\n",
              "        'text': 'Generated plot:'},\n",
              "       {'x': 1689,\n",
              "        'y': 1511,\n",
              "        'w': 88,\n",
              "        'h': 40,\n",
              "        'confidence': '0.999147891998291',\n",
              "        'text': 'Plot'},\n",
              "       {'x': 1788,\n",
              "        'y': 1511,\n",
              "        'w': 70,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999858219572908',\n",
              "        'text': 'bar'},\n",
              "       {'x': 1869,\n",
              "        'y': 1511,\n",
              "        'w': 108,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999981873912962',\n",
              "        'text': 'chart'},\n",
              "       {'x': 678,\n",
              "        'y': 1585,\n",
              "        'w': 252,\n",
              "        'h': 28,\n",
              "        'confidence': '0.8730038287891561',\n",
              "        'text': 'Total Sales per Month'},\n",
              "       {'x': 1650,\n",
              "        'y': 1562,\n",
              "        'w': 460,\n",
              "        'h': 45,\n",
              "        'confidence': '0.4109185573515574',\n",
              "        'text': 'plt.figure( figsize=( 10 ,'},\n",
              "       {'x': 2126,\n",
              "        'y': 1562,\n",
              "        'w': 65,\n",
              "        'h': 40,\n",
              "        'confidence': '0.999287081733639',\n",
              "        'text': '6) )'},\n",
              "       {'x': 207,\n",
              "        'y': 1636,\n",
              "        'w': 55,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9998063445091248',\n",
              "        'text': '3000'},\n",
              "       {'x': 1650,\n",
              "        'y': 1613,\n",
              "        'w': 618,\n",
              "        'h': 45,\n",
              "        'confidence': '0.7269521612266578',\n",
              "        'text': \"plt.bar(monthly_sales [ 'Month' ] ,\"},\n",
              "       {'x': 2284,\n",
              "        'y': 1613,\n",
              "        'w': 461,\n",
              "        'h': 45,\n",
              "        'confidence': '0.7652835623157632',\n",
              "        'text': \"monthly_sales [ 'Sales' ] ,\"},\n",
              "       {'x': 2761,\n",
              "        'y': 1613,\n",
              "        'w': 261,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9642847062319827',\n",
              "        'text': \"color='blue' \"},\n",
              "       {'x': 1649,\n",
              "        'y': 1660,\n",
              "        'w': 210,\n",
              "        'h': 50,\n",
              "        'confidence': '0.981032021858697',\n",
              "        'text': 'plt.xlabel'},\n",
              "       {'x': 1886,\n",
              "        'y': 1662,\n",
              "        'w': 113,\n",
              "        'h': 40,\n",
              "        'confidence': '0.999997121150701',\n",
              "        'text': 'Month'},\n",
              "       {'x': 1650,\n",
              "        'y': 1713,\n",
              "        'w': 346,\n",
              "        'h': 45,\n",
              "        'confidence': '0.9181211015356511',\n",
              "        'text': \"plt.ylabel( 'Total\"},\n",
              "       {'x': 2007,\n",
              "        'y': 1713,\n",
              "        'w': 142,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9599954329019388',\n",
              "        'text': \"Sales' \"},\n",
              "       {'x': 207,\n",
              "        'y': 1738,\n",
              "        'w': 55,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9999409317970276',\n",
              "        'text': '2500'},\n",
              "       {'x': 1650,\n",
              "        'y': 1764,\n",
              "        'w': 188,\n",
              "        'h': 45,\n",
              "        'confidence': '0.9938683596355227',\n",
              "        'text': 'plt.title'},\n",
              "       {'x': 1869,\n",
              "        'y': 1764,\n",
              "        'w': 224,\n",
              "        'h': 40,\n",
              "        'confidence': '0.7499051293116378',\n",
              "        'text': 'Total Sales'},\n",
              "       {'x': 2107,\n",
              "        'y': 1764,\n",
              "        'w': 220,\n",
              "        'h': 45,\n",
              "        'confidence': '0.9096064820340712',\n",
              "        'text': \"per Month' \"},\n",
              "       {'x': 207,\n",
              "        'y': 1838,\n",
              "        'w': 55,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9976677894592285',\n",
              "        'text': '2000'},\n",
              "       {'x': 1650,\n",
              "        'y': 1815,\n",
              "        'w': 169,\n",
              "        'h': 45,\n",
              "        'confidence': '0.7332466243324262',\n",
              "        'text': 'plt. show'},\n",
              "       {'x': 181,\n",
              "        'y': 1885,\n",
              "        'w': 19,\n",
              "        'h': 56,\n",
              "        'confidence': '0.6121551884373275',\n",
              "        'text': '3'},\n",
              "       {'x': 207,\n",
              "        'y': 1940,\n",
              "        'w': 55,\n",
              "        'h': 24,\n",
              "        'confidence': '0.5550395637795608',\n",
              "        'text': '1500'},\n",
              "       {'x': 177,\n",
              "        'y': 1938,\n",
              "        'w': 24,\n",
              "        'h': 52,\n",
              "        'confidence': '0.5174574482498464',\n",
              "        'text': '2'},\n",
              "       {'x': 207,\n",
              "        'y': 2040,\n",
              "        'w': 55,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9992749094963074',\n",
              "        'text': '1000'},\n",
              "       {'x': 219,\n",
              "        'y': 2139,\n",
              "        'w': 43,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9954513335566669',\n",
              "        'text': '500'},\n",
              "       {'x': 350,\n",
              "        'y': 2263,\n",
              "        'w': 38,\n",
              "        'h': 24,\n",
              "        'confidence': '0.9991668448241406',\n",
              "        'text': 'Jan'},\n",
              "       {'x': 474,\n",
              "        'y': 2261,\n",
              "        'w': 40,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9999927733243802',\n",
              "        'text': 'Feb'},\n",
              "       {'x': 586,\n",
              "        'y': 2261,\n",
              "        'w': 65,\n",
              "        'h': 26,\n",
              "        'confidence': '0.7591729262884082',\n",
              "        'text': 'March'},\n",
              "       {'x': 841,\n",
              "        'y': 2263,\n",
              "        'w': 50,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9998754262924194',\n",
              "        'text': 'June'},\n",
              "       {'x': 968,\n",
              "        'y': 2263,\n",
              "        'w': 45,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9995226860046387',\n",
              "        'text': 'July'},\n",
              "       {'x': 1218,\n",
              "        'y': 2261,\n",
              "        'w': 43,\n",
              "        'h': 26,\n",
              "        'confidence': '0.9999847207512171',\n",
              "        'text': 'Dec'},\n",
              "       {'x': 771,\n",
              "        'y': 2290,\n",
              "        'w': 67,\n",
              "        'h': 24,\n",
              "        'confidence': '0.9517245471784188',\n",
              "        'text': 'Month'},\n",
              "       {'x': 641,\n",
              "        'y': 259,\n",
              "        'w': 54,\n",
              "        'h': 82,\n",
              "        'confidence': '0.99369241305034',\n",
              "        'text': 'by'},\n",
              "       {'x': 2542,\n",
              "        'y': 498,\n",
              "        'w': 67,\n",
              "        'h': 52,\n",
              "        'confidence': '0.9999955951683134',\n",
              "        'text': 'May'},\n",
              "       {'x': 442,\n",
              "        'y': 1203,\n",
              "        'w': 96,\n",
              "        'h': 84,\n",
              "        'confidence': '0.9999896761798278',\n",
              "        'text': 'Aug'},\n",
              "       {'x': 720,\n",
              "        'y': 2258,\n",
              "        'w': 43,\n",
              "        'h': 33,\n",
              "        'confidence': '0.9089048504829407',\n",
              "        'text': 'May'},\n",
              "       {'x': 1092,\n",
              "        'y': 2259,\n",
              "        'w': 43,\n",
              "        'h': 32,\n",
              "        'confidence': '0.9987847208976746',\n",
              "        'text': 'Aug'}]},\n",
              "     {'name': 'img_p41_2.png',\n",
              "      'height': 1677,\n",
              "      'width': 3112,\n",
              "      'x': 72,\n",
              "      'y': 449.42674,\n",
              "      'original_width': 3112,\n",
              "      'original_height': 1677,\n",
              "      'ocr': [{'x': 206,\n",
              "        'y': 63,\n",
              "        'w': 365,\n",
              "        'h': 89,\n",
              "        'confidence': '0.7173779620360612',\n",
              "        'text': 'User input'},\n",
              "       {'x': 1758,\n",
              "        'y': 61,\n",
              "        'w': 339,\n",
              "        'h': 99,\n",
              "        'confidence': '0.9999957010176243',\n",
              "        'text': 'Response'},\n",
              "       {'x': 71,\n",
              "        'y': 263,\n",
              "        'w': 1121,\n",
              "        'h': 80,\n",
              "        'confidence': '0.9729644699789103',\n",
              "        'text': 'How many people say the European Union'},\n",
              "       {'x': 1652,\n",
              "        'y': 266,\n",
              "        'w': 1147,\n",
              "        'h': 77,\n",
              "        'confidence': '0.8752973300064123',\n",
              "        'text': 'The chart shows that 57% of people say the'},\n",
              "       {'x': 66,\n",
              "        'y': 333,\n",
              "        'w': 895,\n",
              "        'h': 91,\n",
              "        'confidence': '0.792331710483227',\n",
              "        'text': 'responded well to the pandemic?'},\n",
              "       {'x': 1652,\n",
              "        'y': 334,\n",
              "        'w': 1315,\n",
              "        'h': 88,\n",
              "        'confidence': '0.9537994881216033',\n",
              "        'text': 'European Union responded well to the pandemic'},\n",
              "       {'x': 1652,\n",
              "        'y': 415,\n",
              "        'w': 1378,\n",
              "        'h': 79,\n",
              "        'confidence': '0.9227472362903618',\n",
              "        'text': 'This is indicated by the green bar labeled \"Good\" for'},\n",
              "       {'x': 1649,\n",
              "        'y': 494,\n",
              "        'w': 1221,\n",
              "        'h': 76,\n",
              "        'confidence': '0.9192973521015405',\n",
              "        'text': 'the EU, which extends to a height of 57%. This'},\n",
              "       {'x': 252,\n",
              "        'y': 535,\n",
              "        'w': 949,\n",
              "        'h': 68,\n",
              "        'confidence': '0.588832646391042',\n",
              "        'text': \"Poor marks for America'$ response to\"},\n",
              "       {'x': 251,\n",
              "        'y': 603,\n",
              "        'w': 543,\n",
              "        'h': 55,\n",
              "        'confidence': '0.9908964473696216',\n",
              "        'text': 'coronavirus outbreak'},\n",
              "       {'x': 1648,\n",
              "        'y': 571,\n",
              "        'w': 1372,\n",
              "        'h': 81,\n",
              "        'confidence': '0.6841820721047641',\n",
              "        'text': 'suggests that a majority of the people surveyed had'},\n",
              "       {'x': 253,\n",
              "        'y': 685,\n",
              "        'w': 497,\n",
              "        'h': 53,\n",
              "        'confidence': '0.48357552052554875',\n",
              "        'text': '%6 who say each has done a'},\n",
              "       {'x': 792,\n",
              "        'y': 680,\n",
              "        'w': 375,\n",
              "        'h': 60,\n",
              "        'confidence': '0.6629150608486664',\n",
              "        'text': 'job dealing with the'},\n",
              "       {'x': 1656,\n",
              "        'y': 666,\n",
              "        'w': 30,\n",
              "        'h': 41,\n",
              "        'confidence': '0.9997227384136664',\n",
              "        'text': 'a'},\n",
              "       {'x': 1694,\n",
              "        'y': 644,\n",
              "        'w': 1077,\n",
              "        'h': 84,\n",
              "        'confidence': '0.9777579369347608',\n",
              "        'text': \"positive view of the EU's response to the\"},\n",
              "       {'x': 254,\n",
              "        'y': 740,\n",
              "        'w': 398,\n",
              "        'h': 43,\n",
              "        'confidence': '0.8766075643214496',\n",
              "        'text': 'coronavirus outbreak'},\n",
              "       {'x': 1649,\n",
              "        'y': 717,\n",
              "        'w': 292,\n",
              "        'h': 88,\n",
              "        'confidence': '0.999902048736947',\n",
              "        'text': 'pandemic'},\n",
              "       {'x': 799,\n",
              "        'y': 821,\n",
              "        'w': 79,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999950445645353',\n",
              "        'text': 'Bad'},\n",
              "       {'x': 925,\n",
              "        'y': 821,\n",
              "        'w': 101,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9998270869255066',\n",
              "        'text': 'Good'},\n",
              "       {'x': 261,\n",
              "        'y': 871,\n",
              "        'w': 208,\n",
              "        'h': 50,\n",
              "        'confidence': '0.9555852273924744',\n",
              "        'text': 'Our country'},\n",
              "       {'x': 801,\n",
              "        'y': 874,\n",
              "        'w': 86,\n",
              "        'h': 43,\n",
              "        'confidence': '0.1636164379430478',\n",
              "        'text': '2520'},\n",
              "       {'x': 1137,\n",
              "        'y': 874,\n",
              "        'w': 86,\n",
              "        'h': 40,\n",
              "        'confidence': '0.8047981124765582',\n",
              "        'text': '7496'},\n",
              "       {'x': 373,\n",
              "        'y': 952,\n",
              "        'w': 91,\n",
              "        'h': 38,\n",
              "        'confidence': '0.9939248408949845',\n",
              "        'text': 'WHO'},\n",
              "       {'x': 772,\n",
              "        'y': 952,\n",
              "        'w': 57,\n",
              "        'h': 42,\n",
              "        'confidence': '0.3570665419101715',\n",
              "        'text': '34'},\n",
              "       {'x': 1117,\n",
              "        'y': 952,\n",
              "        'w': 57,\n",
              "        'h': 40,\n",
              "        'confidence': '0.8435346436728003',\n",
              "        'text': '64'},\n",
              "       {'x': 410,\n",
              "        'y': 1030,\n",
              "        'w': 54,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9782828378657022',\n",
              "        'text': 'EU'},\n",
              "       {'x': 748,\n",
              "        'y': 1030,\n",
              "        'w': 59,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999939308592087',\n",
              "        'text': '39'},\n",
              "       {'x': 1086,\n",
              "        'y': 1030,\n",
              "        'w': 54,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9998586423965649',\n",
              "        'text': '57'},\n",
              "       {'x': 359,\n",
              "        'y': 1105,\n",
              "        'w': 105,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999890710298764',\n",
              "        'text': 'China'},\n",
              "       {'x': 653,\n",
              "        'y': 1108,\n",
              "        'w': 59,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9996728718357353',\n",
              "        'text': '60'},\n",
              "       {'x': 996,\n",
              "        'y': 1108,\n",
              "        'w': 54,\n",
              "        'h': 40,\n",
              "        'confidence': '0.6431979711287937',\n",
              "        'text': '37'},\n",
              "       {'x': 388,\n",
              "        'y': 1183,\n",
              "        'w': 69,\n",
              "        'h': 40,\n",
              "        'confidence': '0.8555225097983744',\n",
              "        'text': 'US'},\n",
              "       {'x': 544,\n",
              "        'y': 1185,\n",
              "        'w': 59,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9999972183091315',\n",
              "        'text': '84'},\n",
              "       {'x': 913,\n",
              "        'y': 1185,\n",
              "        'w': 57,\n",
              "        'h': 40,\n",
              "        'confidence': '0.9853182195762988',\n",
              "        'text': '15'},\n",
              "       {'x': 252,\n",
              "        'y': 1273,\n",
              "        'w': 994,\n",
              "        'h': 48,\n",
              "        'confidence': '0.7563140248213207',\n",
              "        'text': 'Note: Percentages are medians hased on 13 countries Those who'},\n",
              "       {'x': 254,\n",
              "        'y': 1319,\n",
              "        'w': 446,\n",
              "        'h': 38,\n",
              "        'confidence': '0.37794979661396966',\n",
              "        'text': 'did not answer are not shcren'},\n",
              "       {'x': 254,\n",
              "        'y': 1362,\n",
              "        'w': 828,\n",
              "        'h': 44,\n",
              "        'confidence': '0.30338914627186014',\n",
              "        'text': 'Source: Summer 2020 Global Attitudes Survey. Q10a-e'},\n",
              "       {'x': 265,\n",
              "        'y': 1402,\n",
              "        'w': 926,\n",
              "        'h': 52,\n",
              "        'confidence': '0.7289514039887456',\n",
              "        'text': 'US. Image Plummets Internationally as Most Say Country Has'},\n",
              "       {'x': 254,\n",
              "        'y': 1450,\n",
              "        'w': 422,\n",
              "        'h': 43,\n",
              "        'confidence': '0.9334844829657865',\n",
              "        'text': 'Handled Coronavirus Badly\"'},\n",
              "       {'x': 254,\n",
              "        'y': 1514,\n",
              "        'w': 385,\n",
              "        'h': 37,\n",
              "        'confidence': '0.9171970273091365',\n",
              "        'text': 'PEW RESEARCH CENTER'}]}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'User input',\n",
              "      'md': '# User input',\n",
              "      'bBox': {'x': 103, 'y': 82, 'w': 55, 'h': 551}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Group this sales data by month using pandas and then create a bar chart of the sales per month:\\n\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nData\\nMonth           Sales\\nJan                     1523.13\\nJan                         957\\nFeb                         133\\nMarch                       452\\nMay                        521\\nJune                        901\\nJuly                      1334\\nJuly                        131\\nAug                        2601\\nDec                      3005\\nCreate DataFrame\\ndf= pd.DataFrame(data)',\n",
              "      'md': 'Group this sales data by month using pandas and then create a bar chart of the sales per month:\\n\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nData\\nMonth           Sales\\nJan                     1523.13\\nJan                         957\\nFeb                         133\\nMarch                       452\\nMay                        521\\nJune                        901\\nJuly                      1334\\nJuly                        131\\nAug                        2601\\nDec                      3005\\nCreate DataFrame\\ndf= pd.DataFrame(data)',\n",
              "      'bBox': {'x': 82, 'y': 109, 'w': 443, 'h': 525}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Group by month and sum sales',\n",
              "      'md': '# Group by month and sum sales',\n",
              "      'bBox': {'x': 138, 'y': 141, 'w': 235, 'h': 187}},\n",
              "     {'type': 'text',\n",
              "      'value': \"monthly_sales = df.groupby('Month')['Sales'].sum().reset_index()\",\n",
              "      'md': \"monthly_sales = df.groupby('Month')['Sales'].sum().reset_index()\",\n",
              "      'bBox': {'x': 138, 'y': 141, 'w': 332, 'h': 187}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Sort months based on calendar',\n",
              "      'md': '# Sort months based on calendar',\n",
              "      'bBox': {'x': 138, 'y': 141, 'w': 276, 'h': 187}},\n",
              "     {'type': 'text',\n",
              "      'value': \"monthly_sales['Month'] = pd.Categorical(monthly_sales['Month'],\\ncategories=['Jan', 'Feb', 'March', 'April', 'May', 'June', 'July', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec'],\\nordered=True)\\nmonthly_sales = monthly_sales.sort_values('Month')\\nPlot barchart\\nplt.figure(figsize=(10, 6))\\nplt.bar(monthly_sales['Month'], monthly_sales['Sales'], color='blue')\\nplt.xlabel('Month')\\nplt.ylabel('Total Sales')\\nplt.title('Total Sales per Month')\\nplt.show()\",\n",
              "      'md': \"monthly_sales['Month'] = pd.Categorical(monthly_sales['Month'],\\ncategories=['Jan', 'Feb', 'March', 'April', 'May', 'June', 'July', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec'],\\nordered=True)\\nmonthly_sales = monthly_sales.sort_values('Month')\\nPlot barchart\\nplt.figure(figsize=(10, 6))\\nplt.bar(monthly_sales['Month'], monthly_sales['Sales'], color='blue')\\nplt.xlabel('Month')\\nplt.ylabel('Total Sales')\\nplt.title('Total Sales per Month')\\nplt.show()\",\n",
              "      'bBox': {'x': 125, 'y': 141, 'w': 401, 'h': 276}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Generated plot:',\n",
              "      'md': '# Generated plot:',\n",
              "      'bBox': {'x': 105, 'y': 295, 'w': 65, 'h': 11}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Total Sales per Month',\n",
              "      'md': '# Total Sales per Month',\n",
              "      'bBox': {'x': 138, 'y': 141, 'w': 235, 'h': 187}},\n",
              "     {'type': 'table',\n",
              "      'rows': [['Month', 'Sales'],\n",
              "       ['Jan', '1523.13'],\n",
              "       ['Jan', '957'],\n",
              "       ['Feb', '133'],\n",
              "       ['March', '452'],\n",
              "       ['May', '521'],\n",
              "       ['June', '901'],\n",
              "       ['July', '1334'],\n",
              "       ['July', '131'],\n",
              "       ['Aug', '2601'],\n",
              "       ['Dec', '3005']],\n",
              "      'md': '|Month|Sales|\\n|Jan|1523.13|\\n|Jan|957|\\n|Feb|133|\\n|March|452|\\n|May|521|\\n|June|901|\\n|July|1334|\\n|July|131|\\n|Aug|2601|\\n|Dec|3005|',\n",
              "      'isPerfectTable': True,\n",
              "      'csv': '\"Month\",\"Sales\"\\n\"Jan\",\"1523.13\"\\n\"Jan\",\"957\"\\n\"Feb\",\"133\"\\n\"March\",\"452\"\\n\"May\",\"521\"\\n\"June\",\"901\"\\n\"July\",\"1334\"\\n\"July\",\"131\"\\n\"Aug\",\"2601\"\\n\"Dec\",\"3005\"',\n",
              "      'bBox': {'x': 82, 'y': 112, 'w': 448, 'h': 309}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'User input',\n",
              "      'md': '# User input',\n",
              "      'bBox': {'x': 103, 'y': 82, 'w': 55, 'h': 551}},\n",
              "     {'type': 'text',\n",
              "      'value': 'How many people say the European Union responded well to the pandemic?',\n",
              "      'md': 'How many people say the European Union responded well to the pandemic?',\n",
              "      'bBox': {'x': 82, 'y': 489, 'w': 436, 'h': 121}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Response',\n",
              "      'md': '# Response',\n",
              "      'bBox': {'x': 336, 'y': 82, 'w': 51, 'h': 392}},\n",
              "     {'type': 'text',\n",
              "      'value': 'The chart shows that 57% of people say the European Union responded well to the pandemic. This is indicated by the green bar labeled \"Good\" for the EU, which extends to a height of 57%. This suggests that a majority of the people surveyed had a positive view of the EU\\'s response to the pandemic.',\n",
              "      'md': 'The chart shows that 57% of people say the European Union responded well to the pandemic. This is indicated by the green bar labeled \"Good\" for the EU, which extends to a height of 57%. This suggests that a majority of the people surveyed had a positive view of the EU\\'s response to the pandemic.',\n",
              "      'bBox': {'x': 134, 'y': 82, 'w': 393, 'h': 528}},\n",
              "     {'type': 'table',\n",
              "      'rows': [[],\n",
              "       ['', 'Poor', 'Good'],\n",
              "       ['---', '---', '---'],\n",
              "       ['Our country', '2520', '7496'],\n",
              "       ['WHO', '34', '64'],\n",
              "       ['EU', '39', '57'],\n",
              "       ['China', '60', '37'],\n",
              "       ['US', '84', '15']],\n",
              "      'md': 'The chart shows that 57% of people say the European Union responded well to the pandemic. This is indicated by the green bar labeled \"Good\" for the EU, which extends to a height of 57%. This suggests that a majority of the people surveyed had a positive view of the EU\\'s response to the pandemic.\\n| |Poor|Good|\\n|---|---|---|\\n|Our country|2520|7496|\\n|WHO|34|64|\\n|EU|39|57|\\n|China|60|37|\\n|US|84|15|',\n",
              "      'isPerfectTable': False,\n",
              "      'csv': '\\n\"\",\"Poor\",\"Good\"\\n\"---\",\"---\",\"---\"\\n\"Our country\",\"2520\",\"7496\"\\n\"WHO\",\"34\",\"64\"\\n\"EU\",\"39\",\"57\"\\n\"China\",\"60\",\"37\"\\n\"US\",\"84\",\"15\"',\n",
              "      'bBox': {'x': 82, 'y': 34.04, 'w': 448, 'h': 716}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Note: Percentages are medians based on 13 countries. Those who did not answer are not shown.\\n\\nSource: Summer 2020 Global Attitudes Survey. Q10a-e\\n\\nUS. Image Plummets Internationally as Most Say Country Has Handled Coronavirus Badly\\n\\nPEW RESEARCH CENTER\\n\\nFigure 10: Created with Nova Lite. Image source: ChartQA dataset [50].',\n",
              "      'md': 'Note: Percentages are medians based on 13 countries. Those who did not answer are not shown.\\n\\nSource: Summer 2020 Global Attitudes Survey. Q10a-e\\n\\nUS. Image Plummets Internationally as Most Say Country Has Handled Coronavirus Badly\\n\\nPEW RESEARCH CENTER\\n\\nFigure 10: Created with Nova Lite. Image source: ChartQA dataset [50].',\n",
              "      'bBox': {'x': 99, 'y': 133, 'w': 352, 'h': 584.04}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False},\n",
              "   {'page': 43,\n",
              "    'text': '                            The Amazon Nova Family of Models\\nD   Correspondence and Contributors\\nPlease direct all correspondences to:\\n                         nova-technical-reportCamazon . com\\nThe Nova family of models were built by the Amazon AGI organization and partner teams.\\n                                         43',\n",
              "    'md': '# The Amazon Nova Family of Models\\n\\n# Correspondence and Contributors\\n\\nPlease direct all correspondences to:\\n\\nnova-technical-report@amazon.com\\n\\nThe Nova family of models were built by the Amazon AGI organization and partner teams.\\n\\n43',\n",
              "    'images': [{'name': 'img_p42_1.png',\n",
              "      'height': 108,\n",
              "      'width': 1538,\n",
              "      'x': 210.239,\n",
              "      'y': 118.69030000000001,\n",
              "      'original_width': 1538,\n",
              "      'original_height': 108,\n",
              "      'ocr': [{'x': 15,\n",
              "        'y': 12,\n",
              "        'w': 1335,\n",
              "        'h': 96,\n",
              "        'confidence': '0.8680535427172303',\n",
              "        'text': 'nova-technical-reportCamazon .'},\n",
              "       {'x': 1375,\n",
              "        'y': 31,\n",
              "        'w': 146,\n",
              "        'h': 62,\n",
              "        'confidence': '0.9942112019913054',\n",
              "        'text': 'com'}]}],\n",
              "    'charts': [],\n",
              "    'items': [{'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'The Amazon Nova Family of Models',\n",
              "      'md': '# The Amazon Nova Family of Models',\n",
              "      'bBox': {'x': 72, 'y': 34.04, 'w': 306, 'h': 48.01}},\n",
              "     {'type': 'heading',\n",
              "      'lvl': 1,\n",
              "      'value': 'Correspondence and Contributors',\n",
              "      'md': '# Correspondence and Contributors',\n",
              "      'bBox': {'x': 72, 'y': 70.04, 'w': 196, 'h': 12}},\n",
              "     {'type': 'text',\n",
              "      'value': 'Please direct all correspondences to:\\n\\nnova-technical-report@amazon.com\\n\\nThe Nova family of models were built by the Amazon AGI organization and partner teams.\\n\\n43',\n",
              "      'md': 'Please direct all correspondences to:\\n\\nnova-technical-report@amazon.com\\n\\nThe Nova family of models were built by the Amazon AGI organization and partner teams.\\n\\n43',\n",
              "      'bBox': {'x': 71, 'y': 70.04, 'w': 363, 'h': 679.99}}],\n",
              "    'status': 'OK',\n",
              "    'links': [],\n",
              "    'width': 612,\n",
              "    'height': 792,\n",
              "    'triggeredAutoMode': False,\n",
              "    'parsingMode': 'accurate',\n",
              "    'structuredData': None,\n",
              "    'noStructuredContent': False,\n",
              "    'noTextContent': False}],\n",
              "  'job_metadata': {'credits_used': 1659.0,\n",
              "   'job_credits_usage': 0,\n",
              "   'job_pages': 0,\n",
              "   'job_auto_mode_triggered_pages': 6,\n",
              "   'job_is_cache_hit': True,\n",
              "   'credits_max': 3000},\n",
              "  'job_id': '52fb78c8-3815-4cca-b12c-4c7c947d36ce',\n",
              "  'file_path': '/content/data/nova_technical_report.pdf'}]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDo_yzBSen1D",
        "outputId": "ec528e94-f8fb-40c6-c5dd-9cdbd9d59430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pages': [{'page': 1,\n",
              "   'text': '                     The Amazon Nova Family of Models:\\n                       Technical Report and Model Card\\n                                 Amazon Artificial General Intelligence\\n                       Nova\\n                       Lite                                                     Nova\\n                       Nova                <l>             <l >                 Micro     Ix          </>\\n                       Pro    AIx\\n</>\\n=\\n                      Nova                                                      Nova\\n                     Canvas                                                      Reel\\n                               Figure 1: The Amazon Nova family of models\\n                                               Abstract\\n       We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver\\n       frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable\\n       multimodal model with the best combination of accuracy, speed, and cost for a wide range of\\n       tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing\\n       images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our\\n       lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that\\n       creates professional grade images with rich customization controls. Amazon Nova Reel is a video\\n       generation model offering high-quality outputs, customization, and motion control. Our models\\n       were built responsibly and with a commitment to customer trust, security, and reliability. We report\\n       benchmarking results for core capabilities, agentic performance, long context, functional adaptation,\\n       runtime performance, and human evaluation.',\n",
              "   'md': '# The Amazon Nova Family of Models: Technical Report and Model Card\\n\\n# Amazon Artificial General Intelligence\\n\\n# Abstract\\n\\nWe present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.\\n\\n# Figure 1: The Amazon Nova family of models',\n",
              "   'images': [{'name': 'img_p0_1.png',\n",
              "     'height': 1540,\n",
              "     'width': 1952,\n",
              "     'x': 72,\n",
              "     'y': 225.5598,\n",
              "     'original_width': 1952,\n",
              "     'original_height': 1540,\n",
              "     'ocr': [{'x': 945,\n",
              "       'y': 471,\n",
              "       'w': 240,\n",
              "       'h': 98,\n",
              "       'confidence': '0.9999932646751404',\n",
              "       'text': 'Nova'},\n",
              "      {'x': 974,\n",
              "       'y': 590,\n",
              "       'w': 180,\n",
              "       'h': 96,\n",
              "       'confidence': '0.9999980926513672',\n",
              "       'text': 'Lite'},\n",
              "      {'x': 946,\n",
              "       'y': 774,\n",
              "       'w': 241,\n",
              "       'h': 99,\n",
              "       'confidence': '0.9999923706054688',\n",
              "       'text': 'Nova'},\n",
              "      {'x': 1802,\n",
              "       'y': 860,\n",
              "       'w': 94,\n",
              "       'h': 50,\n",
              "       'confidence': '0.1643972396850586',\n",
              "       'text': '<l>'},\n",
              "      {'x': 987,\n",
              "       'y': 897,\n",
              "       'w': 158,\n",
              "       'h': 92,\n",
              "       'confidence': '0.9999774252650832',\n",
              "       'text': 'Pro'},\n",
              "      {'x': 1283,\n",
              "       'y': 931,\n",
              "       'w': 40,\n",
              "       'h': 42,\n",
              "       'confidence': '0.9993208608148194',\n",
              "       'text': 'A'},\n",
              "      {'x': 1307,\n",
              "       'y': 941,\n",
              "       'w': 60,\n",
              "       'h': 72,\n",
              "       'confidence': '0.463554484308421',\n",
              "       'text': 'Ix'},\n",
              "      {'x': 66,\n",
              "       'y': 1028,\n",
              "       'w': 94,\n",
              "       'h': 54,\n",
              "       'confidence': '0.31367990980962945',\n",
              "       'text': '</>'},\n",
              "      {'x': 61,\n",
              "       'y': 1313,\n",
              "       'w': 86,\n",
              "       'h': 116,\n",
              "       'confidence': '0.9766882835297253',\n",
              "       'text': '='}]},\n",
              "    {'name': 'img_p0_2.png',\n",
              "     'height': 554,\n",
              "     'width': 1928,\n",
              "     'x': 329.401,\n",
              "     'y': 278.37412,\n",
              "     'original_width': 1928,\n",
              "     'original_height': 554,\n",
              "     'ocr': [{'x': 905,\n",
              "       'y': 144,\n",
              "       'w': 236,\n",
              "       'h': 92,\n",
              "       'confidence': '0.9999958872795105',\n",
              "       'text': 'Nova'},\n",
              "      {'x': 894,\n",
              "       'y': 259,\n",
              "       'w': 261,\n",
              "       'h': 102,\n",
              "       'confidence': '0.6382414265111352',\n",
              "       'text': 'Micro'},\n",
              "      {'x': 1267,\n",
              "       'y': 317,\n",
              "       'w': 60,\n",
              "       'h': 72,\n",
              "       'confidence': '0.463554484308421',\n",
              "       'text': 'Ix'},\n",
              "      {'x': 58,\n",
              "       'y': 366,\n",
              "       'w': 94,\n",
              "       'h': 50,\n",
              "       'confidence': '0.16095946729183197',\n",
              "       'text': '<l >'},\n",
              "      {'x': 1776,\n",
              "       'y': 372,\n",
              "       'w': 98,\n",
              "       'h': 52,\n",
              "       'confidence': '0.17453782055061207',\n",
              "       'text': '</>'}]},\n",
              "    {'name': 'img_p0_3.png',\n",
              "     'height': 572,\n",
              "     'width': 1944,\n",
              "     'x': 72,\n",
              "     'y': 398.66996,\n",
              "     'original_width': 1944,\n",
              "     'original_height': 572,\n",
              "     'ocr': [{'x': 895,\n",
              "       'y': 125,\n",
              "       'w': 241,\n",
              "       'h': 98,\n",
              "       'confidence': '0.7646325282365729',\n",
              "       'text': 'Nova'},\n",
              "      {'x': 844,\n",
              "       'y': 244,\n",
              "       'w': 345,\n",
              "       'h': 104,\n",
              "       'confidence': '0.9997395603041859',\n",
              "       'text': 'Canvas'}]},\n",
              "    {'name': 'img_p0_4.png',\n",
              "     'height': 572,\n",
              "     'width': 1944,\n",
              "     'x': 329.401,\n",
              "     'y': 398.66996,\n",
              "     'original_width': 1944,\n",
              "     'original_height': 572,\n",
              "     'ocr': [{'x': 897,\n",
              "       'y': 127,\n",
              "       'w': 239,\n",
              "       'h': 97,\n",
              "       'confidence': '0.9999892711639404',\n",
              "       'text': 'Nova'},\n",
              "      {'x': 917,\n",
              "       'y': 249,\n",
              "       'w': 202,\n",
              "       'h': 92,\n",
              "       'confidence': '0.9999985098838806',\n",
              "       'text': 'Reel'}]}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models: Technical Report and Model Card',\n",
              "     'md': '# The Amazon Nova Family of Models: Technical Report and Model Card',\n",
              "     'bBox': {'x': 169, 'y': 96.78, 'w': 285, 'h': 326.22}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Amazon Artificial General Intelligence',\n",
              "     'md': '# Amazon Artificial General Intelligence',\n",
              "     'bBox': {'x': 223, 'y': 179.04, 'w': 165, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Abstract',\n",
              "     'md': '# Abstract',\n",
              "     'bBox': {'x': 283, 'y': 524.04, 'w': 44, 'h': 12}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.',\n",
              "     'md': 'We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.',\n",
              "     'bBox': {'x': 107, 'y': 276, 'w': 398, 'h': 393.04}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Figure 1: The Amazon Nova family of models',\n",
              "     'md': '# Figure 1: The Amazon Nova family of models',\n",
              "     'bBox': {'x': 169, 'y': 276, 'w': 285, 'h': 206.04}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 2,\n",
              "   'text': '                                   The Amazon Nova Family of Models\\nContents\\n1  Introduction                                                                                                        3\\n   1.1  Amazon Nova Pro, Lite, and Micro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       3\\n   1.2  Amazon Nova Canvas and Reel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        3\\n2  Amazon Nova Pro, Lite, and Micro Evaluations                                                                        5\\n   2.1  Core capability public benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      5\\n        2.1.1      Core capability text benchmarks and results . . . . . . . . . . . . . . . . . . . . . . . . . . .   5\\n        2.1.2        Core capability multimodal benchmarks and results . . . . . . . . . . . . . . . . . . . . . . .   7\\n   2.2  Agentic workflows    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   8\\n        2.2.1      Agentic text benchmarks and results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   9\\n        2.2.2        Agentic multimodal benchmarks and results . . . . . . . . . . . . . . . . . . . . . . . . . . .   9\\n   2.3  Long context   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  10\\n   2.4  Functional expertise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  11\\n        2.4.1     Software engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12\\n        2.4.2   Financial analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12\\n        2.4.3   Retrieval augmented generation     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12\\n   2.5  Runtime performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     13\\n3  Amazon Nova Canvas Evaluation                                                                                      15\\n   3.1  Automated metrics    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  15\\n   3.2  Human evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    15\\n4  Amazon Nova Reel Evaluation                                                                                        16\\n   4.1  Human evaluation metrics     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  16\\n   4.2  Dataset  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  16\\n   4.3  Implementation details & results   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\\n5  Responsible AI                                                                                                     17\\n   5.1  Defining our RAI objectives    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\\n   5.2  Ensuring adherence to RAI objectives     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  18\\n   5.3  RAI Evaluation   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19\\n   5.4  Red Teaming    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19\\n        5.4.1     Internal Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19\\n        5.4.2   External Red Teaming     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  20\\n        5.4.3        Automated Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21\\n6  Training Infrastructure                                                                                            21\\nA  Amazon Nova Canvas Capabilities                                                                                    28\\nB  Prompts and Scoring                                                                                                30\\nC  Qualitative examples of multimodal intelligence                                                                    39\\nD  Correspondence and Contributors                                                                                    43\\n                                                   2',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# Contents\\n\\n1. Introduction\\n2. 1. Amazon Nova Pro, Lite, and Micro\\n2. Amazon Nova Canvas and Reel\\n\\nAmazon Nova Pro, Lite, and Micro Evaluations\\n3. 1. Core capability public benchmarks\\n2. 1. Core capability text benchmarks and results\\n2. Core capability multimodal benchmarks and results\\n\\nAgentic workflows\\n3. 1. Agentic text benchmarks and results\\n2. Agentic multimodal benchmarks and results\\n\\nLong context\\n4. Functional expertise\\n5. 1. Software engineering\\n2. Financial analysis\\n3. Retrieval augmented generation\\n\\nRuntime performance\\n\\nAmazon Nova Canvas Evaluation\\n4. 1. Automated metrics\\n2. Human evaluation\\n\\nAmazon Nova Reel Evaluation\\n5. 1. Human evaluation metrics\\n2. Dataset\\n3. Implementation details & results\\n\\nResponsible AI\\n6. 1. Defining our RAI objectives\\n2. Ensuring adherence to RAI objectives\\n3. RAI Evaluation\\n4. Red Teaming\\n\\nTraining Infrastructure\\n7. A Amazon Nova Canvas Capabilities\\n8. B Prompts and Scoring\\n9. C Qualitative examples of multimodal intelligence\\n10. D Correspondence and Contributors',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 72, 'y': 34.04, 'w': 306, 'h': 541}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Contents',\n",
              "     'md': '# Contents',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 46, 'h': 483.99}},\n",
              "    {'type': 'text',\n",
              "     'value': '1. Introduction\\n2. 1. Amazon Nova Pro, Lite, and Micro\\n2. Amazon Nova Canvas and Reel\\n\\nAmazon Nova Pro, Lite, and Micro Evaluations\\n3. 1. Core capability public benchmarks\\n2. 1. Core capability text benchmarks and results\\n2. Core capability multimodal benchmarks and results\\n\\nAgentic workflows\\n3. 1. Agentic text benchmarks and results\\n2. Agentic multimodal benchmarks and results\\n\\nLong context\\n4. Functional expertise\\n5. 1. Software engineering\\n2. Financial analysis\\n3. Retrieval augmented generation\\n\\nRuntime performance\\n\\nAmazon Nova Canvas Evaluation\\n4. 1. Automated metrics\\n2. Human evaluation\\n\\nAmazon Nova Reel Evaluation\\n5. 1. Human evaluation metrics\\n2. Dataset\\n3. Implementation details & results\\n\\nResponsible AI\\n6. 1. Defining our RAI objectives\\n2. Ensuring adherence to RAI objectives\\n3. RAI Evaluation\\n4. Red Teaming\\n\\nTraining Infrastructure\\n7. A Amazon Nova Canvas Capabilities\\n8. B Prompts and Scoring\\n9. C Qualitative examples of multimodal intelligence\\n10. D Correspondence and Contributors',\n",
              "     'md': '1. Introduction\\n2. 1. Amazon Nova Pro, Lite, and Micro\\n2. Amazon Nova Canvas and Reel\\n\\nAmazon Nova Pro, Lite, and Micro Evaluations\\n3. 1. Core capability public benchmarks\\n2. 1. Core capability text benchmarks and results\\n2. Core capability multimodal benchmarks and results\\n\\nAgentic workflows\\n3. 1. Agentic text benchmarks and results\\n2. Agentic multimodal benchmarks and results\\n\\nLong context\\n4. Functional expertise\\n5. 1. Software engineering\\n2. Financial analysis\\n3. Retrieval augmented generation\\n\\nRuntime performance\\n\\nAmazon Nova Canvas Evaluation\\n4. 1. Automated metrics\\n2. Human evaluation\\n\\nAmazon Nova Reel Evaluation\\n5. 1. Human evaluation metrics\\n2. Dataset\\n3. Implementation details & results\\n\\nResponsible AI\\n6. 1. Defining our RAI objectives\\n2. Ensuring adherence to RAI objectives\\n3. RAI Evaluation\\n4. Red Teaming\\n\\nTraining Infrastructure\\n7. A Amazon Nova Canvas Capabilities\\n8. B Prompts and Scoring\\n9. C Qualitative examples of multimodal intelligence\\n10. D Correspondence and Contributors',\n",
              "     'bBox': {'x': 72, 'y': 93.04, 'w': 468, 'h': 657}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 3,\n",
              "   'text': '                                      The Amazon Nova Family of Models\\n1   Introduction\\nThis document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier\\nintelligence and industry-leading price performance.\\n1.1   Amazon Nova Pro, Lite, and Micro\\nKey capabilities of Amazon Nova Pro, Lite, and Micro include:\\n       •  Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and\\n          process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in\\n          its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning\\n          (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard).                 Our multimodal models,\\n          Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output.\\n          These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA),\\n          Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web)\\n          in their respective intelligence tiers. These models are the first to offer video understanding capabilities on\\n          Amazon Bedrock, enabling deeper insights from multimedia content.\\n       •  Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one\\n          of the fastest models in their respective intelligence tiers.\\n       •  Agentic Workflows:        Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down\\n          and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel\\n          at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the\\n          developer’s data.\\n       •  Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro,\\n          Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also\\n          run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\\n       •  Price-Performance:    Each model was optimized to deliver exceptional price-performance value, offering\\n          state-of-the-art performance on key benchmarks at low cost.\\nAmazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series\\nof training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal\\ndata. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source\\ndatasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular\\nemphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian,\\nSimplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning\\nstages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and\\nreward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via\\nmethods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that\\nthe final models are aligned with human preferences in both quality and responsibility.\\n1.2   Amazon Nova Canvas and Reel\\n    Amazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images\\nand videos, for a wide range of applications such as advertising, marketing, and entertainment.\\nAmazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\\n       •  Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to\\n          2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M\\n          pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or\\n          color palette, or to generate variations of an image.\\n       •  Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting\\n          through natural language mask prompts. These mask prompts describe the specific area of the input image that\\n          needs to be repainted. The user can also easily change a background with the background removal feature,\\n          leaving the subject of the image unchanged.\\n                                                        3',\n",
              "   'md': \"The Amazon Nova Family of Models\\n\\n# 1 Introduction\\n\\nThis document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance.\\n\\n## 1.1 Amazon Nova Pro, Lite, and Micro\\n\\nKey capabilities of Amazon Nova Pro, Lite, and Micro include:\\n\\n- Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard). Our multimodal models, Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output. These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA), Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web) in their respective intelligence tiers. These models are the first to offer video understanding capabilities on Amazon Bedrock, enabling deeper insights from multimedia content.\\n\\n- Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one of the fastest models in their respective intelligence tiers.\\n\\n- Agentic Workflows: Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the developer's data.\\n\\n- Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro, Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\\n\\n- Price-Performance: Each model was optimized to deliver exceptional price-performance value, offering state-of-the-art performance on key benchmarks at low cost.\\n\\nAmazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series of training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal data. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular emphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning stages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and reward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via methods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that the final models are aligned with human preferences in both quality and responsibility.\\n\\n## 1.2 Amazon Nova Canvas and Reel\\n\\nAmazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images and videos, for a wide range of applications such as advertising, marketing, and entertainment.\\n\\nAmazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\\n\\n- Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to 2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or color palette, or to generate variations of an image.\\n\\n- Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting through natural language mask prompts. These mask prompts describe the specific area of the input image that needs to be repainted. The user can also easily change a background with the background removal feature, leaving the subject of the image unchanged.\",\n",
              "   'images': [{'name': 'page_3.jpg',\n",
              "     'height': 792,\n",
              "     'width': 612,\n",
              "     'x': 0,\n",
              "     'y': 0,\n",
              "     'original_width': 1236,\n",
              "     'original_height': 1600,\n",
              "     'type': 'full_page_screenshot'}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'text',\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': 'The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '1 Introduction',\n",
              "     'md': '# 1 Introduction',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 82, 'h': 12}},\n",
              "    {'type': 'text',\n",
              "     'value': 'This document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance.',\n",
              "     'md': 'This document introduces Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance.',\n",
              "     'bBox': {'x': 71, 'y': 97.04, 'w': 468, 'h': 21}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 2,\n",
              "     'value': '1.1 Amazon Nova Pro, Lite, and Micro',\n",
              "     'md': '## 1.1 Amazon Nova Pro, Lite, and Micro',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 171, 'h': 74.99}},\n",
              "    {'type': 'text',\n",
              "     'value': \"Key capabilities of Amazon Nova Pro, Lite, and Micro include:\\n\\n- Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard). Our multimodal models, Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output. These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA), Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web) in their respective intelligence tiers. These models are the first to offer video understanding capabilities on Amazon Bedrock, enabling deeper insights from multimedia content.\\n\\n- Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one of the fastest models in their respective intelligence tiers.\\n\\n- Agentic Workflows: Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the developer's data.\\n\\n- Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro, Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\\n\\n- Price-Performance: Each model was optimized to deliver exceptional price-performance value, offering state-of-the-art performance on key benchmarks at low cost.\\n\\nAmazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series of training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal data. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular emphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning stages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and reward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via methods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that the final models are aligned with human preferences in both quality and responsibility.\",\n",
              "     'md': \"Key capabilities of Amazon Nova Pro, Lite, and Micro include:\\n\\n- Frontier intelligence: Amazon Nova models possess frontier intelligence, enabling them to understand and process complex language tasks with state-of-the-art accuracy. Amazon Nova Micro sets new standards in its intelligence tier in several text benchmarks such as Language Understanding (MMLU), Deep Reasoning (GPQA), Mathematics (MATH), and Multi-step Reasoning (Big-Bench Hard). Our multimodal models, Amazon Nova Pro and Lite, take text, images, documents, and video as input and generate text as output. These models set standards in several benchmarks such as Video Captioning (VATEX), Visual QA (TextVQA), Function Calling (BFCL), and multimodal agentic benchmarks (GroundUI-1K, VisualWebBench, Mind2Web) in their respective intelligence tiers. These models are the first to offer video understanding capabilities on Amazon Bedrock, enabling deeper insights from multimedia content.\\n\\n- Speed: Amazon Nova has been designed for fast inference, with Amazon Micro, Lite, and Pro each being one of the fastest models in their respective intelligence tiers.\\n\\n- Agentic Workflows: Amazon Nova Pro, Lite, and Micro can power AI agents capable of breaking down and executing multi-step tasks. These models are integrated with Bedrock Knowledge Bases and they excel at retrieval-augmented generation (RAG) to ensure the best accuracy by grounding their responses to the developer's data.\\n\\n- Customizability: Developers can fine-tune these models with multimodal data (Pro and Lite) or text data (Pro, Lite, and Micro), providing the flexibility to achieve desired accuracy, latency, and cost. Developers can also run self-service Custom Fine-Tuning (CFT) and distillation of larger models to smaller ones via Bedrock APIs.\\n\\n- Price-Performance: Each model was optimized to deliver exceptional price-performance value, offering state-of-the-art performance on key benchmarks at low cost.\\n\\nAmazon Nova Pro, Lite, and Micro are based on the Transformer architecture [74]. Each model went through a series of training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal data. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular emphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish, and Turkish. After pretraining, models iteratively went through a series of fine-tuning stages, including Supervised Fine-Tuning (SFT) on instruction-demonstration pairs (including multimodal ones) and reward model (RM) training from human preference data [59]. Finally, the models learned from human preferences via methods like Direct Preference Optimization (DPO) [62] and Proximal Policy Optimization (PPO) [68] to ensure that the final models are aligned with human preferences in both quality and responsibility.\",\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 470, 'h': 471.99}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 2,\n",
              "     'value': '1.2 Amazon Nova Canvas and Reel',\n",
              "     'md': '## 1.2 Amazon Nova Canvas and Reel',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 155, 'h': 497.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Amazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images and videos, for a wide range of applications such as advertising, marketing, and entertainment.\\n\\nAmazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\\n\\n- Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to 2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or color palette, or to generate variations of an image.\\n\\n- Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting through natural language mask prompts. These mask prompts describe the specific area of the input image that needs to be repainted. The user can also easily change a background with the background removal feature, leaving the subject of the image unchanged.',\n",
              "     'md': 'Amazon Nova Canvas and Amazon Nova Reel are designed to create realistic multimodal content, including images and videos, for a wide range of applications such as advertising, marketing, and entertainment.\\n\\nAmazon Nova Canvas offers the following functionalities, with more details provided in Appendix A:\\n\\n- Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to 2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or color palette, or to generate variations of an image.\\n\\n- Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting through natural language mask prompts. These mask prompts describe the specific area of the input image that needs to be repainted. The user can also easily change a background with the background removal feature, leaving the subject of the image unchanged.',\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 469, 'h': 649.99}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': True,\n",
              "   'parsingMode': 'premium',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 4,\n",
              "   'text': '                                            The Amazon Nova Family of Models\\n      Amazon Nova Reel offers the following functionalities:\\n             • Generate videos from a text prompt: Amazon Nova Reel can generate high-quality videos of 6-second duration\\n               (720p resolution at 24 frames per second) from a text prompt.\\n             •          Generate videos from a reference image and a prompt: Amazon Nova Reel brings images to motion and\\n               generates videos that are guided by the input image and a text prompt.\\n             •    Camera motion control using a text prompt: With camera motion control in Amazon Nova Reel, the user can\\n                 guide camera motion with text prompts like “zoom” and “dolly forward” to get the exact visual needed for\\n                 each video. Amazon Nova Reel supports more than 20 camera motions. For more details, please refer to our\\n               prompting guide1.\\n             Amazon Nova Canvas and Reel are latent diffusion models [61] where a Variational AutoEncoder (VAE) [41] maps\\n     the image or video frames to latent variables on which the diffusion process happens. A text encoder tokenizes input\\n     text prompts into tokens which are then passed to the diffusion model as a conditioning signal. At inference time, a\\n     latent variable is initialized with random noise sampled from a Gaussian distribution, which is then denoised by the\\n trained diffusion model iteratively into a clean latent variable. The clean latent variable is decoded back to images or\\n      video frames by the decoder of the VAE. Both models underwent a two-phased approach of pretraining and fine-tuning.\\nPretraining data were sourced from a variety of sources, including licensed data, proprietary data, open source datasets,\\n      and publicly available data where appropriate.    Our highly scalable data filtering, deduplication, and enrichment\\n      pipelines were based on AWS EMR [2] and AWS Batch [1], as well as other AWS services.\\n         1https://docs.aws.amazon.com/nova/latest/userguide\\n                                                              4',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\nAmazon Nova Reel offers the following functionalities:\\n\\n- Generate videos from a text prompt: Amazon Nova Reel can generate high-quality videos of 6-second duration (720p resolution at 24 frames per second) from a text prompt.\\n- Generate videos from a reference image and a prompt: Amazon Nova Reel brings images to motion and generates videos that are guided by the input image and a text prompt.\\n- Camera motion control using a text prompt: With camera motion control in Amazon Nova Reel, the user can guide camera motion with text prompts like “zoom” and “dolly forward” to get the exact visual needed for each video. Amazon Nova Reel supports more than 20 camera motions. For more details, please refer to our prompting guide1.\\n\\nAmazon Nova Canvas and Reel are latent diffusion models[61] where a Variational AutoEncoder (VAE)[41] maps the image or video frames to latent variables on which the diffusion process happens. A text encoder tokenizes input text prompts into tokens which are then passed to the diffusion model as a conditioning signal. At inference time, a latent variable is initialized with random noise sampled from a Gaussian distribution, which is then denoised by the trained diffusion model iteratively into a clean latent variable. The clean latent variable is decoded back to images or video frames by the decoder of the VAE. Both models underwent a two-phased approach of pretraining and fine-tuning. Pretraining data were sourced from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. Our highly scalable data filtering, deduplication, and enrichment pipelines were based on AWS EMR[2] and AWS Batch[1], as well as other AWS services.\\n\\n1https://docs.aws.amazon.com/nova/latest/userguide',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Amazon Nova Reel offers the following functionalities:\\n\\n- Generate videos from a text prompt: Amazon Nova Reel can generate high-quality videos of 6-second duration (720p resolution at 24 frames per second) from a text prompt.\\n- Generate videos from a reference image and a prompt: Amazon Nova Reel brings images to motion and generates videos that are guided by the input image and a text prompt.\\n- Camera motion control using a text prompt: With camera motion control in Amazon Nova Reel, the user can guide camera motion with text prompts like “zoom” and “dolly forward” to get the exact visual needed for each video. Amazon Nova Reel supports more than 20 camera motions. For more details, please refer to our prompting guide1.\\n\\nAmazon Nova Canvas and Reel are latent diffusion models[61] where a Variational AutoEncoder (VAE)[41] maps the image or video frames to latent variables on which the diffusion process happens. A text encoder tokenizes input text prompts into tokens which are then passed to the diffusion model as a conditioning signal. At inference time, a latent variable is initialized with random noise sampled from a Gaussian distribution, which is then denoised by the trained diffusion model iteratively into a clean latent variable. The clean latent variable is decoded back to images or video frames by the decoder of the VAE. Both models underwent a two-phased approach of pretraining and fine-tuning. Pretraining data were sourced from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. Our highly scalable data filtering, deduplication, and enrichment pipelines were based on AWS EMR[2] and AWS Batch[1], as well as other AWS services.\\n\\n1https://docs.aws.amazon.com/nova/latest/userguide',\n",
              "     'md': 'Amazon Nova Reel offers the following functionalities:\\n\\n- Generate videos from a text prompt: Amazon Nova Reel can generate high-quality videos of 6-second duration (720p resolution at 24 frames per second) from a text prompt.\\n- Generate videos from a reference image and a prompt: Amazon Nova Reel brings images to motion and generates videos that are guided by the input image and a text prompt.\\n- Camera motion control using a text prompt: With camera motion control in Amazon Nova Reel, the user can guide camera motion with text prompts like “zoom” and “dolly forward” to get the exact visual needed for each video. Amazon Nova Reel supports more than 20 camera motions. For more details, please refer to our prompting guide1.\\n\\nAmazon Nova Canvas and Reel are latent diffusion models[61] where a Variational AutoEncoder (VAE)[41] maps the image or video frames to latent variables on which the diffusion process happens. A text encoder tokenizes input text prompts into tokens which are then passed to the diffusion model as a conditioning signal. At inference time, a latent variable is initialized with random noise sampled from a Gaussian distribution, which is then denoised by the trained diffusion model iteratively into a clean latent variable. The clean latent variable is decoded back to images or video frames by the decoder of the VAE. Both models underwent a two-phased approach of pretraining and fine-tuning. Pretraining data were sourced from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. Our highly scalable data filtering, deduplication, and enrichment pipelines were based on AWS EMR[2] and AWS Batch[1], as well as other AWS services.\\n\\n1https://docs.aws.amazon.com/nova/latest/userguide',\n",
              "     'bBox': {'x': 71, 'y': 72.04, 'w': 470, 'h': 678}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://docs.aws.amazon.com/nova/latest/userguide',\n",
              "     'text': 'https://docs.aws.amazon.com/nova/latest/userguide'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 5,\n",
              "   'text': '                                           The Amazon Nova Family of Models\\n     2   Amazon Nova Pro, Lite, and Micro Evaluations\\n     In this section, we report benchmarking results for Amazon Nova models and for select publicly-available models,\\n     including by citing existing public results and by measuring their performance.2 In cases for which the result is a\\n     simple average of binary scores, we assume a Gaussian distribution for the sample and approximate the 95% confidence\\n     interval as:                            CI(S) = 1.96 × r S × (1 − S)                                         (1)\\n                                                                     N\\n     where CI  is the 95% confidence interval, S is the measured score for the benchmark, and N is the sample size [48, 45].\\n     2.1  Core capability public benchmarks\\n     We evaluate Amazon Nova models on a suite of automated public benchmarks to assess core capabilities, including for\\n     both text-only (Section 2.1.1) and multimodal (Section 2.1.2) use cases.\\n     2.1.1  Core capability text benchmarks and results\\n        We evaluate select core capabilities of Amazon Nova models on a variety of public text-only benchmarks, spanning\\n     general knowledge, reasoning, language understanding, multilinguality, and instruction following.\\n         The following list briefly describes our selected text-only benchmarks. The prompts used for evaluation of each\\n     benchmark are summarized in Appendix B.1.\\n            •  MMLU [36]: Massive Multitask Language Understanding (MMLU) is a multiple-choice question answering\\n               benchmark that covers 57 subject areas across STEM, humanities, and social sciences. Subjects include law,\\n               physics, mathematics, computer science, history, and more. The difficulty levels vary from elementary level to\\n               advanced professional level, focusing on both world knowledge and problem solving abilities. We use 0-shot\\n               Chain-of-Thought (CoT) [79] for prompting and report the macro average exact match accuracy across all\\n               subjects.\\n            •  ARC-C [22]: The AI2’s Reasoning Challenge (ARC) is a multiple-choice question-answering dataset, which\\n               contains science questions from grade 3 to grade 9 exams. We use 0-shot CoT for prompting and report exact\\n               match accuracy.\\n            •  DROP [26]: Discrete Reasoning Over Paragraphs (DROP) is a crowdsourced reading comprehension dataset\\n               that requires reasoning and operating over multiple input positions from the reference text. We use 0-shot CoT\\n               for prompting and report f1 score.\\n            • GPQA [64]: Graduate-level Google-Proof Question and Answering (GPQA) is a challenging and high-quality\\n               multiple-choice question answering benchmark written by domain experts who have or are pursuing PhDs in\\n               biology, physics, and chemistry. We use 0-shot CoT for prompting and report exact match accuracy on the\\n               main set.\\n            •  MATH [37]: MATH is a mathematics problem solving benchmark, consisting of problems from mathematics\\n               competitions including the American Mathematics Competitions (AMC 10 and AMC 12), the American\\n               Invitational Mathematics Examination (AIME) and more. We use 0-shot CoT for prompting and report the\\n               exact match accuracy on the MATH5k set.\\n                     • GSM8K [23]: Grade School Math 8K (GSM8K) is a math benchmark consisting of 8,500 high-quality and\\n               diverse grade school math problems. The benchmark tests basic mathematical problem solving capabilities,\\n               requiring multi-step reasoning. We use 0-shot CoT for prompting and report the exact match accuracy on the\\n               test set containing 1,319 samples.\\n            •  IFEval [89]: IFeval is an instruction-following benchmark, which evaluates a model’s capability of following\\n        “verifiable instructions” such as “mention the keyword of AI at least 3 times”. The dataset contains 25 types of\\n               verifiable instructions and in total 541 prompts, where each prompt contains one or more verifiable instructions\\n               in natural language. We report the instruction-level accuracy under loose constraints.\\n            •  BBH [72]: Big Bench Hard (BBH) is a diverse benchmark consisting of an aggregate of 23 diverse subjects that\\n               cover algorithmic and NLP tasks ranging from casual logic tasks to word sorting and movie recommendations.\\n               The tasks are both multiple choice and open generation tasks. We report the macro average exact match\\n               accuracy across the subjects.\\n2Results measured internally by Amazon for evaluation purposes after Amazon Nova models completed training using (i) the\\n     Bedrock API for Claude and Meta models or (ii) the OpenAI API or Gemini API, as applicable.\\n                                                            5',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# 2 Amazon Nova Pro, Lite, and Micro Evaluations\\n\\nIn this section, we report benchmarking results for Amazon Nova models and for select publicly-available models, including by citing existing public results and by measuring their performance.2 In cases for which the result is a simple average of binary scores, we assume a Gaussian distribution for the sample and approximate the 95% confidence interval as:\\n\\nCI(S) = 1.96 × r S × (1 − S)\\nN\\n\\nwhere CI is the 95% confidence interval, S is the measured score for the benchmark, and N is the sample size [48, 45].\\n\\n# 2.1 Core capability public benchmarks\\n\\nWe evaluate Amazon Nova models on a suite of automated public benchmarks to assess core capabilities, including for both text-only (Section 2.1.1) and multimodal (Section 2.1.2) use cases.\\n\\n# 2.1.1 Core capability text benchmarks and results\\n\\nWe evaluate select core capabilities of Amazon Nova models on a variety of public text-only benchmarks, spanning general knowledge, reasoning, language understanding, multilinguality, and instruction following.\\n\\nThe following list briefly describes our selected text-only benchmarks. The prompts used for evaluation of each benchmark are summarized in Appendix B.1.\\n\\n- MMLU [36]: Massive Multitask Language Understanding (MMLU) is a multiple-choice question answering benchmark that covers 57 subject areas across STEM, humanities, and social sciences. Subjects include law, physics, mathematics, computer science, history, and more. The difficulty levels vary from elementary level to advanced professional level, focusing on both world knowledge and problem solving abilities. We use 0-shot Chain-of-Thought (CoT) [79] for prompting and report the macro average exact match accuracy across all subjects.\\n- ARC-C [22]: The AI2’s Reasoning Challenge (ARC) is a multiple-choice question-answering dataset, which contains science questions from grade 3 to grade 9 exams. We use 0-shot CoT for prompting and report exact match accuracy.\\n- DROP [26]: Discrete Reasoning Over Paragraphs (DROP) is a crowdsourced reading comprehension dataset that requires reasoning and operating over multiple input positions from the reference text. We use 0-shot CoT for prompting and report f1 score.\\n- GPQA [64]: Graduate-level Google-Proof Question and Answering (GPQA) is a challenging and high-quality multiple-choice question answering benchmark written by domain experts who have or are pursuing PhDs in biology, physics, and chemistry. We use 0-shot CoT for prompting and report exact match accuracy on the main set.\\n- MATH [37]: MATH is a mathematics problem solving benchmark, consisting of problems from mathematics competitions including the American Mathematics Competitions (AMC 10 and AMC 12), the American Invitational Mathematics Examination (AIME) and more. We use 0-shot CoT for prompting and report the exact match accuracy on the MATH5k set.\\n- GSM8K [23]: Grade School Math 8K (GSM8K) is a math benchmark consisting of 8,500 high-quality and diverse grade school math problems. The benchmark tests basic mathematical problem solving capabilities, requiring multi-step reasoning. We use 0-shot CoT for prompting and report the exact match accuracy on the test set containing 1,319 samples.\\n- IFEval [89]: IFeval is an instruction-following benchmark, which evaluates a model’s capability of following “verifiable instructions” such as “mention the keyword of AI at least 3 times”. The dataset contains 25 types of verifiable instructions and in total 541 prompts, where each prompt contains one or more verifiable instructions in natural language. We report the instruction-level accuracy under loose constraints.\\n- BBH [72]: Big Bench Hard (BBH) is a diverse benchmark consisting of an aggregate of 23 diverse subjects that cover algorithmic and NLP tasks ranging from casual logic tasks to word sorting and movie recommendations. The tasks are both multiple choice and open generation tasks. We report the macro average exact match accuracy across the subjects.\\n\\nResults measured internally by Amazon for evaluation purposes after Amazon Nova models completed training using (i) the Bedrock API for Claude and Meta models or (ii) the OpenAI API or Gemini API, as applicable.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 129}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2 Amazon Nova Pro, Lite, and Micro Evaluations',\n",
              "     'md': '# 2 Amazon Nova Pro, Lite, and Micro Evaluations',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 277, 'h': 92.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'In this section, we report benchmarking results for Amazon Nova models and for select publicly-available models, including by citing existing public results and by measuring their performance.2 In cases for which the result is a simple average of binary scores, we assume a Gaussian distribution for the sample and approximate the 95% confidence interval as:\\n\\nCI(S) = 1.96 × r S × (1 − S)\\nN\\n\\nwhere CI is the 95% confidence interval, S is the measured score for the benchmark, and N is the sample size [48, 45].',\n",
              "     'md': 'In this section, we report benchmarking results for Amazon Nova models and for select publicly-available models, including by citing existing public results and by measuring their performance.2 In cases for which the result is a simple average of binary scores, we assume a Gaussian distribution for the sample and approximate the 95% confidence interval as:\\n\\nCI(S) = 1.96 × r S × (1 − S)\\nN\\n\\nwhere CI is the 95% confidence interval, S is the measured score for the benchmark, and N is the sample size [48, 45].',\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 471, 'h': 679.99}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2.1 Core capability public benchmarks',\n",
              "     'md': '# 2.1 Core capability public benchmarks',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 277, 'h': 131.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We evaluate Amazon Nova models on a suite of automated public benchmarks to assess core capabilities, including for both text-only (Section 2.1.1) and multimodal (Section 2.1.2) use cases.',\n",
              "     'md': 'We evaluate Amazon Nova models on a suite of automated public benchmarks to assess core capabilities, including for both text-only (Section 2.1.1) and multimodal (Section 2.1.2) use cases.',\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 469, 'h': 185.99}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2.1.1 Core capability text benchmarks and results',\n",
              "     'md': '# 2.1.1 Core capability text benchmarks and results',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 277, 'h': 185.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We evaluate select core capabilities of Amazon Nova models on a variety of public text-only benchmarks, spanning general knowledge, reasoning, language understanding, multilinguality, and instruction following.\\n\\nThe following list briefly describes our selected text-only benchmarks. The prompts used for evaluation of each benchmark are summarized in Appendix B.1.\\n\\n- MMLU [36]: Massive Multitask Language Understanding (MMLU) is a multiple-choice question answering benchmark that covers 57 subject areas across STEM, humanities, and social sciences. Subjects include law, physics, mathematics, computer science, history, and more. The difficulty levels vary from elementary level to advanced professional level, focusing on both world knowledge and problem solving abilities. We use 0-shot Chain-of-Thought (CoT) [79] for prompting and report the macro average exact match accuracy across all subjects.\\n- ARC-C [22]: The AI2’s Reasoning Challenge (ARC) is a multiple-choice question-answering dataset, which contains science questions from grade 3 to grade 9 exams. We use 0-shot CoT for prompting and report exact match accuracy.\\n- DROP [26]: Discrete Reasoning Over Paragraphs (DROP) is a crowdsourced reading comprehension dataset that requires reasoning and operating over multiple input positions from the reference text. We use 0-shot CoT for prompting and report f1 score.\\n- GPQA [64]: Graduate-level Google-Proof Question and Answering (GPQA) is a challenging and high-quality multiple-choice question answering benchmark written by domain experts who have or are pursuing PhDs in biology, physics, and chemistry. We use 0-shot CoT for prompting and report exact match accuracy on the main set.\\n- MATH [37]: MATH is a mathematics problem solving benchmark, consisting of problems from mathematics competitions including the American Mathematics Competitions (AMC 10 and AMC 12), the American Invitational Mathematics Examination (AIME) and more. We use 0-shot CoT for prompting and report the exact match accuracy on the MATH5k set.\\n- GSM8K [23]: Grade School Math 8K (GSM8K) is a math benchmark consisting of 8,500 high-quality and diverse grade school math problems. The benchmark tests basic mathematical problem solving capabilities, requiring multi-step reasoning. We use 0-shot CoT for prompting and report the exact match accuracy on the test set containing 1,319 samples.\\n- IFEval [89]: IFeval is an instruction-following benchmark, which evaluates a model’s capability of following “verifiable instructions” such as “mention the keyword of AI at least 3 times”. The dataset contains 25 types of verifiable instructions and in total 541 prompts, where each prompt contains one or more verifiable instructions in natural language. We report the instruction-level accuracy under loose constraints.\\n- BBH [72]: Big Bench Hard (BBH) is a diverse benchmark consisting of an aggregate of 23 diverse subjects that cover algorithmic and NLP tasks ranging from casual logic tasks to word sorting and movie recommendations. The tasks are both multiple choice and open generation tasks. We report the macro average exact match accuracy across the subjects.\\n\\nResults measured internally by Amazon for evaluation purposes after Amazon Nova models completed training using (i) the Bedrock API for Claude and Meta models or (ii) the OpenAI API or Gemini API, as applicable.',\n",
              "     'md': 'We evaluate select core capabilities of Amazon Nova models on a variety of public text-only benchmarks, spanning general knowledge, reasoning, language understanding, multilinguality, and instruction following.\\n\\nThe following list briefly describes our selected text-only benchmarks. The prompts used for evaluation of each benchmark are summarized in Appendix B.1.\\n\\n- MMLU [36]: Massive Multitask Language Understanding (MMLU) is a multiple-choice question answering benchmark that covers 57 subject areas across STEM, humanities, and social sciences. Subjects include law, physics, mathematics, computer science, history, and more. The difficulty levels vary from elementary level to advanced professional level, focusing on both world knowledge and problem solving abilities. We use 0-shot Chain-of-Thought (CoT) [79] for prompting and report the macro average exact match accuracy across all subjects.\\n- ARC-C [22]: The AI2’s Reasoning Challenge (ARC) is a multiple-choice question-answering dataset, which contains science questions from grade 3 to grade 9 exams. We use 0-shot CoT for prompting and report exact match accuracy.\\n- DROP [26]: Discrete Reasoning Over Paragraphs (DROP) is a crowdsourced reading comprehension dataset that requires reasoning and operating over multiple input positions from the reference text. We use 0-shot CoT for prompting and report f1 score.\\n- GPQA [64]: Graduate-level Google-Proof Question and Answering (GPQA) is a challenging and high-quality multiple-choice question answering benchmark written by domain experts who have or are pursuing PhDs in biology, physics, and chemistry. We use 0-shot CoT for prompting and report exact match accuracy on the main set.\\n- MATH [37]: MATH is a mathematics problem solving benchmark, consisting of problems from mathematics competitions including the American Mathematics Competitions (AMC 10 and AMC 12), the American Invitational Mathematics Examination (AIME) and more. We use 0-shot CoT for prompting and report the exact match accuracy on the MATH5k set.\\n- GSM8K [23]: Grade School Math 8K (GSM8K) is a math benchmark consisting of 8,500 high-quality and diverse grade school math problems. The benchmark tests basic mathematical problem solving capabilities, requiring multi-step reasoning. We use 0-shot CoT for prompting and report the exact match accuracy on the test set containing 1,319 samples.\\n- IFEval [89]: IFeval is an instruction-following benchmark, which evaluates a model’s capability of following “verifiable instructions” such as “mention the keyword of AI at least 3 times”. The dataset contains 25 types of verifiable instructions and in total 541 prompts, where each prompt contains one or more verifiable instructions in natural language. We report the instruction-level accuracy under loose constraints.\\n- BBH [72]: Big Bench Hard (BBH) is a diverse benchmark consisting of an aggregate of 23 diverse subjects that cover algorithmic and NLP tasks ranging from casual logic tasks to word sorting and movie recommendations. The tasks are both multiple choice and open generation tasks. We report the macro average exact match accuracy across the subjects.\\n\\nResults measured internally by Amazon for evaluation purposes after Amazon Nova models completed training using (i) the Bedrock API for Claude and Meta models or (ii) the OpenAI API or Gemini API, as applicable.',\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 470, 'h': 679.99}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 6,\n",
              "   'text': '                                                 The Amazon Nova Family of Models\\n                                          MMLU   ARC-C        DROP   GPQA        MATH      GSM8k     IFEval      BBH\\n                                                                                                    instruction-\\n                               tok/     accuracy  accuracy  F1-score  accuracy accuracy   accuracy   level     accuracy\\n                                sec                                                                  loose\\n                                                                                                    accuracy\\n Nova Pro                      100        85.9   94.8         85.4    46.9       76.6       94.8      92.1       86.9\\n                                                 ±1.3      ±0.7       ±4.6       ±1.2       ±1.2      ±1.8\\n Nova Lite                     157        80.5   92.4         80.2    42.0       73.3       94.5      89.7       82.4\\n                                                 ±1.5         ±0.8    ±4.6       ±1.2       ±1.2      ±2.1\\n Nova Micro                    210        77.6   90.2         79.3    40.0       69.3       92.3      87.2       79.5\\n                                                 ±1.7         ±0.8    ±4.5       ±1.3       ±1.4      ±2.3\\n                                         0-shot  0-shot      6-shot   0-shot    0-shot     0-shot    0-shot     3-shot\\n                                        CoT                   CoT       CoT      CoT        CoT                CoT\\n Claude 3.5 Sonnet (Oct)        57        89.3   96.3ᴹ        88.3    58.0ᴹ      78.3     96.5ᴹ      90.2*       93.2\\n                                                 ±1.1         ±0.6    ±4.6       ±1.1       ±1.0      ±2.0\\n Claude 3.5 Haiku               64        80.3   90.9ᴹ     83.1       37.5ᴹ      69.4     93.8ᴹ      85.9*       86.6\\n                                                 ±1.6         ±0.8    ±4.5       ±1.3       ±1.3      ±2.4\\n                                         0-shot   25-shot    3-shot   0-shot    0-shot     0-shot    0-shot     3-shot\\n                                        CoT                           CoT        CoT        CoT                CoT\\n Gemini 1.5 Pro (002)           58        85.9   95.4ᴹ        74.9    55.1ᴹ      86.5       90.8     91.7ᴹ       89.2\\n                                                 ±1.2         ±0.9    ±4.6       ±0.9       ±1.6      ±1.9\\n Gemini 1.5 Flash (002)        190        78.9   94.3ᴹ        78.4    45.1ᴹ      77.9       86.2     91.6ᴹ       85.5\\n                                                 ±1.3         ±0.8    ±4.6       ±1.2       ±1.9      ±1.9\\n Gemini 1.5 Flash 8B (001)     283        68.1   88.7ᴹ     68.1ᴹ      33.5ᴹ      58.7     84.5ᴹ      86.1ᴹ       69.5\\n                                                 ±1.8         ±0.9    ±4.4       ±1.4       ±2.0      ±2.3\\n                                         5-shot   25-shot    3-shot   0-shot    4-shot    11-shot   0-shot      3-shot\\n GPT-4o                        163        88.7   96.2ᴹ        83.4    48.4ᴹ      76.6     92.6ᴹ      89.8ᴹ    83.0ᴹ\\n                                                 ±1.1      ±0.7       ±4.6       ±1.2       ±1.4      ±2.1\\n GPT-4o Mini                   113        82.0   92.3ᴹ        79.7    41.7ᴹ      70.2     86.4ᴹ      87.4ᴹ    81.0ᴹ\\n                                                 ±1.5         ±0.8    ±4.6       ±1.3       ±1.8      ±2.3\\n                                         0-shot   25-shot    3-shot   0-shot    0-shot     0-shot   0-shot      3-shot\\n                                                                                 CoT        CoT\\n Llama 3.2 90B                  40        86.0   94.8          -      46.7       68.0     95.1       90.9ᴹ        -\\n                                                 ±1.3                 ±4.6       ±1.3       ±1.2      ±2.0\\n Llama 3.2 11B                 124        73.0   83.4          -      32.8       51.9       84.5     85.0ᴹ        -\\n                                                 ±2.1                 ±4.3       ±1.4       ±2.0      ±2.4\\n Llama 3.1 8B                  157        73.0   83.4          -      30.4       51.9       84.5     85.0ᴹ        -\\n                                                 ±2.1                 ±4.3       ±1.4       ±2.0      ±2.4\\n                                         0-shot   25-shot      -      0-shot    0-shot     8-shot     -         -\\n                                        CoT                                      CoT        CoT\\nTable 1: Quantitative results on core capability benchmarks (MMLU [36], ARC-C [22], DROP [26], GPQA [64],\\nMATH [37]), GSM8K [23], IFEval [89] and BigBench-Hard (BBH) [72]). Unless otherwise noted, all reference\\nnumbers are taken from the original technical reports and websites for Claude models [14, 11], GPT4 models [58, 57],\\nLlama models [45] and Gemini models [32]. Results marked with M were measured by us2. Claude numbers for\\nIFEval (taken from [14]) are marked with an asterisk (∗), as the scoring methodology is unspecified in the report.\\nToken generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from\\nSection 2.5.\\n                                                         6',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n| |tok/ sec|accuracy|accuracy|F1-score|accuracy|accuracy|accuracy|instruction- level accuracy| |\\n|---|---|---|---|---|---|---|---|---|---|\\n|Nova Pro|100|85.9 ±1.3|94.8 ±0.7|85.4 ±4.6|46.9 ±1.2|76.6 ±1.2|94.8 ±1.8|92.1|86.9|\\n|Nova Lite|157|80.5 ±1.5|92.4 ±0.8|80.2 ±4.6|42.0 ±1.2|73.3 ±1.2|94.5 ±2.1|89.7|82.4|\\n|Nova Micro|210|77.6 ±1.7|90.2 ±0.8|79.3 ±4.5|40.0 ±1.3|69.3 ±1.4|92.3 ±2.3|87.2|79.5|\\n\\n| |0-shot|0-shot|6-shot|0-shot|0-shot|0-shot|0-shot|3-shot| |\\n|---|---|---|---|---|---|---|---|---|---|\\n|Claude 3.5 Sonnet (Oct)|57|89.3 ±1.1|96.3ᴹ ±0.6|88.3 ±4.6|58.0ᴹ ±1.1|78.3 ±1.0|96.5ᴹ ±2.0|90.2*|93.2|\\n|Claude 3.5 Haiku|64|80.3 ±1.6|90.9ᴹ ±0.8|83.1 ±4.5|37.5ᴹ ±1.3|69.4 ±1.3|93.8ᴹ ±2.4|85.9*|86.6|\\n\\n| |0-shot|25-shot|3-shot|0-shot|0-shot|0-shot|0-shot|3-shot| |\\n|---|---|---|---|---|---|---|---|---|---|\\n|Gemini 1.5 Pro (002)|58|85.9 ±1.2|95.4ᴹ ±0.9|74.9 ±4.6|55.1ᴹ ±0.9|86.5 ±1.6|90.8 ±1.9|91.7ᴹ|89.2|\\n|Gemini 1.5 Flash (002)|190|78.9 ±1.3|94.3ᴹ ±0.8|78.4 ±4.6|45.1ᴹ ±1.2|77.9 ±1.9|86.2 ±1.9|91.6ᴹ|85.5|\\n|Gemini 1.5 Flash 8B (001)|283|68.1 ±1.8|88.7ᴹ ±0.9|68.1ᴹ ±4.4|33.5ᴹ ±1.4|58.7 ±2.0|84.5ᴹ ±2.3|86.1ᴹ|69.5|\\n\\n| |5-shot|25-shot|3-shot|0-shot|4-shot|11-shot|0-shot|3-shot| |\\n|---|---|---|---|---|---|---|---|---|---|\\n|GPT-4o|163|88.7 ±1.1|96.2ᴹ ±0.7|83.4 ±4.6|48.4ᴹ ±1.2|76.6 ±1.4|92.6ᴹ ±2.1|89.8ᴹ|83.0ᴹ|\\n|GPT-4o Mini|113|82.0 ±1.5|92.3ᴹ ±0.8|79.7 ±4.6|41.7ᴹ ±1.3|70.2 ±1.8|86.4ᴹ ±2.3|87.4ᴹ|81.0ᴹ|\\n\\n| |0-shot|25-shot|3-shot|0-shot|0-shot|0-shot|0-shot|3-shot|\\n|---|---|---|---|---|---|---|---|---|\\n|Llama 3.2 90B|40|86.0 ±1.3|-|46.7|68.0 ±1.3|95.1 ±1.2|90.9ᴹ|-|\\n|Llama 3.2 11B|124|73.0 ±2.1|-|32.8|51.9 ±1.4|84.5 ±2.0|85.0ᴹ|-|\\n|Llama 3.1 8B|157|73.0 ±2.1|-|30.4|51.9 ±1.4|84.5 ±2.0|85.0ᴹ|-|\\n\\nTable 1: Quantitative results on core capability benchmarks (MMLU [36], ARC-C [22], DROP [26], GPQA [64], MATH [37]), GSM8K [23], IFEval [89] and BigBench-Hard (BBH) [72]). Unless otherwise noted, all reference numbers are taken from the original technical reports and websites for Claude models [14, 11], GPT4 models [58, 57], Llama models [45] and Gemini models [32]. Results marked with M were measured by us. Claude numbers for IFEval (taken from [14]) are marked with an asterisk (∗), as the scoring methodology is unspecified in the report. Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['',\n",
              "       'tok/ sec',\n",
              "       'accuracy',\n",
              "       'accuracy',\n",
              "       'F1-score',\n",
              "       'accuracy',\n",
              "       'accuracy',\n",
              "       'accuracy',\n",
              "       'instruction- level accuracy',\n",
              "       ''],\n",
              "      ['Nova Pro',\n",
              "       '100',\n",
              "       '85.9 ±1.3',\n",
              "       '94.8 ±0.7',\n",
              "       '85.4 ±4.6',\n",
              "       '46.9 ±1.2',\n",
              "       '76.6 ±1.2',\n",
              "       '94.8 ±1.8',\n",
              "       '92.1',\n",
              "       '86.9'],\n",
              "      ['Nova Lite',\n",
              "       '157',\n",
              "       '80.5 ±1.5',\n",
              "       '92.4 ±0.8',\n",
              "       '80.2 ±4.6',\n",
              "       '42.0 ±1.2',\n",
              "       '73.3 ±1.2',\n",
              "       '94.5 ±2.1',\n",
              "       '89.7',\n",
              "       '82.4'],\n",
              "      ['Nova Micro',\n",
              "       '210',\n",
              "       '77.6 ±1.7',\n",
              "       '90.2 ±0.8',\n",
              "       '79.3 ±4.5',\n",
              "       '40.0 ±1.3',\n",
              "       '69.3 ±1.4',\n",
              "       '92.3 ±2.3',\n",
              "       '87.2',\n",
              "       '79.5']],\n",
              "     'md': '| |tok/ sec|accuracy|accuracy|F1-score|accuracy|accuracy|accuracy|instruction- level accuracy| |\\n|Nova Pro|100|85.9 ±1.3|94.8 ±0.7|85.4 ±4.6|46.9 ±1.2|76.6 ±1.2|94.8 ±1.8|92.1|86.9|\\n|Nova Lite|157|80.5 ±1.5|92.4 ±0.8|80.2 ±4.6|42.0 ±1.2|73.3 ±1.2|94.5 ±2.1|89.7|82.4|\\n|Nova Micro|210|77.6 ±1.7|90.2 ±0.8|79.3 ±4.5|40.0 ±1.3|69.3 ±1.4|92.3 ±2.3|87.2|79.5|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"\",\"tok/ sec\",\"accuracy\",\"accuracy\",\"F1-score\",\"accuracy\",\"accuracy\",\"accuracy\",\"instruction- level accuracy\",\"\"\\n\"Nova Pro\",\"100\",\"85.9 ±1.3\",\"94.8 ±0.7\",\"85.4 ±4.6\",\"46.9 ±1.2\",\"76.6 ±1.2\",\"94.8 ±1.8\",\"92.1\",\"86.9\"\\n\"Nova Lite\",\"157\",\"80.5 ±1.5\",\"92.4 ±0.8\",\"80.2 ±4.6\",\"42.0 ±1.2\",\"73.3 ±1.2\",\"94.5 ±2.1\",\"89.7\",\"82.4\"\\n\"Nova Micro\",\"210\",\"77.6 ±1.7\",\"90.2 ±0.8\",\"79.3 ±4.5\",\"40.0 ±1.3\",\"69.3 ±1.4\",\"92.3 ±2.3\",\"87.2\",\"79.5\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '6-shot',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '3-shot',\n",
              "       ''],\n",
              "      ['Claude 3.5 Sonnet (Oct)',\n",
              "       '57',\n",
              "       '89.3 ±1.1',\n",
              "       '96.3ᴹ ±0.6',\n",
              "       '88.3 ±4.6',\n",
              "       '58.0ᴹ ±1.1',\n",
              "       '78.3 ±1.0',\n",
              "       '96.5ᴹ ±2.0',\n",
              "       '90.2*',\n",
              "       '93.2'],\n",
              "      ['Claude 3.5 Haiku',\n",
              "       '64',\n",
              "       '80.3 ±1.6',\n",
              "       '90.9ᴹ ±0.8',\n",
              "       '83.1 ±4.5',\n",
              "       '37.5ᴹ ±1.3',\n",
              "       '69.4 ±1.3',\n",
              "       '93.8ᴹ ±2.4',\n",
              "       '85.9*',\n",
              "       '86.6']],\n",
              "     'md': '| |0-shot|0-shot|6-shot|0-shot|0-shot|0-shot|0-shot|3-shot| |\\n|Claude 3.5 Sonnet (Oct)|57|89.3 ±1.1|96.3ᴹ ±0.6|88.3 ±4.6|58.0ᴹ ±1.1|78.3 ±1.0|96.5ᴹ ±2.0|90.2*|93.2|\\n|Claude 3.5 Haiku|64|80.3 ±1.6|90.9ᴹ ±0.8|83.1 ±4.5|37.5ᴹ ±1.3|69.4 ±1.3|93.8ᴹ ±2.4|85.9*|86.6|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"\",\"0-shot\",\"0-shot\",\"6-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"3-shot\",\"\"\\n\"Claude 3.5 Sonnet (Oct)\",\"57\",\"89.3 ±1.1\",\"96.3ᴹ ±0.6\",\"88.3 ±4.6\",\"58.0ᴹ ±1.1\",\"78.3 ±1.0\",\"96.5ᴹ ±2.0\",\"90.2*\",\"93.2\"\\n\"Claude 3.5 Haiku\",\"64\",\"80.3 ±1.6\",\"90.9ᴹ ±0.8\",\"83.1 ±4.5\",\"37.5ᴹ ±1.3\",\"69.4 ±1.3\",\"93.8ᴹ ±2.4\",\"85.9*\",\"86.6\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['',\n",
              "       '0-shot',\n",
              "       '25-shot',\n",
              "       '3-shot',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '3-shot',\n",
              "       ''],\n",
              "      ['Gemini 1.5 Pro (002)',\n",
              "       '58',\n",
              "       '85.9 ±1.2',\n",
              "       '95.4ᴹ ±0.9',\n",
              "       '74.9 ±4.6',\n",
              "       '55.1ᴹ ±0.9',\n",
              "       '86.5 ±1.6',\n",
              "       '90.8 ±1.9',\n",
              "       '91.7ᴹ',\n",
              "       '89.2'],\n",
              "      ['Gemini 1.5 Flash (002)',\n",
              "       '190',\n",
              "       '78.9 ±1.3',\n",
              "       '94.3ᴹ ±0.8',\n",
              "       '78.4 ±4.6',\n",
              "       '45.1ᴹ ±1.2',\n",
              "       '77.9 ±1.9',\n",
              "       '86.2 ±1.9',\n",
              "       '91.6ᴹ',\n",
              "       '85.5'],\n",
              "      ['Gemini 1.5 Flash 8B (001)',\n",
              "       '283',\n",
              "       '68.1 ±1.8',\n",
              "       '88.7ᴹ ±0.9',\n",
              "       '68.1ᴹ ±4.4',\n",
              "       '33.5ᴹ ±1.4',\n",
              "       '58.7 ±2.0',\n",
              "       '84.5ᴹ ±2.3',\n",
              "       '86.1ᴹ',\n",
              "       '69.5']],\n",
              "     'md': '| |0-shot|25-shot|3-shot|0-shot|0-shot|0-shot|0-shot|3-shot| |\\n|Gemini 1.5 Pro (002)|58|85.9 ±1.2|95.4ᴹ ±0.9|74.9 ±4.6|55.1ᴹ ±0.9|86.5 ±1.6|90.8 ±1.9|91.7ᴹ|89.2|\\n|Gemini 1.5 Flash (002)|190|78.9 ±1.3|94.3ᴹ ±0.8|78.4 ±4.6|45.1ᴹ ±1.2|77.9 ±1.9|86.2 ±1.9|91.6ᴹ|85.5|\\n|Gemini 1.5 Flash 8B (001)|283|68.1 ±1.8|88.7ᴹ ±0.9|68.1ᴹ ±4.4|33.5ᴹ ±1.4|58.7 ±2.0|84.5ᴹ ±2.3|86.1ᴹ|69.5|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"\",\"0-shot\",\"25-shot\",\"3-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"3-shot\",\"\"\\n\"Gemini 1.5 Pro (002)\",\"58\",\"85.9 ±1.2\",\"95.4ᴹ ±0.9\",\"74.9 ±4.6\",\"55.1ᴹ ±0.9\",\"86.5 ±1.6\",\"90.8 ±1.9\",\"91.7ᴹ\",\"89.2\"\\n\"Gemini 1.5 Flash (002)\",\"190\",\"78.9 ±1.3\",\"94.3ᴹ ±0.8\",\"78.4 ±4.6\",\"45.1ᴹ ±1.2\",\"77.9 ±1.9\",\"86.2 ±1.9\",\"91.6ᴹ\",\"85.5\"\\n\"Gemini 1.5 Flash 8B (001)\",\"283\",\"68.1 ±1.8\",\"88.7ᴹ ±0.9\",\"68.1ᴹ ±4.4\",\"33.5ᴹ ±1.4\",\"58.7 ±2.0\",\"84.5ᴹ ±2.3\",\"86.1ᴹ\",\"69.5\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['',\n",
              "       '5-shot',\n",
              "       '25-shot',\n",
              "       '3-shot',\n",
              "       '0-shot',\n",
              "       '4-shot',\n",
              "       '11-shot',\n",
              "       '0-shot',\n",
              "       '3-shot',\n",
              "       ''],\n",
              "      ['GPT-4o',\n",
              "       '163',\n",
              "       '88.7 ±1.1',\n",
              "       '96.2ᴹ ±0.7',\n",
              "       '83.4 ±4.6',\n",
              "       '48.4ᴹ ±1.2',\n",
              "       '76.6 ±1.4',\n",
              "       '92.6ᴹ ±2.1',\n",
              "       '89.8ᴹ',\n",
              "       '83.0ᴹ'],\n",
              "      ['GPT-4o Mini',\n",
              "       '113',\n",
              "       '82.0 ±1.5',\n",
              "       '92.3ᴹ ±0.8',\n",
              "       '79.7 ±4.6',\n",
              "       '41.7ᴹ ±1.3',\n",
              "       '70.2 ±1.8',\n",
              "       '86.4ᴹ ±2.3',\n",
              "       '87.4ᴹ',\n",
              "       '81.0ᴹ']],\n",
              "     'md': '| |5-shot|25-shot|3-shot|0-shot|4-shot|11-shot|0-shot|3-shot| |\\n|GPT-4o|163|88.7 ±1.1|96.2ᴹ ±0.7|83.4 ±4.6|48.4ᴹ ±1.2|76.6 ±1.4|92.6ᴹ ±2.1|89.8ᴹ|83.0ᴹ|\\n|GPT-4o Mini|113|82.0 ±1.5|92.3ᴹ ±0.8|79.7 ±4.6|41.7ᴹ ±1.3|70.2 ±1.8|86.4ᴹ ±2.3|87.4ᴹ|81.0ᴹ|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"\",\"5-shot\",\"25-shot\",\"3-shot\",\"0-shot\",\"4-shot\",\"11-shot\",\"0-shot\",\"3-shot\",\"\"\\n\"GPT-4o\",\"163\",\"88.7 ±1.1\",\"96.2ᴹ ±0.7\",\"83.4 ±4.6\",\"48.4ᴹ ±1.2\",\"76.6 ±1.4\",\"92.6ᴹ ±2.1\",\"89.8ᴹ\",\"83.0ᴹ\"\\n\"GPT-4o Mini\",\"113\",\"82.0 ±1.5\",\"92.3ᴹ ±0.8\",\"79.7 ±4.6\",\"41.7ᴹ ±1.3\",\"70.2 ±1.8\",\"86.4ᴹ ±2.3\",\"87.4ᴹ\",\"81.0ᴹ\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['',\n",
              "       '0-shot',\n",
              "       '25-shot',\n",
              "       '3-shot',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '0-shot',\n",
              "       '3-shot'],\n",
              "      ['Llama 3.2 90B',\n",
              "       '40',\n",
              "       '86.0 ±1.3',\n",
              "       '-',\n",
              "       '46.7',\n",
              "       '68.0 ±1.3',\n",
              "       '95.1 ±1.2',\n",
              "       '90.9ᴹ',\n",
              "       '-'],\n",
              "      ['Llama 3.2 11B',\n",
              "       '124',\n",
              "       '73.0 ±2.1',\n",
              "       '-',\n",
              "       '32.8',\n",
              "       '51.9 ±1.4',\n",
              "       '84.5 ±2.0',\n",
              "       '85.0ᴹ',\n",
              "       '-'],\n",
              "      ['Llama 3.1 8B',\n",
              "       '157',\n",
              "       '73.0 ±2.1',\n",
              "       '-',\n",
              "       '30.4',\n",
              "       '51.9 ±1.4',\n",
              "       '84.5 ±2.0',\n",
              "       '85.0ᴹ',\n",
              "       '-']],\n",
              "     'md': '| |0-shot|25-shot|3-shot|0-shot|0-shot|0-shot|0-shot|3-shot|\\n|Llama 3.2 90B|40|86.0 ±1.3|-|46.7|68.0 ±1.3|95.1 ±1.2|90.9ᴹ|-|\\n|Llama 3.2 11B|124|73.0 ±2.1|-|32.8|51.9 ±1.4|84.5 ±2.0|85.0ᴹ|-|\\n|Llama 3.1 8B|157|73.0 ±2.1|-|30.4|51.9 ±1.4|84.5 ±2.0|85.0ᴹ|-|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"\",\"0-shot\",\"25-shot\",\"3-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"0-shot\",\"3-shot\"\\n\"Llama 3.2 90B\",\"40\",\"86.0 ±1.3\",\"-\",\"46.7\",\"68.0 ±1.3\",\"95.1 ±1.2\",\"90.9ᴹ\",\"-\"\\n\"Llama 3.2 11B\",\"124\",\"73.0 ±2.1\",\"-\",\"32.8\",\"51.9 ±1.4\",\"84.5 ±2.0\",\"85.0ᴹ\",\"-\"\\n\"Llama 3.1 8B\",\"157\",\"73.0 ±2.1\",\"-\",\"30.4\",\"51.9 ±1.4\",\"84.5 ±2.0\",\"85.0ᴹ\",\"-\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 1: Quantitative results on core capability benchmarks (MMLU [36], ARC-C [22], DROP [26], GPQA [64], MATH [37]), GSM8K [23], IFEval [89] and BigBench-Hard (BBH) [72]). Unless otherwise noted, all reference numbers are taken from the original technical reports and websites for Claude models [14, 11], GPT4 models [58, 57], Llama models [45] and Gemini models [32]. Results marked with M were measured by us. Claude numbers for IFEval (taken from [14]) are marked with an asterisk (∗), as the scoring methodology is unspecified in the report. Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "     'md': 'Table 1: Quantitative results on core capability benchmarks (MMLU [36], ARC-C [22], DROP [26], GPQA [64], MATH [37]), GSM8K [23], IFEval [89] and BigBench-Hard (BBH) [72]). Unless otherwise noted, all reference numbers are taken from the original technical reports and websites for Claude models [14, 11], GPT4 models [58, 57], Llama models [45] and Gemini models [32]. Results marked with M were measured by us. Claude numbers for IFEval (taken from [14]) are marked with an asterisk (∗), as the scoring methodology is unspecified in the report. Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "     'bBox': {'x': 71, 'y': 106.03, 'w': 470, 'h': 644.01}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 7,\n",
              "   'text': '                                     The Amazon Nova Family of Models\\nTable 1 summarizes the quantitative results of Nova models and select public models on the aforementioned benchmarks\\nfor core capabilities. When available, we reference the highest publicly-reported numbers for each benchmark from the\\nofficial technical reports and websites for Claude, Gemini, OpenAI and Llama family of models. Amazon Nova Pro,\\nLite, and Micro demonstrate strong performance across all benchmarks, showcasing their advanced core intelligence,\\nparticularly Amazon Nova Micro and Lite on math, reasoning, and instruction following benchmarks.\\nWe also evaluate the translation capabilities of Nova models. Flores200 [73, 34, 35], or simply Flores, is a machine\\ntranslation benchmark consisting of translations from 842 distinct web articles, which tests the translation capabilities\\nbetween English and non-English languages. Sentences are 21 words long on average. We use a 0-shot setup and\\nreport the macro average of two metrics, spBleu and COMET22 score [63] across a set of languages (Arabic, German,\\nSpanish, French, Hindi, Italian, Japanese, Korean, Portuguese, Hebrew, Turkish, Simplified Chinese, Russian, Dutch)\\nfor translation from and into English. The prompts used for evaluation are summarized in Appendix B.1. Table 2\\nsummarizes our quantitative results on Flores, demonstrating strong multilingual performance on translation for Amazon\\nNova Pro, Lite, and Micro.\\n                                                                FLORES (0-shot)\\n                                                     en → Set1                      Set1 → en\\n                                  tok/sec     spBleu (↑)   COMET22 (↑)       spBleu (↑)    COMET22 (↑)\\n       Nova Pro                     100         43.4           89.1            44.4            89.0\\n       Nova Lite                    157         41.5           88.8            43.1            88.8\\n       Nova Micro                   210         40.2           88.5            42.6            88.7\\n       Claude 3.5 Sonnet (Oct)       57         42.5ᴹ          89.4ᴹ           43.5ᴹ           89.1ᴹ\\n       Claude 3.5 Haiku              64         40.0ᴹ          88.5ᴹ           40.2ᴹ           88.3ᴹ\\n       Gemini 1.5 Pro (002)          57         43.0M*         89.1M*          45.6M*          89.1M*\\n       Gemini 1.5 Flash (002)       190         40.0M*         88.5M*          42.9M*          88.8M*\\n       Gemini 1.5 Flash 8B (001)    283         38.2M*         88.0M*          41.4M*          88.5M*\\n       GPT-4o                       163         43.1M*         89.2M*          43.9M*          89.0M*\\n       GPT-4o Mini                  113         41.1M*         88.7M*          41.9M*          88.7M*\\n       Llama 3.2 90B                 40         39.7ᴹ          88.2ᴹ           43.7ᴹ           88.5ᴹ\\n       Llama 3.2 11B                124         33.0ᴹ          85.7ᴹ           36.3ᴹ           86.3ᴹ\\n       Llama 3.1 8B                 157         32.7ᴹ          85.5ᴹ           36.5ᴹ           86.5ᴹ\\nTable 2: Quantitative results on Flores200 [34], a machine translation benchmark. Set1 refers to {de, es, fr, it, pt, ja,\\nar, hi, ru, nl, tr, he, ko, zh}. Results marked with M were measured by us.2. Results marked with an asterisk (∗) were\\nobtained using an alternate prompt which can be found in Appendix B.1 Token generation speed in tokens per second\\n(tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n2.1.2  Core capability multimodal benchmarks and results\\nIn this section we evaluate the multimodal capabilities of Amazon Nova models on a diverse set of public benchmarks.\\nOur selection of multimodal benchmarks aims to probe for various capabilities, including natural image understanding,\\ndocument understanding with charts and graphs, text understanding, and temporal reasoning in videos.    For all\\nbenchmarks, we follow the suggested metrics and choice of data split for evaluation. The following list briefly describes\\nthe selected benchmarks.\\n      •    MMMU [85]: The Massive Multi-discipline Multimodal Understanding benchmark consists of college-level\\n          multiple-choice and open-ended questions from 30 different disciplines. We use Chain-of-Thought (CoT)\\n         prompting for this benchmark and report accuracy.\\n      •  ChartQA [50]: The 2,500 questions of this benchmark cover three different types of charts (bar, line and pie)\\n   and require strong visual, logical, and arithmetical reasoning capabilities. We evaluate on the test set and\\n         report relaxed accuracy.\\n      • DocVQA [51]: This benchmark probes capabilities on document analysis and recognition, including Optical\\n       Character Recognition (OCR). The 5,349 questions contain images from a diverse set of documents, ranging\\n                                                      7',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\nTable 1 summarizes the quantitative results of Nova models and select public models on the aforementioned benchmarks for core capabilities. When available, we reference the highest publicly-reported numbers for each benchmark from the official technical reports and websites for Claude, Gemini, OpenAI and Llama family of models. Amazon Nova Pro, Lite, and Micro demonstrate strong performance across all benchmarks, showcasing their advanced core intelligence, particularly Amazon Nova Micro and Lite on math, reasoning, and instruction following benchmarks.\\n\\nWe also evaluate the translation capabilities of Nova models. Flores200 [73, 34, 35], or simply Flores, is a machine translation benchmark consisting of translations from 842 distinct web articles, which tests the translation capabilities between English and non-English languages. Sentences are 21 words long on average. We use a 0-shot setup and report the macro average of two metrics, spBleu and COMET22 score [63] across a set of languages (Arabic, German, Spanish, French, Hindi, Italian, Japanese, Korean, Portuguese, Hebrew, Turkish, Simplified Chinese, Russian, Dutch) for translation from and into English. The prompts used for evaluation are summarized in Appendix B.1. Table 2 summarizes our quantitative results on Flores, demonstrating strong multilingual performance on translation for Amazon Nova Pro, Lite, and Micro.\\n\\n# FLORES (0-shot)\\n\\n| | |en → Set1| |Set1 → en| |\\n|---|---|---|---|---|---|\\n| |tok/sec|spBleu (↑)|COMET22 (↑)|spBleu (↑)|COMET22 (↑)|\\n|Nova Pro|100|43.4|89.1|44.4|89.0|\\n|Nova Lite|157|41.5|88.8|43.1|88.8|\\n|Nova Micro|210|40.2|88.5|42.6|88.7|\\n|Claude 3.5 Sonnet (Oct)|57|42.5ᴹ|89.4ᴹ|43.5ᴹ|89.1ᴹ|\\n|Claude 3.5 Haiku|64|40.0ᴹ|88.5ᴹ|40.2ᴹ|88.3ᴹ|\\n|Gemini 1.5 Pro (002)|57|43.0M*|89.1M*|45.6M*|89.1M*|\\n|Gemini 1.5 Flash (002)|190|40.0M*|88.5M*|42.9M*|88.8M*|\\n|Gemini 1.5 Flash 8B (001)|283|38.2M*|88.0M*|41.4M*|88.5M*|\\n|GPT-4o|163|43.1M*|89.2M*|43.9M*|89.0M*|\\n|GPT-4o Mini|113|41.1M*|88.7M*|41.9M*|88.7M*|\\n|Llama 3.2 90B|40|39.7ᴹ|88.2ᴹ|43.7ᴹ|88.5ᴹ|\\n|Llama 3.2 11B|124|33.0ᴹ|85.7ᴹ|36.3ᴹ|86.3ᴹ|\\n|Llama 3.1 8B|157|32.7ᴹ|85.5ᴹ|36.5ᴹ|86.5ᴹ|\\n\\nTable 2: Quantitative results on Flores200 [34], a machine translation benchmark. Set1 refers to {de, es, fr, it, pt, ja, ar, hi, ru, nl, tr, he, ko, zh}. Results marked with M were measured by us. Results marked with an asterisk (∗) were obtained using an alternate prompt which can be found in Appendix B.1 Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n\\n# 2.1.2 Core capability multimodal benchmarks and results\\n\\nIn this section we evaluate the multimodal capabilities of Amazon Nova models on a diverse set of public benchmarks. Our selection of multimodal benchmarks aims to probe for various capabilities, including natural image understanding, document understanding with charts and graphs, text understanding, and temporal reasoning in videos. For all benchmarks, we follow the suggested metrics and choice of data split for evaluation. The following list briefly describes the selected benchmarks.\\n\\n- MMMU [85]: The Massive Multi-discipline Multimodal Understanding benchmark consists of college-level multiple-choice and open-ended questions from 30 different disciplines. We use Chain-of-Thought (CoT) prompting for this benchmark and report accuracy.\\n- ChartQA [50]: The 2,500 questions of this benchmark cover three different types of charts (bar, line and pie) and require strong visual, logical, and arithmetical reasoning capabilities. We evaluate on the test set and report relaxed accuracy.\\n- DocVQA [51]: This benchmark probes capabilities on document analysis and recognition, including Optical Character Recognition (OCR). The 5,349 questions contain images from a diverse set of documents, ranging.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 1 summarizes the quantitative results of Nova models and select public models on the aforementioned benchmarks for core capabilities. When available, we reference the highest publicly-reported numbers for each benchmark from the official technical reports and websites for Claude, Gemini, OpenAI and Llama family of models. Amazon Nova Pro, Lite, and Micro demonstrate strong performance across all benchmarks, showcasing their advanced core intelligence, particularly Amazon Nova Micro and Lite on math, reasoning, and instruction following benchmarks.\\n\\nWe also evaluate the translation capabilities of Nova models. Flores200 [73, 34, 35], or simply Flores, is a machine translation benchmark consisting of translations from 842 distinct web articles, which tests the translation capabilities between English and non-English languages. Sentences are 21 words long on average. We use a 0-shot setup and report the macro average of two metrics, spBleu and COMET22 score [63] across a set of languages (Arabic, German, Spanish, French, Hindi, Italian, Japanese, Korean, Portuguese, Hebrew, Turkish, Simplified Chinese, Russian, Dutch) for translation from and into English. The prompts used for evaluation are summarized in Appendix B.1. Table 2 summarizes our quantitative results on Flores, demonstrating strong multilingual performance on translation for Amazon Nova Pro, Lite, and Micro.',\n",
              "     'md': 'Table 1 summarizes the quantitative results of Nova models and select public models on the aforementioned benchmarks for core capabilities. When available, we reference the highest publicly-reported numbers for each benchmark from the official technical reports and websites for Claude, Gemini, OpenAI and Llama family of models. Amazon Nova Pro, Lite, and Micro demonstrate strong performance across all benchmarks, showcasing their advanced core intelligence, particularly Amazon Nova Micro and Lite on math, reasoning, and instruction following benchmarks.\\n\\nWe also evaluate the translation capabilities of Nova models. Flores200 [73, 34, 35], or simply Flores, is a machine translation benchmark consisting of translations from 842 distinct web articles, which tests the translation capabilities between English and non-English languages. Sentences are 21 words long on average. We use a 0-shot setup and report the macro average of two metrics, spBleu and COMET22 score [63] across a set of languages (Arabic, German, Spanish, French, Hindi, Italian, Japanese, Korean, Portuguese, Hebrew, Turkish, Simplified Chinese, Russian, Dutch) for translation from and into English. The prompts used for evaluation are summarized in Appendix B.1. Table 2 summarizes our quantitative results on Flores, demonstrating strong multilingual performance on translation for Amazon Nova Pro, Lite, and Micro.',\n",
              "     'bBox': {'x': 71, 'y': 72.04, 'w': 470, 'h': 678}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'FLORES (0-shot)',\n",
              "     'md': '# FLORES (0-shot)',\n",
              "     'bBox': {'x': 346, 'y': 232.04, 'w': 75, 'h': 10}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['', '', 'en → Set1', '', 'Set1 → en', ''],\n",
              "      ['',\n",
              "       'tok/sec',\n",
              "       'spBleu (↑)',\n",
              "       'COMET22 (↑)',\n",
              "       'spBleu (↑)',\n",
              "       'COMET22 (↑)'],\n",
              "      ['Nova Pro', '100', '43.4', '89.1', '44.4', '89.0'],\n",
              "      ['Nova Lite', '157', '41.5', '88.8', '43.1', '88.8'],\n",
              "      ['Nova Micro', '210', '40.2', '88.5', '42.6', '88.7'],\n",
              "      ['Claude 3.5 Sonnet (Oct)', '57', '42.5ᴹ', '89.4ᴹ', '43.5ᴹ', '89.1ᴹ'],\n",
              "      ['Claude 3.5 Haiku', '64', '40.0ᴹ', '88.5ᴹ', '40.2ᴹ', '88.3ᴹ'],\n",
              "      ['Gemini 1.5 Pro (002)', '57', '43.0M*', '89.1M*', '45.6M*', '89.1M*'],\n",
              "      ['Gemini 1.5 Flash (002)',\n",
              "       '190',\n",
              "       '40.0M*',\n",
              "       '88.5M*',\n",
              "       '42.9M*',\n",
              "       '88.8M*'],\n",
              "      ['Gemini 1.5 Flash 8B (001)',\n",
              "       '283',\n",
              "       '38.2M*',\n",
              "       '88.0M*',\n",
              "       '41.4M*',\n",
              "       '88.5M*'],\n",
              "      ['GPT-4o', '163', '43.1M*', '89.2M*', '43.9M*', '89.0M*'],\n",
              "      ['GPT-4o Mini', '113', '41.1M*', '88.7M*', '41.9M*', '88.7M*'],\n",
              "      ['Llama 3.2 90B', '40', '39.7ᴹ', '88.2ᴹ', '43.7ᴹ', '88.5ᴹ'],\n",
              "      ['Llama 3.2 11B', '124', '33.0ᴹ', '85.7ᴹ', '36.3ᴹ', '86.3ᴹ'],\n",
              "      ['Llama 3.1 8B', '157', '32.7ᴹ', '85.5ᴹ', '36.5ᴹ', '86.5ᴹ']],\n",
              "     'md': '| | |en → Set1| |Set1 → en| |\\n| |tok/sec|spBleu (↑)|COMET22 (↑)|spBleu (↑)|COMET22 (↑)|\\n|Nova Pro|100|43.4|89.1|44.4|89.0|\\n|Nova Lite|157|41.5|88.8|43.1|88.8|\\n|Nova Micro|210|40.2|88.5|42.6|88.7|\\n|Claude 3.5 Sonnet (Oct)|57|42.5ᴹ|89.4ᴹ|43.5ᴹ|89.1ᴹ|\\n|Claude 3.5 Haiku|64|40.0ᴹ|88.5ᴹ|40.2ᴹ|88.3ᴹ|\\n|Gemini 1.5 Pro (002)|57|43.0M*|89.1M*|45.6M*|89.1M*|\\n|Gemini 1.5 Flash (002)|190|40.0M*|88.5M*|42.9M*|88.8M*|\\n|Gemini 1.5 Flash 8B (001)|283|38.2M*|88.0M*|41.4M*|88.5M*|\\n|GPT-4o|163|43.1M*|89.2M*|43.9M*|89.0M*|\\n|GPT-4o Mini|113|41.1M*|88.7M*|41.9M*|88.7M*|\\n|Llama 3.2 90B|40|39.7ᴹ|88.2ᴹ|43.7ᴹ|88.5ᴹ|\\n|Llama 3.2 11B|124|33.0ᴹ|85.7ᴹ|36.3ᴹ|86.3ᴹ|\\n|Llama 3.1 8B|157|32.7ᴹ|85.5ᴹ|36.5ᴹ|86.5ᴹ|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"\",\"\",\"en → Set1\",\"\",\"Set1 → en\",\"\"\\n\"\",\"tok/sec\",\"spBleu (↑)\",\"COMET22 (↑)\",\"spBleu (↑)\",\"COMET22 (↑)\"\\n\"Nova Pro\",\"100\",\"43.4\",\"89.1\",\"44.4\",\"89.0\"\\n\"Nova Lite\",\"157\",\"41.5\",\"88.8\",\"43.1\",\"88.8\"\\n\"Nova Micro\",\"210\",\"40.2\",\"88.5\",\"42.6\",\"88.7\"\\n\"Claude 3.5 Sonnet (Oct)\",\"57\",\"42.5ᴹ\",\"89.4ᴹ\",\"43.5ᴹ\",\"89.1ᴹ\"\\n\"Claude 3.5 Haiku\",\"64\",\"40.0ᴹ\",\"88.5ᴹ\",\"40.2ᴹ\",\"88.3ᴹ\"\\n\"Gemini 1.5 Pro (002)\",\"57\",\"43.0M*\",\"89.1M*\",\"45.6M*\",\"89.1M*\"\\n\"Gemini 1.5 Flash (002)\",\"190\",\"40.0M*\",\"88.5M*\",\"42.9M*\",\"88.8M*\"\\n\"Gemini 1.5 Flash 8B (001)\",\"283\",\"38.2M*\",\"88.0M*\",\"41.4M*\",\"88.5M*\"\\n\"GPT-4o\",\"163\",\"43.1M*\",\"89.2M*\",\"43.9M*\",\"89.0M*\"\\n\"GPT-4o Mini\",\"113\",\"41.1M*\",\"88.7M*\",\"41.9M*\",\"88.7M*\"\\n\"Llama 3.2 90B\",\"40\",\"39.7ᴹ\",\"88.2ᴹ\",\"43.7ᴹ\",\"88.5ᴹ\"\\n\"Llama 3.2 11B\",\"124\",\"33.0ᴹ\",\"85.7ᴹ\",\"36.3ᴹ\",\"86.3ᴹ\"\\n\"Llama 3.1 8B\",\"157\",\"32.7ᴹ\",\"85.5ᴹ\",\"36.5ᴹ\",\"86.5ᴹ\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 2: Quantitative results on Flores200 [34], a machine translation benchmark. Set1 refers to {de, es, fr, it, pt, ja, ar, hi, ru, nl, tr, he, ko, zh}. Results marked with M were measured by us. Results marked with an asterisk (∗) were obtained using an alternate prompt which can be found in Appendix B.1 Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "     'md': 'Table 2: Quantitative results on Flores200 [34], a machine translation benchmark. Set1 refers to {de, es, fr, it, pt, ja, ar, hi, ru, nl, tr, he, ko, zh}. Results marked with M were measured by us. Results marked with an asterisk (∗) were obtained using an alternate prompt which can be found in Appendix B.1 Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.',\n",
              "     'bBox': {'x': 71, 'y': 262.03, 'w': 470, 'h': 257}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2.1.2 Core capability multimodal benchmarks and results',\n",
              "     'md': '# 2.1.2 Core capability multimodal benchmarks and results',\n",
              "     'bBox': {'x': 72, 'y': 546.04, 'w': 250, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'In this section we evaluate the multimodal capabilities of Amazon Nova models on a diverse set of public benchmarks. Our selection of multimodal benchmarks aims to probe for various capabilities, including natural image understanding, document understanding with charts and graphs, text understanding, and temporal reasoning in videos. For all benchmarks, we follow the suggested metrics and choice of data split for evaluation. The following list briefly describes the selected benchmarks.\\n\\n- MMMU [85]: The Massive Multi-discipline Multimodal Understanding benchmark consists of college-level multiple-choice and open-ended questions from 30 different disciplines. We use Chain-of-Thought (CoT) prompting for this benchmark and report accuracy.\\n- ChartQA [50]: The 2,500 questions of this benchmark cover three different types of charts (bar, line and pie) and require strong visual, logical, and arithmetical reasoning capabilities. We evaluate on the test set and report relaxed accuracy.\\n- DocVQA [51]: This benchmark probes capabilities on document analysis and recognition, including Optical Character Recognition (OCR). The 5,349 questions contain images from a diverse set of documents, ranging.',\n",
              "     'md': 'In this section we evaluate the multimodal capabilities of Amazon Nova models on a diverse set of public benchmarks. Our selection of multimodal benchmarks aims to probe for various capabilities, including natural image understanding, document understanding with charts and graphs, text understanding, and temporal reasoning in videos. For all benchmarks, we follow the suggested metrics and choice of data split for evaluation. The following list briefly describes the selected benchmarks.\\n\\n- MMMU [85]: The Massive Multi-discipline Multimodal Understanding benchmark consists of college-level multiple-choice and open-ended questions from 30 different disciplines. We use Chain-of-Thought (CoT) prompting for this benchmark and report accuracy.\\n- ChartQA [50]: The 2,500 questions of this benchmark cover three different types of charts (bar, line and pie) and require strong visual, logical, and arithmetical reasoning capabilities. We evaluate on the test set and report relaxed accuracy.\\n- DocVQA [51]: This benchmark probes capabilities on document analysis and recognition, including Optical Character Recognition (OCR). The 5,349 questions contain images from a diverse set of documents, ranging.',\n",
              "     'bBox': {'x': 72, 'y': 564.04, 'w': 470, 'h': 156}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 8,\n",
              "   'text': '                                                  The Amazon Nova Family of Models\\n                                    MMMU            Chart         Doc           Text          VATEX         Ego\\n                                     (CoT)           QAC          VQA             VQA                      Schema\\n                                      val            test          test           val          test         test\\n                            tok/    accuracy       relaxed        ANLS          weighted      CIDEr       accuracy\\n                            sec                    accuracy                     accuracy\\n Amazon Nova Pro            100     61.7 ±3.2     89.2 ±1.2       93.5          81.5           77.8      72.1 ±5.4\\n Amazon Nova Lite           157     56.2 ±3.2     86.8 ±1.3       92.4          80.2           77.8      71.4 ±5.4\\n Claude 3.5 Sonnet (Oct)     57     70.4 ±3.0     90.8 ±1.1       94.2          61.7ᴹ           -            -\\n Claude 3 Haiku              64     50.2 ±3.3     82.0 ±1.5       88.8            -             -            -\\n Gemini 1.5 Pro (001)        58     65.9 ±3.1 E   87.2 ±1.3      93.1ᴮ          78.7          64.6ᴬ      72.2 ±5.4\\n Gemini 1.5 Flash (001)     190     62.3 ±3.2 E   85.4 ±1.4      89.9ᴮ          78.7           57.1      65.7 ±5.7\\n Gemini 1.5 Flash 8B (001)  283     53.7 ±3.3 F   78.2 ±1.6 G     73.6          66.7          53.2ᴬ          -\\n GPT-4o (May)                -      69.1 ±3.0     85.7 ±1.4       92.8          77.2D,M         -        72.2 ±5.4\\n GPT-4o Mini (Jul)          113     59.4 ±3.2     79.2 ±1.6 M      -            70.3ᴹ           -            -\\n Llama 3.2 90B               40     60.3 ±3.2     85.5 ±1.4       90.1          80.7ᴹ           -            -\\n Llama 3.2 11B              124     50.7 ±3.3     83.4 ±1.5       88.4          71.3ᴹ           -            -\\nTable 3: Quantitative results on four image understanding benchmarks (MMMU [85], ChartQA [50], DocVQA [51],\\nTextVQA [70]) and 2 video understanding benchmarks (VATEX [78] and EgoSchema [49]). Higher numbers are better\\nfor all benchmarks (↑). Unless otherwise noted, all evaluations are 0-shot and reference numbers are taken from the\\noriginal technical reports and websites for Claude models [11, 12], GPT4 models [56, 55], Llama models [45, 53] and\\nGemini models [32, 33]. Remarks: (A) 4-shot evaluation; (B) External Optical Character Recognition (OCR) was used;\\n(C) All models except Amazon Nova use CoT; (D) GPT-4o (Nov); (E) Gemini 1.5 Flash/Pro (002) models; (F) Reported\\nin [33]; (G) Reported in [4]; (M) Claude 3.5 Sonnet and Llama 3.2 results for TextVQA as well as GPT4o and GPT4o\\nmini results on ChartQA, TextVQA and VATEX were measured by us.2 Token generation speed in tokens per second\\n(tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n              from 1940 to 2020 and covering multiple industries. We report Average Normalized Levenshtein Similarity\\n         (ANLS).\\n       •     TextVQA [70]: The 5,000 samples of this dataset focus specifically on text-reading capabilities (OCR) in\\n         natural images. We report weighted accuracy on the validation set.\\n       •         VATEX [78]: This video captioning benchmark covers a diverse set of human activities. We evaluate on\\n         the public test set containing videos with a length of around 10 seconds. The CIDEr [75] score is used for\\n         evaluation.\\n       •   EgoSchema [49]: The unique characteristic of this long-form video question answering benchmark is its high\\n                                                  “certificate length” [15], which is, loosely speaking, the time it takes a human to verify the video description.\\n               The videos cover a broad range of natural human activities and come with human-curated multiple-choice\\n         question-answer pairs.\\nTable 3 summarizes our quantitative results on multiple image and video understanding benchmarks. Amazon Nova\\nPro and Lite achieve high scores across all benchmarks. Chart understanding on ChartQA and video understanding on\\nVATEX stand out, where Nova models rank either first or second. We provide the prompt templates for all benchmarks\\nin Appendix B.2, as well as qualitative examples in Appendix C.\\n2.2   Agentic workflows\\nAmazon Nova Pro, Lite, and Micro models can be used as agents. An agent considers a suite of tools and APIs, reasons\\nabout the user’s request and past conversational history, chooses if a tool should be used and, if so, decides which tool\\nto use, invokes the tool, assesses the outcome from the tool, and then communicates back with the user [83, 67, 46, 60].\\nTo this end, we evaluated our Nova models on agentic workflows that require textual understanding and visual reasoning.\\nFor textual understanding (Section 2.2.1), we used the Berkeley Function Calling Leaderboard benchmark to test our\\nmodels’ capabilities in function calling and orchestrating real-world applications. For visual reasoning (Section 2.2.2),\\n                                                         8',\n",
              "   'md': 'The Amazon Nova Family of Models\\n\\n| Model | tok/sec | MMMU (CoT) accuracy | Chart QA^C relaxed accuracy | Doc VQA ANLS | Text VQA weighted accuracy | VATEX CIDEr | Ego Schema accuracy |\\n|-------|---------|----------------------|----------------------------|---------------|----------------------------|-------------|----------------------|\\n| Amazon Nova Pro | 100 | 61.7 ±3.2 | 89.2 ±1.2 | 93.5 | 81.5 | 77.8 | 72.1 ±5.4 |\\n| Amazon Nova Lite | 157 | 56.2 ±3.2 | 86.8 ±1.3 | 92.4 | 80.2 | 77.8 | 71.4 ±5.4 |\\n| Claude 3.5 Sonnet (Oct) | 57 | 70.4 ±3.0 | 90.8 ±1.1 | 94.2 | 61.7^M | - | - |\\n| Claude 3 Haiku | 64 | 50.2 ±3.3 | 82.0 ±1.5 | 88.8 | - | - | - |\\n| Gemini 1.5 Pro (001) | 58 | 65.9 ±3.1^E | 87.2 ±1.3 | 93.1^B | 78.7 | 64.6^A | 72.2 ±5.4 |\\n| Gemini 1.5 Flash (001) | 190 | 62.3 ±3.2^E | 85.4 ±1.4 | 89.9^B | 78.7 | 57.1 | 65.7 ±5.7 |\\n| Gemini 1.5 Flash 8B (001) | 283 | 53.7 ±3.3^F | 78.2 ±1.6^G | 73.6 | 66.7 | 53.2^A | - |\\n| GPT-4o (May) | - | 69.1 ±3.0 | 85.7 ±1.4 | 92.8 | 77.2^D,M | - | 72.2 ±5.4 |\\n| GPT-4o Mini (Jul) | 113 | 59.4 ±3.2 | 79.2 ±1.6^M | - | 70.3^M | - | - |\\n| Llama 3.2 90B | 40 | 60.3 ±3.2 | 85.5 ±1.4 | 90.1 | 80.7^M | - | - |\\n| Llama 3.2 11B | 124 | 50.7 ±3.3 | 83.4 ±1.5 | 88.4 | 71.3^M | - | - |\\n\\nTable 3: Quantitative results on four image understanding benchmarks (MMMU [85], ChartQA [50], DocVQA [51], TextVQA [70]) and 2 video understanding benchmarks (VATEX [78] and EgoSchema [49]). Higher numbers are better for all benchmarks (↑). Unless otherwise noted, all evaluations are 0-shot and reference numbers are taken from the original technical reports and websites for Claude models [11, 12], GPT4 models [56, 55], Llama models [45, 53] and Gemini models [32, 33]. Remarks: (A) 4-shot evaluation; (B) External Optical Character Recognition (OCR) was used; (C) All models except Amazon Nova use CoT; (D) GPT-4o (Nov); (E) Gemini 1.5 Flash/Pro (002) models; (F) Reported in [33]; (G) Reported in [4]; (M) Claude 3.5 Sonnet and Llama 3.2 results for TextVQA as well as GPT4o and GPT4o mini results on ChartQA, TextVQA and VATEX were measured by us.² Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n\\nfrom 1940 to 2020 and covering multiple industries. We report Average Normalized Levenshtein Similarity (ANLS).\\n\\n- TextVQA [70]: The 5,000 samples of this dataset focus specifically on text-reading capabilities (OCR) in natural images. We report weighted accuracy on the validation set.\\n\\n- VATEX [78]: This video captioning benchmark covers a diverse set of human activities. We evaluate on the public test set containing videos with a length of around 10 seconds. The CIDEr [75] score is used for evaluation.\\n\\n- EgoSchema [49]: The unique characteristic of this long-form video question answering benchmark is its high \"certificate length\" [15], which is, loosely speaking, the time it takes a human to verify the video description. The videos cover a broad range of natural human activities and come with human-curated multiple-choice question-answer pairs.\\n\\nTable 3 summarizes our quantitative results on multiple image and video understanding benchmarks. Amazon Nova Pro and Lite achieve high scores across all benchmarks. Chart understanding on ChartQA and video understanding on VATEX stand out, where Nova models rank either first or second. We provide the prompt templates for all benchmarks in Appendix B.2, as well as qualitative examples in Appendix C.\\n\\n## 2.2 Agentic workflows\\n\\nAmazon Nova Pro, Lite, and Micro models can be used as agents. An agent considers a suite of tools and APIs, reasons about the user\\'s request and past conversational history, chooses if a tool should be used and, if so, decides which tool to use, invokes the tool, assesses the outcome from the tool, and then communicates back with the user [83, 67, 46, 60].\\n\\nTo this end, we evaluated our Nova models on agentic workflows that require textual understanding and visual reasoning. For textual understanding (Section 2.2.1), we used the Berkeley Function Calling Leaderboard benchmark to test our models\\' capabilities in function calling and orchestrating real-world applications. For visual reasoning (Section 2.2.2),',\n",
              "   'images': [{'name': 'page_8.jpg',\n",
              "     'height': 792,\n",
              "     'width': 612,\n",
              "     'x': 0,\n",
              "     'y': 0,\n",
              "     'original_width': 1236,\n",
              "     'original_height': 1600,\n",
              "     'type': 'full_page_screenshot'}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'text',\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': 'The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      ['Model',\n",
              "       'tok/sec',\n",
              "       'MMMU (CoT) accuracy',\n",
              "       'Chart QA^C relaxed accuracy',\n",
              "       'Doc VQA ANLS',\n",
              "       'Text VQA weighted accuracy',\n",
              "       'VATEX CIDEr',\n",
              "       'Ego Schema accuracy'],\n",
              "      ['-------',\n",
              "       '---------',\n",
              "       '----------------------',\n",
              "       '----------------------------',\n",
              "       '---------------',\n",
              "       '----------------------------',\n",
              "       '-------------',\n",
              "       '----------------------'],\n",
              "      ['Amazon Nova Pro',\n",
              "       '100',\n",
              "       '61.7 ±3.2',\n",
              "       '89.2 ±1.2',\n",
              "       '93.5',\n",
              "       '81.5',\n",
              "       '77.8',\n",
              "       '72.1 ±5.4'],\n",
              "      ['Amazon Nova Lite',\n",
              "       '157',\n",
              "       '56.2 ±3.2',\n",
              "       '86.8 ±1.3',\n",
              "       '92.4',\n",
              "       '80.2',\n",
              "       '77.8',\n",
              "       '71.4 ±5.4'],\n",
              "      ['Claude 3.5 Sonnet (Oct)',\n",
              "       '57',\n",
              "       '70.4 ±3.0',\n",
              "       '90.8 ±1.1',\n",
              "       '94.2',\n",
              "       '61.7^M',\n",
              "       '-',\n",
              "       '-'],\n",
              "      ['Claude 3 Haiku',\n",
              "       '64',\n",
              "       '50.2 ±3.3',\n",
              "       '82.0 ±1.5',\n",
              "       '88.8',\n",
              "       '-',\n",
              "       '-',\n",
              "       '-'],\n",
              "      ['Gemini 1.5 Pro (001)',\n",
              "       '58',\n",
              "       '65.9 ±3.1^E',\n",
              "       '87.2 ±1.3',\n",
              "       '93.1^B',\n",
              "       '78.7',\n",
              "       '64.6^A',\n",
              "       '72.2 ±5.4'],\n",
              "      ['Gemini 1.5 Flash (001)',\n",
              "       '190',\n",
              "       '62.3 ±3.2^E',\n",
              "       '85.4 ±1.4',\n",
              "       '89.9^B',\n",
              "       '78.7',\n",
              "       '57.1',\n",
              "       '65.7 ±5.7'],\n",
              "      ['Gemini 1.5 Flash 8B (001)',\n",
              "       '283',\n",
              "       '53.7 ±3.3^F',\n",
              "       '78.2 ±1.6^G',\n",
              "       '73.6',\n",
              "       '66.7',\n",
              "       '53.2^A',\n",
              "       '-'],\n",
              "      ['GPT-4o (May)',\n",
              "       '-',\n",
              "       '69.1 ±3.0',\n",
              "       '85.7 ±1.4',\n",
              "       '92.8',\n",
              "       '77.2^D,M',\n",
              "       '-',\n",
              "       '72.2 ±5.4'],\n",
              "      ['GPT-4o Mini (Jul)',\n",
              "       '113',\n",
              "       '59.4 ±3.2',\n",
              "       '79.2 ±1.6^M',\n",
              "       '-',\n",
              "       '70.3^M',\n",
              "       '-',\n",
              "       '-'],\n",
              "      ['Llama 3.2 90B',\n",
              "       '40',\n",
              "       '60.3 ±3.2',\n",
              "       '85.5 ±1.4',\n",
              "       '90.1',\n",
              "       '80.7^M',\n",
              "       '-',\n",
              "       '-'],\n",
              "      ['Llama 3.2 11B',\n",
              "       '124',\n",
              "       '50.7 ±3.3',\n",
              "       '83.4 ±1.5',\n",
              "       '88.4',\n",
              "       '71.3^M',\n",
              "       '-',\n",
              "       '-']],\n",
              "     'md': 'The Amazon Nova Family of Models\\n| Model | tok/sec | MMMU (CoT) accuracy | Chart QA^C relaxed accuracy | Doc VQA ANLS | Text VQA weighted accuracy | VATEX CIDEr | Ego Schema accuracy |\\n|-------|---------|----------------------|----------------------------|---------------|----------------------------|-------------|----------------------|\\n| Amazon Nova Pro | 100 | 61.7 ±3.2 | 89.2 ±1.2 | 93.5 | 81.5 | 77.8 | 72.1 ±5.4 |\\n| Amazon Nova Lite | 157 | 56.2 ±3.2 | 86.8 ±1.3 | 92.4 | 80.2 | 77.8 | 71.4 ±5.4 |\\n| Claude 3.5 Sonnet (Oct) | 57 | 70.4 ±3.0 | 90.8 ±1.1 | 94.2 | 61.7^M | - | - |\\n| Claude 3 Haiku | 64 | 50.2 ±3.3 | 82.0 ±1.5 | 88.8 | - | - | - |\\n| Gemini 1.5 Pro (001) | 58 | 65.9 ±3.1^E | 87.2 ±1.3 | 93.1^B | 78.7 | 64.6^A | 72.2 ±5.4 |\\n| Gemini 1.5 Flash (001) | 190 | 62.3 ±3.2^E | 85.4 ±1.4 | 89.9^B | 78.7 | 57.1 | 65.7 ±5.7 |\\n| Gemini 1.5 Flash 8B (001) | 283 | 53.7 ±3.3^F | 78.2 ±1.6^G | 73.6 | 66.7 | 53.2^A | - |\\n| GPT-4o (May) | - | 69.1 ±3.0 | 85.7 ±1.4 | 92.8 | 77.2^D,M | - | 72.2 ±5.4 |\\n| GPT-4o Mini (Jul) | 113 | 59.4 ±3.2 | 79.2 ±1.6^M | - | 70.3^M | - | - |\\n| Llama 3.2 90B | 40 | 60.3 ±3.2 | 85.5 ±1.4 | 90.1 | 80.7^M | - | - |\\n| Llama 3.2 11B | 124 | 50.7 ±3.3 | 83.4 ±1.5 | 88.4 | 71.3^M | - | - |',\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\"Model\",\"tok/sec\",\"MMMU (CoT) accuracy\",\"Chart QA^C relaxed accuracy\",\"Doc VQA ANLS\",\"Text VQA weighted accuracy\",\"VATEX CIDEr\",\"Ego Schema accuracy\"\\n\"-------\",\"---------\",\"----------------------\",\"----------------------------\",\"---------------\",\"----------------------------\",\"-------------\",\"----------------------\"\\n\"Amazon Nova Pro\",\"100\",\"61.7 ±3.2\",\"89.2 ±1.2\",\"93.5\",\"81.5\",\"77.8\",\"72.1 ±5.4\"\\n\"Amazon Nova Lite\",\"157\",\"56.2 ±3.2\",\"86.8 ±1.3\",\"92.4\",\"80.2\",\"77.8\",\"71.4 ±5.4\"\\n\"Claude 3.5 Sonnet (Oct)\",\"57\",\"70.4 ±3.0\",\"90.8 ±1.1\",\"94.2\",\"61.7^M\",\"-\",\"-\"\\n\"Claude 3 Haiku\",\"64\",\"50.2 ±3.3\",\"82.0 ±1.5\",\"88.8\",\"-\",\"-\",\"-\"\\n\"Gemini 1.5 Pro (001)\",\"58\",\"65.9 ±3.1^E\",\"87.2 ±1.3\",\"93.1^B\",\"78.7\",\"64.6^A\",\"72.2 ±5.4\"\\n\"Gemini 1.5 Flash (001)\",\"190\",\"62.3 ±3.2^E\",\"85.4 ±1.4\",\"89.9^B\",\"78.7\",\"57.1\",\"65.7 ±5.7\"\\n\"Gemini 1.5 Flash 8B (001)\",\"283\",\"53.7 ±3.3^F\",\"78.2 ±1.6^G\",\"73.6\",\"66.7\",\"53.2^A\",\"-\"\\n\"GPT-4o (May)\",\"-\",\"69.1 ±3.0\",\"85.7 ±1.4\",\"92.8\",\"77.2^D,M\",\"-\",\"72.2 ±5.4\"\\n\"GPT-4o Mini (Jul)\",\"113\",\"59.4 ±3.2\",\"79.2 ±1.6^M\",\"-\",\"70.3^M\",\"-\",\"-\"\\n\"Llama 3.2 90B\",\"40\",\"60.3 ±3.2\",\"85.5 ±1.4\",\"90.1\",\"80.7^M\",\"-\",\"-\"\\n\"Llama 3.2 11B\",\"124\",\"50.7 ±3.3\",\"83.4 ±1.5\",\"88.4\",\"71.3^M\",\"-\",\"-\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 686}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 3: Quantitative results on four image understanding benchmarks (MMMU [85], ChartQA [50], DocVQA [51], TextVQA [70]) and 2 video understanding benchmarks (VATEX [78] and EgoSchema [49]). Higher numbers are better for all benchmarks (↑). Unless otherwise noted, all evaluations are 0-shot and reference numbers are taken from the original technical reports and websites for Claude models [11, 12], GPT4 models [56, 55], Llama models [45, 53] and Gemini models [32, 33]. Remarks: (A) 4-shot evaluation; (B) External Optical Character Recognition (OCR) was used; (C) All models except Amazon Nova use CoT; (D) GPT-4o (Nov); (E) Gemini 1.5 Flash/Pro (002) models; (F) Reported in [33]; (G) Reported in [4]; (M) Claude 3.5 Sonnet and Llama 3.2 results for TextVQA as well as GPT4o and GPT4o mini results on ChartQA, TextVQA and VATEX were measured by us.² Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n\\nfrom 1940 to 2020 and covering multiple industries. We report Average Normalized Levenshtein Similarity (ANLS).\\n\\n- TextVQA [70]: The 5,000 samples of this dataset focus specifically on text-reading capabilities (OCR) in natural images. We report weighted accuracy on the validation set.\\n\\n- VATEX [78]: This video captioning benchmark covers a diverse set of human activities. We evaluate on the public test set containing videos with a length of around 10 seconds. The CIDEr [75] score is used for evaluation.\\n\\n- EgoSchema [49]: The unique characteristic of this long-form video question answering benchmark is its high \"certificate length\" [15], which is, loosely speaking, the time it takes a human to verify the video description. The videos cover a broad range of natural human activities and come with human-curated multiple-choice question-answer pairs.\\n\\nTable 3 summarizes our quantitative results on multiple image and video understanding benchmarks. Amazon Nova Pro and Lite achieve high scores across all benchmarks. Chart understanding on ChartQA and video understanding on VATEX stand out, where Nova models rank either first or second. We provide the prompt templates for all benchmarks in Appendix B.2, as well as qualitative examples in Appendix C.',\n",
              "     'md': 'Table 3: Quantitative results on four image understanding benchmarks (MMMU [85], ChartQA [50], DocVQA [51], TextVQA [70]) and 2 video understanding benchmarks (VATEX [78] and EgoSchema [49]). Higher numbers are better for all benchmarks (↑). Unless otherwise noted, all evaluations are 0-shot and reference numbers are taken from the original technical reports and websites for Claude models [11, 12], GPT4 models [56, 55], Llama models [45, 53] and Gemini models [32, 33]. Remarks: (A) 4-shot evaluation; (B) External Optical Character Recognition (OCR) was used; (C) All models except Amazon Nova use CoT; (D) GPT-4o (Nov); (E) Gemini 1.5 Flash/Pro (002) models; (F) Reported in [33]; (G) Reported in [4]; (M) Claude 3.5 Sonnet and Llama 3.2 results for TextVQA as well as GPT4o and GPT4o mini results on ChartQA, TextVQA and VATEX were measured by us.² Token generation speed in tokens per second (tok/sec), the inverse of per-token generation latency, is reproduced from Section 2.5.\\n\\nfrom 1940 to 2020 and covering multiple industries. We report Average Normalized Levenshtein Similarity (ANLS).\\n\\n- TextVQA [70]: The 5,000 samples of this dataset focus specifically on text-reading capabilities (OCR) in natural images. We report weighted accuracy on the validation set.\\n\\n- VATEX [78]: This video captioning benchmark covers a diverse set of human activities. We evaluate on the public test set containing videos with a length of around 10 seconds. The CIDEr [75] score is used for evaluation.\\n\\n- EgoSchema [49]: The unique characteristic of this long-form video question answering benchmark is its high \"certificate length\" [15], which is, loosely speaking, the time it takes a human to verify the video description. The videos cover a broad range of natural human activities and come with human-curated multiple-choice question-answer pairs.\\n\\nTable 3 summarizes our quantitative results on multiple image and video understanding benchmarks. Amazon Nova Pro and Lite achieve high scores across all benchmarks. Chart understanding on ChartQA and video understanding on VATEX stand out, where Nova models rank either first or second. We provide the prompt templates for all benchmarks in Appendix B.2, as well as qualitative examples in Appendix C.',\n",
              "     'bBox': {'x': 71, 'y': 73.03, 'w': 470, 'h': 677}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 2,\n",
              "     'value': '2.2 Agentic workflows',\n",
              "     'md': '## 2.2 Agentic workflows',\n",
              "     'bBox': {'x': 72, 'y': 630.04, 'w': 101, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': \"Amazon Nova Pro, Lite, and Micro models can be used as agents. An agent considers a suite of tools and APIs, reasons about the user's request and past conversational history, chooses if a tool should be used and, if so, decides which tool to use, invokes the tool, assesses the outcome from the tool, and then communicates back with the user [83, 67, 46, 60].\\n\\nTo this end, we evaluated our Nova models on agentic workflows that require textual understanding and visual reasoning. For textual understanding (Section 2.2.1), we used the Berkeley Function Calling Leaderboard benchmark to test our models' capabilities in function calling and orchestrating real-world applications. For visual reasoning (Section 2.2.2),\",\n",
              "     'md': \"Amazon Nova Pro, Lite, and Micro models can be used as agents. An agent considers a suite of tools and APIs, reasons about the user's request and past conversational history, chooses if a tool should be used and, if so, decides which tool to use, invokes the tool, assesses the outcome from the tool, and then communicates back with the user [83, 67, 46, 60].\\n\\nTo this end, we evaluated our Nova models on agentic workflows that require textual understanding and visual reasoning. For textual understanding (Section 2.2.1), we used the Berkeley Function Calling Leaderboard benchmark to test our models' capabilities in function calling and orchestrating real-world applications. For visual reasoning (Section 2.2.2),\",\n",
              "     'bBox': {'x': 71, 'y': 73.03, 'w': 470, 'h': 677}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': True,\n",
              "   'parsingMode': 'premium',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 9,\n",
              "   'text': '                                     The Amazon Nova Family of Models\\nwe evaluate on three benchmarks that require image understanding capabilities for correct function calling. We highlight\\nthat both Amazon Nova Pro and Lite models set a new state of the art on these challenging benchmarks.\\n2.2.1  Agentic text benchmarks and results\\nTable 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL).3 Stemming from the\\nGorilla project [60], the revamped BFCL [81] benchmark evaluates a model’s ability to accurately call and utilize\\nreal-world functions, or tools, based on a user’s natural language request. Amazon Nova models particularly excel in\\nthe Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models.\\nAmazon Nova Lite and Micro also had the lowest latency of the selected models.\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names\\nand argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based\\non manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call’s\\naccuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\nTo measure the rate of hallucination, Irrelevance measures the model’s ability to recognize that it does not have the\\nappropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of\\nirrelevance, measures the model’s ability to recognize it indeed does have the functions necessary to help the user (but\\ndoes not verify function signature accuracy). For both metrics, higher numbers are better.\\n                             Overall  Latency        Non-Live          Live      Multi-      Hallucination\\n                                                                                 Turn\\n                             accuracy seconds     AST      execution overall   overall   relevance   irrelevance\\n                               (↑)      (↓)        (↑)       (↑)       (↑)       (↑)        (↑)        (↑)\\n Nova Pro                      68.4     1.0       90.1      89.8       71.5       45.1      95.1       65.1\\n Nova Lite                     66.6     0.6       87.5      86.4       66.0      50.3       97.6       49.1\\n Nova Micro                    56.2     0.5       87.2      89.7       67.4      15.5       87.8       57.6\\n Claude Sonnet 3.5 (Jun)       61.3     3.9       70.0      66.3       74.7      40.0       68.3       74.6\\n Claude Haiku 3              40.4       1.5       41.7      47.5       57.7      20.6       97.6       29.4\\n Gemini 1.5 Pro (002)          59.8     3.0       88.0      91.4       74.3      16.3       75.6       75.1\\n Gemini 1.5 Flash (002)        55.3     1.1       79.7      80.6       73.2      12.5       78.1       75.7\\n Llama 3.2 90Bᴬ                54.3     N/A       88.9      89.3       61.1      14.3       92.7       58.4\\n Llama 3.2 11Bᴬ              49.9       N/A       83.6      87.3       57.9      10.5       78.1       41.6\\n GPT-4o (Aug)                  68.9     1.5       85.9      85.6       75.4      45.3       63.4       82.9\\n GPT-4o-mini (Jul)             60.7     1.6       84.3      84.1       70.2      28.3       80.5       71.8\\nTable 4: Results on the Berkeley Function Calling Leaderboard (BFCL) v3 as of the Nov 17th, 2024 update. We include\\nthe latest versions of the models available on the leaderboard at that time. (A) We use leaderboard results for Llama 3.1\\n8B and 70B for Llama 3.2 11B and 90B, respectively, given the shared text LLM.\\n2.2.2  Agentic multimodal benchmarks and results\\nThe Amazon Nova Pro and Lite models provide native support for multimodal inputs, including agentic workflows. In\\nthis section, we present results from our models on three different benchmarks that require agents to navigate websites\\nto solve real-world tasks. Websites are typically represented as screenshots in these datasets to correctly convey all style\\nelements and visual data as rendered in a standard web browser.\\n      •  VisualWebBench [43]: This benchmark includes seven core tasks related to web browsing, including captioning,\\n         question answering, OCR, action prediction, and grounding. All models are evaluated on 1,536 samples that\\n         span more than 100 websites from 12 domains. The final metric is the average over different metrics for the\\n         individual core tasks.\\n                             3BFCL is a fast-moving, live benchmark. We report results using the state of the repository and website leaderboard as of Nov\\n17th, 2024 (commit 8226d).\\n                                                      9',\n",
              "   'md': \"The Amazon Nova Family of Models\\n\\nwe evaluate on three benchmarks that require image understanding capabilities for correct function calling. We highlight that both Amazon Nova Pro and Lite models set a new state of the art on these challenging benchmarks.\\n\\n## 2.2.1 Agentic text benchmarks and results\\n\\nTable 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL)³. Stemming from the Gorilla project [60], the revamped BFCL [81] benchmark evaluates a model's ability to accurately call and utilize real-world functions, or tools, based on a user's natural language request. Amazon Nova models particularly excel in the Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models. Amazon Nova Lite and Micro also had the lowest latency of the selected models.\\n\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names and argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based on manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call's accuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\n\\nTo measure the rate of hallucination, Irrelevance measures the model's ability to recognize that it does not have the appropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of irrelevance, measures the model's ability to recognize it indeed does have the functions necessary to help the user (but does not verify function signature accuracy). For both metrics, higher numbers are better.\\n\\n| Model | Overall accuracy (↑) | Latency seconds (↓) | Non-Live AST (↑) | Non-Live execution (↑) | Live overall (↑) | Multi-Turn overall (↑) | Hallucination relevance (↑) | Hallucination irrelevance (↑) |\\n|---|---|---|---|---|---|---|---|---|\\n| Nova Pro | 68.4 | 1.0 | 90.1 | 89.8 | 71.5 | 45.1 | 95.1 | 65.1 |\\n| Nova Lite | 66.6 | 0.6 | 87.5 | 86.4 | 66.0 | 50.3 | 97.6 | 49.1 |\\n| Nova Micro | 56.2 | 0.5 | 87.2 | 89.7 | 67.4 | 15.5 | 87.8 | 57.6 |\\n| Claude Sonnet 3.5 (Jun) | 61.3 | 3.9 | 70.0 | 66.3 | 74.7 | 40.0 | 68.3 | 74.6 |\\n| Claude Haiku 3 | 40.4 | 1.5 | 41.7 | 47.5 | 57.7 | 20.6 | 97.6 | 29.4 |\\n| Gemini 1.5 Pro (002) | 59.8 | 3.0 | 88.0 | 91.4 | 74.3 | 16.3 | 75.6 | 75.1 |\\n| Gemini 1.5 Flash (002) | 55.3 | 1.1 | 79.7 | 80.6 | 73.2 | 12.5 | 78.1 | 75.7 |\\n| Llama 3.2 90B^A | 54.3 | N/A | 88.9 | 89.3 | 61.1 | 14.3 | 92.7 | 58.4 |\\n| Llama 3.2 11B^A | 49.9 | N/A | 83.6 | 87.3 | 57.9 | 10.5 | 78.1 | 41.6 |\\n| GPT-4o (Aug) | 68.9 | 1.5 | 85.9 | 85.6 | 75.4 | 45.3 | 63.4 | 82.9 |\\n| GPT-4o-mini (Jul) | 60.7 | 1.6 | 84.3 | 84.1 | 70.2 | 28.3 | 80.5 | 71.8 |\\n\\nTable 4: Results on the Berkeley Function Calling Leaderboard (BFCL) v3 as of the Nov 17th, 2024 update. We include the latest versions of the models available on the leaderboard at that time. (A) We use leaderboard results for Llama 3.1 8B and 70B for Llama 3.2 11B and 90B, respectively, given the shared text LLM.\\n\\n## 2.2.2 Agentic multimodal benchmarks and results\\n\\nThe Amazon Nova Pro and Lite models provide native support for multimodal inputs, including agentic workflows. In this section, we present results from our models on three different benchmarks that require agents to navigate websites to solve real-world tasks. Websites are typically represented as screenshots in these datasets to correctly convey all style elements and visual data as rendered in a standard web browser.\\n\\n- VisualWebBench [43]: This benchmark includes seven core tasks related to web browsing, including captioning, question answering, OCR, action prediction, and grounding. All models are evaluated on 1,536 samples that span more than 100 websites from 12 domains. The final metric is the average over different metrics for the individual core tasks.\\n\\n³BFCL is a fast-moving, live benchmark. We report results using the state of the repository and website leaderboard as of Nov 17th, 2024 (commit 8226d).\",\n",
              "   'images': [{'name': 'page_9.jpg',\n",
              "     'height': 792,\n",
              "     'width': 612,\n",
              "     'x': 0,\n",
              "     'y': 0,\n",
              "     'original_width': 1236,\n",
              "     'original_height': 1600,\n",
              "     'type': 'full_page_screenshot'}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'text',\n",
              "     'value': 'The Amazon Nova Family of Models\\n\\nwe evaluate on three benchmarks that require image understanding capabilities for correct function calling. We highlight that both Amazon Nova Pro and Lite models set a new state of the art on these challenging benchmarks.',\n",
              "     'md': 'The Amazon Nova Family of Models\\n\\nwe evaluate on three benchmarks that require image understanding capabilities for correct function calling. We highlight that both Amazon Nova Pro and Lite models set a new state of the art on these challenging benchmarks.',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 468, 'h': 321}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 2,\n",
              "     'value': '2.2.1 Agentic text benchmarks and results',\n",
              "     'md': '## 2.2.1 Agentic text benchmarks and results',\n",
              "     'bBox': {'x': 72, 'y': 108.04, 'w': 184, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': \"Table 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL)³. Stemming from the Gorilla project [60], the revamped BFCL [81] benchmark evaluates a model's ability to accurately call and utilize real-world functions, or tools, based on a user's natural language request. Amazon Nova models particularly excel in the Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models. Amazon Nova Lite and Micro also had the lowest latency of the selected models.\\n\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names and argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based on manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call's accuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\n\\nTo measure the rate of hallucination, Irrelevance measures the model's ability to recognize that it does not have the appropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of irrelevance, measures the model's ability to recognize it indeed does have the functions necessary to help the user (but does not verify function signature accuracy). For both metrics, higher numbers are better.\",\n",
              "     'md': \"Table 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL)³. Stemming from the Gorilla project [60], the revamped BFCL [81] benchmark evaluates a model's ability to accurately call and utilize real-world functions, or tools, based on a user's natural language request. Amazon Nova models particularly excel in the Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models. Amazon Nova Lite and Micro also had the lowest latency of the selected models.\\n\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names and argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based on manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call's accuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\n\\nTo measure the rate of hallucination, Irrelevance measures the model's ability to recognize that it does not have the appropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of irrelevance, measures the model's ability to recognize it indeed does have the functions necessary to help the user (but does not verify function signature accuracy). For both metrics, higher numbers are better.\",\n",
              "     'bBox': {'x': 71, 'y': 160.04, 'w': 471, 'h': 209}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      [],\n",
              "      [],\n",
              "      [],\n",
              "      [],\n",
              "      ['Model',\n",
              "       'Overall accuracy (↑)',\n",
              "       'Latency seconds (↓)',\n",
              "       'Non-Live AST (↑)',\n",
              "       'Non-Live execution (↑)',\n",
              "       'Live overall (↑)',\n",
              "       'Multi-Turn overall (↑)',\n",
              "       'Hallucination relevance (↑)',\n",
              "       'Hallucination irrelevance (↑)'],\n",
              "      ['---', '---', '---', '---', '---', '---', '---', '---', '---'],\n",
              "      ['Nova Pro',\n",
              "       '68.4',\n",
              "       '1.0',\n",
              "       '90.1',\n",
              "       '89.8',\n",
              "       '71.5',\n",
              "       '45.1',\n",
              "       '95.1',\n",
              "       '65.1'],\n",
              "      ['Nova Lite',\n",
              "       '66.6',\n",
              "       '0.6',\n",
              "       '87.5',\n",
              "       '86.4',\n",
              "       '66.0',\n",
              "       '50.3',\n",
              "       '97.6',\n",
              "       '49.1'],\n",
              "      ['Nova Micro',\n",
              "       '56.2',\n",
              "       '0.5',\n",
              "       '87.2',\n",
              "       '89.7',\n",
              "       '67.4',\n",
              "       '15.5',\n",
              "       '87.8',\n",
              "       '57.6'],\n",
              "      ['Claude Sonnet 3.5 (Jun)',\n",
              "       '61.3',\n",
              "       '3.9',\n",
              "       '70.0',\n",
              "       '66.3',\n",
              "       '74.7',\n",
              "       '40.0',\n",
              "       '68.3',\n",
              "       '74.6'],\n",
              "      ['Claude Haiku 3',\n",
              "       '40.4',\n",
              "       '1.5',\n",
              "       '41.7',\n",
              "       '47.5',\n",
              "       '57.7',\n",
              "       '20.6',\n",
              "       '97.6',\n",
              "       '29.4'],\n",
              "      ['Gemini 1.5 Pro (002)',\n",
              "       '59.8',\n",
              "       '3.0',\n",
              "       '88.0',\n",
              "       '91.4',\n",
              "       '74.3',\n",
              "       '16.3',\n",
              "       '75.6',\n",
              "       '75.1'],\n",
              "      ['Gemini 1.5 Flash (002)',\n",
              "       '55.3',\n",
              "       '1.1',\n",
              "       '79.7',\n",
              "       '80.6',\n",
              "       '73.2',\n",
              "       '12.5',\n",
              "       '78.1',\n",
              "       '75.7'],\n",
              "      ['Llama 3.2 90B^A',\n",
              "       '54.3',\n",
              "       'N/A',\n",
              "       '88.9',\n",
              "       '89.3',\n",
              "       '61.1',\n",
              "       '14.3',\n",
              "       '92.7',\n",
              "       '58.4'],\n",
              "      ['Llama 3.2 11B^A',\n",
              "       '49.9',\n",
              "       'N/A',\n",
              "       '83.6',\n",
              "       '87.3',\n",
              "       '57.9',\n",
              "       '10.5',\n",
              "       '78.1',\n",
              "       '41.6'],\n",
              "      ['GPT-4o (Aug)',\n",
              "       '68.9',\n",
              "       '1.5',\n",
              "       '85.9',\n",
              "       '85.6',\n",
              "       '75.4',\n",
              "       '45.3',\n",
              "       '63.4',\n",
              "       '82.9'],\n",
              "      ['GPT-4o-mini (Jul)',\n",
              "       '60.7',\n",
              "       '1.6',\n",
              "       '84.3',\n",
              "       '84.1',\n",
              "       '70.2',\n",
              "       '28.3',\n",
              "       '80.5',\n",
              "       '71.8']],\n",
              "     'md': \"Table 4 presents quantitative results on the Berkeley Function Calling Leaderboard v3 (BFCL)³. Stemming from the Gorilla project [60], the revamped BFCL [81] benchmark evaluates a model's ability to accurately call and utilize real-world functions, or tools, based on a user's natural language request. Amazon Nova models particularly excel in the Abstract Syntax Tree (AST), Execution, and Relevance metrics, as well as overall scores versus comparable models. Amazon Nova Lite and Micro also had the lowest latency of the selected models.\\nIn Table 4, AST measures the exact match function calling performance of the model when comparing function names and argument/value signatures to a human-curated ground truth. While AST allows for some soft matching based on manually-defined, permitted argument values (e.g., different date formats), Execution measures a function call's accuracy not by the call signature itself, but by comparing the return value of the call when executed against a real API.\\n\\nTo measure the rate of hallucination, Irrelevance measures the model's ability to recognize that it does not have the appropriate functions available to help the user, and should therefore not call any. Relevance, as the opposite of irrelevance, measures the model's ability to recognize it indeed does have the functions necessary to help the user (but does not verify function signature accuracy). For both metrics, higher numbers are better.\\n\\n| Model | Overall accuracy (↑) | Latency seconds (↓) | Non-Live AST (↑) | Non-Live execution (↑) | Live overall (↑) | Multi-Turn overall (↑) | Hallucination relevance (↑) | Hallucination irrelevance (↑) |\\n|---|---|---|---|---|---|---|---|---|\\n| Nova Pro | 68.4 | 1.0 | 90.1 | 89.8 | 71.5 | 45.1 | 95.1 | 65.1 |\\n| Nova Lite | 66.6 | 0.6 | 87.5 | 86.4 | 66.0 | 50.3 | 97.6 | 49.1 |\\n| Nova Micro | 56.2 | 0.5 | 87.2 | 89.7 | 67.4 | 15.5 | 87.8 | 57.6 |\\n| Claude Sonnet 3.5 (Jun) | 61.3 | 3.9 | 70.0 | 66.3 | 74.7 | 40.0 | 68.3 | 74.6 |\\n| Claude Haiku 3 | 40.4 | 1.5 | 41.7 | 47.5 | 57.7 | 20.6 | 97.6 | 29.4 |\\n| Gemini 1.5 Pro (002) | 59.8 | 3.0 | 88.0 | 91.4 | 74.3 | 16.3 | 75.6 | 75.1 |\\n| Gemini 1.5 Flash (002) | 55.3 | 1.1 | 79.7 | 80.6 | 73.2 | 12.5 | 78.1 | 75.7 |\\n| Llama 3.2 90B^A | 54.3 | N/A | 88.9 | 89.3 | 61.1 | 14.3 | 92.7 | 58.4 |\\n| Llama 3.2 11B^A | 49.9 | N/A | 83.6 | 87.3 | 57.9 | 10.5 | 78.1 | 41.6 |\\n| GPT-4o (Aug) | 68.9 | 1.5 | 85.9 | 85.6 | 75.4 | 45.3 | 63.4 | 82.9 |\\n| GPT-4o-mini (Jul) | 60.7 | 1.6 | 84.3 | 84.1 | 70.2 | 28.3 | 80.5 | 71.8 |\",\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\\n\\n\\n\\n\"Model\",\"Overall accuracy (↑)\",\"Latency seconds (↓)\",\"Non-Live AST (↑)\",\"Non-Live execution (↑)\",\"Live overall (↑)\",\"Multi-Turn overall (↑)\",\"Hallucination relevance (↑)\",\"Hallucination irrelevance (↑)\"\\n\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\\n\"Nova Pro\",\"68.4\",\"1.0\",\"90.1\",\"89.8\",\"71.5\",\"45.1\",\"95.1\",\"65.1\"\\n\"Nova Lite\",\"66.6\",\"0.6\",\"87.5\",\"86.4\",\"66.0\",\"50.3\",\"97.6\",\"49.1\"\\n\"Nova Micro\",\"56.2\",\"0.5\",\"87.2\",\"89.7\",\"67.4\",\"15.5\",\"87.8\",\"57.6\"\\n\"Claude Sonnet 3.5 (Jun)\",\"61.3\",\"3.9\",\"70.0\",\"66.3\",\"74.7\",\"40.0\",\"68.3\",\"74.6\"\\n\"Claude Haiku 3\",\"40.4\",\"1.5\",\"41.7\",\"47.5\",\"57.7\",\"20.6\",\"97.6\",\"29.4\"\\n\"Gemini 1.5 Pro (002)\",\"59.8\",\"3.0\",\"88.0\",\"91.4\",\"74.3\",\"16.3\",\"75.6\",\"75.1\"\\n\"Gemini 1.5 Flash (002)\",\"55.3\",\"1.1\",\"79.7\",\"80.6\",\"73.2\",\"12.5\",\"78.1\",\"75.7\"\\n\"Llama 3.2 90B^A\",\"54.3\",\"N/A\",\"88.9\",\"89.3\",\"61.1\",\"14.3\",\"92.7\",\"58.4\"\\n\"Llama 3.2 11B^A\",\"49.9\",\"N/A\",\"83.6\",\"87.3\",\"57.9\",\"10.5\",\"78.1\",\"41.6\"\\n\"GPT-4o (Aug)\",\"68.9\",\"1.5\",\"85.9\",\"85.6\",\"75.4\",\"45.3\",\"63.4\",\"82.9\"\\n\"GPT-4o-mini (Jul)\",\"60.7\",\"1.6\",\"84.3\",\"84.1\",\"70.2\",\"28.3\",\"80.5\",\"71.8\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 636}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 4: Results on the Berkeley Function Calling Leaderboard (BFCL) v3 as of the Nov 17th, 2024 update. We include the latest versions of the models available on the leaderboard at that time. (A) We use leaderboard results for Llama 3.1 8B and 70B for Llama 3.2 11B and 90B, respectively, given the shared text LLM.',\n",
              "     'md': 'Table 4: Results on the Berkeley Function Calling Leaderboard (BFCL) v3 as of the Nov 17th, 2024 update. We include the latest versions of the models available on the leaderboard at that time. (A) We use leaderboard results for Llama 3.1 8B and 70B for Llama 3.2 11B and 90B, respectively, given the shared text LLM.',\n",
              "     'bBox': {'x': 71, 'y': 514.04, 'w': 469, 'h': 236}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 2,\n",
              "     'value': '2.2.2 Agentic multimodal benchmarks and results',\n",
              "     'md': '## 2.2.2 Agentic multimodal benchmarks and results',\n",
              "     'bBox': {'x': 72, 'y': 576.04, 'w': 218, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'The Amazon Nova Pro and Lite models provide native support for multimodal inputs, including agentic workflows. In this section, we present results from our models on three different benchmarks that require agents to navigate websites to solve real-world tasks. Websites are typically represented as screenshots in these datasets to correctly convey all style elements and visual data as rendered in a standard web browser.\\n\\n- VisualWebBench [43]: This benchmark includes seven core tasks related to web browsing, including captioning, question answering, OCR, action prediction, and grounding. All models are evaluated on 1,536 samples that span more than 100 websites from 12 domains. The final metric is the average over different metrics for the individual core tasks.\\n\\n³BFCL is a fast-moving, live benchmark. We report results using the state of the repository and website leaderboard as of Nov 17th, 2024 (commit 8226d).',\n",
              "     'md': 'The Amazon Nova Pro and Lite models provide native support for multimodal inputs, including agentic workflows. In this section, we present results from our models on three different benchmarks that require agents to navigate websites to solve real-world tasks. Websites are typically represented as screenshots in these datasets to correctly convey all style elements and visual data as rendered in a standard web browser.\\n\\n- VisualWebBench [43]: This benchmark includes seven core tasks related to web browsing, including captioning, question answering, OCR, action prediction, and grounding. All models are evaluated on 1,536 samples that span more than 100 websites from 12 domains. The final metric is the average over different metrics for the individual core tasks.\\n\\n³BFCL is a fast-moving, live benchmark. We report results using the state of the repository and website leaderboard as of Nov 17th, 2024 (commit 8226d).',\n",
              "     'bBox': {'x': 71, 'y': 300.03, 'w': 470, 'h': 420}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': True,\n",
              "   'parsingMode': 'premium',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 10,\n",
              "   'text': '                                                      The Amazon Nova Family of Models\\n               •  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the\\n            original website screenshots, making it multimodal. An agent needs to select an element and pick one of three\\n             elementary actions (click, type, or select) alongside a value for some actions. We report micro average over\\n          the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the\\n                  predicted value, are correct.\\n               •  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24],\\n                 and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an\\n            instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location\\n         of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding\\n                  box.\\n                                                      Table 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results.\\n         Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve\\n         high scores on all three benchmarks.\\n                                                      VisualWebBench        MM-Mind2Web   GroundUI-1K\\n                                                        compositeᴰ         step accuracy     accuracy\\n                     Nova Pro                         79.7                 63.7                 81.4\\n                     Nova Lite                        77.7                 60.7                 80.2\\n                     Claude 3.5 Sonnet (Oct)          76.7ᴹ                61.6ᴹ                16.3\\n                     GPT-4o (Nov)                     77.5ᴹ                55.0ᴹ                13.4C\\n                     GPT-4o Mini (Jul)                71.3ᴹ                58.6ᴹ                 7.2ᴹ\\n                     GPT-4 (Apr)                      64.6                 36.8ᴬ                  -\\n                     Gemini 1.5 Pro (002)             76.4ᴹ                58.4ᴹ                35.2ᴮ\\n                     Gemini 1.5 Flash (002)           76.1ᴹ                46.2ᴹ                59.9ᴹ\\n                     Gemini 1.0 Pro (001)             48.0                 17.9ᴬ                  -\\n                     Llama 3.2 90B                    73.2ᴹ                21.6ᴹ                 8.3ᴹ\\n                     Llama 3.2 11B                    65.1ᴹ                22.1ᴹ                 3.7ᴹ\\n             Table 5: Quantitative results on three multi-modal agentic benchmarks: VisualWebBench [43], MM-Mind2Web [86]\\n               and GroundUI-1K [87]. Reference numbers are taken from the corresponding benchmark papers [43, 86, 87] and\\n    leaderboard [3]. Remarks: (A) uses in-context learning (ICL) (please note that Amazon Nova models do not need to rely\\n      on in-context examples); (B) Gemini 1.5 Pro (001); (C) GPT-4o (May); (D) Macro average over individual metrics; (M)\\n         Measured by us.2\\n         2.3  Long context\\n         We evaluate Amazon Nova Pro, Lite, and Micro on tasks that require the models to understand and reason over long\\ncontext. These skills are crucial for tasks such as long multi-turn conversations, reasoning over long lists of retrieved\\n              documents, or understanding long videos. Amazon Nova Micro, Lite, and Pro models support context lengths of\\n         128k, 300k, and 300k tokens, respectively. We used the following benchmarks to evaluate our models’ long context\\n         performance:\\n               •    Text Needle-in-a-Haystack (NIAH): Following [40], we assessed each model’s ability to locate specific\\n                  information (the “needle”) within extensive contexts (the “haystack”). This “needle-in-a-haystack” test\\n              evaluates the model’s performance on context lengths starting at 32k, allowing us to measure its ability to\\n                  accurately retrieve information across varying lengths of input context.\\n               • SQuALITY [76] (ZeroScrolls Benchmark [69]): Focused on query-based summarization of literary stories,\\n                  this task evaluates the model’s capacity to generate relevant summaries from large contexts.\\n               •    LVBench [77]: This multimodal benchmark includes questions about YouTube videos4 from various domains\\n           such as TV series, sports, broadcasts, and surveillance footage. The LVBench dataset consists of 99 videos and\\n                  1,549 questions, covering six different types of tasks such as reasoning, event understanding and summarization.\\n            4https://huggingface.co/datasets/AIWinter/LVBench\\n                                                              10',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n•  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the original website screenshots, making it multimodal. An agent needs to select an element and pick one of three elementary actions (click, type, or select) alongside a value for some actions. We report micro average over the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the predicted value, are correct.\\n\\n•  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24], and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding box.\\n\\nTable 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results. Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve high scores on all three benchmarks.\\n\\n| |Model|Results| |\\n|---|---|---|---|\\n|VisualWebBench compositeᴰ|MM-Mind2Web step accuracy|GroundUI-1K accuracy| |\\n|Nova Pro|79.7|63.7|81.4|\\n|Nova Lite|77.7|60.7|80.2|\\n|Claude 3.5 Sonnet (Oct)|76.7ᴹ|61.6ᴹ|16.3|\\n|GPT-4o (Nov)|77.5ᴹ|55.0ᴹ|13.4C|\\n|GPT-4o Mini (Jul)|71.3ᴹ|58.6ᴹ|7.2ᴹ|\\n|GPT-4 (Apr)|64.6|36.8ᴬ|-|\\n|Gemini 1.5 Pro (002)|76.4ᴹ|58.4ᴹ|35.2ᴮ|\\n|Gemini 1.5 Flash (002)|76.1ᴹ|46.2ᴹ|59.9ᴹ|\\n|Gemini 1.0 Pro (001)|48.0|17.9ᴬ|-|\\n|Llama 3.2 90B|73.2ᴹ|21.6ᴹ|8.3ᴹ|\\n|Llama 3.2 11B|65.1ᴹ|22.1ᴹ|3.7ᴹ|\\n\\nTable 5: Quantitative results on three multi-modal agentic benchmarks: VisualWebBench [43], MM-Mind2Web [86] and GroundUI-1K [87]. Reference numbers are taken from the corresponding benchmark papers [43, 86, 87] and leaderboard [3]. Remarks: (A) uses in-context learning (ICL) (please note that Amazon Nova models do not need to rely on in-context examples); (B) Gemini 1.5 Pro (001); (C) GPT-4o (May); (D) Macro average over individual metrics; (M) Measured by us.\\n\\n# 2.3  Long context\\n\\nWe evaluate Amazon Nova Pro, Lite, and Micro on tasks that require the models to understand and reason over long context. These skills are crucial for tasks such as long multi-turn conversations, reasoning over long lists of retrieved documents, or understanding long videos. Amazon Nova Micro, Lite, and Pro models support context lengths of 128k, 300k, and 300k tokens, respectively. We used the following benchmarks to evaluate our models’ long context performance:\\n\\n•    Text Needle-in-a-Haystack (NIAH): Following [40], we assessed each model’s ability to locate specific information (the “needle”) within extensive contexts (the “haystack”). This “needle-in-a-haystack” test evaluates the model’s performance on context lengths starting at 32k, allowing us to measure its ability to accurately retrieve information across varying lengths of input context.\\n\\n• SQuALITY [76] (ZeroScrolls Benchmark [69]): Focused on query-based summarization of literary stories, this task evaluates the model’s capacity to generate relevant summaries from large contexts.\\n\\n•    LVBench [77]: This multimodal benchmark includes questions about YouTube videos from various domains such as TV series, sports, broadcasts, and surveillance footage. The LVBench dataset consists of 99 videos and 1,549 questions, covering six different types of tasks such as reasoning, event understanding and summarization.\\n\\n4https://huggingface.co/datasets/AIWinter/LVBench',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': '•  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the original website screenshots, making it multimodal. An agent needs to select an element and pick one of three elementary actions (click, type, or select) alongside a value for some actions. We report micro average over the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the predicted value, are correct.\\n\\n•  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24], and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding box.\\n\\nTable 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results. Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve high scores on all three benchmarks.',\n",
              "     'md': '•  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the original website screenshots, making it multimodal. An agent needs to select an element and pick one of three elementary actions (click, type, or select) alongside a value for some actions. We report micro average over the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the predicted value, are correct.\\n\\n•  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24], and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding box.\\n\\nTable 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results. Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve high scores on all three benchmarks.',\n",
              "     'bBox': {'x': 71, 'y': 72.04, 'w': 470, 'h': 609}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      [],\n",
              "      [],\n",
              "      [],\n",
              "      [],\n",
              "      ['', 'Model', 'Results', ''],\n",
              "      ['---', '---', '---', '---'],\n",
              "      ['VisualWebBench compositeᴰ',\n",
              "       'MM-Mind2Web step accuracy',\n",
              "       'GroundUI-1K accuracy',\n",
              "       ''],\n",
              "      ['Nova Pro', '79.7', '63.7', '81.4'],\n",
              "      ['Nova Lite', '77.7', '60.7', '80.2'],\n",
              "      ['Claude 3.5 Sonnet (Oct)', '76.7ᴹ', '61.6ᴹ', '16.3'],\n",
              "      ['GPT-4o (Nov)', '77.5ᴹ', '55.0ᴹ', '13.4C'],\n",
              "      ['GPT-4o Mini (Jul)', '71.3ᴹ', '58.6ᴹ', '7.2ᴹ'],\n",
              "      ['GPT-4 (Apr)', '64.6', '36.8ᴬ', '-'],\n",
              "      ['Gemini 1.5 Pro (002)', '76.4ᴹ', '58.4ᴹ', '35.2ᴮ'],\n",
              "      ['Gemini 1.5 Flash (002)', '76.1ᴹ', '46.2ᴹ', '59.9ᴹ'],\n",
              "      ['Gemini 1.0 Pro (001)', '48.0', '17.9ᴬ', '-'],\n",
              "      ['Llama 3.2 90B', '73.2ᴹ', '21.6ᴹ', '8.3ᴹ'],\n",
              "      ['Llama 3.2 11B', '65.1ᴹ', '22.1ᴹ', '3.7ᴹ']],\n",
              "     'md': '•  MM-Mind2Web [86]: This extension of the original Mind2Web [24] benchmark links samples with the original website screenshots, making it multimodal. An agent needs to select an element and pick one of three elementary actions (click, type, or select) alongside a value for some actions. We report micro average over the per-sample step accuracy, where an agent is successful only if element and action selection, as well as the predicted value, are correct.\\n•  GroundUI-1K [87]: This benchmark is composed of multiple existing datasets, including Mind2Web [24], and repurposes them as a grounding task. On 1,000 samples for evaluation, a multimodal agent is given an instruction and a screenshot of a website from a wide variety of domains and asked to predict the 2D location of the desired UI element. The agent is correct if its predicted 2D location is within the ground truth bounding box.\\n\\nTable 5 shows the results of our models on multimodal agent workflows along with other publicly-reported results. Both Amazon Nova models, Lite and Pro, demonstrate strong visual reasoning and agentic capabilities and achieve high scores on all three benchmarks.\\n\\n| |Model|Results| |\\n|---|---|---|---|\\n|VisualWebBench compositeᴰ|MM-Mind2Web step accuracy|GroundUI-1K accuracy| |\\n|Nova Pro|79.7|63.7|81.4|\\n|Nova Lite|77.7|60.7|80.2|\\n|Claude 3.5 Sonnet (Oct)|76.7ᴹ|61.6ᴹ|16.3|\\n|GPT-4o (Nov)|77.5ᴹ|55.0ᴹ|13.4C|\\n|GPT-4o Mini (Jul)|71.3ᴹ|58.6ᴹ|7.2ᴹ|\\n|GPT-4 (Apr)|64.6|36.8ᴬ|-|\\n|Gemini 1.5 Pro (002)|76.4ᴹ|58.4ᴹ|35.2ᴮ|\\n|Gemini 1.5 Flash (002)|76.1ᴹ|46.2ᴹ|59.9ᴹ|\\n|Gemini 1.0 Pro (001)|48.0|17.9ᴬ|-|\\n|Llama 3.2 90B|73.2ᴹ|21.6ᴹ|8.3ᴹ|\\n|Llama 3.2 11B|65.1ᴹ|22.1ᴹ|3.7ᴹ|',\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\\n\\n\\n\\n\"\",\"Model\",\"Results\",\"\"\\n\"---\",\"---\",\"---\",\"---\"\\n\"VisualWebBench compositeᴰ\",\"MM-Mind2Web step accuracy\",\"GroundUI-1K accuracy\",\"\"\\n\"Nova Pro\",\"79.7\",\"63.7\",\"81.4\"\\n\"Nova Lite\",\"77.7\",\"60.7\",\"80.2\"\\n\"Claude 3.5 Sonnet (Oct)\",\"76.7ᴹ\",\"61.6ᴹ\",\"16.3\"\\n\"GPT-4o (Nov)\",\"77.5ᴹ\",\"55.0ᴹ\",\"13.4C\"\\n\"GPT-4o Mini (Jul)\",\"71.3ᴹ\",\"58.6ᴹ\",\"7.2ᴹ\"\\n\"GPT-4 (Apr)\",\"64.6\",\"36.8ᴬ\",\"-\"\\n\"Gemini 1.5 Pro (002)\",\"76.4ᴹ\",\"58.4ᴹ\",\"35.2ᴮ\"\\n\"Gemini 1.5 Flash (002)\",\"76.1ᴹ\",\"46.2ᴹ\",\"59.9ᴹ\"\\n\"Gemini 1.0 Pro (001)\",\"48.0\",\"17.9ᴬ\",\"-\"\\n\"Llama 3.2 90B\",\"73.2ᴹ\",\"21.6ᴹ\",\"8.3ᴹ\"\\n\"Llama 3.2 11B\",\"65.1ᴹ\",\"22.1ᴹ\",\"3.7ᴹ\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 5: Quantitative results on three multi-modal agentic benchmarks: VisualWebBench [43], MM-Mind2Web [86] and GroundUI-1K [87]. Reference numbers are taken from the corresponding benchmark papers [43, 86, 87] and leaderboard [3]. Remarks: (A) uses in-context learning (ICL) (please note that Amazon Nova models do not need to rely on in-context examples); (B) Gemini 1.5 Pro (001); (C) GPT-4o (May); (D) Macro average over individual metrics; (M) Measured by us.',\n",
              "     'md': 'Table 5: Quantitative results on three multi-modal agentic benchmarks: VisualWebBench [43], MM-Mind2Web [86] and GroundUI-1K [87]. Reference numbers are taken from the corresponding benchmark papers [43, 86, 87] and leaderboard [3]. Remarks: (A) uses in-context learning (ICL) (please note that Amazon Nova models do not need to rely on in-context examples); (B) Gemini 1.5 Pro (001); (C) GPT-4o (May); (D) Macro average over individual metrics; (M) Measured by us.',\n",
              "     'bBox': {'x': 71, 'y': 237.04, 'w': 469, 'h': 243}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2.3  Long context',\n",
              "     'md': '# 2.3  Long context',\n",
              "     'bBox': {'x': 72, 'y': 517.04, 'w': 78, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We evaluate Amazon Nova Pro, Lite, and Micro on tasks that require the models to understand and reason over long context. These skills are crucial for tasks such as long multi-turn conversations, reasoning over long lists of retrieved documents, or understanding long videos. Amazon Nova Micro, Lite, and Pro models support context lengths of 128k, 300k, and 300k tokens, respectively. We used the following benchmarks to evaluate our models’ long context performance:\\n\\n•    Text Needle-in-a-Haystack (NIAH): Following [40], we assessed each model’s ability to locate specific information (the “needle”) within extensive contexts (the “haystack”). This “needle-in-a-haystack” test evaluates the model’s performance on context lengths starting at 32k, allowing us to measure its ability to accurately retrieve information across varying lengths of input context.\\n\\n• SQuALITY [76] (ZeroScrolls Benchmark [69]): Focused on query-based summarization of literary stories, this task evaluates the model’s capacity to generate relevant summaries from large contexts.\\n\\n•    LVBench [77]: This multimodal benchmark includes questions about YouTube videos from various domains such as TV series, sports, broadcasts, and surveillance footage. The LVBench dataset consists of 99 videos and 1,549 questions, covering six different types of tasks such as reasoning, event understanding and summarization.\\n\\n4https://huggingface.co/datasets/AIWinter/LVBench',\n",
              "     'md': 'We evaluate Amazon Nova Pro, Lite, and Micro on tasks that require the models to understand and reason over long context. These skills are crucial for tasks such as long multi-turn conversations, reasoning over long lists of retrieved documents, or understanding long videos. Amazon Nova Micro, Lite, and Pro models support context lengths of 128k, 300k, and 300k tokens, respectively. We used the following benchmarks to evaluate our models’ long context performance:\\n\\n•    Text Needle-in-a-Haystack (NIAH): Following [40], we assessed each model’s ability to locate specific information (the “needle”) within extensive contexts (the “haystack”). This “needle-in-a-haystack” test evaluates the model’s performance on context lengths starting at 32k, allowing us to measure its ability to accurately retrieve information across varying lengths of input context.\\n\\n• SQuALITY [76] (ZeroScrolls Benchmark [69]): Focused on query-based summarization of literary stories, this task evaluates the model’s capacity to generate relevant summaries from large contexts.\\n\\n•    LVBench [77]: This multimodal benchmark includes questions about YouTube videos from various domains such as TV series, sports, broadcasts, and surveillance footage. The LVBench dataset consists of 99 videos and 1,549 questions, covering six different types of tasks such as reasoning, event understanding and summarization.\\n\\n4https://huggingface.co/datasets/AIWinter/LVBench',\n",
              "     'bBox': {'x': 71, 'y': 72.04, 'w': 471, 'h': 644.98}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://huggingface.co/datasets/AIWinter/LVBench',\n",
              "     'text': 'https://huggingface.co/datasets/AIWinter/LVBench'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 11,\n",
              "   'text': '                                       The Amazon Nova Family of Models\\n                      Nova Micro               Nova Lite                       Nova Pro\\n                   10                  10                            10\\n                   20                  20                            20                          100\\n                   30                  30                            30\\n                2  40                  40                            40                          75\\n                [  50                  50                            50                          50\\n                1  60                  60                            60\\n                   70                  70                            70\\n                   80                  80                            80                          25\\n                   90                  90                            90\\n                  100                 100                           100\\n                      3   3  8            3  3 4 % 3       8             3  3 8    8   3  8\\n                     Context Length           Context Length                 Context Length\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and\\nNova Pro (up-to 300k) models.\\n                                                         SQuALITY              LVBench\\n                                                         ROUGE-L               accuracy\\n                         Nova Pro                        19.8 ±8.7            41.6 ±2.5\\n                         Nova Lite                       19.2 ±8.6            40.4 ±2.4\\n                         Nova Micro                      18.8 ±8.6                -\\n                         Claude 3.5 Sonnet (Jun)         13.4 ±7.5                -\\n                         Gemini 1.5 Pro (001)                -                33.1 ±2.3\\n                         Gemini 1.5 Pro (002)            19.1 ±8.6 M              -\\n                         Gemini 1.5 Flash (002)          18.1 ±8.4 M              -\\n                         GPT-4o                          18.8 ±8.6            30.8 ±2.3\\n                         Llama 3 - 70B                   16.4 ±8.1                -\\n                         Llama 3 - 8B                    15.3 ±7.9                -\\nTable 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For\\nSQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3\\nreport [45]. Gemini results were measured by us2                       (M). For LVBench, Gemini and GPT-4o numbers were taken from the\\ncorresponding benchmark leaderboard [77].\\nResults for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering\\ntask, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other\\nmodels. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any\\ndepth across both text and multimodal understanding use cases, delivering high accuracy and reliability.\\n2.4   Functional expertise\\nIn addition to core capabilities, foundation models must perform well in particular specialties and domains. Across\\nour many areas of performance analyses, we have selected four domains for which to present benchmarking results:\\nSoftware engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can\\nbe found in Appendix B.3.\\n                                                         11',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n|Nova Micro|Nova Lite|Nova Pro|\\n|---|---|---|\\n|10|10|10|\\n|20|20|20|\\n|30|30|30|\\n|40|40|40|\\n|50|50|50|\\n|60|60|60|\\n|70|70|70|\\n|80|80|80|\\n|90|90|90|\\n|100|100|100|\\n\\nContext Length\\n\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.\\n\\n| |SQuALITY|LVBench|\\n|---|---|---|\\n|Nova Pro|19.8 ±8.7|41.6 ±2.5|\\n|Nova Lite|19.2 ±8.6|40.4 ±2.4|\\n|Nova Micro|18.8 ±8.6|-|\\n|Claude 3.5 Sonnet (Jun)|13.4 ±7.5|-|\\n|Gemini 1.5 Pro (001)|-|33.1 ±2.3|\\n|Gemini 1.5 Pro (002)|19.1 ±8.6 M|-|\\n|Gemini 1.5 Flash (002)|18.1 ±8.4 M|-|\\n|GPT-4o|18.8 ±8.6|30.8 ±2.3|\\n|Llama 3 - 70B|16.4 ±8.1|-|\\n|Llama 3 - 8B|15.3 ±7.9|-|\\n\\nTable 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For SQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3 report [45]. Gemini results were measured by us2 (M). For LVBench, Gemini and GPT-4o numbers were taken from the corresponding benchmark leaderboard [77].\\n\\nResults for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering task, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other models. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any depth across both text and multimodal understanding use cases, delivering high accuracy and reliability.\\n\\n# 2.4 Functional expertise\\n\\nIn addition to core capabilities, foundation models must perform well in particular specialties and domains. Across our many areas of performance analyses, we have selected four domains for which to present benchmarking results: Software engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can be found in Appendix B.3.',\n",
              "   'images': [{'name': 'img_p10_1.png',\n",
              "     'height': 1028,\n",
              "     'width': 2528,\n",
              "     'x': 72,\n",
              "     'y': 72.00149999999994,\n",
              "     'original_width': 2528,\n",
              "     'original_height': 1028,\n",
              "     'ocr': [{'x': 475,\n",
              "       'y': 37,\n",
              "       'w': 208,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999322100776649',\n",
              "       'text': 'Nova Micro'},\n",
              "      {'x': 1023,\n",
              "       'y': 37,\n",
              "       'w': 176,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999627985879241',\n",
              "       'text': 'Nova Lite'},\n",
              "      {'x': 1695,\n",
              "       'y': 39,\n",
              "       'w': 168,\n",
              "       'h': 38,\n",
              "       'confidence': '0.9999550178555574',\n",
              "       'text': 'Nova Pro'},\n",
              "      {'x': 407,\n",
              "       'y': 103,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999944366205292',\n",
              "       'text': '10'},\n",
              "      {'x': 829,\n",
              "       'y': 103,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999944366205292',\n",
              "       'text': '10'},\n",
              "      {'x': 1495,\n",
              "       'y': 103,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999944366205292',\n",
              "       'text': '10'},\n",
              "      {'x': 2096,\n",
              "       'y': 148,\n",
              "       'w': 56,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9996983438622489',\n",
              "       'text': '100'},\n",
              "      {'x': 405,\n",
              "       'y': 177,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999942680334141',\n",
              "       'text': '20'},\n",
              "      {'x': 827,\n",
              "       'y': 177,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999942680334141',\n",
              "       'text': '20'},\n",
              "      {'x': 1493,\n",
              "       'y': 177,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999942680334141',\n",
              "       'text': '20'},\n",
              "      {'x': 405,\n",
              "       'y': 251,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.5168530382282581',\n",
              "       'text': '30'},\n",
              "      {'x': 827,\n",
              "       'y': 251,\n",
              "       'w': 46,\n",
              "       'h': 36,\n",
              "       'confidence': '0.5394472539629145',\n",
              "       'text': '30'},\n",
              "      {'x': 1493,\n",
              "       'y': 251,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.5168530382282581',\n",
              "       'text': '30'},\n",
              "      {'x': 2094,\n",
              "       'y': 296,\n",
              "       'w': 38,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999962910793456',\n",
              "       'text': '75'},\n",
              "      {'x': 341,\n",
              "       'y': 315,\n",
              "       'w': 36,\n",
              "       'h': 52,\n",
              "       'confidence': '0.3187985916726035',\n",
              "       'text': '2'},\n",
              "      {'x': 408,\n",
              "       'y': 326,\n",
              "       'w': 42,\n",
              "       'h': 32,\n",
              "       'confidence': '0.6243105911491494',\n",
              "       'text': '40'},\n",
              "      {'x': 830,\n",
              "       'y': 326,\n",
              "       'w': 42,\n",
              "       'h': 32,\n",
              "       'confidence': '0.6243105911491494',\n",
              "       'text': '40'},\n",
              "      {'x': 1496,\n",
              "       'y': 326,\n",
              "       'w': 42,\n",
              "       'h': 32,\n",
              "       'confidence': '0.6243105911491494',\n",
              "       'text': '40'},\n",
              "      {'x': 405,\n",
              "       'y': 399,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9997859850114889',\n",
              "       'text': '50'},\n",
              "      {'x': 827,\n",
              "       'y': 399,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9997859850114889',\n",
              "       'text': '50'},\n",
              "      {'x': 1493,\n",
              "       'y': 399,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9997859850114889',\n",
              "       'text': '50'},\n",
              "      {'x': 341,\n",
              "       'y': 369,\n",
              "       'w': 36,\n",
              "       'h': 126,\n",
              "       'confidence': '0.2668490313661245',\n",
              "       'text': '['},\n",
              "      {'x': 2094,\n",
              "       'y': 444,\n",
              "       'w': 40,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9998446502509013',\n",
              "       'text': '50'},\n",
              "      {'x': 406,\n",
              "       'y': 474,\n",
              "       'w': 42,\n",
              "       'h': 32,\n",
              "       'confidence': '0.9999579377042895',\n",
              "       'text': '60'},\n",
              "      {'x': 827,\n",
              "       'y': 473,\n",
              "       'w': 46,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999658612233858',\n",
              "       'text': '60'},\n",
              "      {'x': 1493,\n",
              "       'y': 473,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999684743028646',\n",
              "       'text': '60'},\n",
              "      {'x': 341,\n",
              "       'y': 497,\n",
              "       'w': 36,\n",
              "       'h': 116,\n",
              "       'confidence': '0.8209942657591149',\n",
              "       'text': '1'},\n",
              "      {'x': 408,\n",
              "       'y': 548,\n",
              "       'w': 40,\n",
              "       'h': 32,\n",
              "       'confidence': '0.9999754706222831',\n",
              "       'text': '70'},\n",
              "      {'x': 827,\n",
              "       'y': 547,\n",
              "       'w': 44,\n",
              "       'h': 36,\n",
              "       'confidence': '0.999979432399878',\n",
              "       'text': '70'},\n",
              "      {'x': 1495,\n",
              "       'y': 547,\n",
              "       'w': 42,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999858386882693',\n",
              "       'text': '70'},\n",
              "      {'x': 2094,\n",
              "       'y': 592,\n",
              "       'w': 40,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999998314126102',\n",
              "       'text': '25'},\n",
              "      {'x': 405,\n",
              "       'y': 621,\n",
              "       'w': 46,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999895475975691',\n",
              "       'text': '80'},\n",
              "      {'x': 827,\n",
              "       'y': 621,\n",
              "       'w': 46,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999895475975691',\n",
              "       'text': '80'},\n",
              "      {'x': 1493,\n",
              "       'y': 621,\n",
              "       'w': 46,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999895475975691',\n",
              "       'text': '80'},\n",
              "      {'x': 405,\n",
              "       'y': 695,\n",
              "       'w': 46,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9998102601606897',\n",
              "       'text': '90'},\n",
              "      {'x': 827,\n",
              "       'y': 695,\n",
              "       'w': 46,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9998102601606897',\n",
              "       'text': '90'},\n",
              "      {'x': 1493,\n",
              "       'y': 695,\n",
              "       'w': 46,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9998102601606897',\n",
              "       'text': '90'},\n",
              "      {'x': 387,\n",
              "       'y': 769,\n",
              "       'w': 64,\n",
              "       'h': 36,\n",
              "       'confidence': '0.5027778744697571',\n",
              "       'text': '100'},\n",
              "      {'x': 809,\n",
              "       'y': 769,\n",
              "       'w': 64,\n",
              "       'h': 36,\n",
              "       'confidence': '0.5027778744697571',\n",
              "       'text': '100'},\n",
              "      {'x': 1475,\n",
              "       'y': 769,\n",
              "       'w': 64,\n",
              "       'h': 36,\n",
              "       'confidence': '0.5027778744697571',\n",
              "       'text': '100'},\n",
              "      {'x': 483,\n",
              "       'y': 847,\n",
              "       'w': 36,\n",
              "       'h': 62,\n",
              "       'confidence': '0.6133456089595803',\n",
              "       'text': '3'},\n",
              "      {'x': 557,\n",
              "       'y': 847,\n",
              "       'w': 36,\n",
              "       'h': 62,\n",
              "       'confidence': '0.8439624514381059',\n",
              "       'text': '3'},\n",
              "      {'x': 633,\n",
              "       'y': 845,\n",
              "       'w': 36,\n",
              "       'h': 82,\n",
              "       'confidence': '0.46584164892928825',\n",
              "       'text': '8'},\n",
              "      {'x': 905,\n",
              "       'y': 847,\n",
              "       'w': 36,\n",
              "       'h': 62,\n",
              "       'confidence': '0.6133456089595803',\n",
              "       'text': '3'},\n",
              "      {'x': 980,\n",
              "       'y': 848,\n",
              "       'w': 32,\n",
              "       'h': 60,\n",
              "       'confidence': '0.9520704241185393',\n",
              "       'text': '3'},\n",
              "      {'x': 1046,\n",
              "       'y': 838,\n",
              "       'w': 122,\n",
              "       'h': 96,\n",
              "       'confidence': '0.18577231466770172',\n",
              "       'text': '4 %'},\n",
              "      {'x': 1201,\n",
              "       'y': 847,\n",
              "       'w': 36,\n",
              "       'h': 82,\n",
              "       'confidence': '0.6826460477094685',\n",
              "       'text': '3'},\n",
              "      {'x': 1273,\n",
              "       'y': 847,\n",
              "       'w': 38,\n",
              "       'h': 84,\n",
              "       'confidence': '0.9727422895918068',\n",
              "       'text': '8'},\n",
              "      {'x': 1571,\n",
              "       'y': 847,\n",
              "       'w': 36,\n",
              "       'h': 62,\n",
              "       'confidence': '0.6133456089595803',\n",
              "       'text': '3'},\n",
              "      {'x': 1645,\n",
              "       'y': 847,\n",
              "       'w': 36,\n",
              "       'h': 62,\n",
              "       'confidence': '0.8439624514381059',\n",
              "       'text': '3'},\n",
              "      {'x': 1719,\n",
              "       'y': 845,\n",
              "       'w': 38,\n",
              "       'h': 82,\n",
              "       'confidence': '0.3407077789243864',\n",
              "       'text': '8'},\n",
              "      {'x': 1791,\n",
              "       'y': 845,\n",
              "       'w': 38,\n",
              "       'h': 84,\n",
              "       'confidence': '0.9719812684425619',\n",
              "       'text': '8'},\n",
              "      {'x': 1868,\n",
              "       'y': 848,\n",
              "       'w': 32,\n",
              "       'h': 82,\n",
              "       'confidence': '0.7704166704233018',\n",
              "       'text': '3'},\n",
              "      {'x': 1941,\n",
              "       'y': 847,\n",
              "       'w': 36,\n",
              "       'h': 84,\n",
              "       'confidence': '0.9845581018018947',\n",
              "       'text': '8'},\n",
              "      {'x': 457,\n",
              "       'y': 949,\n",
              "       'w': 242,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999922742657932',\n",
              "       'text': 'Context Length'},\n",
              "      {'x': 989,\n",
              "       'y': 949,\n",
              "       'w': 242,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999922742657932',\n",
              "       'text': 'Context Length'},\n",
              "      {'x': 1655,\n",
              "       'y': 949,\n",
              "       'w': 242,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999922742657932',\n",
              "       'text': 'Context Length'}]}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['Nova Micro', 'Nova Lite', 'Nova Pro'],\n",
              "      ['10', '10', '10'],\n",
              "      ['20', '20', '20'],\n",
              "      ['30', '30', '30'],\n",
              "      ['40', '40', '40'],\n",
              "      ['50', '50', '50'],\n",
              "      ['60', '60', '60'],\n",
              "      ['70', '70', '70'],\n",
              "      ['80', '80', '80'],\n",
              "      ['90', '90', '90'],\n",
              "      ['100', '100', '100']],\n",
              "     'md': '|Nova Micro|Nova Lite|Nova Pro|\\n|10|10|10|\\n|20|20|20|\\n|30|30|30|\\n|40|40|40|\\n|50|50|50|\\n|60|60|60|\\n|70|70|70|\\n|80|80|80|\\n|90|90|90|\\n|100|100|100|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"Nova Micro\",\"Nova Lite\",\"Nova Pro\"\\n\"10\",\"10\",\"10\"\\n\"20\",\"20\",\"20\"\\n\"30\",\"30\",\"30\"\\n\"40\",\"40\",\"40\"\\n\"50\",\"50\",\"50\"\\n\"60\",\"60\",\"60\"\\n\"70\",\"70\",\"70\"\\n\"80\",\"80\",\"80\"\\n\"90\",\"90\",\"90\"\\n\"100\",\"100\",\"100\"',\n",
              "     'bBox': {'x': 72, 'y': 79, 'w': 468, 'h': 528.04}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Context Length\\n\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.',\n",
              "     'md': 'Context Length\\n\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.',\n",
              "     'bBox': {'x': 72, 'y': 79, 'w': 468, 'h': 426.04}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      [],\n",
              "      [],\n",
              "      ['', 'SQuALITY', 'LVBench'],\n",
              "      ['---', '---', '---'],\n",
              "      ['Nova Pro', '19.8 ±8.7', '41.6 ±2.5'],\n",
              "      ['Nova Lite', '19.2 ±8.6', '40.4 ±2.4'],\n",
              "      ['Nova Micro', '18.8 ±8.6', '-'],\n",
              "      ['Claude 3.5 Sonnet (Jun)', '13.4 ±7.5', '-'],\n",
              "      ['Gemini 1.5 Pro (001)', '-', '33.1 ±2.3'],\n",
              "      ['Gemini 1.5 Pro (002)', '19.1 ±8.6 M', '-'],\n",
              "      ['Gemini 1.5 Flash (002)', '18.1 ±8.4 M', '-'],\n",
              "      ['GPT-4o', '18.8 ±8.6', '30.8 ±2.3'],\n",
              "      ['Llama 3 - 70B', '16.4 ±8.1', '-'],\n",
              "      ['Llama 3 - 8B', '15.3 ±7.9', '-']],\n",
              "     'md': 'Context Length\\nFigure 2: Text Needle-in-a-Haystack recall performance for Nova Micro (up-to 128k), Nova Lite (up-to 300k) and Nova Pro (up-to 300k) models.\\n\\n| |SQuALITY|LVBench|\\n|---|---|---|\\n|Nova Pro|19.8 ±8.7|41.6 ±2.5|\\n|Nova Lite|19.2 ±8.6|40.4 ±2.4|\\n|Nova Micro|18.8 ±8.6|-|\\n|Claude 3.5 Sonnet (Jun)|13.4 ±7.5|-|\\n|Gemini 1.5 Pro (001)|-|33.1 ±2.3|\\n|Gemini 1.5 Pro (002)|19.1 ±8.6 M|-|\\n|Gemini 1.5 Flash (002)|18.1 ±8.4 M|-|\\n|GPT-4o|18.8 ±8.6|30.8 ±2.3|\\n|Llama 3 - 70B|16.4 ±8.1|-|\\n|Llama 3 - 8B|15.3 ±7.9|-|',\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\\n\\n\"\",\"SQuALITY\",\"LVBench\"\\n\"---\",\"---\",\"---\"\\n\"Nova Pro\",\"19.8 ±8.7\",\"41.6 ±2.5\"\\n\"Nova Lite\",\"19.2 ±8.6\",\"40.4 ±2.4\"\\n\"Nova Micro\",\"18.8 ±8.6\",\"-\"\\n\"Claude 3.5 Sonnet (Jun)\",\"13.4 ±7.5\",\"-\"\\n\"Gemini 1.5 Pro (001)\",\"-\",\"33.1 ±2.3\"\\n\"Gemini 1.5 Pro (002)\",\"19.1 ±8.6 M\",\"-\"\\n\"Gemini 1.5 Flash (002)\",\"18.1 ±8.4 M\",\"-\"\\n\"GPT-4o\",\"18.8 ±8.6\",\"30.8 ±2.3\"\\n\"Llama 3 - 70B\",\"16.4 ±8.1\",\"-\"\\n\"Llama 3 - 8B\",\"15.3 ±7.9\",\"-\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For SQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3 report [45]. Gemini results were measured by us2 (M). For LVBench, Gemini and GPT-4o numbers were taken from the corresponding benchmark leaderboard [77].\\n\\nResults for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering task, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other models. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any depth across both text and multimodal understanding use cases, delivering high accuracy and reliability.',\n",
              "     'md': 'Table 6: Text and Multimodal long context performance on SQuALITY (ROUGE-L) and LVBench (Accuracy). For SQuALITY, measurements for Claude 3.5 Sonnet, GPT-4o, Llama 3 70B and Llama 3 8B are taken from the Llama 3 report [45]. Gemini results were measured by us2 (M). For LVBench, Gemini and GPT-4o numbers were taken from the corresponding benchmark leaderboard [77].\\n\\nResults for text and multimodal long context benchmarks are presented in Table 6. In the long video question answering task, both Amazon Nova Pro and Lite demonstrate robust performance on the LVBench dataset, surpassing other models. Amazon Nova models consistently demonstrate exceptional performance in retrieving information from any depth across both text and multimodal understanding use cases, delivering high accuracy and reliability.',\n",
              "     'bBox': {'x': 71, 'y': 79, 'w': 469, 'h': 550.04}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2.4 Functional expertise',\n",
              "     'md': '# 2.4 Functional expertise',\n",
              "     'bBox': {'x': 72, 'y': 130, 'w': 109, 'h': 534.04}},\n",
              "    {'type': 'text',\n",
              "     'value': 'In addition to core capabilities, foundation models must perform well in particular specialties and domains. Across our many areas of performance analyses, we have selected four domains for which to present benchmarking results: Software engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can be found in Appendix B.3.',\n",
              "     'md': 'In addition to core capabilities, foundation models must perform well in particular specialties and domains. Across our many areas of performance analyses, we have selected four domains for which to present benchmarking results: Software engineering, financial analysis, and retrieval-augmented generation. Prompt templates for all benchmarks can be found in Appendix B.3.',\n",
              "     'bBox': {'x': 72, 'y': 229, 'w': 469, 'h': 491.04}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 12,\n",
              "   'text': '                                      The Amazon Nova Family of Models\\n                                                    Software           Finance              RAG\\n                                                   HumanEval            FinQA               CRAG\\n                                                     Python\\n                                          tok/       0-shot             0-shot           accuracy\\n                                          sec        pass@1           accuracy\\n            Nova Pro                      100      89.0 ±4.8          77.2 ±0.9          50.3 ±1.9\\n            Nova Lite                     157      85.4 ±5.4          73.6 ±0.9          43.8 ±1.9\\n            Nova Micro                    210      81.1 ±6.0          65.2 ±1.0          43.1 ±1.9\\n            Claude 3.5 Sonnet (Oct)        57      93.7 ±3.7          77.3 ±0.9 M        52.6 ±1.8 M\\n            Claude 3.5 Haiku               64      88.1 ±5.0          73.9 ±0.9 M        31.9 ±1.8 M\\n            Gemini 1.5 Pro (002)           58      87.8 ±5.0 M        74.4 ±0.9 M        48.9 ±1.9 M\\n            Gemini 1.5 Flash (002)        190      81.1 ±6.0 M        73.5 ±1.0 M        42.4 ±1.9 M\\n            Gemini 1.5 Flash 8B (001)     283      81.1 ±6.0 M        63.7 ±1.0 M        37.7 ±1.8 M\\n            GPT-4o                        163      90.2 ±4.6          71.1 ±1.0 M        52.0 ±1.9 M\\n            GPT-4o Mini                   113      87.2 ±5.1          70.6 ±1.0 M        49.9 ±1.9 M\\n            Llama 3.2 90B                  40      80.5 ±6.1          72.8 ±1.0 M        45.2 ±1.9 M\\n            Llama 3.2 11B                 124      72.6 ±6.8          60.8 ±1.1 M        42.2 ±1.9 M\\n            Llama 3.1 8B                  157      72.6 ±6.8          61.2 ±1.0 M        42.2 ±1.8 M\\nTable 7: Performance on select functional benchmarks, including software engineering benchmarks in Python with\\nHumanEval [19], financial reasoning with FinQA [20], and retrieval augmented generation with CRAG [82]. CRAG uses\\nour scoring method described in Section 2.4.3. Where available, reference numbers are taken from the corresponding\\nbenchmark papers and technical reports [13, 11, 32, 39, 45, 58]. Additional results were measured (M) by us2. Model\\nspeed in tokens per second (Tok/Sec) is reproduced from section 2.5.\\n2.4.1  Software engineering\\nWe assessed Amazon Nova’s code generation capabilities on the Python coding task HumanEval [19]. The bench-\\nmark contains 164 original programming problems with unit tests. These problems assess language comprehension,\\nalgorithms, and simple mathematics. Some problems are comparable to simple software interview questions. Table 7\\nprovides the performance of our Nova models and select public models.\\n2.4.2  Financial analysis\\nWe use FinQA [20] to evaluate Amazon Nova’s ability to understand financial data. FinQA is an expert-annotated\\ndataset comprising 8,281 financial question-answer pairs derived from the earnings reports of S&P 500 companies. It\\nevaluates a model’s ability to extract information from both tables and unstructured text, while accurately performing\\ncalculations using relevant financial knowledge. We report the average post-rounding accuracy under the 0-shot CoT\\nsetting. Table 7 provides the performance of Amazon Nova models and select public models on FinQA.\\n2.4.3  Retrieval augmented generation\\nWe evaluate RAG capabilities on the CRAG [82] benchmark using the Task 1 setup, which considers five pre-selected\\nHTML pages as external knowledge to each input question. We extract top-20 text snippets from these pages following\\nthe standard retrieval approach used in CRAG’s official repository, whereby pages are first cleaned using BeautifulSoup\\nto remove HTML tags, after which the text is then split into sentences or chunks no longer than 1000 characters.\\nThese are then encoded using the sentence-transformers/all-MiniLM-L6-v2 model, which is also used to encode the\\nquestion. The top 20 chunks with highest similarity are passed as context in the input for model inference. We report\\nthe percentage of correct responses as judged by an LLM (gpt-4-turbo-2024-04-09), which compares each model’s\\nanswer with the expected answer using the prompt shown in Appendix B.3.2. Table 7 provides the performance of\\nAmazon Nova models and selected public models on a combined validation and test set of 2,706 examples.\\n                                                       12',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n| | |Software|Finance|RAG| |\\n|---|---|---|---|---|---|\\n| | |HumanEval|FinQA|CRAG| |\\n|tok/ sec|0-shot pass@1| | |accuracy| |\\n|Nova Pro|100|89.0 ±4.8|77.2 ±0.9|50.3 ±1.9| |\\n|Nova Lite|157|85.4 ±5.4|73.6 ±0.9|43.8 ±1.9| |\\n|Nova Micro|210|81.1 ±6.0|65.2 ±1.0|43.1 ±1.9| |\\n|Claude 3.5 Sonnet (Oct)|57|93.7 ±3.7|77.3 ±0.9 M|52.6 ±1.8 M| |\\n|Claude 3.5 Haiku|64|88.1 ±5.0|73.9 ±0.9 M|31.9 ±1.8 M| |\\n|Gemini 1.5 Pro (002)|58|87.8 ±5.0 M|74.4 ±0.9 M|48.9 ±1.9 M| |\\n|Gemini 1.5 Flash (002)|190|81.1 ±6.0 M|73.5 ±1.0 M|42.4 ±1.9 M| |\\n|Gemini 1.5 Flash 8B (001)|283|81.1 ±6.0 M|63.7 ±1.0 M|37.7 ±1.8 M| |\\n|GPT-4o|163|90.2 ±4.6|71.1 ±1.0 M|52.0 ±1.9 M| |\\n|GPT-4o Mini|113|87.2 ±5.1|70.6 ±1.0 M|49.9 ±1.9 M| |\\n|Llama 3.2 90B|40|80.5 ±6.1|72.8 ±1.0 M|45.2 ±1.9 M| |\\n|Llama 3.2 11B|124|72.6 ±6.8|60.8 ±1.1 M|42.2 ±1.9 M| |\\n|Llama 3.1 8B|157|72.6 ±6.8|61.2 ±1.0 M|42.2 ±1.8 M| |\\n\\nTable 7: Performance on select functional benchmarks, including software engineering benchmarks in Python with HumanEval [19], financial reasoning with FinQA [20], and retrieval augmented generation with CRAG [82]. CRAG uses our scoring method described in Section 2.4.3. Where available, reference numbers are taken from the corresponding benchmark papers and technical reports [13, 11, 32, 39, 45, 58]. Additional results were measured (M) by us. Model speed in tokens per second (Tok/Sec) is reproduced from section 2.5.\\n\\n# 2.4.1 Software engineering\\n\\nWe assessed Amazon Nova’s code generation capabilities on the Python coding task HumanEval [19]. The benchmark contains 164 original programming problems with unit tests. These problems assess language comprehension, algorithms, and simple mathematics. Some problems are comparable to simple software interview questions. Table 7 provides the performance of our Nova models and select public models.\\n\\n# 2.4.2 Financial analysis\\n\\nWe use FinQA [20] to evaluate Amazon Nova’s ability to understand financial data. FinQA is an expert-annotated dataset comprising 8,281 financial question-answer pairs derived from the earnings reports of S&P 500 companies. It evaluates a model’s ability to extract information from both tables and unstructured text, while accurately performing calculations using relevant financial knowledge. We report the average post-rounding accuracy under the 0-shot CoT setting. Table 7 provides the performance of Amazon Nova models and select public models on FinQA.\\n\\n# 2.4.3 Retrieval augmented generation\\n\\nWe evaluate RAG capabilities on the CRAG [82] benchmark using the Task 1 setup, which considers five pre-selected HTML pages as external knowledge to each input question. We extract top-20 text snippets from these pages following the standard retrieval approach used in CRAG’s official repository, whereby pages are first cleaned using BeautifulSoup to remove HTML tags, after which the text is then split into sentences or chunks no longer than 1000 characters. These are then encoded using the sentence-transformers/all-MiniLM-L6-v2 model, which is also used to encode the question. The top 20 chunks with highest similarity are passed as context in the input for model inference. We report the percentage of correct responses as judged by an LLM (gpt-4-turbo-2024-04-09), which compares each model’s answer with the expected answer using the prompt shown in Appendix B.3.2. Table 7 provides the performance of Amazon Nova models and selected public models on a combined validation and test set of 2,706 examples.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['', '', 'Software', 'Finance', 'RAG', ''],\n",
              "      ['', '', 'HumanEval', 'FinQA', 'CRAG', ''],\n",
              "      ['tok/ sec', '0-shot pass@1', '', '', 'accuracy', ''],\n",
              "      ['Nova Pro', '100', '89.0 ±4.8', '77.2 ±0.9', '50.3 ±1.9', ''],\n",
              "      ['Nova Lite', '157', '85.4 ±5.4', '73.6 ±0.9', '43.8 ±1.9', ''],\n",
              "      ['Nova Micro', '210', '81.1 ±6.0', '65.2 ±1.0', '43.1 ±1.9', ''],\n",
              "      ['Claude 3.5 Sonnet (Oct)',\n",
              "       '57',\n",
              "       '93.7 ±3.7',\n",
              "       '77.3 ±0.9 M',\n",
              "       '52.6 ±1.8 M',\n",
              "       ''],\n",
              "      ['Claude 3.5 Haiku',\n",
              "       '64',\n",
              "       '88.1 ±5.0',\n",
              "       '73.9 ±0.9 M',\n",
              "       '31.9 ±1.8 M',\n",
              "       ''],\n",
              "      ['Gemini 1.5 Pro (002)',\n",
              "       '58',\n",
              "       '87.8 ±5.0 M',\n",
              "       '74.4 ±0.9 M',\n",
              "       '48.9 ±1.9 M',\n",
              "       ''],\n",
              "      ['Gemini 1.5 Flash (002)',\n",
              "       '190',\n",
              "       '81.1 ±6.0 M',\n",
              "       '73.5 ±1.0 M',\n",
              "       '42.4 ±1.9 M',\n",
              "       ''],\n",
              "      ['Gemini 1.5 Flash 8B (001)',\n",
              "       '283',\n",
              "       '81.1 ±6.0 M',\n",
              "       '63.7 ±1.0 M',\n",
              "       '37.7 ±1.8 M',\n",
              "       ''],\n",
              "      ['GPT-4o', '163', '90.2 ±4.6', '71.1 ±1.0 M', '52.0 ±1.9 M', ''],\n",
              "      ['GPT-4o Mini', '113', '87.2 ±5.1', '70.6 ±1.0 M', '49.9 ±1.9 M', ''],\n",
              "      ['Llama 3.2 90B', '40', '80.5 ±6.1', '72.8 ±1.0 M', '45.2 ±1.9 M', ''],\n",
              "      ['Llama 3.2 11B', '124', '72.6 ±6.8', '60.8 ±1.1 M', '42.2 ±1.9 M', ''],\n",
              "      ['Llama 3.1 8B', '157', '72.6 ±6.8', '61.2 ±1.0 M', '42.2 ±1.8 M', '']],\n",
              "     'md': '| | |Software|Finance|RAG| |\\n| | |HumanEval|FinQA|CRAG| |\\n|tok/ sec|0-shot pass@1| | |accuracy| |\\n|Nova Pro|100|89.0 ±4.8|77.2 ±0.9|50.3 ±1.9| |\\n|Nova Lite|157|85.4 ±5.4|73.6 ±0.9|43.8 ±1.9| |\\n|Nova Micro|210|81.1 ±6.0|65.2 ±1.0|43.1 ±1.9| |\\n|Claude 3.5 Sonnet (Oct)|57|93.7 ±3.7|77.3 ±0.9 M|52.6 ±1.8 M| |\\n|Claude 3.5 Haiku|64|88.1 ±5.0|73.9 ±0.9 M|31.9 ±1.8 M| |\\n|Gemini 1.5 Pro (002)|58|87.8 ±5.0 M|74.4 ±0.9 M|48.9 ±1.9 M| |\\n|Gemini 1.5 Flash (002)|190|81.1 ±6.0 M|73.5 ±1.0 M|42.4 ±1.9 M| |\\n|Gemini 1.5 Flash 8B (001)|283|81.1 ±6.0 M|63.7 ±1.0 M|37.7 ±1.8 M| |\\n|GPT-4o|163|90.2 ±4.6|71.1 ±1.0 M|52.0 ±1.9 M| |\\n|GPT-4o Mini|113|87.2 ±5.1|70.6 ±1.0 M|49.9 ±1.9 M| |\\n|Llama 3.2 90B|40|80.5 ±6.1|72.8 ±1.0 M|45.2 ±1.9 M| |\\n|Llama 3.2 11B|124|72.6 ±6.8|60.8 ±1.1 M|42.2 ±1.9 M| |\\n|Llama 3.1 8B|157|72.6 ±6.8|61.2 ±1.0 M|42.2 ±1.8 M| |',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"\",\"\",\"Software\",\"Finance\",\"RAG\",\"\"\\n\"\",\"\",\"HumanEval\",\"FinQA\",\"CRAG\",\"\"\\n\"tok/ sec\",\"0-shot pass@1\",\"\",\"\",\"accuracy\",\"\"\\n\"Nova Pro\",\"100\",\"89.0 ±4.8\",\"77.2 ±0.9\",\"50.3 ±1.9\",\"\"\\n\"Nova Lite\",\"157\",\"85.4 ±5.4\",\"73.6 ±0.9\",\"43.8 ±1.9\",\"\"\\n\"Nova Micro\",\"210\",\"81.1 ±6.0\",\"65.2 ±1.0\",\"43.1 ±1.9\",\"\"\\n\"Claude 3.5 Sonnet (Oct)\",\"57\",\"93.7 ±3.7\",\"77.3 ±0.9 M\",\"52.6 ±1.8 M\",\"\"\\n\"Claude 3.5 Haiku\",\"64\",\"88.1 ±5.0\",\"73.9 ±0.9 M\",\"31.9 ±1.8 M\",\"\"\\n\"Gemini 1.5 Pro (002)\",\"58\",\"87.8 ±5.0 M\",\"74.4 ±0.9 M\",\"48.9 ±1.9 M\",\"\"\\n\"Gemini 1.5 Flash (002)\",\"190\",\"81.1 ±6.0 M\",\"73.5 ±1.0 M\",\"42.4 ±1.9 M\",\"\"\\n\"Gemini 1.5 Flash 8B (001)\",\"283\",\"81.1 ±6.0 M\",\"63.7 ±1.0 M\",\"37.7 ±1.8 M\",\"\"\\n\"GPT-4o\",\"163\",\"90.2 ±4.6\",\"71.1 ±1.0 M\",\"52.0 ±1.9 M\",\"\"\\n\"GPT-4o Mini\",\"113\",\"87.2 ±5.1\",\"70.6 ±1.0 M\",\"49.9 ±1.9 M\",\"\"\\n\"Llama 3.2 90B\",\"40\",\"80.5 ±6.1\",\"72.8 ±1.0 M\",\"45.2 ±1.9 M\",\"\"\\n\"Llama 3.2 11B\",\"124\",\"72.6 ±6.8\",\"60.8 ±1.1 M\",\"42.2 ±1.9 M\",\"\"\\n\"Llama 3.1 8B\",\"157\",\"72.6 ±6.8\",\"61.2 ±1.0 M\",\"42.2 ±1.8 M\",\"\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 7: Performance on select functional benchmarks, including software engineering benchmarks in Python with HumanEval [19], financial reasoning with FinQA [20], and retrieval augmented generation with CRAG [82]. CRAG uses our scoring method described in Section 2.4.3. Where available, reference numbers are taken from the corresponding benchmark papers and technical reports [13, 11, 32, 39, 45, 58]. Additional results were measured (M) by us. Model speed in tokens per second (Tok/Sec) is reproduced from section 2.5.',\n",
              "     'md': 'Table 7: Performance on select functional benchmarks, including software engineering benchmarks in Python with HumanEval [19], financial reasoning with FinQA [20], and retrieval augmented generation with CRAG [82]. CRAG uses our scoring method described in Section 2.4.3. Where available, reference numbers are taken from the corresponding benchmark papers and technical reports [13, 11, 32, 39, 45, 58]. Additional results were measured (M) by us. Model speed in tokens per second (Tok/Sec) is reproduced from section 2.5.',\n",
              "     'bBox': {'x': 71, 'y': 73.04, 'w': 469, 'h': 539}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2.4.1 Software engineering',\n",
              "     'md': '# 2.4.1 Software engineering',\n",
              "     'bBox': {'x': 72, 'y': 73.04, 'w': 249, 'h': 365}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We assessed Amazon Nova’s code generation capabilities on the Python coding task HumanEval [19]. The benchmark contains 164 original programming problems with unit tests. These problems assess language comprehension, algorithms, and simple mathematics. Some problems are comparable to simple software interview questions. Table 7 provides the performance of our Nova models and select public models.',\n",
              "     'md': 'We assessed Amazon Nova’s code generation capabilities on the Python coding task HumanEval [19]. The benchmark contains 164 original programming problems with unit tests. These problems assess language comprehension, algorithms, and simple mathematics. Some problems are comparable to simple software interview questions. Table 7 provides the performance of our Nova models and select public models.',\n",
              "     'bBox': {'x': 72, 'y': 73.04, 'w': 469, 'h': 418}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2.4.2 Financial analysis',\n",
              "     'md': '# 2.4.2 Financial analysis',\n",
              "     'bBox': {'x': 72, 'y': 510.04, 'w': 105, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We use FinQA [20] to evaluate Amazon Nova’s ability to understand financial data. FinQA is an expert-annotated dataset comprising 8,281 financial question-answer pairs derived from the earnings reports of S&P 500 companies. It evaluates a model’s ability to extract information from both tables and unstructured text, while accurately performing calculations using relevant financial knowledge. We report the average post-rounding accuracy under the 0-shot CoT setting. Table 7 provides the performance of Amazon Nova models and select public models on FinQA.',\n",
              "     'md': 'We use FinQA [20] to evaluate Amazon Nova’s ability to understand financial data. FinQA is an expert-annotated dataset comprising 8,281 financial question-answer pairs derived from the earnings reports of S&P 500 companies. It evaluates a model’s ability to extract information from both tables and unstructured text, while accurately performing calculations using relevant financial knowledge. We report the average post-rounding accuracy under the 0-shot CoT setting. Table 7 provides the performance of Amazon Nova models and select public models on FinQA.',\n",
              "     'bBox': {'x': 71, 'y': 73.04, 'w': 469, 'h': 511}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2.4.3 Retrieval augmented generation',\n",
              "     'md': '# 2.4.3 Retrieval augmented generation',\n",
              "     'bBox': {'x': 72, 'y': 602.04, 'w': 165, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We evaluate RAG capabilities on the CRAG [82] benchmark using the Task 1 setup, which considers five pre-selected HTML pages as external knowledge to each input question. We extract top-20 text snippets from these pages following the standard retrieval approach used in CRAG’s official repository, whereby pages are first cleaned using BeautifulSoup to remove HTML tags, after which the text is then split into sentences or chunks no longer than 1000 characters. These are then encoded using the sentence-transformers/all-MiniLM-L6-v2 model, which is also used to encode the question. The top 20 chunks with highest similarity are passed as context in the input for model inference. We report the percentage of correct responses as judged by an LLM (gpt-4-turbo-2024-04-09), which compares each model’s answer with the expected answer using the prompt shown in Appendix B.3.2. Table 7 provides the performance of Amazon Nova models and selected public models on a combined validation and test set of 2,706 examples.',\n",
              "     'md': 'We evaluate RAG capabilities on the CRAG [82] benchmark using the Task 1 setup, which considers five pre-selected HTML pages as external knowledge to each input question. We extract top-20 text snippets from these pages following the standard retrieval approach used in CRAG’s official repository, whereby pages are first cleaned using BeautifulSoup to remove HTML tags, after which the text is then split into sentences or chunks no longer than 1000 characters. These are then encoded using the sentence-transformers/all-MiniLM-L6-v2 model, which is also used to encode the question. The top 20 chunks with highest similarity are passed as context in the input for model inference. We report the percentage of correct responses as judged by an LLM (gpt-4-turbo-2024-04-09), which compares each model’s answer with the expected answer using the prompt shown in Appendix B.3.2. Table 7 provides the performance of Amazon Nova models and selected public models on a combined validation and test set of 2,706 examples.',\n",
              "     'bBox': {'x': 71, 'y': 73.04, 'w': 471, 'h': 647}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 13,\n",
              "   'text': '                                      The Amazon Nova Family of Models\\n2.5  Runtime performance\\nWe evaluate the runtime performance of Amazon Nova models using three metrics: Time to First Token (TTFT), Output\\nTokens per Second (OTPS) and Total Response Time. TTFT is measured as the time, in seconds, it takes to receive the\\nfirst token from the model after an API request is sent. OTPS is measured as the number of tokens generated per second\\n(tok/sec). It is the rate at which a model produces subsequent output tokens after the first token, reflecting overall\\nthroughput and efficiency during inference. Total Response Time measures the total duration in seconds from the\\nsubmission of the input prompt to the end of generation sequence for a given input-output prompt length. It represents\\nthe overall user experience for a model.\\nIn Figure 3, we show TTFT, OTPS, and Total Response Time using 1000 tokens of input and 100 tokens of output\\nfor Amazon Nova models and select public models as reported by Artificial Analysis5, an independent entity that\\nbenchmarks AI models and hosting providers. Amazon Nova Micro, Lite and Pro models are among the fastest models\\nin their respective intelligence tiers. Together, all three Amazon Nova models demonstrate state-of-the-art runtime\\nperformance, ensuring a smooth and responsive user experience in many real world use cases.\\n  5https://artificialanalysis.ai/methodology\\n                                                       13',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# 2.5 Runtime performance\\n\\nWe evaluate the runtime performance of Amazon Nova models using three metrics: Time to First Token (TTFT), Output Tokens per Second (OTPS) and Total Response Time. TTFT is measured as the time, in seconds, it takes to receive the first token from the model after an API request is sent. OTPS is measured as the number of tokens generated per second (tok/sec). It is the rate at which a model produces subsequent output tokens after the first token, reflecting overall throughput and efficiency during inference. Total Response Time measures the total duration in seconds from the submission of the input prompt to the end of generation sequence for a given input-output prompt length. It represents the overall user experience for a model.\\n\\nIn Figure 3, we show TTFT, OTPS, and Total Response Time using 1000 tokens of input and 100 tokens of output for Amazon Nova models and select public models as reported by Artificial Analysis5, an independent entity that benchmarks AI models and hosting providers. Amazon Nova Micro, Lite and Pro models are among the fastest models in their respective intelligence tiers. Together, all three Amazon Nova models demonstrate state-of-the-art runtime performance, ensuring a smooth and responsive user experience in many real world use cases.\\n\\n5',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '2.5 Runtime performance',\n",
              "     'md': '# 2.5 Runtime performance',\n",
              "     'bBox': {'x': 72, 'y': 72.04, 'w': 116, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We evaluate the runtime performance of Amazon Nova models using three metrics: Time to First Token (TTFT), Output Tokens per Second (OTPS) and Total Response Time. TTFT is measured as the time, in seconds, it takes to receive the first token from the model after an API request is sent. OTPS is measured as the number of tokens generated per second (tok/sec). It is the rate at which a model produces subsequent output tokens after the first token, reflecting overall throughput and efficiency during inference. Total Response Time measures the total duration in seconds from the submission of the input prompt to the end of generation sequence for a given input-output prompt length. It represents the overall user experience for a model.\\n\\nIn Figure 3, we show TTFT, OTPS, and Total Response Time using 1000 tokens of input and 100 tokens of output for Amazon Nova models and select public models as reported by Artificial Analysis5, an independent entity that benchmarks AI models and hosting providers. Amazon Nova Micro, Lite and Pro models are among the fastest models in their respective intelligence tiers. Together, all three Amazon Nova models demonstrate state-of-the-art runtime performance, ensuring a smooth and responsive user experience in many real world use cases.\\n\\n5',\n",
              "     'md': 'We evaluate the runtime performance of Amazon Nova models using three metrics: Time to First Token (TTFT), Output Tokens per Second (OTPS) and Total Response Time. TTFT is measured as the time, in seconds, it takes to receive the first token from the model after an API request is sent. OTPS is measured as the number of tokens generated per second (tok/sec). It is the rate at which a model produces subsequent output tokens after the first token, reflecting overall throughput and efficiency during inference. Total Response Time measures the total duration in seconds from the submission of the input prompt to the end of generation sequence for a given input-output prompt length. It represents the overall user experience for a model.\\n\\nIn Figure 3, we show TTFT, OTPS, and Total Response Time using 1000 tokens of input and 100 tokens of output for Amazon Nova models and select public models as reported by Artificial Analysis5, an independent entity that benchmarks AI models and hosting providers. Amazon Nova Micro, Lite and Pro models are among the fastest models in their respective intelligence tiers. Together, all three Amazon Nova models demonstrate state-of-the-art runtime performance, ensuring a smooth and responsive user experience in many real world use cases.\\n\\n5',\n",
              "     'bBox': {'x': 71, 'y': 72.04, 'w': 469, 'h': 156}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://artificialanalysis.ai/methodology',\n",
              "     'text': 'https://artificialanalysis.ai/methodology'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 14,\n",
              "   'text': \"                                       The Amazon Nova Family of Models\\n                                                         Model Family\\n                                Meta      Amazon      Google     Mistral Al  OpenAI     Anthropic\\n        1.0                                                                                               0.98\\n       80.8                                                                             0.72  0.72  0.87\\n       80.6                                                            0.46  0.53  0.62\\n       20.4  0.29  0.32  0.35 0.35  0.36  0.36  0.37  0.38 0.42  0.42\\n       2\\n       20.2\\n         0.0 1~  9  1 1  8      2   1 8  1ᴬ⁸    1  9  1 2 1⁵ 2     8  1~ 8  Tv 2   81   A 8   3  2  8 2   2 2\\n                                                                                       0'\\n                     J                                                                 !    3    3     A\\n                            K                                            {                             J\\n                           J                                                                    8\\n             283\\n       8²⁵⁰        210   190\\n        200                    163  157   157\\n      8 150                                     124   115   113   100\\n      2 100                                                             73    64   58    57    40    35\\n       3 50          1                9         9                  2     2           {    2     8    2      29\\n             2 8  {     2  2    8           1⁵⁸ 3    1 2   8 1   1    ~     3  1  2     8     ~           3 8\\n          J                                    1                      1    8    N?\\n                     J                                                         J     0'          {\\n                                                                                    8\\n        4.0                                                                                               4.0\\n       83.5                                                                              2.8   2.9   3.4\\n       23.0                                                                  2.4   2.7\\n       L2.5\\n        2.0                                           1.3   1.4   1.5   1.7\\n        1.5        0.8   0.9   1.0   1.0        1,.2\\n      2 1.0  0.7\\n        0.5\\n        0.0   8   {  0  2 2   1 g   58    ~ 9    8   1 2   1 2   8 1   K 2  3  2  8  2  8 {   3 8     2   3 8\\n          N?                                                               8          K\\n          J          N                                                          3    J           {\\n                     J                                                        8\\nFigure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of\\ninput and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov\\n29th, 2024).\\n                                                         14\",\n",
              "   'md': \"# The Amazon Nova Family of Models\\n\\n# Model Family\\n\\n| | |Meta|Amazon|Google|Mistral AI|OpenAI|Anthropic| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|1.0| | | |0.98| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|80.8| | |0.72|0.72|0.87| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|80.6| | |0.46|0.53|0.62| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|20.4|0.29|0.32|0.35|0.35|0.36|0.36|0.37|0.38|0.42|0.42| | | | | | | | | | | | | | | | | | | | | | | | | |\\n| | |2|20.2|0.0|1~|9|1|1|8|2|1|8|1ᴬ⁸|1|9|1|2|1⁵|2|8|1~|8|Tv|2|81|A|8|3|2|8|2|2|2| | |\\n| |0'| |J| |!|3|3|A| | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |K| |{| |J| |8| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|283|8²⁵⁰|210|190|200|163|157|157| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|8|150| |124|115|113|100| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|2|100| |73|64|58|57|40|35| | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|3|50|1|9|9|2|2|{|2|8|2|29| | | | | | | | | | | | | | | | | | | | | | | | |\\n| |2|8|{|2|2|8|1⁵⁸|3|1|2|8|1|1|~|3|1|2|8|~|3|8| | | | | | | | | | | | | | |\\n| |J|1| |1|8|N?| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |J| | |0'|{| |8| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|4.0| | | |4.0| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|83.5| | |2.8|2.9|3.4| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|23.0| |2.4|2.7| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|L2.5| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|2.0| |1.3|1.4|1.5|1.7| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|1.5|0.8|0.9|1.0|1.0|1,.2| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |2|1.0|0.7|0.5|0.0|8|{|0|2|2|1|g|58|~|9|8|1|2|1|2|8|1|K|2|3|2|8|2|8|{|3|8|2|3|8|\\n| |N?| |8|K| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |J| |N| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n\\nFigure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of input and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov 29th, 2024).\\n\\n14\",\n",
              "   'images': [{'name': 'img_p13_1.png',\n",
              "     'height': 10505,\n",
              "     'width': 8503,\n",
              "     'x': 95.401,\n",
              "     'y': 115.97222899999997,\n",
              "     'original_width': 8503,\n",
              "     'original_height': 10505,\n",
              "     'ocr': [{'x': 4088,\n",
              "       'y': 43,\n",
              "       'w': 774,\n",
              "       'h': 181,\n",
              "       'confidence': '0.9839083488780975',\n",
              "       'text': 'Model Family'},\n",
              "      {'x': 2099,\n",
              "       'y': 236,\n",
              "       'w': 323,\n",
              "       'h': 126,\n",
              "       'confidence': '0.9999979138374329',\n",
              "       'text': 'Meta'},\n",
              "      {'x': 2928,\n",
              "       'y': 236,\n",
              "       'w': 438,\n",
              "       'h': 126,\n",
              "       'confidence': '0.9999618935846967',\n",
              "       'text': 'Amazon'},\n",
              "      {'x': 3861,\n",
              "       'y': 234,\n",
              "       'w': 394,\n",
              "       'h': 147,\n",
              "       'confidence': '0.8736473632916529',\n",
              "       'text': 'Google'},\n",
              "      {'x': 4766,\n",
              "       'y': 236,\n",
              "       'w': 471,\n",
              "       'h': 126,\n",
              "       'confidence': '0.991574360549493',\n",
              "       'text': 'Mistral Al'},\n",
              "      {'x': 5749,\n",
              "       'y': 234,\n",
              "       'w': 410,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9944369465507542',\n",
              "       'text': 'OpenAI'},\n",
              "      {'x': 6660,\n",
              "       'y': 234,\n",
              "       'w': 508,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999943574190016',\n",
              "       'text': 'Anthropic'},\n",
              "      {'x': 226,\n",
              "       'y': 447,\n",
              "       'w': 204,\n",
              "       'h': 147,\n",
              "       'confidence': '0.8053494691848755',\n",
              "       'text': '1.0'},\n",
              "      {'x': 8070,\n",
              "       'y': 397,\n",
              "       'w': 314,\n",
              "       'h': 157,\n",
              "       'confidence': '0.999979555606842',\n",
              "       'text': '0.98'},\n",
              "      {'x': 7603,\n",
              "       'y': 602,\n",
              "       'w': 313,\n",
              "       'h': 157,\n",
              "       'confidence': '0.8997139530749945',\n",
              "       'text': '0.87'},\n",
              "      {'x': 71,\n",
              "       'y': 661,\n",
              "       'w': 136,\n",
              "       'h': 302,\n",
              "       'confidence': '0.688268856047646',\n",
              "       'text': '8'},\n",
              "      {'x': 210,\n",
              "       'y': 817,\n",
              "       'w': 219,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9999953198664192',\n",
              "       'text': '0.8'},\n",
              "      {'x': 6660,\n",
              "       'y': 882,\n",
              "       'w': 311,\n",
              "       'h': 147,\n",
              "       'confidence': '0.999975860118866',\n",
              "       'text': '0.72'},\n",
              "      {'x': 7136,\n",
              "       'y': 882,\n",
              "       'w': 311,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9981343976945429',\n",
              "       'text': '0.72'},\n",
              "      {'x': 53,\n",
              "       'y': 997,\n",
              "       'w': 147,\n",
              "       'h': 377,\n",
              "       'confidence': '0.8895793391360804',\n",
              "       'text': '8'},\n",
              "      {'x': 210,\n",
              "       'y': 1187,\n",
              "       'w': 219,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9999767370120424',\n",
              "       'text': '0.6'},\n",
              "      {'x': 6192,\n",
              "       'y': 1063,\n",
              "       'w': 311,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999938607215881',\n",
              "       'text': '0.62'},\n",
              "      {'x': 5724,\n",
              "       'y': 1227,\n",
              "       'w': 311,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9988323863847912',\n",
              "       'text': '0.53'},\n",
              "      {'x': 5247,\n",
              "       'y': 1349,\n",
              "       'w': 322,\n",
              "       'h': 157,\n",
              "       'confidence': '0.9999840259552002',\n",
              "       'text': '0.46'},\n",
              "      {'x': 57,\n",
              "       'y': 1370,\n",
              "       'w': 149,\n",
              "       'h': 304,\n",
              "       'confidence': '0.9471026370546013',\n",
              "       'text': '2'},\n",
              "      {'x': 210,\n",
              "       'y': 1548,\n",
              "       'w': 219,\n",
              "       'h': 137,\n",
              "       'confidence': '0.73024243558224',\n",
              "       'text': '0.4'},\n",
              "      {'x': 1489,\n",
              "       'y': 1555,\n",
              "       'w': 312,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9725168958543228',\n",
              "       'text': '0.35'},\n",
              "      {'x': 1957,\n",
              "       'y': 1555,\n",
              "       'w': 311,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999983906745911',\n",
              "       'text': '0.35'},\n",
              "      {'x': 2425,\n",
              "       'y': 1539,\n",
              "       'w': 319,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9998807311058044',\n",
              "       'text': '0.36'},\n",
              "      {'x': 2893,\n",
              "       'y': 1539,\n",
              "       'w': 319,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9166766923509',\n",
              "       'text': '0.36'},\n",
              "      {'x': 3369,\n",
              "       'y': 1522,\n",
              "       'w': 303,\n",
              "       'h': 147,\n",
              "       'confidence': '0.5533049631119245',\n",
              "       'text': '0.37'},\n",
              "      {'x': 3836,\n",
              "       'y': 1497,\n",
              "       'w': 313,\n",
              "       'h': 157,\n",
              "       'confidence': '0.9059684872627258',\n",
              "       'text': '0.38'},\n",
              "      {'x': 4303,\n",
              "       'y': 1423,\n",
              "       'w': 322,\n",
              "       'h': 157,\n",
              "       'confidence': '0.9999935626983643',\n",
              "       'text': '0.42'},\n",
              "      {'x': 4779,\n",
              "       'y': 1423,\n",
              "       'w': 314,\n",
              "       'h': 157,\n",
              "       'confidence': '0.926105747451197',\n",
              "       'text': '0.42'},\n",
              "      {'x': 62,\n",
              "       'y': 1671,\n",
              "       'w': 137,\n",
              "       'h': 161,\n",
              "       'confidence': '0.6138422000714492',\n",
              "       'text': '2'},\n",
              "      {'x': 545,\n",
              "       'y': 1661,\n",
              "       'w': 313,\n",
              "       'h': 157,\n",
              "       'confidence': '0.9449063940231073',\n",
              "       'text': '0.29'},\n",
              "      {'x': 1013,\n",
              "       'y': 1612,\n",
              "       'w': 312,\n",
              "       'h': 148,\n",
              "       'confidence': '0.9686722281280554',\n",
              "       'text': '0.32'},\n",
              "      {'x': 45,\n",
              "       'y': 1809,\n",
              "       'w': 169,\n",
              "       'h': 345,\n",
              "       'confidence': '0.8821796873711527',\n",
              "       'text': '2'},\n",
              "      {'x': 209,\n",
              "       'y': 1908,\n",
              "       'w': 221,\n",
              "       'h': 147,\n",
              "       'confidence': '0.8353034853935242',\n",
              "       'text': '0.2'},\n",
              "      {'x': 210,\n",
              "       'y': 2278,\n",
              "       'w': 211,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9999902267832098',\n",
              "       'text': '0.0'},\n",
              "      {'x': 1544,\n",
              "       'y': 2397,\n",
              "       'w': 186,\n",
              "       'h': 203,\n",
              "       'confidence': '0.9887108552995336',\n",
              "       'text': '8'},\n",
              "      {'x': 2913,\n",
              "       'y': 2569,\n",
              "       'w': 115,\n",
              "       'h': 98,\n",
              "       'confidence': '0.5001947202801524',\n",
              "       'text': 'A'},\n",
              "      {'x': 5651,\n",
              "       'y': 2697,\n",
              "       'w': 137,\n",
              "       'h': 145,\n",
              "       'confidence': '0.25478391924324834',\n",
              "       'text': 'Tv'},\n",
              "      {'x': 6606,\n",
              "       'y': 2700,\n",
              "       'w': 115,\n",
              "       'h': 98,\n",
              "       'confidence': '0.1331048338541132',\n",
              "       'text': 'A'},\n",
              "      {'x': 6547,\n",
              "       'y': 2788,\n",
              "       'w': 159,\n",
              "       'h': 127,\n",
              "       'confidence': '0.5560608740220718',\n",
              "       'text': \"0'\"},\n",
              "      {'x': 7838,\n",
              "       'y': 3021,\n",
              "       'w': 105,\n",
              "       'h': 88,\n",
              "       'confidence': '0.16249399394812158',\n",
              "       'text': 'A'},\n",
              "      {'x': 1659,\n",
              "       'y': 3119,\n",
              "       'w': 112,\n",
              "       'h': 88,\n",
              "       'confidence': '0.33426752427784834',\n",
              "       'text': 'K'},\n",
              "      {'x': 570,\n",
              "       'y': 3755,\n",
              "       'w': 270,\n",
              "       'h': 147,\n",
              "       'confidence': '1.0',\n",
              "       'text': '283'},\n",
              "      {'x': 176,\n",
              "       'y': 4075,\n",
              "       'w': 254,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999971781544015',\n",
              "       'text': '250'},\n",
              "      {'x': 1029,\n",
              "       'y': 4205,\n",
              "       'w': 280,\n",
              "       'h': 157,\n",
              "       'confidence': '0.9999995870469189',\n",
              "       'text': '210'},\n",
              "      {'x': 20,\n",
              "       'y': 4255,\n",
              "       'w': 147,\n",
              "       'h': 484,\n",
              "       'confidence': '0.4429378318724595',\n",
              "       'text': '8'},\n",
              "      {'x': 176,\n",
              "       'y': 4386,\n",
              "       'w': 254,\n",
              "       'h': 148,\n",
              "       'confidence': '0.9999741904763433',\n",
              "       'text': '200'},\n",
              "      {'x': 1513,\n",
              "       'y': 4336,\n",
              "       'w': 264,\n",
              "       'h': 158,\n",
              "       'confidence': '0.9999987611408254',\n",
              "       'text': '190'},\n",
              "      {'x': 1989,\n",
              "       'y': 4509,\n",
              "       'w': 264,\n",
              "       'h': 157,\n",
              "       'confidence': '0.9999995870469189',\n",
              "       'text': '163'},\n",
              "      {'x': 2450,\n",
              "       'y': 4551,\n",
              "       'w': 262,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999987611408254',\n",
              "       'text': '157'},\n",
              "      {'x': 2926,\n",
              "       'y': 4551,\n",
              "       'w': 253,\n",
              "       'h': 147,\n",
              "       'confidence': '0.7397971749305725',\n",
              "       'text': '157'},\n",
              "      {'x': 34,\n",
              "       'y': 4696,\n",
              "       'w': 407,\n",
              "       'h': 284,\n",
              "       'confidence': '0.4340921058134146',\n",
              "       'text': '8 150'},\n",
              "      {'x': 3402,\n",
              "       'y': 4756,\n",
              "       'w': 253,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9920586943626404',\n",
              "       'text': '124'},\n",
              "      {'x': 3878,\n",
              "       'y': 4813,\n",
              "       'w': 253,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9996430277824402',\n",
              "       'text': '115'},\n",
              "      {'x': 4345,\n",
              "       'y': 4830,\n",
              "       'w': 254,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999969716779362',\n",
              "       'text': '113'},\n",
              "      {'x': 4813,\n",
              "       'y': 4912,\n",
              "       'w': 262,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999154827455907',\n",
              "       'text': '100'},\n",
              "      {'x': 184,\n",
              "       'y': 5018,\n",
              "       'w': 246,\n",
              "       'h': 148,\n",
              "       'confidence': '0.998988151550293',\n",
              "       'text': '100'},\n",
              "      {'x': 20,\n",
              "       'y': 4969,\n",
              "       'w': 147,\n",
              "       'h': 451,\n",
              "       'confidence': '0.2324521445270875',\n",
              "       'text': '2'},\n",
              "      {'x': 5324,\n",
              "       'y': 5094,\n",
              "       'w': 176,\n",
              "       'h': 127,\n",
              "       'confidence': '0.9367439723640099',\n",
              "       'text': '73'},\n",
              "      {'x': 5782,\n",
              "       'y': 5142,\n",
              "       'w': 188,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999997471189183',\n",
              "       'text': '64'},\n",
              "      {'x': 6249,\n",
              "       'y': 5174,\n",
              "       'w': 197,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999988198884456',\n",
              "       'text': '58'},\n",
              "      {'x': 6725,\n",
              "       'y': 5183,\n",
              "       'w': 180,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999967968410157',\n",
              "       'text': '57'},\n",
              "      {'x': 251,\n",
              "       'y': 5348,\n",
              "       'w': 178,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9999833098887841',\n",
              "       'text': '50'},\n",
              "      {'x': 7193,\n",
              "       'y': 5289,\n",
              "       'w': 188,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9800780570169296',\n",
              "       'text': '40'},\n",
              "      {'x': 7661,\n",
              "       'y': 5322,\n",
              "       'w': 196,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9038006883505015',\n",
              "       'text': '35'},\n",
              "      {'x': 8137,\n",
              "       'y': 5363,\n",
              "       'w': 188,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999989884757856',\n",
              "       'text': '29'},\n",
              "      {'x': 14,\n",
              "       'y': 5423,\n",
              "       'w': 171,\n",
              "       'h': 446,\n",
              "       'confidence': '0.5015159722621689',\n",
              "       'text': '3'},\n",
              "      {'x': 2955,\n",
              "       'y': 5770,\n",
              "       'w': 186,\n",
              "       'h': 211,\n",
              "       'confidence': '0.6754507575908626',\n",
              "       'text': '8'},\n",
              "      {'x': 3273,\n",
              "       'y': 5981,\n",
              "       'w': 216,\n",
              "       'h': 249,\n",
              "       'confidence': '0.10451296868180382',\n",
              "       'text': '3'},\n",
              "      {'x': 5163,\n",
              "       'y': 6008,\n",
              "       'w': 186,\n",
              "       'h': 227,\n",
              "       'confidence': '0.1981989766605352',\n",
              "       'text': '~'},\n",
              "      {'x': 5942,\n",
              "       'y': 6295,\n",
              "       'w': 196,\n",
              "       'h': 212,\n",
              "       'confidence': '0.5486785747508187',\n",
              "       'text': 'N?'},\n",
              "      {'x': 6324,\n",
              "       'y': 6546,\n",
              "       'w': 162,\n",
              "       'h': 137,\n",
              "       'confidence': '0.10241533919824125',\n",
              "       'text': \"0'\"},\n",
              "      {'x': 8114,\n",
              "       'y': 7130,\n",
              "       'w': 226,\n",
              "       'h': 126,\n",
              "       'confidence': '0.9994204044342041',\n",
              "       'text': '4.0'},\n",
              "      {'x': 219,\n",
              "       'y': 7253,\n",
              "       'w': 201,\n",
              "       'h': 126,\n",
              "       'confidence': '0.9995023392916654',\n",
              "       'text': '4.0'},\n",
              "      {'x': 7653,\n",
              "       'y': 7390,\n",
              "       'w': 221,\n",
              "       'h': 147,\n",
              "       'confidence': '0.72256789348978',\n",
              "       'text': '3.4'},\n",
              "      {'x': 71,\n",
              "       'y': 7391,\n",
              "       'w': 136,\n",
              "       'h': 268,\n",
              "       'confidence': '0.9989661267748353',\n",
              "       'text': '8'},\n",
              "      {'x': 218,\n",
              "       'y': 7473,\n",
              "       'w': 203,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9999552635490495',\n",
              "       'text': '3.5'},\n",
              "      {'x': 218,\n",
              "       'y': 7695,\n",
              "       'w': 203,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9999300047790276',\n",
              "       'text': '3.0'},\n",
              "      {'x': 6232,\n",
              "       'y': 7701,\n",
              "       'w': 231,\n",
              "       'h': 157,\n",
              "       'confidence': '0.6016059802823547',\n",
              "       'text': '2.7'},\n",
              "      {'x': 6701,\n",
              "       'y': 7661,\n",
              "       'w': 229,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9999894499778748',\n",
              "       'text': '2.8'},\n",
              "      {'x': 7176,\n",
              "       'y': 7611,\n",
              "       'w': 231,\n",
              "       'h': 157,\n",
              "       'confidence': '0.9999029566541391',\n",
              "       'text': '2.9'},\n",
              "      {'x': 47,\n",
              "       'y': 7687,\n",
              "       'w': 163,\n",
              "       'h': 325,\n",
              "       'confidence': '0.8541750535076886',\n",
              "       'text': '2'},\n",
              "      {'x': 5765,\n",
              "       'y': 7833,\n",
              "       'w': 221,\n",
              "       'h': 148,\n",
              "       'confidence': '0.7943207621574402',\n",
              "       'text': '2.4'},\n",
              "      {'x': 209,\n",
              "       'y': 7915,\n",
              "       'w': 221,\n",
              "       'h': 148,\n",
              "       'confidence': '0.5617242455482483',\n",
              "       'text': '2.5'},\n",
              "      {'x': 218,\n",
              "       'y': 8146,\n",
              "       'w': 211,\n",
              "       'h': 137,\n",
              "       'confidence': '0.7588341236114502',\n",
              "       'text': '2.0'},\n",
              "      {'x': 5308,\n",
              "       'y': 8164,\n",
              "       'w': 208,\n",
              "       'h': 126,\n",
              "       'confidence': '0.8548241594752413',\n",
              "       'text': '1.7'},\n",
              "      {'x': 56,\n",
              "       'y': 8017,\n",
              "       'w': 175,\n",
              "       'h': 634,\n",
              "       'confidence': '0.604393879951207',\n",
              "       'text': 'L'},\n",
              "      {'x': 227,\n",
              "       'y': 8368,\n",
              "       'w': 202,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9992104754561254',\n",
              "       'text': '1.5'},\n",
              "      {'x': 3418,\n",
              "       'y': 8375,\n",
              "       'w': 221,\n",
              "       'h': 147,\n",
              "       'confidence': '0.307715336994227',\n",
              "       'text': '1,.2'},\n",
              "      {'x': 3887,\n",
              "       'y': 8335,\n",
              "       'w': 227,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9866940975189209',\n",
              "       'text': '1.3'},\n",
              "      {'x': 4354,\n",
              "       'y': 8285,\n",
              "       'w': 229,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9974104766619256',\n",
              "       'text': '1.4'},\n",
              "      {'x': 4830,\n",
              "       'y': 8244,\n",
              "       'w': 221,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9987100460591064',\n",
              "       'text': '1.5'},\n",
              "      {'x': 1523,\n",
              "       'y': 8516,\n",
              "       'w': 236,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9998371601104736',\n",
              "       'text': '0.9'},\n",
              "      {'x': 2006,\n",
              "       'y': 8465,\n",
              "       'w': 221,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9916629195213318',\n",
              "       'text': '1.0'},\n",
              "      {'x': 2482,\n",
              "       'y': 8465,\n",
              "       'w': 221,\n",
              "       'h': 147,\n",
              "       'confidence': '0.9993635975959063',\n",
              "       'text': '1.0'},\n",
              "      {'x': 226,\n",
              "       'y': 8588,\n",
              "       'w': 204,\n",
              "       'h': 148,\n",
              "       'confidence': '0.9998610425779199',\n",
              "       'text': '1.0'},\n",
              "      {'x': 588,\n",
              "       'y': 8606,\n",
              "       'w': 210,\n",
              "       'h': 137,\n",
              "       'confidence': '0.5166886317871929',\n",
              "       'text': '0.7'},\n",
              "      {'x': 1054,\n",
              "       'y': 8556,\n",
              "       'w': 230,\n",
              "       'h': 147,\n",
              "       'confidence': '0.7083911299705505',\n",
              "       'text': '0.8'},\n",
              "      {'x': 53,\n",
              "       'y': 8646,\n",
              "       'w': 147,\n",
              "       'h': 319,\n",
              "       'confidence': '0.19658218897651072',\n",
              "       'text': '2'},\n",
              "      {'x': 211,\n",
              "       'y': 8820,\n",
              "       'w': 209,\n",
              "       'h': 127,\n",
              "       'confidence': '0.9998821020126343',\n",
              "       'text': '0.5'},\n",
              "      {'x': 210,\n",
              "       'y': 9041,\n",
              "       'w': 211,\n",
              "       'h': 137,\n",
              "       'confidence': '0.9999913968155315',\n",
              "       'text': '0.0'},\n",
              "      {'x': 2478,\n",
              "       'y': 9143,\n",
              "       'w': 197,\n",
              "       'h': 212,\n",
              "       'confidence': '0.49838369517604875',\n",
              "       'text': '8'},\n",
              "      {'x': 5228,\n",
              "       'y': 9397,\n",
              "       'w': 114,\n",
              "       'h': 98,\n",
              "       'confidence': '0.17841355045820606',\n",
              "       'text': 'K'},\n",
              "      {'x': 320,\n",
              "       'y': 9635,\n",
              "       'w': 196,\n",
              "       'h': 213,\n",
              "       'confidence': '0.12168862952506394',\n",
              "       'text': 'N?'},\n",
              "      {'x': 6426,\n",
              "       'y': 9774,\n",
              "       'w': 114,\n",
              "       'h': 98,\n",
              "       'confidence': '0.3055300490850641',\n",
              "       'text': 'K'},\n",
              "      {'x': 1191,\n",
              "       'y': 9882,\n",
              "       'w': 104,\n",
              "       'h': 88,\n",
              "       'confidence': '0.10631055459378391',\n",
              "       'text': 'N'},\n",
              "      {'x': 6340,\n",
              "       'y': 2349,\n",
              "       'w': 16,\n",
              "       'h': 349,\n",
              "       'confidence': '0.7712873667529188',\n",
              "       'text': '1'},\n",
              "      {'x': 689,\n",
              "       'y': 2338,\n",
              "       'w': 17,\n",
              "       'h': 342,\n",
              "       'confidence': '0.24297760018134884',\n",
              "       'text': '9'},\n",
              "      {'x': 2560,\n",
              "       'y': 2337,\n",
              "       'w': 34,\n",
              "       'h': 425,\n",
              "       'confidence': '0.24729664785958505',\n",
              "       'text': '8'},\n",
              "      {'x': 3025,\n",
              "       'y': 2354,\n",
              "       'w': 47,\n",
              "       'h': 277,\n",
              "       'confidence': '0.9975590122924736',\n",
              "       'text': '8'},\n",
              "      {'x': 3509,\n",
              "       'y': 2335,\n",
              "       'w': 28,\n",
              "       'h': 399,\n",
              "       'confidence': '0.2454178347646243',\n",
              "       'text': '9'},\n",
              "      {'x': 4001,\n",
              "       'y': 2356,\n",
              "       'w': 44,\n",
              "       'h': 348,\n",
              "       'confidence': '0.23636700062355942',\n",
              "       'text': '2'},\n",
              "      {'x': 4446,\n",
              "       'y': 2337,\n",
              "       'w': 10,\n",
              "       'h': 368,\n",
              "       'confidence': '0.8387163422538926',\n",
              "       'text': '2'},\n",
              "      {'x': 5381,\n",
              "       'y': 2338,\n",
              "       'w': 1,\n",
              "       'h': 363,\n",
              "       'confidence': '0.6375358123601451',\n",
              "       'text': '8'},\n",
              "      {'x': 6797,\n",
              "       'y': 2340,\n",
              "       'w': 38,\n",
              "       'h': 425,\n",
              "       'confidence': '0.8758150447147841',\n",
              "       'text': '8'},\n",
              "      {'x': 1152,\n",
              "       'y': 2338,\n",
              "       'w': 87,\n",
              "       'h': 503,\n",
              "       'confidence': '0.492085690319616',\n",
              "       'text': '1'},\n",
              "      {'x': 7291,\n",
              "       'y': 2354,\n",
              "       'w': 85,\n",
              "       'h': 431,\n",
              "       'confidence': '0.5462918639807555',\n",
              "       'text': '2'},\n",
              "      {'x': 7741,\n",
              "       'y': 2334,\n",
              "       'w': 24,\n",
              "       'h': 429,\n",
              "       'confidence': '0.6988871726352954',\n",
              "       'text': '2'},\n",
              "      {'x': 4921,\n",
              "       'y': 2356,\n",
              "       'w': 111,\n",
              "       'h': 533,\n",
              "       'confidence': '0.9614069831212682',\n",
              "       'text': '8'},\n",
              "      {'x': 8211,\n",
              "       'y': 2340,\n",
              "       'w': 41,\n",
              "       'h': 452,\n",
              "       'confidence': '0.16219706043338888',\n",
              "       'text': '2'},\n",
              "      {'x': 5852,\n",
              "       'y': 2344,\n",
              "       'w': 38,\n",
              "       'h': 442,\n",
              "       'confidence': '0.31891921946466084',\n",
              "       'text': '2'},\n",
              "      {'x': 2088,\n",
              "       'y': 2373,\n",
              "       'w': 45,\n",
              "       'h': 415,\n",
              "       'confidence': '0.5449010616340253',\n",
              "       'text': '2'},\n",
              "      {'x': 555,\n",
              "       'y': 2554,\n",
              "       'w': 13,\n",
              "       'h': 332,\n",
              "       'confidence': '0.7109246713671382',\n",
              "       'text': '~'},\n",
              "      {'x': 5253,\n",
              "       'y': 2581,\n",
              "       'w': 27,\n",
              "       'h': 308,\n",
              "       'confidence': '0.48903673352641874',\n",
              "       'text': '~'},\n",
              "      {'x': 4307,\n",
              "       'y': 2589,\n",
              "       'w': 31,\n",
              "       'h': 300,\n",
              "       'confidence': '0.13600577883881115',\n",
              "       'text': '5'},\n",
              "      {'x': 6187,\n",
              "       'y': 2594,\n",
              "       'w': 109,\n",
              "       'h': 533,\n",
              "       'confidence': '0.9765466791082531',\n",
              "       'text': '8'},\n",
              "      {'x': 3835,\n",
              "       'y': 2590,\n",
              "       'w': 77,\n",
              "       'h': 483,\n",
              "       'confidence': '0.2795716485537447',\n",
              "       'text': '1'},\n",
              "      {'x': 2409,\n",
              "       'y': 2647,\n",
              "       'w': 97,\n",
              "       'h': 477,\n",
              "       'confidence': '0.33115697486482887',\n",
              "       'text': '1'},\n",
              "      {'x': 3356,\n",
              "       'y': 2626,\n",
              "       'w': 89,\n",
              "       'h': 473,\n",
              "       'confidence': '0.3041710120271546',\n",
              "       'text': '1'},\n",
              "      {'x': 7591,\n",
              "       'y': 2636,\n",
              "       'w': 108,\n",
              "       'h': 511,\n",
              "       'confidence': '0.27097032986881686',\n",
              "       'text': '8'},\n",
              "      {'x': 7088,\n",
              "       'y': 2673,\n",
              "       'w': 12,\n",
              "       'h': 306,\n",
              "       'confidence': '0.8785177079196025',\n",
              "       'text': '3'},\n",
              "      {'x': 8064,\n",
              "       'y': 2672,\n",
              "       'w': 39,\n",
              "       'h': 314,\n",
              "       'confidence': '0.6479517934494652',\n",
              "       'text': '2'},\n",
              "      {'x': 2845,\n",
              "       'y': 2727,\n",
              "       'w': 91,\n",
              "       'h': 448,\n",
              "       'confidence': '0.10543889472213497',\n",
              "       'text': '1'},\n",
              "      {'x': 929,\n",
              "       'y': 2737,\n",
              "       'w': 77,\n",
              "       'h': 475,\n",
              "       'confidence': '0.2847387905387997',\n",
              "       'text': '1'},\n",
              "      {'x': 453,\n",
              "       'y': 2780,\n",
              "       'w': 82,\n",
              "       'h': 449,\n",
              "       'confidence': '0.10974362991058584',\n",
              "       'text': '1'},\n",
              "      {'x': 4212,\n",
              "       'y': 2789,\n",
              "       'w': 82,\n",
              "       'h': 448,\n",
              "       'confidence': '0.13955380649372273',\n",
              "       'text': '1'},\n",
              "      {'x': 5156,\n",
              "       'y': 2789,\n",
              "       'w': 84,\n",
              "       'h': 452,\n",
              "       'confidence': '0.15025342471499759',\n",
              "       'text': '1'},\n",
              "      {'x': 6527,\n",
              "       'y': 2854,\n",
              "       'w': 80,\n",
              "       'h': 450,\n",
              "       'confidence': '0.1012778842840456',\n",
              "       'text': '!'},\n",
              "      {'x': 6977,\n",
              "       'y': 2871,\n",
              "       'w': 99,\n",
              "       'h': 507,\n",
              "       'confidence': '0.21164851314102862',\n",
              "       'text': '3'},\n",
              "      {'x': 1244,\n",
              "       'y': 3027,\n",
              "       'w': 108,\n",
              "       'h': 508,\n",
              "       'confidence': '0.8597333670758331',\n",
              "       'text': 'J'},\n",
              "      {'x': 7351,\n",
              "       'y': 3032,\n",
              "       'w': 19,\n",
              "       'h': 305,\n",
              "       'confidence': '0.8849601028396705',\n",
              "       'text': '3'},\n",
              "      {'x': 7811,\n",
              "       'y': 3065,\n",
              "       'w': 109,\n",
              "       'h': 510,\n",
              "       'confidence': '0.8648671976815514',\n",
              "       'text': 'J'},\n",
              "      {'x': 5451,\n",
              "       'y': 3091,\n",
              "       'w': 101,\n",
              "       'h': 477,\n",
              "       'confidence': '0.13479203859975897',\n",
              "       'text': '{'},\n",
              "      {'x': 1629,\n",
              "       'y': 3166,\n",
              "       'w': 98,\n",
              "       'h': 513,\n",
              "       'confidence': '0.8796104708736152',\n",
              "       'text': 'J'},\n",
              "      {'x': 7242,\n",
              "       'y': 3235,\n",
              "       'w': 99,\n",
              "       'h': 499,\n",
              "       'confidence': '0.543002418566175',\n",
              "       'text': '8'},\n",
              "      {'x': 4453,\n",
              "       'y': 5727,\n",
              "       'w': 10,\n",
              "       'h': 355,\n",
              "       'confidence': '0.6726373501387606',\n",
              "       'text': '1'},\n",
              "      {'x': 690,\n",
              "       'y': 5737,\n",
              "       'w': 22,\n",
              "       'h': 276,\n",
              "       'confidence': '0.8826998490445135',\n",
              "       'text': '8'},\n",
              "      {'x': 2560,\n",
              "       'y': 5717,\n",
              "       'w': 19,\n",
              "       'h': 402,\n",
              "       'confidence': '0.4157630524396758',\n",
              "       'text': '9'},\n",
              "      {'x': 3505,\n",
              "       'y': 5719,\n",
              "       'w': 0,\n",
              "       'h': 343,\n",
              "       'confidence': '0.6983019021479464',\n",
              "       'text': '9'},\n",
              "      {'x': 3977,\n",
              "       'y': 5732,\n",
              "       'w': 38,\n",
              "       'h': 406,\n",
              "       'confidence': '0.6568197627317183',\n",
              "       'text': '2'},\n",
              "      {'x': 5383,\n",
              "       'y': 5721,\n",
              "       'w': 4,\n",
              "       'h': 363,\n",
              "       'confidence': '0.9163023418543084',\n",
              "       'text': '2'},\n",
              "      {'x': 6799,\n",
              "       'y': 5712,\n",
              "       'w': 26,\n",
              "       'h': 431,\n",
              "       'confidence': '0.7325677511144022',\n",
              "       'text': '2'},\n",
              "      {'x': 7266,\n",
              "       'y': 5720,\n",
              "       'w': 1,\n",
              "       'h': 364,\n",
              "       'confidence': '0.7464041683822451',\n",
              "       'text': '8'},\n",
              "      {'x': 8203,\n",
              "       'y': 5721,\n",
              "       'w': 31,\n",
              "       'h': 425,\n",
              "       'confidence': '0.752803959460735',\n",
              "       'text': '8'},\n",
              "      {'x': 1154,\n",
              "       'y': 5722,\n",
              "       'w': 88,\n",
              "       'h': 497,\n",
              "       'confidence': '0.36279112424157134',\n",
              "       'text': '1'},\n",
              "      {'x': 4917,\n",
              "       'y': 5716,\n",
              "       'w': 14,\n",
              "       'h': 375,\n",
              "       'confidence': '0.8206169081171346',\n",
              "       'text': '2'},\n",
              "      {'x': 5880,\n",
              "       'y': 5737,\n",
              "       'w': 85,\n",
              "       'h': 428,\n",
              "       'confidence': '0.2896119153831478',\n",
              "       'text': '1'},\n",
              "      {'x': 6335,\n",
              "       'y': 5718,\n",
              "       'w': 49,\n",
              "       'h': 455,\n",
              "       'confidence': '0.3211120523353692',\n",
              "       'text': '{'},\n",
              "      {'x': 7743,\n",
              "       'y': 5719,\n",
              "       'w': 43,\n",
              "       'h': 451,\n",
              "       'confidence': '0.6244501327313543',\n",
              "       'text': '2'},\n",
              "      {'x': 2097,\n",
              "       'y': 5738,\n",
              "       'w': 112,\n",
              "       'h': 533,\n",
              "       'confidence': '0.9753512262401252',\n",
              "       'text': '8'},\n",
              "      {'x': 1619,\n",
              "       'y': 5753,\n",
              "       'w': 44,\n",
              "       'h': 416,\n",
              "       'confidence': '0.45764921581437434',\n",
              "       'text': '2'},\n",
              "      {'x': 2921,\n",
              "       'y': 5904,\n",
              "       'w': 45,\n",
              "       'h': 302,\n",
              "       'confidence': '0.313358388153663',\n",
              "       'text': '5'},\n",
              "      {'x': 591,\n",
              "       'y': 5894,\n",
              "       'w': 66,\n",
              "       'h': 442,\n",
              "       'confidence': '0.22184205355435527',\n",
              "       'text': '2'},\n",
              "      {'x': 7142,\n",
              "       'y': 5959,\n",
              "       'w': 19,\n",
              "       'h': 316,\n",
              "       'confidence': '0.43460846827020205',\n",
              "       'text': '~'},\n",
              "      {'x': 4305,\n",
              "       'y': 5973,\n",
              "       'w': 107,\n",
              "       'h': 536,\n",
              "       'confidence': '0.9448837734965743',\n",
              "       'text': '8'},\n",
              "      {'x': 4783,\n",
              "       'y': 5978,\n",
              "       'w': 90,\n",
              "       'h': 475,\n",
              "       'confidence': '0.34080415774502626',\n",
              "       'text': '1'},\n",
              "      {'x': 6657,\n",
              "       'y': 6014,\n",
              "       'w': 115,\n",
              "       'h': 512,\n",
              "       'confidence': '0.19618836920231164',\n",
              "       'text': '8'},\n",
              "      {'x': 3819,\n",
              "       'y': 6028,\n",
              "       'w': 94,\n",
              "       'h': 478,\n",
              "       'confidence': '0.49902155932421977',\n",
              "       'text': '1'},\n",
              "      {'x': 8051,\n",
              "       'y': 6038,\n",
              "       'w': 1,\n",
              "       'h': 298,\n",
              "       'confidence': '0.39238979616271763',\n",
              "       'text': '3'},\n",
              "      {'x': 5676,\n",
              "       'y': 6054,\n",
              "       'w': 13,\n",
              "       'h': 307,\n",
              "       'confidence': '0.8778884230085993',\n",
              "       'text': '3'},\n",
              "      {'x': 1455,\n",
              "       'y': 6039,\n",
              "       'w': 68,\n",
              "       'h': 438,\n",
              "       'confidence': '0.11561597225997478',\n",
              "       'text': '2'},\n",
              "      {'x': 6163,\n",
              "       'y': 6048,\n",
              "       'w': 11,\n",
              "       'h': 318,\n",
              "       'confidence': '0.9886742284800363',\n",
              "       'text': '2'},\n",
              "      {'x': 2845,\n",
              "       'y': 6108,\n",
              "       'w': 91,\n",
              "       'h': 448,\n",
              "       'confidence': '0.14013630215256345',\n",
              "       'text': '1'},\n",
              "      {'x': 935,\n",
              "       'y': 6132,\n",
              "       'w': 79,\n",
              "       'h': 454,\n",
              "       'confidence': '0.15676671819172672',\n",
              "       'text': '{'},\n",
              "      {'x': 3277,\n",
              "       'y': 6162,\n",
              "       'w': 82,\n",
              "       'h': 448,\n",
              "       'confidence': '0.11329884152377545',\n",
              "       'text': '1'},\n",
              "      {'x': 5161,\n",
              "       'y': 6171,\n",
              "       'w': 92,\n",
              "       'h': 451,\n",
              "       'confidence': '0.20488957690674514',\n",
              "       'text': '1'},\n",
              "      {'x': 5565,\n",
              "       'y': 6252,\n",
              "       'w': 98,\n",
              "       'h': 507,\n",
              "       'confidence': '0.2753797594087395',\n",
              "       'text': '8'},\n",
              "      {'x': 306,\n",
              "       'y': 6411,\n",
              "       'w': 111,\n",
              "       'h': 506,\n",
              "       'confidence': '0.8480184737341325',\n",
              "       'text': 'J'},\n",
              "      {'x': 5930,\n",
              "       'y': 6449,\n",
              "       'w': 106,\n",
              "       'h': 505,\n",
              "       'confidence': '0.7721643162033196',\n",
              "       'text': 'J'},\n",
              "      {'x': 7336,\n",
              "       'y': 6472,\n",
              "       'w': 104,\n",
              "       'h': 479,\n",
              "       'confidence': '0.22608909993781712',\n",
              "       'text': '{'},\n",
              "      {'x': 1165,\n",
              "       'y': 6548,\n",
              "       'w': 109,\n",
              "       'h': 511,\n",
              "       'confidence': '0.8339597518561845',\n",
              "       'text': 'J'},\n",
              "      {'x': 6302,\n",
              "       'y': 6612,\n",
              "       'w': 101,\n",
              "       'h': 506,\n",
              "       'confidence': '0.25724476859199186',\n",
              "       'text': '8'},\n",
              "      {'x': 4929,\n",
              "       'y': 9111,\n",
              "       'w': 16,\n",
              "       'h': 349,\n",
              "       'confidence': '0.8487730314816169',\n",
              "       'text': '1'},\n",
              "      {'x': 3031,\n",
              "       'y': 9102,\n",
              "       'w': 8,\n",
              "       'h': 341,\n",
              "       'confidence': '0.4761241798133007',\n",
              "       'text': '9'},\n",
              "      {'x': 3977,\n",
              "       'y': 9113,\n",
              "       'w': 38,\n",
              "       'h': 406,\n",
              "       'confidence': '0.6229611265155341',\n",
              "       'text': '2'},\n",
              "      {'x': 5383,\n",
              "       'y': 9102,\n",
              "       'w': 2,\n",
              "       'h': 361,\n",
              "       'confidence': '0.8835775843134606',\n",
              "       'text': '2'},\n",
              "      {'x': 5874,\n",
              "       'y': 9113,\n",
              "       'w': 73,\n",
              "       'h': 436,\n",
              "       'confidence': '0.27616171355795416',\n",
              "       'text': '2'},\n",
              "      {'x': 6326,\n",
              "       'y': 9097,\n",
              "       'w': 18,\n",
              "       'h': 423,\n",
              "       'confidence': '0.6489268059539519',\n",
              "       'text': '2'},\n",
              "      {'x': 7264,\n",
              "       'y': 9099,\n",
              "       'w': 0,\n",
              "       'h': 366,\n",
              "       'confidence': '0.4996460120341659',\n",
              "       'text': '8'},\n",
              "      {'x': 8201,\n",
              "       'y': 9100,\n",
              "       'w': 30,\n",
              "       'h': 427,\n",
              "       'confidence': '0.9644932681694627',\n",
              "       'text': '8'},\n",
              "      {'x': 681,\n",
              "       'y': 9120,\n",
              "       'w': 36,\n",
              "       'h': 272,\n",
              "       'confidence': '0.986155955974656',\n",
              "       'text': '8'},\n",
              "      {'x': 1154,\n",
              "       'y': 9103,\n",
              "       'w': 90,\n",
              "       'h': 501,\n",
              "       'confidence': '0.22169996646216905',\n",
              "       'text': '0'},\n",
              "      {'x': 2094,\n",
              "       'y': 9098,\n",
              "       'w': 23,\n",
              "       'h': 398,\n",
              "       'confidence': '0.43386847952749363',\n",
              "       'text': 'g'},\n",
              "      {'x': 4470,\n",
              "       'y': 9107,\n",
              "       'w': 50,\n",
              "       'h': 365,\n",
              "       'confidence': '0.24057624916276055',\n",
              "       'text': '2'},\n",
              "      {'x': 6807,\n",
              "       'y': 9100,\n",
              "       'w': 51,\n",
              "       'h': 450,\n",
              "       'confidence': '0.16420180079251967',\n",
              "       'text': '{'},\n",
              "      {'x': 3510,\n",
              "       'y': 9121,\n",
              "       'w': 113,\n",
              "       'h': 528,\n",
              "       'confidence': '0.9802094238780548',\n",
              "       'text': '8'},\n",
              "      {'x': 7740,\n",
              "       'y': 9107,\n",
              "       'w': 38,\n",
              "       'h': 441,\n",
              "       'confidence': '0.5758991549193446',\n",
              "       'text': '2'},\n",
              "      {'x': 1615,\n",
              "       'y': 9133,\n",
              "       'w': 39,\n",
              "       'h': 416,\n",
              "       'confidence': '0.32065367285563084',\n",
              "       'text': '2'},\n",
              "      {'x': 2462,\n",
              "       'y': 9291,\n",
              "       'w': 29,\n",
              "       'h': 295,\n",
              "       'confidence': '0.20765122716035478',\n",
              "       'text': '5'},\n",
              "      {'x': 2906,\n",
              "       'y': 9319,\n",
              "       'w': 18,\n",
              "       'h': 333,\n",
              "       'confidence': '0.5646714622819218',\n",
              "       'text': '~'},\n",
              "      {'x': 7140,\n",
              "       'y': 9344,\n",
              "       'w': 21,\n",
              "       'h': 312,\n",
              "       'confidence': '0.33463615120894374',\n",
              "       'text': '3'},\n",
              "      {'x': 4776,\n",
              "       'y': 9356,\n",
              "       'w': 110,\n",
              "       'h': 533,\n",
              "       'confidence': '0.962003312750781',\n",
              "       'text': '8'},\n",
              "      {'x': 4304,\n",
              "       'y': 9357,\n",
              "       'w': 78,\n",
              "       'h': 478,\n",
              "       'confidence': '0.3146560545851038',\n",
              "       'text': '1'},\n",
              "      {'x': 6178,\n",
              "       'y': 9398,\n",
              "       'w': 105,\n",
              "       'h': 510,\n",
              "       'confidence': '0.21285099410804165',\n",
              "       'text': '8'},\n",
              "      {'x': 1942,\n",
              "       'y': 9386,\n",
              "       'w': 85,\n",
              "       'h': 476,\n",
              "       'confidence': '0.4507907802444642',\n",
              "       'text': '1'},\n",
              "      {'x': 3817,\n",
              "       'y': 9404,\n",
              "       'w': 92,\n",
              "       'h': 484,\n",
              "       'confidence': '0.5698845067533718',\n",
              "       'text': '1'},\n",
              "      {'x': 8051,\n",
              "       'y': 9419,\n",
              "       'w': 1,\n",
              "       'h': 298,\n",
              "       'confidence': '0.38998629950219765',\n",
              "       'text': '3'},\n",
              "      {'x': 5676,\n",
              "       'y': 9436,\n",
              "       'w': 14,\n",
              "       'h': 304,\n",
              "       'confidence': '0.8627207911337251',\n",
              "       'text': '3'},\n",
              "      {'x': 1455,\n",
              "       'y': 9421,\n",
              "       'w': 68,\n",
              "       'h': 437,\n",
              "       'confidence': '0.11508754847687896',\n",
              "       'text': '2'},\n",
              "      {'x': 6658,\n",
              "       'y': 9438,\n",
              "       'w': 47,\n",
              "       'h': 311,\n",
              "       'confidence': '0.4233042690388267',\n",
              "       'text': '8'},\n",
              "      {'x': 935,\n",
              "       'y': 9514,\n",
              "       'w': 79,\n",
              "       'h': 453,\n",
              "       'confidence': '0.18732901806753777',\n",
              "       'text': '{'},\n",
              "      {'x': 5565,\n",
              "       'y': 9634,\n",
              "       'w': 97,\n",
              "       'h': 504,\n",
              "       'confidence': '0.7119009867910933',\n",
              "       'text': '8'},\n",
              "      {'x': 306,\n",
              "       'y': 9792,\n",
              "       'w': 110,\n",
              "       'h': 504,\n",
              "       'confidence': '0.8108557003475312',\n",
              "       'text': 'J'},\n",
              "      {'x': 5939,\n",
              "       'y': 9797,\n",
              "       'w': 15,\n",
              "       'h': 301,\n",
              "       'confidence': '0.7546431551077184',\n",
              "       'text': '3'},\n",
              "      {'x': 6399,\n",
              "       'y': 9828,\n",
              "       'w': 106,\n",
              "       'h': 507,\n",
              "       'confidence': '0.8658196580981894',\n",
              "       'text': 'J'},\n",
              "      {'x': 7336,\n",
              "       'y': 9853,\n",
              "       'w': 104,\n",
              "       'h': 479,\n",
              "       'confidence': '0.22608909993781712',\n",
              "       'text': '{'},\n",
              "      {'x': 1165,\n",
              "       'y': 9929,\n",
              "       'w': 108,\n",
              "       'h': 510,\n",
              "       'confidence': '0.8334826709984533',\n",
              "       'text': 'J'},\n",
              "      {'x': 5830,\n",
              "       'y': 9991,\n",
              "       'w': 98,\n",
              "       'h': 507,\n",
              "       'confidence': '0.47171733264362103',\n",
              "       'text': '8'}]}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 154, 'y': 34.04, 'w': 335, 'h': 574.96}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Model Family',\n",
              "     'md': '# Model Family',\n",
              "     'bBox': {'x': 298, 'y': 118, 'w': 191, 'h': 152}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['',\n",
              "       '',\n",
              "       'Meta',\n",
              "       'Amazon',\n",
              "       'Google',\n",
              "       'Mistral AI',\n",
              "       'OpenAI',\n",
              "       'Anthropic',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['1.0',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '0.98',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['80.8',\n",
              "       '',\n",
              "       '',\n",
              "       '0.72',\n",
              "       '0.72',\n",
              "       '0.87',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['80.6',\n",
              "       '',\n",
              "       '',\n",
              "       '0.46',\n",
              "       '0.53',\n",
              "       '0.62',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['20.4',\n",
              "       '0.29',\n",
              "       '0.32',\n",
              "       '0.35',\n",
              "       '0.35',\n",
              "       '0.36',\n",
              "       '0.36',\n",
              "       '0.37',\n",
              "       '0.38',\n",
              "       '0.42',\n",
              "       '0.42',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['',\n",
              "       '',\n",
              "       '2',\n",
              "       '20.2',\n",
              "       '0.0',\n",
              "       '1~',\n",
              "       '9',\n",
              "       '1',\n",
              "       '1',\n",
              "       '8',\n",
              "       '2',\n",
              "       '1',\n",
              "       '8',\n",
              "       '1ᴬ⁸',\n",
              "       '1',\n",
              "       '9',\n",
              "       '1',\n",
              "       '2',\n",
              "       '1⁵',\n",
              "       '2',\n",
              "       '8',\n",
              "       '1~',\n",
              "       '8',\n",
              "       'Tv',\n",
              "       '2',\n",
              "       '81',\n",
              "       'A',\n",
              "       '8',\n",
              "       '3',\n",
              "       '2',\n",
              "       '8',\n",
              "       '2',\n",
              "       '2',\n",
              "       '2',\n",
              "       '',\n",
              "       ''],\n",
              "      ['',\n",
              "       \"0'\",\n",
              "       '',\n",
              "       'J',\n",
              "       '',\n",
              "       '!',\n",
              "       '3',\n",
              "       '3',\n",
              "       'A',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['',\n",
              "       'K',\n",
              "       '',\n",
              "       '{',\n",
              "       '',\n",
              "       'J',\n",
              "       '',\n",
              "       '8',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['283',\n",
              "       '8²⁵⁰',\n",
              "       '210',\n",
              "       '190',\n",
              "       '200',\n",
              "       '163',\n",
              "       '157',\n",
              "       '157',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['8',\n",
              "       '150',\n",
              "       '',\n",
              "       '124',\n",
              "       '115',\n",
              "       '113',\n",
              "       '100',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['2',\n",
              "       '100',\n",
              "       '',\n",
              "       '73',\n",
              "       '64',\n",
              "       '58',\n",
              "       '57',\n",
              "       '40',\n",
              "       '35',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['3',\n",
              "       '50',\n",
              "       '1',\n",
              "       '9',\n",
              "       '9',\n",
              "       '2',\n",
              "       '2',\n",
              "       '{',\n",
              "       '2',\n",
              "       '8',\n",
              "       '2',\n",
              "       '29',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['',\n",
              "       '2',\n",
              "       '8',\n",
              "       '{',\n",
              "       '2',\n",
              "       '2',\n",
              "       '8',\n",
              "       '1⁵⁸',\n",
              "       '3',\n",
              "       '1',\n",
              "       '2',\n",
              "       '8',\n",
              "       '1',\n",
              "       '1',\n",
              "       '~',\n",
              "       '3',\n",
              "       '1',\n",
              "       '2',\n",
              "       '8',\n",
              "       '~',\n",
              "       '3',\n",
              "       '8',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['',\n",
              "       'J',\n",
              "       '1',\n",
              "       '',\n",
              "       '1',\n",
              "       '8',\n",
              "       'N?',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['',\n",
              "       'J',\n",
              "       '',\n",
              "       '',\n",
              "       \"0'\",\n",
              "       '{',\n",
              "       '',\n",
              "       '8',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['4.0',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '4.0',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['83.5',\n",
              "       '',\n",
              "       '',\n",
              "       '2.8',\n",
              "       '2.9',\n",
              "       '3.4',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['23.0',\n",
              "       '',\n",
              "       '2.4',\n",
              "       '2.7',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['L2.5',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['2.0',\n",
              "       '',\n",
              "       '1.3',\n",
              "       '1.4',\n",
              "       '1.5',\n",
              "       '1.7',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['1.5',\n",
              "       '0.8',\n",
              "       '0.9',\n",
              "       '1.0',\n",
              "       '1.0',\n",
              "       '1,.2',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['',\n",
              "       '2',\n",
              "       '1.0',\n",
              "       '0.7',\n",
              "       '0.5',\n",
              "       '0.0',\n",
              "       '8',\n",
              "       '{',\n",
              "       '0',\n",
              "       '2',\n",
              "       '2',\n",
              "       '1',\n",
              "       'g',\n",
              "       '58',\n",
              "       '~',\n",
              "       '9',\n",
              "       '8',\n",
              "       '1',\n",
              "       '2',\n",
              "       '1',\n",
              "       '2',\n",
              "       '8',\n",
              "       '1',\n",
              "       'K',\n",
              "       '2',\n",
              "       '3',\n",
              "       '2',\n",
              "       '8',\n",
              "       '2',\n",
              "       '8',\n",
              "       '{',\n",
              "       '3',\n",
              "       '8',\n",
              "       '2',\n",
              "       '3',\n",
              "       '8'],\n",
              "      ['',\n",
              "       'N?',\n",
              "       '',\n",
              "       '8',\n",
              "       'K',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       ''],\n",
              "      ['',\n",
              "       'J',\n",
              "       '',\n",
              "       'N',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '',\n",
              "       '']],\n",
              "     'md': \"| | |Meta|Amazon|Google|Mistral AI|OpenAI|Anthropic| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|1.0| | | |0.98| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|80.8| | |0.72|0.72|0.87| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|80.6| | |0.46|0.53|0.62| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|20.4|0.29|0.32|0.35|0.35|0.36|0.36|0.37|0.38|0.42|0.42| | | | | | | | | | | | | | | | | | | | | | | | | |\\n| | |2|20.2|0.0|1~|9|1|1|8|2|1|8|1ᴬ⁸|1|9|1|2|1⁵|2|8|1~|8|Tv|2|81|A|8|3|2|8|2|2|2| | |\\n| |0'| |J| |!|3|3|A| | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |K| |{| |J| |8| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|283|8²⁵⁰|210|190|200|163|157|157| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|8|150| |124|115|113|100| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|2|100| |73|64|58|57|40|35| | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|3|50|1|9|9|2|2|{|2|8|2|29| | | | | | | | | | | | | | | | | | | | | | | | |\\n| |2|8|{|2|2|8|1⁵⁸|3|1|2|8|1|1|~|3|1|2|8|~|3|8| | | | | | | | | | | | | | |\\n| |J|1| |1|8|N?| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |J| | |0'|{| |8| | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|4.0| | | |4.0| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|83.5| | |2.8|2.9|3.4| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|23.0| |2.4|2.7| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|L2.5| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|2.0| |1.3|1.4|1.5|1.7| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n|1.5|0.8|0.9|1.0|1.0|1,.2| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |2|1.0|0.7|0.5|0.0|8|{|0|2|2|1|g|58|~|9|8|1|2|1|2|8|1|K|2|3|2|8|2|8|{|3|8|2|3|8|\\n| |N?| |8|K| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\\n| |J| |N| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\",\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"\",\"\",\"Meta\",\"Amazon\",\"Google\",\"Mistral AI\",\"OpenAI\",\"Anthropic\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"1.0\",\"\",\"\",\"\",\"0.98\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"80.8\",\"\",\"\",\"0.72\",\"0.72\",\"0.87\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"80.6\",\"\",\"\",\"0.46\",\"0.53\",\"0.62\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"20.4\",\"0.29\",\"0.32\",\"0.35\",\"0.35\",\"0.36\",\"0.36\",\"0.37\",\"0.38\",\"0.42\",\"0.42\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"\",\"2\",\"20.2\",\"0.0\",\"1~\",\"9\",\"1\",\"1\",\"8\",\"2\",\"1\",\"8\",\"1ᴬ⁸\",\"1\",\"9\",\"1\",\"2\",\"1⁵\",\"2\",\"8\",\"1~\",\"8\",\"Tv\",\"2\",\"81\",\"A\",\"8\",\"3\",\"2\",\"8\",\"2\",\"2\",\"2\",\"\",\"\"\\n\"\",\"0\\'\",\"\",\"J\",\"\",\"!\",\"3\",\"3\",\"A\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"K\",\"\",\"{\",\"\",\"J\",\"\",\"8\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"283\",\"8²⁵⁰\",\"210\",\"190\",\"200\",\"163\",\"157\",\"157\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"8\",\"150\",\"\",\"124\",\"115\",\"113\",\"100\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"2\",\"100\",\"\",\"73\",\"64\",\"58\",\"57\",\"40\",\"35\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"3\",\"50\",\"1\",\"9\",\"9\",\"2\",\"2\",\"{\",\"2\",\"8\",\"2\",\"29\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"2\",\"8\",\"{\",\"2\",\"2\",\"8\",\"1⁵⁸\",\"3\",\"1\",\"2\",\"8\",\"1\",\"1\",\"~\",\"3\",\"1\",\"2\",\"8\",\"~\",\"3\",\"8\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"J\",\"1\",\"\",\"1\",\"8\",\"N?\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"J\",\"\",\"\",\"0\\'\",\"{\",\"\",\"8\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"4.0\",\"\",\"\",\"\",\"4.0\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"83.5\",\"\",\"\",\"2.8\",\"2.9\",\"3.4\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"23.0\",\"\",\"2.4\",\"2.7\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"L2.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"2.0\",\"\",\"1.3\",\"1.4\",\"1.5\",\"1.7\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"1.5\",\"0.8\",\"0.9\",\"1.0\",\"1.0\",\"1,.2\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"2\",\"1.0\",\"0.7\",\"0.5\",\"0.0\",\"8\",\"{\",\"0\",\"2\",\"2\",\"1\",\"g\",\"58\",\"~\",\"9\",\"8\",\"1\",\"2\",\"1\",\"2\",\"8\",\"1\",\"K\",\"2\",\"3\",\"2\",\"8\",\"2\",\"8\",\"{\",\"3\",\"8\",\"2\",\"3\",\"8\"\\n\"\",\"N?\",\"\",\"8\",\"K\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\\n\"\",\"J\",\"\",\"N\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"',\n",
              "     'bBox': {'x': 72, 'y': 34.04, 'w': 468, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Figure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of input and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov 29th, 2024).\\n\\n14',\n",
              "     'md': 'Figure 3: Time to First Token (↓), Output Tokens per Second (↑), and Total Response Time (↓) using 1,000 tokens of input and 100 tokens of output for Amazon Nova models and select publicly-available models (Artificial Analysis, Nov 29th, 2024).\\n\\n14',\n",
              "     'bBox': {'x': 72, 'y': 128, 'w': 468, 'h': 622.04}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 15,\n",
              "   'text': '                                    The Amazon Nova Family of Models\\n3  Amazon Nova Canvas Evaluation\\nAmazon Nova Canvas is a diffusion model that takes a text prompt and an optional RGB image as input and generates\\nan image as an output conditioned on the input text and optional image. Illustrative examples of the images generated\\nby Amazon Nova Canvas can be found in our Amazon Science blog post 6. In this section, we provide details on the\\nevaluation strategy and performance of the model both in terms of automated metrics and human evaluation.\\n3.1  Automated metrics\\nWe use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n      •            ImageReward score is generated from a standardized reward model that aligns human preference with the\\n        predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-\\n        2014 [42] validation set and use this set for calculating the score.\\n      • Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated\\n        image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-\\n        selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66],\\n        PartiPrompts [84], and PaintSkill [21] datasets.\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3\\nMedium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.\\n                                                         TIFA    ImageReward\\n                            Amazon Nova Canvas          0.897       1.250\\n                            DALL.E 3                    0.863       1.052\\n                            Stable Diffusion 3.5 Large  0.891       1.082\\n                            Stable Diffusion 3 Medium   0.881       0.952\\n                            Flux Pro 1.0                0.875       1.075\\n                            Flux Schnell                0.882       0.999\\n       Table 8: Comparison of TIFA and ImageReward metrics of Amazon Nova Canvas with other models.\\n3.2  Human evaluation\\nWe conduct A/B testing to compare Amazon Nova Canvas with other third-party text-to-image models. The A/B\\ntesting prompt set is composed of approximately 1,000 prompts designed to capture customer usage of text-to-image\\nmodels. This set include prompts from datasets such as MSCOCO [42], Drawbench [66], OpenParti [84], DALL.E 3\\nEval [16], and DOCCI [54] and covers a broad set of categories such as humans, landscapes, natural scenarios, indoor\\nenvironments, creative themes, artistic themes, and so forth. A few prompts were randomly selected and repeated in\\norder to get additional data points on the quality of the model.\\nWith each prompt we generate an image from Amazon Nova Canvas as well as each other text-to-image model. We used\\nrandom seeds to generate the images from Amazon Nova Canvas and all images were generated at 1k x 1k resolution. If\\nthe prompts trigger filters such that an image is not generated, for either the Amazon Nova Canvas model or the public\\ntext-to-image model, we ignore that prompt and do not show it to the human raters. All human evaluation is done in\\na single-blind manner where the annotator is provided two sets of images, one from Amazon Nova Canvas and the\\nother from the third-party model. The order of the images are randomized for each prompt and annotator. In our blind\\ntesting, we ask human annotators to select images that they prefer based on (1) text-image alignment, which measures\\nthe instruction-following capability of the model, and (2) image quality, which quantifies the overall preference of the\\nannotators. To ensure rigorous, consistent, and unbiased evaluation, we used a third-party vendor for human evaluation.\\nWe created guidelines that were used to train the annotators so that the decision-making criteria were clear to them in\\neach dimension.\\nThe pair-wise results comparing Amazon Nova Canvas with OpenAI DALL.E 3 and Google Imagen 3 are shown in\\nTable 9, including win, tie, loss rate. The win rate reflects the percentage of samples where Amazon Nova Canvas was\\n  6 https://www.amazon.science/blog/amazon-nova-canvas-examples\\n                                                    15',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# 3  Amazon Nova Canvas Evaluation\\n\\nAmazon Nova Canvas is a diffusion model that takes a text prompt and an optional RGB image as input and generates an image as an output conditioned on the input text and optional image. Illustrative examples of the images generated by Amazon Nova Canvas can be found in our Amazon Science blog post 6. In this section, we provide details on the evaluation strategy and performance of the model both in terms of automated metrics and human evaluation.\\n\\n# 3.1  Automated metrics\\n\\nWe use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n\\n- ImageReward score is generated from a standardized reward model that aligns human preference with the predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-2014 [42] validation set and use this set for calculating the score.\\n- Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66], PartiPrompts [84], and PaintSkill [21] datasets.\\n\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3 Medium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.\\n\\n|TIFA|TIFA|ImageReward|\\n|---|---|\\n|Amazon Nova Canvas|0.897|1.250|\\n|DALL.E 3|0.863|1.052|\\n|Stable Diffusion 3.5 Large|0.891|1.082|\\n|Stable Diffusion 3 Medium|0.881|0.952|\\n|Flux Pro 1.0|0.875|1.075|\\n|Flux Schnell|0.882|0.999|\\n\\nTable 8: Comparison of TIFA and ImageReward metrics of Amazon Nova Canvas with other models.\\n\\n# 3.2  Human evaluation\\n\\nWe conduct A/B testing to compare Amazon Nova Canvas with other third-party text-to-image models. The A/B testing prompt set is composed of approximately 1,000 prompts designed to capture customer usage of text-to-image models. This set includes prompts from datasets such as MSCOCO [42], Drawbench [66], OpenParti [84], DALL.E 3 Eval [16], and DOCCI [54] and covers a broad set of categories such as humans, landscapes, natural scenarios, indoor environments, creative themes, artistic themes, and so forth. A few prompts were randomly selected and repeated in order to get additional data points on the quality of the model.\\n\\nWith each prompt we generate an image from Amazon Nova Canvas as well as each other text-to-image model. We used random seeds to generate the images from Amazon Nova Canvas and all images were generated at 1k x 1k resolution. If the prompts trigger filters such that an image is not generated, for either the Amazon Nova Canvas model or the public text-to-image model, we ignore that prompt and do not show it to the human raters. All human evaluation is done in a single-blind manner where the annotator is provided two sets of images, one from Amazon Nova Canvas and the other from the third-party model. The order of the images are randomized for each prompt and annotator. In our blind testing, we ask human annotators to select images that they prefer based on (1) text-image alignment, which measures the instruction-following capability of the model, and (2) image quality, which quantifies the overall preference of the annotators. To ensure rigorous, consistent, and unbiased evaluation, we used a third-party vendor for human evaluation. We created guidelines that were used to train the annotators so that the decision-making criteria were clear to them in each dimension.\\n\\nThe pair-wise results comparing Amazon Nova Canvas with OpenAI DALL.E 3 and Google Imagen 3 are shown in Table 9, including win, tie, loss rate. The win rate reflects the percentage of samples where Amazon Nova Canvas was...',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '3  Amazon Nova Canvas Evaluation',\n",
              "     'md': '# 3  Amazon Nova Canvas Evaluation',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 214, 'h': 272.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Amazon Nova Canvas is a diffusion model that takes a text prompt and an optional RGB image as input and generates an image as an output conditioned on the input text and optional image. Illustrative examples of the images generated by Amazon Nova Canvas can be found in our Amazon Science blog post 6. In this section, we provide details on the evaluation strategy and performance of the model both in terms of automated metrics and human evaluation.',\n",
              "     'md': 'Amazon Nova Canvas is a diffusion model that takes a text prompt and an optional RGB image as input and generates an image as an output conditioned on the input text and optional image. Illustrative examples of the images generated by Amazon Nova Canvas can be found in our Amazon Science blog post 6. In this section, we provide details on the evaluation strategy and performance of the model both in terms of automated metrics and human evaluation.',\n",
              "     'bBox': {'x': 71, 'y': 95.04, 'w': 469, 'h': 380}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '3.1  Automated metrics',\n",
              "     'md': '# 3.1  Automated metrics',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 103, 'h': 91.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n\\n- ImageReward score is generated from a standardized reward model that aligns human preference with the predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-2014 [42] validation set and use this set for calculating the score.\\n- Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66], PartiPrompts [84], and PaintSkill [21] datasets.\\n\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3 Medium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.',\n",
              "     'md': 'We use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n\\n- ImageReward score is generated from a standardized reward model that aligns human preference with the predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-2014 [42] validation set and use this set for calculating the score.\\n- Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66], PartiPrompts [84], and PaintSkill [21] datasets.\\n\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3 Medium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.',\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 470, 'h': 317.99}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      [],\n",
              "      [],\n",
              "      [],\n",
              "      [],\n",
              "      [],\n",
              "      ['TIFA', 'TIFA', 'ImageReward'],\n",
              "      ['---', '---'],\n",
              "      ['Amazon Nova Canvas', '0.897', '1.250'],\n",
              "      ['DALL.E 3', '0.863', '1.052'],\n",
              "      ['Stable Diffusion 3.5 Large', '0.891', '1.082'],\n",
              "      ['Stable Diffusion 3 Medium', '0.881', '0.952'],\n",
              "      ['Flux Pro 1.0', '0.875', '1.075'],\n",
              "      ['Flux Schnell', '0.882', '0.999']],\n",
              "     'md': 'We use ImageReward [80] and Text-to-Image Faithfulness (TIFA) [38] as automated metrics.\\n- ImageReward score is generated from a standardized reward model that aligns human preference with the predicted score. To compute the ImageReward score, we randomly sample 10k prompts from MSCOCO-2014 [42] validation set and use this set for calculating the score.\\n- Text-to-Image Faithfulness (TIFA) score is a reference-free metric that measures the faithfulness of a generated image to the input text via visual question answering (VQA). The evaluation set for TIFA score is a pre-selected 4k prompts in the TIFA-v1.0 benchmark, sampled from MSCOCO captions [42], DrawBench [66], PartiPrompts [84], and PaintSkill [21] datasets.\\n\\nWe compare Amazon Nova Canvas with other publicly-available models including DALL.E 3 [16], Stable Diffusion 3 Medium [27], Stable Diffusion 3.5 Large [28] and Flux (Schnell and Pro) [17]. The results are shown in Table 8.\\n\\n|TIFA|TIFA|ImageReward|\\n|---|---|\\n|Amazon Nova Canvas|0.897|1.250|\\n|DALL.E 3|0.863|1.052|\\n|Stable Diffusion 3.5 Large|0.891|1.082|\\n|Stable Diffusion 3 Medium|0.881|0.952|\\n|Flux Pro 1.0|0.875|1.075|\\n|Flux Schnell|0.882|0.999|',\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\\n\\n\\n\\n\\n\"TIFA\",\"TIFA\",\"ImageReward\"\\n\"---\",\"---\"\\n\"Amazon Nova Canvas\",\"0.897\",\"1.250\"\\n\"DALL.E 3\",\"0.863\",\"1.052\"\\n\"Stable Diffusion 3.5 Large\",\"0.891\",\"1.082\"\\n\"Stable Diffusion 3 Medium\",\"0.881\",\"0.952\"\\n\"Flux Pro 1.0\",\"0.875\",\"1.075\"\\n\"Flux Schnell\",\"0.882\",\"0.999\"',\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 470, 'h': 631.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 8: Comparison of TIFA and ImageReward metrics of Amazon Nova Canvas with other models.',\n",
              "     'md': 'Table 8: Comparison of TIFA and ImageReward metrics of Amazon Nova Canvas with other models.',\n",
              "     'bBox': {'x': 103, 'y': 318.04, 'w': 404, 'h': 118}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '3.2  Human evaluation',\n",
              "     'md': '# 3.2  Human evaluation',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 101, 'h': 404.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We conduct A/B testing to compare Amazon Nova Canvas with other third-party text-to-image models. The A/B testing prompt set is composed of approximately 1,000 prompts designed to capture customer usage of text-to-image models. This set includes prompts from datasets such as MSCOCO [42], Drawbench [66], OpenParti [84], DALL.E 3 Eval [16], and DOCCI [54] and covers a broad set of categories such as humans, landscapes, natural scenarios, indoor environments, creative themes, artistic themes, and so forth. A few prompts were randomly selected and repeated in order to get additional data points on the quality of the model.\\n\\nWith each prompt we generate an image from Amazon Nova Canvas as well as each other text-to-image model. We used random seeds to generate the images from Amazon Nova Canvas and all images were generated at 1k x 1k resolution. If the prompts trigger filters such that an image is not generated, for either the Amazon Nova Canvas model or the public text-to-image model, we ignore that prompt and do not show it to the human raters. All human evaluation is done in a single-blind manner where the annotator is provided two sets of images, one from Amazon Nova Canvas and the other from the third-party model. The order of the images are randomized for each prompt and annotator. In our blind testing, we ask human annotators to select images that they prefer based on (1) text-image alignment, which measures the instruction-following capability of the model, and (2) image quality, which quantifies the overall preference of the annotators. To ensure rigorous, consistent, and unbiased evaluation, we used a third-party vendor for human evaluation. We created guidelines that were used to train the annotators so that the decision-making criteria were clear to them in each dimension.\\n\\nThe pair-wise results comparing Amazon Nova Canvas with OpenAI DALL.E 3 and Google Imagen 3 are shown in Table 9, including win, tie, loss rate. The win rate reflects the percentage of samples where Amazon Nova Canvas was...',\n",
              "     'md': 'We conduct A/B testing to compare Amazon Nova Canvas with other third-party text-to-image models. The A/B testing prompt set is composed of approximately 1,000 prompts designed to capture customer usage of text-to-image models. This set includes prompts from datasets such as MSCOCO [42], Drawbench [66], OpenParti [84], DALL.E 3 Eval [16], and DOCCI [54] and covers a broad set of categories such as humans, landscapes, natural scenarios, indoor environments, creative themes, artistic themes, and so forth. A few prompts were randomly selected and repeated in order to get additional data points on the quality of the model.\\n\\nWith each prompt we generate an image from Amazon Nova Canvas as well as each other text-to-image model. We used random seeds to generate the images from Amazon Nova Canvas and all images were generated at 1k x 1k resolution. If the prompts trigger filters such that an image is not generated, for either the Amazon Nova Canvas model or the public text-to-image model, we ignore that prompt and do not show it to the human raters. All human evaluation is done in a single-blind manner where the annotator is provided two sets of images, one from Amazon Nova Canvas and the other from the third-party model. The order of the images are randomized for each prompt and annotator. In our blind testing, we ask human annotators to select images that they prefer based on (1) text-image alignment, which measures the instruction-following capability of the model, and (2) image quality, which quantifies the overall preference of the annotators. To ensure rigorous, consistent, and unbiased evaluation, we used a third-party vendor for human evaluation. We created guidelines that were used to train the annotators so that the decision-making criteria were clear to them in each dimension.\\n\\nThe pair-wise results comparing Amazon Nova Canvas with OpenAI DALL.E 3 and Google Imagen 3 are shown in Table 9, including win, tie, loss rate. The win rate reflects the percentage of samples where Amazon Nova Canvas was...',\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 471, 'h': 631.99}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://www.amazon.science/blog/amazon-nova-canvas-examples',\n",
              "     'text': 'https://www.amazon.science/blog/amazon-nova-canvas-examples'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 16,\n",
              "   'text': '                                      The Amazon Nova Family of Models\\npreferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a\\ndifference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared\\nto the other text-to-image models.\\n   Nova Canvas versus:                                    DALL.E 3                         Imagen 3\\n                                                 win rate   tie rate loss rate   win rate   tie rate loss rate\\n   Overall preference (image quality)              54.5       6.4       39.1       48.2       5.3       46.5\\n   Instruction following (text-image alignment)    39.4       22.5      38.1       38.4       28.1      33.5\\nTable 9: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Canvas versus (a) DALL.E 3 and (b)\\nImagen 3.\\n4   Amazon Nova Reel Evaluation\\nAmazon Nova Reel is a diffusion model that takes a text prompt and an optional RGB image as input and generates\\na video as an output conditioned on the input text and optional image. Illustrative examples of the videos generated\\nby the Amazon Nova Reel can be found in our Amazon Science blog post.7   In this section, we provide details on the\\nevaluation strategy and performance of the model.\\n4.1  Human evaluation metrics\\nTo evaluate Amazon Nova Reel, we rely on human feedback to assess the generated videos across two primary axes:\\nvideo quality and video consistency. All evaluations are conducted through single-blind pairwise comparisons. Human\\nannotators are provided a set of two videos shown side-by-side and are asked to choose the better video or mark them\\nas equal if they find the videos to be equally performant across the metric on which they are evaluating. All videos were\\ngenerated in 720p resolution and different random seeds were used during generation.\\nThe video quality axis encapsulates the technical and perceptual aspects of the generated video via four primary\\ncomponents:\\n      • Image quality: The visual appeal of individual frames, including resolution, sharpness, object clarity, and\\n        overall composition, where each frame is visually pleasing and artifact-free.\\n      • Motion quality: The fluidity of movement across frames, including motion consistency and smooth transitions\\n        without flickering, distortion, or abrupt shifts, contributing to natural and realistic motion portrayal.\\n      • Image-text alignment: How closely individual frames match the prompt, considering the presence of described\\n        entities, their attributes, spatial relationships, colors, and other static visual details.\\n      •     Motion-text alignment: The accuracy of dynamic elements, including the correctness of actions performed\\n            by entities, camera movements, and temporal changes in attributes, as well as adherence to the provided\\n        description.\\nThe video quality axis additionally includes factors influencing overall appeal, such as motion degree, entity size,\\ncreative composition, and general video likability.\\nThe video consistency axis encapsulates the temporal coherence of both subjects and backgrounds throughout the\\nvideo. It includes assessments of the maintenance of entity size, shape, and appearance, as well as background stability\\nwithout unexpected morphing or changes. A high score in this dimension means believable spatial relationships between\\nforeground and background elements throughout the video duration.\\nIn combination, the video quality and video consistency metrics provide a holistic and robust evaluation framework for\\nvideo generation models by considering both technical accuracy and perceptual appeal.\\n4.2  Dataset\\nWe curated a diverse set of prompts designed to capture various aspects of video generation. The prompts are distributed\\nacross 6 broad categories: human and activities, animals, natural scenery and landscapes, indoor scenes, objects\\n   7https://www.amazon.science/blog/amazon-nova-reel-examples\\n                                                       16',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\npreferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a difference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared to the other text-to-image models.\\n\\n| | |DALL.E 3| | |Imagen 3| |\\n|---|---|---|---|---|---|---|\\n| |win rate|tie rate|loss rate|win rate|tie rate|loss rate|\\n|Overall preference (image quality)|54.5|6.4|39.1|48.2|5.3|46.5|\\n|Instruction following (text-image alignment)|39.4|22.5|38.1|38.4|28.1|33.5|\\n\\nTable 9: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Canvas versus (a) DALL.E 3 and (b) Imagen 3.\\n\\n# 4 Amazon Nova Reel Evaluation\\n\\nAmazon Nova Reel is a diffusion model that takes a text prompt and an optional RGB image as input and generates a video as an output conditioned on the input text and optional image. Illustrative examples of the videos generated by the Amazon Nova Reel can be found in our Amazon Science blog post.7 In this section, we provide details on the evaluation strategy and performance of the model.\\n\\n# 4.1 Human evaluation metrics\\n\\nTo evaluate Amazon Nova Reel, we rely on human feedback to assess the generated videos across two primary axes: video quality and video consistency. All evaluations are conducted through single-blind pairwise comparisons. Human annotators are provided a set of two videos shown side-by-side and are asked to choose the better video or mark them as equal if they find the videos to be equally performant across the metric on which they are evaluating. All videos were generated in 720p resolution and different random seeds were used during generation.\\n\\nThe video quality axis encapsulates the technical and perceptual aspects of the generated video via four primary components:\\n\\n- Image quality: The visual appeal of individual frames, including resolution, sharpness, object clarity, and overall composition, where each frame is visually pleasing and artifact-free.\\n- Motion quality: The fluidity of movement across frames, including motion consistency and smooth transitions without flickering, distortion, or abrupt shifts, contributing to natural and realistic motion portrayal.\\n- Image-text alignment: How closely individual frames match the prompt, considering the presence of described entities, their attributes, spatial relationships, colors, and other static visual details.\\n- Motion-text alignment: The accuracy of dynamic elements, including the correctness of actions performed by entities, camera movements, and temporal changes in attributes, as well as adherence to the provided description.\\n\\nThe video quality axis additionally includes factors influencing overall appeal, such as motion degree, entity size, creative composition, and general video likability.\\n\\nThe video consistency axis encapsulates the temporal coherence of both subjects and backgrounds throughout the video. It includes assessments of the maintenance of entity size, shape, and appearance, as well as background stability without unexpected morphing or changes. A high score in this dimension means believable spatial relationships between foreground and background elements throughout the video duration.\\n\\nIn combination, the video quality and video consistency metrics provide a holistic and robust evaluation framework for video generation models by considering both technical accuracy and perceptual appeal.\\n\\n# 4.2 Dataset\\n\\nWe curated a diverse set of prompts designed to capture various aspects of video generation. The prompts are distributed across 6 broad categories: human and activities, animals, natural scenery and landscapes, indoor scenes, objects.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'preferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a difference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared to the other text-to-image models.',\n",
              "     'md': 'preferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a difference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared to the other text-to-image models.',\n",
              "     'bBox': {'x': 72, 'y': 72.04, 'w': 468, 'h': 72}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      ['', '', 'DALL.E 3', '', '', 'Imagen 3', ''],\n",
              "      ['---', '---', '---', '---', '---', '---', '---'],\n",
              "      ['',\n",
              "       'win rate',\n",
              "       'tie rate',\n",
              "       'loss rate',\n",
              "       'win rate',\n",
              "       'tie rate',\n",
              "       'loss rate'],\n",
              "      ['Overall preference (image quality)',\n",
              "       '54.5',\n",
              "       '6.4',\n",
              "       '39.1',\n",
              "       '48.2',\n",
              "       '5.3',\n",
              "       '46.5'],\n",
              "      ['Instruction following (text-image alignment)',\n",
              "       '39.4',\n",
              "       '22.5',\n",
              "       '38.1',\n",
              "       '38.4',\n",
              "       '28.1',\n",
              "       '33.5']],\n",
              "     'md': 'preferred over the other model while the tie rate indicates the scenario where the human annotator did not perceive a difference between the two models. As can be seen in the results, Amazon Nova Canvas has a higher win rate compared to the other text-to-image models.\\n| | |DALL.E 3| | |Imagen 3| |\\n|---|---|---|---|---|---|---|\\n| |win rate|tie rate|loss rate|win rate|tie rate|loss rate|\\n|Overall preference (image quality)|54.5|6.4|39.1|48.2|5.3|46.5|\\n|Instruction following (text-image alignment)|39.4|22.5|38.1|38.4|28.1|33.5|',\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\"\",\"\",\"DALL.E 3\",\"\",\"\",\"Imagen 3\",\"\"\\n\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\\n\"\",\"win rate\",\"tie rate\",\"loss rate\",\"win rate\",\"tie rate\",\"loss rate\"\\n\"Overall preference (image quality)\",\"54.5\",\"6.4\",\"39.1\",\"48.2\",\"5.3\",\"46.5\"\\n\"Instruction following (text-image alignment)\",\"39.4\",\"22.5\",\"38.1\",\"38.4\",\"28.1\",\"33.5\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 9: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Canvas versus (a) DALL.E 3 and (b) Imagen 3.',\n",
              "     'md': 'Table 9: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Canvas versus (a) DALL.E 3 and (b) Imagen 3.',\n",
              "     'bBox': {'x': 71, 'y': 119.04, 'w': 469, 'h': 85}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '4 Amazon Nova Reel Evaluation',\n",
              "     'md': '# 4 Amazon Nova Reel Evaluation',\n",
              "     'bBox': {'x': 72, 'y': 235.04, 'w': 173, 'h': 12}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Amazon Nova Reel is a diffusion model that takes a text prompt and an optional RGB image as input and generates a video as an output conditioned on the input text and optional image. Illustrative examples of the videos generated by the Amazon Nova Reel can be found in our Amazon Science blog post.7 In this section, we provide details on the evaluation strategy and performance of the model.',\n",
              "     'md': 'Amazon Nova Reel is a diffusion model that takes a text prompt and an optional RGB image as input and generates a video as an output conditioned on the input text and optional image. Illustrative examples of the videos generated by the Amazon Nova Reel can be found in our Amazon Science blog post.7 In this section, we provide details on the evaluation strategy and performance of the model.',\n",
              "     'bBox': {'x': 71, 'y': 260.04, 'w': 469, 'h': 43}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '4.1 Human evaluation metrics',\n",
              "     'md': '# 4.1 Human evaluation metrics',\n",
              "     'bBox': {'x': 72, 'y': 235.04, 'w': 135, 'h': 91.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'To evaluate Amazon Nova Reel, we rely on human feedback to assess the generated videos across two primary axes: video quality and video consistency. All evaluations are conducted through single-blind pairwise comparisons. Human annotators are provided a set of two videos shown side-by-side and are asked to choose the better video or mark them as equal if they find the videos to be equally performant across the metric on which they are evaluating. All videos were generated in 720p resolution and different random seeds were used during generation.\\n\\nThe video quality axis encapsulates the technical and perceptual aspects of the generated video via four primary components:\\n\\n- Image quality: The visual appeal of individual frames, including resolution, sharpness, object clarity, and overall composition, where each frame is visually pleasing and artifact-free.\\n- Motion quality: The fluidity of movement across frames, including motion consistency and smooth transitions without flickering, distortion, or abrupt shifts, contributing to natural and realistic motion portrayal.\\n- Image-text alignment: How closely individual frames match the prompt, considering the presence of described entities, their attributes, spatial relationships, colors, and other static visual details.\\n- Motion-text alignment: The accuracy of dynamic elements, including the correctness of actions performed by entities, camera movements, and temporal changes in attributes, as well as adherence to the provided description.\\n\\nThe video quality axis additionally includes factors influencing overall appeal, such as motion degree, entity size, creative composition, and general video likability.\\n\\nThe video consistency axis encapsulates the temporal coherence of both subjects and backgrounds throughout the video. It includes assessments of the maintenance of entity size, shape, and appearance, as well as background stability without unexpected morphing or changes. A high score in this dimension means believable spatial relationships between foreground and background elements throughout the video duration.\\n\\nIn combination, the video quality and video consistency metrics provide a holistic and robust evaluation framework for video generation models by considering both technical accuracy and perceptual appeal.',\n",
              "     'md': 'To evaluate Amazon Nova Reel, we rely on human feedback to assess the generated videos across two primary axes: video quality and video consistency. All evaluations are conducted through single-blind pairwise comparisons. Human annotators are provided a set of two videos shown side-by-side and are asked to choose the better video or mark them as equal if they find the videos to be equally performant across the metric on which they are evaluating. All videos were generated in 720p resolution and different random seeds were used during generation.\\n\\nThe video quality axis encapsulates the technical and perceptual aspects of the generated video via four primary components:\\n\\n- Image quality: The visual appeal of individual frames, including resolution, sharpness, object clarity, and overall composition, where each frame is visually pleasing and artifact-free.\\n- Motion quality: The fluidity of movement across frames, including motion consistency and smooth transitions without flickering, distortion, or abrupt shifts, contributing to natural and realistic motion portrayal.\\n- Image-text alignment: How closely individual frames match the prompt, considering the presence of described entities, their attributes, spatial relationships, colors, and other static visual details.\\n- Motion-text alignment: The accuracy of dynamic elements, including the correctness of actions performed by entities, camera movements, and temporal changes in attributes, as well as adherence to the provided description.\\n\\nThe video quality axis additionally includes factors influencing overall appeal, such as motion degree, entity size, creative composition, and general video likability.\\n\\nThe video consistency axis encapsulates the temporal coherence of both subjects and backgrounds throughout the video. It includes assessments of the maintenance of entity size, shape, and appearance, as well as background stability without unexpected morphing or changes. A high score in this dimension means believable spatial relationships between foreground and background elements throughout the video duration.\\n\\nIn combination, the video quality and video consistency metrics provide a holistic and robust evaluation framework for video generation models by considering both technical accuracy and perceptual appeal.',\n",
              "     'bBox': {'x': 71, 'y': 337.04, 'w': 470, 'h': 309}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '4.2 Dataset',\n",
              "     'md': '# 4.2 Dataset',\n",
              "     'bBox': {'x': 72, 'y': 235.04, 'w': 54, 'h': 435.99}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We curated a diverse set of prompts designed to capture various aspects of video generation. The prompts are distributed across 6 broad categories: human and activities, animals, natural scenery and landscapes, indoor scenes, objects.',\n",
              "     'md': 'We curated a diverse set of prompts designed to capture various aspects of video generation. The prompts are distributed across 6 broad categories: human and activities, animals, natural scenery and landscapes, indoor scenes, objects.',\n",
              "     'bBox': {'x': 71, 'y': 681.04, 'w': 469, 'h': 20}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://www.amazon.science/blog/amazon-nova-reel-examples',\n",
              "     'text': 'https://www.amazon.science/blog/amazon-nova-reel-examples'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 17,\n",
              "   'text': '                                       The Amazon Nova Family of Models\\ninteractions, and creative scenes and activities. This broad categorization ensures that the evaluation covers a wide\\nrange of real-world scenarios. We structured the prompt set to cover various motion-related aspects, which is critical\\nfor assessing motion-text alignment in the generated videos. For example, we included prompts with a variety of\\ncamera motions to evaluate how well the models follow instructions related to camera movement. Additionally, we\\nincorporated dynamic attributes [71], in which the subject or background undergoes state or shape changes over time,\\nwhich allows us to evaluate the model’s ability to generate evolving entities. Finally, we added prompts that require\\nmotion binding [71], where specific compositions of movements and actions are requested, enabling us to assess how\\nwell models can generate complex, coordinated motions. The curated prompt set consists of approximately 700 prompts,\\nall from various open source benchmarks.\\n4.3   Implementation details & results\\nTo ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a\\nthird-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and\\nexamples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These\\nguidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside\\nthis, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to\\nspot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators.\\nBased on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high\\nstandard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus\\nvoting system. For each video comparison, annotations were collected from three different evaluators, and a majority\\nvoting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that\\nthe final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art\\nmodels including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of\\nwin, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over\\nthe other model, while the tie rate indicates cases where no perceptible difference between the two models was found\\nby the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions\\noutlined above, and report the results in Table 10.\\n                Nova Reel versus:        Runway Gen3 Alpha                     Luma 1.6\\n                                      win rate  tie rate loss rate   win rate  tie rate  loss rate\\n                Video Quality          56.4       9.9       33.7       51.1      3.4        45.5\\n                Video Consistency      67.0       9.1       23.9       74.7       5.1       20.2\\nTable 10: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Reel versus (a) Gen3-Alpha and (b)\\nLuma1.6.\\nIn video consistency, Amazon Nova Reel achieved win rates of 67.0% against Gen3 Alpha and 74.7% against Luma\\n1.6, demonstrating superior subject and background coherence. For video quality, Amazon Nova Reel secured win rates\\nof 56.4% against Gen3 Alpha and 51.1% against Luma 1.6.\\n5    Responsible AI\\nOur approach to Responsible AI (RAI) is structured around eight foundational dimensions [10] shown in Table 11.\\nThese dimensions guide our approach to RAI for the Amazon Nova family of models, which we articulate in the\\nfollowing three sections: (1) defining our RAI design objectives, (2) our actions to ensure adherence to these objectives,\\nand (3) system evaluation and red teaming. The last two components form a continuous loop of model development and\\nhuman/automated verification to ensure that our Amazon Nova models are aligned with our RAI objectives and deliver\\nan exceptional and delightful customer experience.\\n5.1   Defining our RAI objectives\\nWe operationalize our RAI dimensions into a series of detailed design objectives that guide our decision-making\\nthroughout the entire model development lifecycle, from initial data collection and pre-training to the implementation\\nof post-deployment runtime mitigations.\\n                                                       17',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\ninteractions, and creative scenes and activities. This broad categorization ensures that the evaluation covers a wide range of real-world scenarios. We structured the prompt set to cover various motion-related aspects, which is critical for assessing motion-text alignment in the generated videos. For example, we included prompts with a variety of camera motions to evaluate how well the models follow instructions related to camera movement. Additionally, we incorporated dynamic attributes [71], in which the subject or background undergoes state or shape changes over time, which allows us to evaluate the model’s ability to generate evolving entities. Finally, we added prompts that require motion binding [71], where specific compositions of movements and actions are requested, enabling us to assess how well models can generate complex, coordinated motions. The curated prompt set consists of approximately 700 prompts, all from various open source benchmarks.\\n\\n# 4.3 Implementation details & results\\n\\nTo ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a third-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and examples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These guidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside this, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to spot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators. Based on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high standard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus voting system. For each video comparison, annotations were collected from three different evaluators, and a majority voting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that the final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\n\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art models including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of win, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over the other model, while the tie rate indicates cases where no perceptible difference between the two models was found by the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions outlined above, and report the results in Table 10.\\n\\n|Nova Reel versus:|Runway Gen3 Alpha| |Luma 1.6| | | |\\n|---|---|---|---|---|---|---|\\n| |win rate|tie rate|loss rate|win rate|tie rate|loss rate|\\n|Video Quality|56.4|9.9|33.7|51.1|3.4|45.5|\\n|Video Consistency|67.0|9.1|23.9|74.7|5.1|20.2|\\n\\nTable 10: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Reel versus (a) Gen3-Alpha and (b) Luma1.6.\\n\\nIn video consistency, Amazon Nova Reel achieved win rates of 67.0% against Gen3 Alpha and 74.7% against Luma 1.6, demonstrating superior subject and background coherence. For video quality, Amazon Nova Reel secured win rates of 56.4% against Gen3 Alpha and 51.1% against Luma 1.6.\\n\\n# 5 Responsible AI\\n\\nOur approach to Responsible AI (RAI) is structured around eight foundational dimensions [10] shown in Table 11. These dimensions guide our approach to RAI for the Amazon Nova family of models, which we articulate in the following three sections: (1) defining our RAI design objectives, (2) our actions to ensure adherence to these objectives, and (3) system evaluation and red teaming. The last two components form a continuous loop of model development and human/automated verification to ensure that our Amazon Nova models are aligned with our RAI objectives and deliver an exceptional and delightful customer experience.\\n\\n# 5.1 Defining our RAI objectives\\n\\nWe operationalize our RAI dimensions into a series of detailed design objectives that guide our decision-making throughout the entire model development lifecycle, from initial data collection and pre-training to the implementation of post-deployment runtime mitigations.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'interactions, and creative scenes and activities. This broad categorization ensures that the evaluation covers a wide range of real-world scenarios. We structured the prompt set to cover various motion-related aspects, which is critical for assessing motion-text alignment in the generated videos. For example, we included prompts with a variety of camera motions to evaluate how well the models follow instructions related to camera movement. Additionally, we incorporated dynamic attributes [71], in which the subject or background undergoes state or shape changes over time, which allows us to evaluate the model’s ability to generate evolving entities. Finally, we added prompts that require motion binding [71], where specific compositions of movements and actions are requested, enabling us to assess how well models can generate complex, coordinated motions. The curated prompt set consists of approximately 700 prompts, all from various open source benchmarks.',\n",
              "     'md': 'interactions, and creative scenes and activities. This broad categorization ensures that the evaluation covers a wide range of real-world scenarios. We structured the prompt set to cover various motion-related aspects, which is critical for assessing motion-text alignment in the generated videos. For example, we included prompts with a variety of camera motions to evaluate how well the models follow instructions related to camera movement. Additionally, we incorporated dynamic attributes [71], in which the subject or background undergoes state or shape changes over time, which allows us to evaluate the model’s ability to generate evolving entities. Finally, we added prompts that require motion binding [71], where specific compositions of movements and actions are requested, enabling us to assess how well models can generate complex, coordinated motions. The curated prompt set consists of approximately 700 prompts, all from various open source benchmarks.',\n",
              "     'bBox': {'x': 71, 'y': 72.04, 'w': 470, 'h': 98}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '4.3 Implementation details & results',\n",
              "     'md': '# 4.3 Implementation details & results',\n",
              "     'bBox': {'x': 72, 'y': 185.04, 'w': 162, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'To ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a third-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and examples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These guidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside this, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to spot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators. Based on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high standard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus voting system. For each video comparison, annotations were collected from three different evaluators, and a majority voting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that the final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\n\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art models including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of win, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over the other model, while the tie rate indicates cases where no perceptible difference between the two models was found by the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions outlined above, and report the results in Table 10.',\n",
              "     'md': 'To ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a third-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and examples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These guidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside this, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to spot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators. Based on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high standard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus voting system. For each video comparison, annotations were collected from three different evaluators, and a majority voting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that the final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\n\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art models including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of win, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over the other model, while the tie rate indicates cases where no perceptible difference between the two models was found by the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions outlined above, and report the results in Table 10.',\n",
              "     'bBox': {'x': 71, 'y': 205.04, 'w': 471, 'h': 371.01}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      [],\n",
              "      [],\n",
              "      ['Nova Reel versus:', 'Runway Gen3 Alpha', '', 'Luma 1.6', '', '', ''],\n",
              "      ['---', '---', '---', '---', '---', '---', '---'],\n",
              "      ['',\n",
              "       'win rate',\n",
              "       'tie rate',\n",
              "       'loss rate',\n",
              "       'win rate',\n",
              "       'tie rate',\n",
              "       'loss rate'],\n",
              "      ['Video Quality', '56.4', '9.9', '33.7', '51.1', '3.4', '45.5'],\n",
              "      ['Video Consistency', '67.0', '9.1', '23.9', '74.7', '5.1', '20.2']],\n",
              "     'md': 'To ensure a rigorous, consistent and unbiased evaluation process, we outsourced the annotation collection process to a third-party vendor. We created detailed guidelines, in which annotators were given comprehensive instructions and examples for each evaluation dimension, ensuring clarity on the criteria for marking preferences between videos. These guidelines included examples of different scenarios to aid in decision-making across our evaluation axes. Alongside this, we ensured that annotators were trained using expert-provided examples, with each round of annotations subject to spot checks. Specifically, 5-10% of the data from each batch was randomly selected and reviewed by expert annotators. Based on this feedback, the vendor continuously refined the annotators’ understanding and accuracy, ensuring a high standard of evaluation across the board. To further enhance the reliability of the results, we employed a consensus voting system. For each video comparison, annotations were collected from three different evaluators, and a majority voting approach was used to determine the final outcome. This method helps reduce individual biases and ensures that the final assessments are based on collective judgment, thereby increasing the robustness of the evaluation.\\nFor reporting performance, we conducted pairwise comparisons between Amazon Nova Reel and other state-of-the-art models including Gen3 Alpha [65] by Runway ML and Luma 1.6 [47] by Luma Labs. We report results in terms of win, tie, and loss rates. The win rate reflects the percentage of samples where Amazon Nova Reel was preferred over the other model, while the tie rate indicates cases where no perceptible difference between the two models was found by the evaluators. Using the curated prompt set described earlier, we evaluate the models across all the dimensions outlined above, and report the results in Table 10.\\n\\n|Nova Reel versus:|Runway Gen3 Alpha| |Luma 1.6| | | |\\n|---|---|---|---|---|---|---|\\n| |win rate|tie rate|loss rate|win rate|tie rate|loss rate|\\n|Video Quality|56.4|9.9|33.7|51.1|3.4|45.5|\\n|Video Consistency|67.0|9.1|23.9|74.7|5.1|20.2|',\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\\n\\n\"Nova Reel versus:\",\"Runway Gen3 Alpha\",\"\",\"Luma 1.6\",\"\",\"\",\"\"\\n\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\\n\"\",\"win rate\",\"tie rate\",\"loss rate\",\"win rate\",\"tie rate\",\"loss rate\"\\n\"Video Quality\",\"56.4\",\"9.9\",\"33.7\",\"51.1\",\"3.4\",\"45.5\"\\n\"Video Consistency\",\"67.0\",\"9.1\",\"23.9\",\"74.7\",\"5.1\",\"20.2\"',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 10: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Reel versus (a) Gen3-Alpha and (b) Luma1.6.\\n\\nIn video consistency, Amazon Nova Reel achieved win rates of 67.0% against Gen3 Alpha and 74.7% against Luma 1.6, demonstrating superior subject and background coherence. For video quality, Amazon Nova Reel secured win rates of 56.4% against Gen3 Alpha and 51.1% against Luma 1.6.',\n",
              "     'md': 'Table 10: The win, tie, and loss rates (%) from human evaluation of Amazon Nova Reel versus (a) Gen3-Alpha and (b) Luma1.6.\\n\\nIn video consistency, Amazon Nova Reel achieved win rates of 67.0% against Gen3 Alpha and 74.7% against Luma 1.6, demonstrating superior subject and background coherence. For video quality, Amazon Nova Reel secured win rates of 56.4% against Gen3 Alpha and 51.1% against Luma 1.6.',\n",
              "     'bBox': {'x': 71, 'y': 409.04, 'w': 469, 'h': 167.01}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '5 Responsible AI',\n",
              "     'md': '# 5 Responsible AI',\n",
              "     'bBox': {'x': 72, 'y': 564.04, 'w': 94, 'h': 12}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Our approach to Responsible AI (RAI) is structured around eight foundational dimensions [10] shown in Table 11. These dimensions guide our approach to RAI for the Amazon Nova family of models, which we articulate in the following three sections: (1) defining our RAI design objectives, (2) our actions to ensure adherence to these objectives, and (3) system evaluation and red teaming. The last two components form a continuous loop of model development and human/automated verification to ensure that our Amazon Nova models are aligned with our RAI objectives and deliver an exceptional and delightful customer experience.',\n",
              "     'md': 'Our approach to Responsible AI (RAI) is structured around eight foundational dimensions [10] shown in Table 11. These dimensions guide our approach to RAI for the Amazon Nova family of models, which we articulate in the following three sections: (1) defining our RAI design objectives, (2) our actions to ensure adherence to these objectives, and (3) system evaluation and red teaming. The last two components form a continuous loop of model development and human/automated verification to ensure that our Amazon Nova models are aligned with our RAI objectives and deliver an exceptional and delightful customer experience.',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 470, 'h': 620}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '5.1 Defining our RAI objectives',\n",
              "     'md': '# 5.1 Defining our RAI objectives',\n",
              "     'bBox': {'x': 72, 'y': 455.04, 'w': 349, 'h': 224}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We operationalize our RAI dimensions into a series of detailed design objectives that guide our decision-making throughout the entire model development lifecycle, from initial data collection and pre-training to the implementation of post-deployment runtime mitigations.',\n",
              "     'md': 'We operationalize our RAI dimensions into a series of detailed design objectives that guide our decision-making throughout the entire model development lifecycle, from initial data collection and pre-training to the implementation of post-deployment runtime mitigations.',\n",
              "     'bBox': {'x': 71, 'y': 689.04, 'w': 469, 'h': 31}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 18,\n",
              "   'text': '                                       The Amazon Nova Family of Models\\n  Term                          Definition\\n  Fairness                      Considering impacts on different groups of stakeholders\\n  Explainability                Understanding and evaluating system outputs\\n  Privacy and security          Appropriately obtaining, using, and protecting data and models\\n  Safety                        Preventing harmful system output and misuse\\n  Controllability               Having mechanisms to monitor and steer AI system behavior\\n  Veracity and robustness       Achieving correct system outputs, even with unexpected or adversarial inputs\\n  Governance                    Incorporating best practices into the AI supply chain, including providers and\\n                                deployers\\n  Transparency                  Enabling stakeholders to make informed choices about their engagement with an AI\\n                                system\\n                                Table 11: Our eight core Responsible AI dimensions\\n In addition to being grounded on the RAI dimensions, our objectives are informed by relevant laws and regulations,\\n voluntary frameworks, and our commitments to our customers, and they undergo an internal alignment process that\\n includes reviews from a number of stakeholders.      We will continue to iterate on these objections as we engage\\n with external experts and participate in industry and government forums, including the Frontier Model Forum [29],\\n Partnership on AI [5], and various forums organized by government agencies such as the National Institute of Standards\\n and Technology (NIST) of the U.S. Department of Commerce [7].\\n Our commitment to Responsible Scaling:      As the capabilities of AI models increase (through increased training\\n data, model size or architecture innovations), so do the potential risks that they present. We joined other technology\\n companies in signing on to the White House’s voluntary commitments on the safe, secure, and transparent development\\n and use of foundation models [6]. Since then we have actively participated in other efforts, including the AI Safety\\n Summits in the UK and Seoul, and we have committed to new standards like the G7 AI Hiroshima Process Code of\\n Conduct [30] in accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy\\n Development and Use of Artificial Intelligence. We also started a partnership with the Model Evaluation and Threat\\n Research (METR) center8 to enrich our Controllability design objectives.\\n 5.2  Ensuring adherence to RAI objectives\\nWe employed a number of methods to measure and ensure compliance for each of our core RAI dimensions depending\\n on their scope (i.e., whether they apply to model output, data management or other processes). For the dimensions\\n that govern model behavior (Safety, Fairness, Veracity and Robustness, Controllability, and Privacy and Security), we\\n curated the pre-training data and we used both Supervised Fine Tuning (SFT) and Reinforcement Learning from Human\\n Feedback (RLHF) methods to align our models. Based on the objectives for each RAI dimension, we created single-\\n and multi-turn RAI demonstrations in multiple languages and conducted helpfulness/harmfulness studies to decide on\\n SFT data mixes. We collected human preference data to be used as inputs to RLHF training where we also provided an\\n RAI-specific reward model. We also identify risk areas during our offline evaluation or red teaming exercises (Section\\n 5.4) and collect semantically similar examples to be included in future SFT and RLHF rounds.\\n In addition to the RAI model alignment, we built runtime input and output moderation models which serve as a first and\\n last line of defense and allow us to respond more quickly to newly identified threats or gaps in model alignment. The\\n main role of the input moderation model is to detect prompts that contain malicious, insecure or illegal material, or\\n attempt to bypass the core model alignment (prompt injection, jailbreaking). Similarly, the output moderation ensures\\n that the content adheres to our RAI objectives.\\n      We have a rigorous Governance methodology, developing our models in a working-backwards product process that\\n incorporates RAI at the design phase, design consultations and implementation assessments by dedicated RAI science\\n and data experts, and includes routine testing, reviews with customers, best practice development, dissemination, and\\n training.\\n   8https://metr.org/\\n                                                       18',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n|Term|Definition|\\n|---|---|\\n|Fairness|Considering impacts on different groups of stakeholders|\\n|Explainability|Understanding and evaluating system outputs|\\n|Privacy and security|Appropriately obtaining, using, and protecting data and models|\\n|Safety|Preventing harmful system output and misuse|\\n|Controllability|Having mechanisms to monitor and steer AI system behavior|\\n|Veracity and robustness|Achieving correct system outputs, even with unexpected or adversarial inputs|\\n|Governance|Incorporating best practices into the AI supply chain, including providers and deployers|\\n|Transparency|Enabling stakeholders to make informed choices about their engagement with an AI system|\\n\\nTable 11: Our eight core Responsible AI dimensions\\n\\nIn addition to being grounded on the RAI dimensions, our objectives are informed by relevant laws and regulations, voluntary frameworks, and our commitments to our customers, and they undergo an internal alignment process that includes reviews from a number of stakeholders. We will continue to iterate on these objections as we engage with external experts and participate in industry and government forums, including the Frontier Model Forum [29], Partnership on AI [5], and various forums organized by government agencies such as the National Institute of Standards and Technology (NIST) of the U.S. Department of Commerce [7].\\n\\nOur commitment to Responsible Scaling: As the capabilities of AI models increase (through increased training data, model size or architecture innovations), so do the potential risks that they present. We joined other technology companies in signing on to the White House’s voluntary commitments on the safe, secure, and transparent development and use of foundation models [6]. Since then we have actively participated in other efforts, including the AI Safety Summits in the UK and Seoul, and we have committed to new standards like the G7 AI Hiroshima Process Code of Conduct [30] in accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We also started a partnership with the Model Evaluation and Threat Research (METR) center8 to enrich our Controllability design objectives.\\n\\n# 5.2 Ensuring adherence to RAI objectives\\n\\nWe employed a number of methods to measure and ensure compliance for each of our core RAI dimensions depending on their scope (i.e., whether they apply to model output, data management or other processes). For the dimensions that govern model behavior (Safety, Fairness, Veracity and Robustness, Controllability, and Privacy and Security), we curated the pre-training data and we used both Supervised Fine Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methods to align our models. Based on the objectives for each RAI dimension, we created single- and multi-turn RAI demonstrations in multiple languages and conducted helpfulness/harmfulness studies to decide on SFT data mixes. We collected human preference data to be used as inputs to RLHF training where we also provided an RAI-specific reward model. We also identify risk areas during our offline evaluation or red teaming exercises (Section 5.4) and collect semantically similar examples to be included in future SFT and RLHF rounds.\\n\\nIn addition to the RAI model alignment, we built runtime input and output moderation models which serve as a first and last line of defense and allow us to respond more quickly to newly identified threats or gaps in model alignment. The main role of the input moderation model is to detect prompts that contain malicious, insecure or illegal material, or attempt to bypass the core model alignment (prompt injection, jailbreaking). Similarly, the output moderation ensures that the content adheres to our RAI objectives.\\n\\nWe have a rigorous Governance methodology, developing our models in a working-backwards product process that incorporates RAI at the design phase, design consultations and implementation assessments by dedicated RAI science and data experts, and includes routine testing, reviews with customers, best practice development, dissemination, and training.\\n\\n8https://metr.org/',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['Term', 'Definition'],\n",
              "      ['Fairness', 'Considering impacts on different groups of stakeholders'],\n",
              "      ['Explainability', 'Understanding and evaluating system outputs'],\n",
              "      ['Privacy and security',\n",
              "       'Appropriately obtaining, using, and protecting data and models'],\n",
              "      ['Safety', 'Preventing harmful system output and misuse'],\n",
              "      ['Controllability',\n",
              "       'Having mechanisms to monitor and steer AI system behavior'],\n",
              "      ['Veracity and robustness',\n",
              "       'Achieving correct system outputs, even with unexpected or adversarial inputs'],\n",
              "      ['Governance',\n",
              "       'Incorporating best practices into the AI supply chain, including providers and deployers'],\n",
              "      ['Transparency',\n",
              "       'Enabling stakeholders to make informed choices about their engagement with an AI system']],\n",
              "     'md': '|Term|Definition|\\n|Fairness|Considering impacts on different groups of stakeholders|\\n|Explainability|Understanding and evaluating system outputs|\\n|Privacy and security|Appropriately obtaining, using, and protecting data and models|\\n|Safety|Preventing harmful system output and misuse|\\n|Controllability|Having mechanisms to monitor and steer AI system behavior|\\n|Veracity and robustness|Achieving correct system outputs, even with unexpected or adversarial inputs|\\n|Governance|Incorporating best practices into the AI supply chain, including providers and deployers|\\n|Transparency|Enabling stakeholders to make informed choices about their engagement with an AI system|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"Term\",\"Definition\"\\n\"Fairness\",\"Considering impacts on different groups of stakeholders\"\\n\"Explainability\",\"Understanding and evaluating system outputs\"\\n\"Privacy and security\",\"Appropriately obtaining, using, and protecting data and models\"\\n\"Safety\",\"Preventing harmful system output and misuse\"\\n\"Controllability\",\"Having mechanisms to monitor and steer AI system behavior\"\\n\"Veracity and robustness\",\"Achieving correct system outputs, even with unexpected or adversarial inputs\"\\n\"Governance\",\"Incorporating best practices into the AI supply chain, including providers and deployers\"\\n\"Transparency\",\"Enabling stakeholders to make informed choices about their engagement with an AI system\"',\n",
              "     'bBox': {'x': 71, 'y': 73.04, 'w': 469, 'h': 588}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Table 11: Our eight core Responsible AI dimensions\\n\\nIn addition to being grounded on the RAI dimensions, our objectives are informed by relevant laws and regulations, voluntary frameworks, and our commitments to our customers, and they undergo an internal alignment process that includes reviews from a number of stakeholders. We will continue to iterate on these objections as we engage with external experts and participate in industry and government forums, including the Frontier Model Forum [29], Partnership on AI [5], and various forums organized by government agencies such as the National Institute of Standards and Technology (NIST) of the U.S. Department of Commerce [7].\\n\\nOur commitment to Responsible Scaling: As the capabilities of AI models increase (through increased training data, model size or architecture innovations), so do the potential risks that they present. We joined other technology companies in signing on to the White House’s voluntary commitments on the safe, secure, and transparent development and use of foundation models [6]. Since then we have actively participated in other efforts, including the AI Safety Summits in the UK and Seoul, and we have committed to new standards like the G7 AI Hiroshima Process Code of Conduct [30] in accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We also started a partnership with the Model Evaluation and Threat Research (METR) center8 to enrich our Controllability design objectives.',\n",
              "     'md': 'Table 11: Our eight core Responsible AI dimensions\\n\\nIn addition to being grounded on the RAI dimensions, our objectives are informed by relevant laws and regulations, voluntary frameworks, and our commitments to our customers, and they undergo an internal alignment process that includes reviews from a number of stakeholders. We will continue to iterate on these objections as we engage with external experts and participate in industry and government forums, including the Frontier Model Forum [29], Partnership on AI [5], and various forums organized by government agencies such as the National Institute of Standards and Technology (NIST) of the U.S. Department of Commerce [7].\\n\\nOur commitment to Responsible Scaling: As the capabilities of AI models increase (through increased training data, model size or architecture innovations), so do the potential risks that they present. We joined other technology companies in signing on to the White House’s voluntary commitments on the safe, secure, and transparent development and use of foundation models [6]. Since then we have actively participated in other efforts, including the AI Safety Summits in the UK and Seoul, and we have committed to new standards like the G7 AI Hiroshima Process Code of Conduct [30] in accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We also started a partnership with the Model Evaluation and Threat Research (METR) center8 to enrich our Controllability design objectives.',\n",
              "     'bBox': {'x': 71, 'y': 133.04, 'w': 470, 'h': 313}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '5.2 Ensuring adherence to RAI objectives',\n",
              "     'md': '# 5.2 Ensuring adherence to RAI objectives',\n",
              "     'bBox': {'x': 72, 'y': 465.04, 'w': 183, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We employed a number of methods to measure and ensure compliance for each of our core RAI dimensions depending on their scope (i.e., whether they apply to model output, data management or other processes). For the dimensions that govern model behavior (Safety, Fairness, Veracity and Robustness, Controllability, and Privacy and Security), we curated the pre-training data and we used both Supervised Fine Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methods to align our models. Based on the objectives for each RAI dimension, we created single- and multi-turn RAI demonstrations in multiple languages and conducted helpfulness/harmfulness studies to decide on SFT data mixes. We collected human preference data to be used as inputs to RLHF training where we also provided an RAI-specific reward model. We also identify risk areas during our offline evaluation or red teaming exercises (Section 5.4) and collect semantically similar examples to be included in future SFT and RLHF rounds.\\n\\nIn addition to the RAI model alignment, we built runtime input and output moderation models which serve as a first and last line of defense and allow us to respond more quickly to newly identified threats or gaps in model alignment. The main role of the input moderation model is to detect prompts that contain malicious, insecure or illegal material, or attempt to bypass the core model alignment (prompt injection, jailbreaking). Similarly, the output moderation ensures that the content adheres to our RAI objectives.\\n\\nWe have a rigorous Governance methodology, developing our models in a working-backwards product process that incorporates RAI at the design phase, design consultations and implementation assessments by dedicated RAI science and data experts, and includes routine testing, reviews with customers, best practice development, dissemination, and training.\\n\\n8https://metr.org/',\n",
              "     'md': 'We employed a number of methods to measure and ensure compliance for each of our core RAI dimensions depending on their scope (i.e., whether they apply to model output, data management or other processes). For the dimensions that govern model behavior (Safety, Fairness, Veracity and Robustness, Controllability, and Privacy and Security), we curated the pre-training data and we used both Supervised Fine Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methods to align our models. Based on the objectives for each RAI dimension, we created single- and multi-turn RAI demonstrations in multiple languages and conducted helpfulness/harmfulness studies to decide on SFT data mixes. We collected human preference data to be used as inputs to RLHF training where we also provided an RAI-specific reward model. We also identify risk areas during our offline evaluation or red teaming exercises (Section 5.4) and collect semantically similar examples to be included in future SFT and RLHF rounds.\\n\\nIn addition to the RAI model alignment, we built runtime input and output moderation models which serve as a first and last line of defense and allow us to respond more quickly to newly identified threats or gaps in model alignment. The main role of the input moderation model is to detect prompts that contain malicious, insecure or illegal material, or attempt to bypass the core model alignment (prompt injection, jailbreaking). Similarly, the output moderation ensures that the content adheres to our RAI objectives.\\n\\nWe have a rigorous Governance methodology, developing our models in a working-backwards product process that incorporates RAI at the design phase, design consultations and implementation assessments by dedicated RAI science and data experts, and includes routine testing, reviews with customers, best practice development, dissemination, and training.\\n\\n8https://metr.org/',\n",
              "     'bBox': {'x': 71, 'y': 88.04, 'w': 471, 'h': 628.98}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://metr.org/', 'text': 'https://metr.org/'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 19,\n",
              "   'text': '                                       The Amazon Nova Family of Models\\nWe work to ensure that our Privacy and Security objectives are adhered to for both the model and training data.\\nIn addition to the model output alignment described above, we take measures that include data access controls [9]\\nprotecting our model training data, resulting weights, and model versions, and watermarking model outputs (see below).\\nWe address the latter through several layers of defense, including de-identifying or removing certain types of personal\\ndata from our training data, when feasible, as well as evaluation through red teaming exercises that cover data privacy\\nassessments.\\nFor Explainability of our models’ outputs we conduct and leverage the current active research in the area of Explainable\\nAI to deeply understand our models’ current behavior, their potential future behavior, and to build capabilities to\\ncontinuously correct their behavior as and when necessary. We use various explainable AI methods throughout our\\nmodel development to guide our decisions regarding RAI alignment and other mitigations. Services like Clarify [8]\\nalso enable our downstream developers to easily explain model predictions.\\nTo work to ensure our models’ Robustness against adversarial inputs such as those that attempt to bypass alignment\\nguardrails, we focused on risks applicable to both developers building applications using our models, and users\\ninteracting with our models via those applications. We organized those risks in broad categories such as sensitive data\\nexfiltration, execution of unauthorized action, degradation of run-time model service availability, and malicious content\\ngeneration. We used this risk organization to build model resiliency against interactions that lead to the prioritized risks.\\nFinally, to maximize Transparency, we incorporate an invisible watermark during the image or video generation\\nprocess and add C2PA9 metadata in all Canvas generated content. We enhanced the robustness to alterations like\\nrotation, resizing, color inversion, and flipping. For videos, we embed our watermark in each frame and ensure that our\\nwatermarking and detection methods withstand H264 compression. To enable anyone to easily detect the watermarks in\\nAmazon Nova generated content, an API will be available soon after launch. Our watermark detection system introduces\\nseveral enhancements such as making confidence score-based predictions instead of a single binary prediction that\\nreflects the extent to which the generated content has been edited even when using external tools. The new detection\\nsystem covers both images and videos.\\n5.3   RAI Evaluation\\nThroughout model development we perform extensive RAI evaluations using publicly available benchmarks like BOLD\\n[25], RealToxicityPrompts [31], and MM-SafetyBench [44]. We also built a series of proprietary, dynamically updating\\nbenchmarks. To build them, our internal data annotation team created a diverse set of examples for each of our RAI\\ndimensions. In addition, we leveraged subject-matter experts in specific areas, such as Security and Controllability, to\\ncollect adversarial prompts. We continued updating and enhancing each dataset based on evaluation and red teaming\\nresults (see Section 5.4 for more details on red teaming). This kept the internal benchmarks evergreen, avoiding\\noverfitting during development, but also made sure the models do not regress against previously identified risks.\\nOur datasets comprise inputs in multiple languages and multiple modalities, and contain single-turn and multi-turn\\nconversation examples.\\n5.4   Red Teaming\\nStatic benchmarks give us a view of how well models perform per RAI dimension against a user’s “plain” intent (i.e.\\nthe prompts explicitly state the intent of the user to generate prohibited content). To test our models’ resilience against\\ntechniques that mask the users’ intent we rely on red teaming. We employed a multi-pronged evaluation strategy\\nconsisting of internal red teaming, red teaming with third party and subject matter experts and, automated red teaming.\\n5.4.1  Internal Red Teaming\\nWe used a team of trained data analysts and subject-matter experts to perform regular red teaming exercises to\\nevaluate the model’s robustness against adversarial prompts across all our RAI dimensions. We enhanced the diversity\\nof manually curated adversarial prompts by employing linguistic, structural, and modality based prompt mutation\\ntechniques, assessing each mutation for its effectiveness at generating a response that does not adhere to our RAI\\nobjectives, likelihood of its success, and the technique’s novelty to a model revision. In total, we identified and\\ndeveloped over 300 distinct techniques (see Figure 4), and tested techniques individually and via chaining various\\ncombinations. The attacks covered multiple languages and modalities, targeting each language/modality individually\\nand in combination. We designed cross-modality attacks, such as embedding adversarial content within seemingly\\nbenign visual inputs, to evaluate the models’ ability to handle complex scenarios involving multiple input types. Where\\nappropriate, we implemented automation to further improve the diversity, reliability, and efficiency of red teaming.\\n   9https://c2pa.org/\\n                                                       19',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\nWe work to ensure that our Privacy and Security objectives are adhered to for both the model and training data. In addition to the model output alignment described above, we take measures that include data access controls [9] protecting our model training data, resulting weights, and model versions, and watermarking model outputs (see below). We address the latter through several layers of defense, including de-identifying or removing certain types of personal data from our training data, when feasible, as well as evaluation through red teaming exercises that cover data privacy assessments.\\n\\nFor Explainability of our models’ outputs we conduct and leverage the current active research in the area of Explainable AI to deeply understand our models’ current behavior, their potential future behavior, and to build capabilities to continuously correct their behavior as and when necessary. We use various explainable AI methods throughout our model development to guide our decisions regarding RAI alignment and other mitigations. Services like Clarify [8] also enable our downstream developers to easily explain model predictions.\\n\\nTo work to ensure our models’ Robustness against adversarial inputs such as those that attempt to bypass alignment guardrails, we focused on risks applicable to both developers building applications using our models, and users interacting with our models via those applications. We organized those risks in broad categories such as sensitive data exfiltration, execution of unauthorized action, degradation of run-time model service availability, and malicious content generation. We used this risk organization to build model resiliency against interactions that lead to the prioritized risks.\\n\\nFinally, to maximize Transparency, we incorporate an invisible watermark during the image or video generation process and add C2PA9 metadata in all Canvas generated content. We enhanced the robustness to alterations like rotation, resizing, color inversion, and flipping. For videos, we embed our watermark in each frame and ensure that our watermarking and detection methods withstand H264 compression. To enable anyone to easily detect the watermarks in Amazon Nova generated content, an API will be available soon after launch. Our watermark detection system introduces several enhancements such as making confidence score-based predictions instead of a single binary prediction that reflects the extent to which the generated content has been edited even when using external tools. The new detection system covers both images and videos.\\n\\n# 5.3 RAI Evaluation\\n\\nThroughout model development we perform extensive RAI evaluations using publicly available benchmarks like BOLD [25], RealToxicityPrompts [31], and MM-SafetyBench [44]. We also built a series of proprietary, dynamically updating benchmarks. To build them, our internal data annotation team created a diverse set of examples for each of our RAI dimensions. In addition, we leveraged subject-matter experts in specific areas, such as Security and Controllability, to collect adversarial prompts. We continued updating and enhancing each dataset based on evaluation and red teaming results (see Section 5.4 for more details on red teaming). This kept the internal benchmarks evergreen, avoiding overfitting during development, but also made sure the models do not regress against previously identified risks. Our datasets comprise inputs in multiple languages and multiple modalities, and contain single-turn and multi-turn conversation examples.\\n\\n# 5.4 Red Teaming\\n\\nStatic benchmarks give us a view of how well models perform per RAI dimension against a user’s “plain” intent (i.e. the prompts explicitly state the intent of the user to generate prohibited content). To test our models’ resilience against techniques that mask the users’ intent we rely on red teaming. We employed a multi-pronged evaluation strategy consisting of internal red teaming, red teaming with third party and subject matter experts and, automated red teaming.\\n\\n# 5.4.1 Internal Red Teaming\\n\\nWe used a team of trained data analysts and subject-matter experts to perform regular red teaming exercises to evaluate the model’s robustness against adversarial prompts across all our RAI dimensions. We enhanced the diversity of manually curated adversarial prompts by employing linguistic, structural, and modality based prompt mutation techniques, assessing each mutation for its effectiveness at generating a response that does not adhere to our RAI objectives, likelihood of its success, and the technique’s novelty to a model revision. In total, we identified and developed over 300 distinct techniques (see Figure 4), and tested techniques individually and via chaining various combinations. The attacks covered multiple languages and modalities, targeting each language/modality individually and in combination. We designed cross-modality attacks, such as embedding adversarial content within seemingly benign visual inputs, to evaluate the models’ ability to handle complex scenarios involving multiple input types. Where appropriate, we implemented automation to further improve the diversity, reliability, and efficiency of red teaming.\\n\\n9 https://c2pa.org/',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We work to ensure that our Privacy and Security objectives are adhered to for both the model and training data. In addition to the model output alignment described above, we take measures that include data access controls [9] protecting our model training data, resulting weights, and model versions, and watermarking model outputs (see below). We address the latter through several layers of defense, including de-identifying or removing certain types of personal data from our training data, when feasible, as well as evaluation through red teaming exercises that cover data privacy assessments.\\n\\nFor Explainability of our models’ outputs we conduct and leverage the current active research in the area of Explainable AI to deeply understand our models’ current behavior, their potential future behavior, and to build capabilities to continuously correct their behavior as and when necessary. We use various explainable AI methods throughout our model development to guide our decisions regarding RAI alignment and other mitigations. Services like Clarify [8] also enable our downstream developers to easily explain model predictions.\\n\\nTo work to ensure our models’ Robustness against adversarial inputs such as those that attempt to bypass alignment guardrails, we focused on risks applicable to both developers building applications using our models, and users interacting with our models via those applications. We organized those risks in broad categories such as sensitive data exfiltration, execution of unauthorized action, degradation of run-time model service availability, and malicious content generation. We used this risk organization to build model resiliency against interactions that lead to the prioritized risks.\\n\\nFinally, to maximize Transparency, we incorporate an invisible watermark during the image or video generation process and add C2PA9 metadata in all Canvas generated content. We enhanced the robustness to alterations like rotation, resizing, color inversion, and flipping. For videos, we embed our watermark in each frame and ensure that our watermarking and detection methods withstand H264 compression. To enable anyone to easily detect the watermarks in Amazon Nova generated content, an API will be available soon after launch. Our watermark detection system introduces several enhancements such as making confidence score-based predictions instead of a single binary prediction that reflects the extent to which the generated content has been edited even when using external tools. The new detection system covers both images and videos.',\n",
              "     'md': 'We work to ensure that our Privacy and Security objectives are adhered to for both the model and training data. In addition to the model output alignment described above, we take measures that include data access controls [9] protecting our model training data, resulting weights, and model versions, and watermarking model outputs (see below). We address the latter through several layers of defense, including de-identifying or removing certain types of personal data from our training data, when feasible, as well as evaluation through red teaming exercises that cover data privacy assessments.\\n\\nFor Explainability of our models’ outputs we conduct and leverage the current active research in the area of Explainable AI to deeply understand our models’ current behavior, their potential future behavior, and to build capabilities to continuously correct their behavior as and when necessary. We use various explainable AI methods throughout our model development to guide our decisions regarding RAI alignment and other mitigations. Services like Clarify [8] also enable our downstream developers to easily explain model predictions.\\n\\nTo work to ensure our models’ Robustness against adversarial inputs such as those that attempt to bypass alignment guardrails, we focused on risks applicable to both developers building applications using our models, and users interacting with our models via those applications. We organized those risks in broad categories such as sensitive data exfiltration, execution of unauthorized action, degradation of run-time model service availability, and malicious content generation. We used this risk organization to build model resiliency against interactions that lead to the prioritized risks.\\n\\nFinally, to maximize Transparency, we incorporate an invisible watermark during the image or video generation process and add C2PA9 metadata in all Canvas generated content. We enhanced the robustness to alterations like rotation, resizing, color inversion, and flipping. For videos, we embed our watermark in each frame and ensure that our watermarking and detection methods withstand H264 compression. To enable anyone to easily detect the watermarks in Amazon Nova generated content, an API will be available soon after launch. Our watermark detection system introduces several enhancements such as making confidence score-based predictions instead of a single binary prediction that reflects the extent to which the generated content has been edited even when using external tools. The new detection system covers both images and videos.',\n",
              "     'bBox': {'x': 71, 'y': 72.04, 'w': 471, 'h': 435}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '5.3 RAI Evaluation',\n",
              "     'md': '# 5.3 RAI Evaluation',\n",
              "     'bBox': {'x': 72, 'y': 365.04, 'w': 89, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Throughout model development we perform extensive RAI evaluations using publicly available benchmarks like BOLD [25], RealToxicityPrompts [31], and MM-SafetyBench [44]. We also built a series of proprietary, dynamically updating benchmarks. To build them, our internal data annotation team created a diverse set of examples for each of our RAI dimensions. In addition, we leveraged subject-matter experts in specific areas, such as Security and Controllability, to collect adversarial prompts. We continued updating and enhancing each dataset based on evaluation and red teaming results (see Section 5.4 for more details on red teaming). This kept the internal benchmarks evergreen, avoiding overfitting during development, but also made sure the models do not regress against previously identified risks. Our datasets comprise inputs in multiple languages and multiple modalities, and contain single-turn and multi-turn conversation examples.',\n",
              "     'md': 'Throughout model development we perform extensive RAI evaluations using publicly available benchmarks like BOLD [25], RealToxicityPrompts [31], and MM-SafetyBench [44]. We also built a series of proprietary, dynamically updating benchmarks. To build them, our internal data annotation team created a diverse set of examples for each of our RAI dimensions. In addition, we leveraged subject-matter experts in specific areas, such as Security and Controllability, to collect adversarial prompts. We continued updating and enhancing each dataset based on evaluation and red teaming results (see Section 5.4 for more details on red teaming). This kept the internal benchmarks evergreen, avoiding overfitting during development, but also made sure the models do not regress against previously identified risks. Our datasets comprise inputs in multiple languages and multiple modalities, and contain single-turn and multi-turn conversation examples.',\n",
              "     'bBox': {'x': 71, 'y': 365.04, 'w': 471, 'h': 142}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '5.4 Red Teaming',\n",
              "     'md': '# 5.4 Red Teaming',\n",
              "     'bBox': {'x': 72, 'y': 497.04, 'w': 78, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Static benchmarks give us a view of how well models perform per RAI dimension against a user’s “plain” intent (i.e. the prompts explicitly state the intent of the user to generate prohibited content). To test our models’ resilience against techniques that mask the users’ intent we rely on red teaming. We employed a multi-pronged evaluation strategy consisting of internal red teaming, red teaming with third party and subject matter experts and, automated red teaming.',\n",
              "     'md': 'Static benchmarks give us a view of how well models perform per RAI dimension against a user’s “plain” intent (i.e. the prompts explicitly state the intent of the user to generate prohibited content). To test our models’ resilience against techniques that mask the users’ intent we rely on red teaming. We employed a multi-pronged evaluation strategy consisting of internal red teaming, red teaming with third party and subject matter experts and, automated red teaming.',\n",
              "     'bBox': {'x': 72, 'y': 497.04, 'w': 470, 'h': 87}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '5.4.1 Internal Red Teaming',\n",
              "     'md': '# 5.4.1 Internal Red Teaming',\n",
              "     'bBox': {'x': 72, 'y': 497.04, 'w': 123, 'h': 87}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We used a team of trained data analysts and subject-matter experts to perform regular red teaming exercises to evaluate the model’s robustness against adversarial prompts across all our RAI dimensions. We enhanced the diversity of manually curated adversarial prompts by employing linguistic, structural, and modality based prompt mutation techniques, assessing each mutation for its effectiveness at generating a response that does not adhere to our RAI objectives, likelihood of its success, and the technique’s novelty to a model revision. In total, we identified and developed over 300 distinct techniques (see Figure 4), and tested techniques individually and via chaining various combinations. The attacks covered multiple languages and modalities, targeting each language/modality individually and in combination. We designed cross-modality attacks, such as embedding adversarial content within seemingly benign visual inputs, to evaluate the models’ ability to handle complex scenarios involving multiple input types. Where appropriate, we implemented automation to further improve the diversity, reliability, and efficiency of red teaming.\\n\\n9 https://c2pa.org/',\n",
              "     'md': 'We used a team of trained data analysts and subject-matter experts to perform regular red teaming exercises to evaluate the model’s robustness against adversarial prompts across all our RAI dimensions. We enhanced the diversity of manually curated adversarial prompts by employing linguistic, structural, and modality based prompt mutation techniques, assessing each mutation for its effectiveness at generating a response that does not adhere to our RAI objectives, likelihood of its success, and the technique’s novelty to a model revision. In total, we identified and developed over 300 distinct techniques (see Figure 4), and tested techniques individually and via chaining various combinations. The attacks covered multiple languages and modalities, targeting each language/modality individually and in combination. We designed cross-modality attacks, such as embedding adversarial content within seemingly benign visual inputs, to evaluate the models’ ability to handle complex scenarios involving multiple input types. Where appropriate, we implemented automation to further improve the diversity, reliability, and efficiency of red teaming.\\n\\n9 https://c2pa.org/',\n",
              "     'bBox': {'x': 71, 'y': 497.04, 'w': 471, 'h': 219.98}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://c2pa.org/', 'text': 'https://c2pa.org/'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 20,\n",
              "   'text': '                                                          The Amazon Nova Family of Models\\n                                                                                                                                                Cipher-based: 3\\n                                                                                                                                      Text based obfuscation: 5\\n                                                                                                    Obfuscation: 8                            Code injection: 9\\n                                                                                                                                         Recursive injection: 1\\n                                                                                                                                              Virtualization: 9\\n                                                                                                                                   Defined dictionary attack: 1\\n                                                                                                                                           Payload splitting: 6\\n                                                                                                                                             Token smuggling: 6\\n                        Prompt injections (direct and indirect): 40                                                                                one-shot: 12\\n                                                                                                                                                  many-shot: 12\\n                                                      Jailbreak: 24                                                                  Multilingual prompting: 14\\n                                                                                                                                        Translation requests: 8\\n                                             Multiple languages: 33                                                                 Mixed language requests: 11\\n                                                                                                                                        In-Context Learning: 14\\n                                                  Context-based: 32                                                                     Syntactic separators: 8\\n                                                                                              Context switching: 18                     Semantic separators: 10\\n                                                                                                                                     Instruction repetition: 19\\n          Attacks: 307                                                                                                                Completion Compliance: 13\\n                                                                                                                                       Affirmative Suffixes: 13\\n                                                    Persuasion: 119                                                                     One-sided arguments: 12\\n                                                                                                                                        Refusal suppression: 10\\n                                                                                                                                        Chain of utterances: 10\\n                                                                                                                             Socratic Questioning Technique: 15\\n                                                                                                                                            Personification: 10\\n                                                   Obfuscations: 59                                                                        Task constraints: 17\\n                                                                                                                                         Veiled Expressions: 16\\n                                                                                                                                         Output constraints: 20\\n                                                                                                                                     Euphemisms via Ciphers: 11\\n                                                                                                                                      Decoding Manipulation: 11\\n                                                                                                                                            Macaronic prompt: 1\\n                 Figure 4: Broad taxonomy and count of attack techniques we use for our red teaming exercises\\nAfter each round of red teaming, we gathered feedback from the team regarding failure patterns which guided the next\\nstage of the model development.\\n5.4.2      External Red Teaming\\nIn accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Artificial\\nIntelligence, we partner with a variety of third parties to conduct red teaming against our AI models. These initiatives\\nare in addition to our extensive in-house efforts, which includes all aspects of Cybersecurity red teaming. Just like with\\nour internal red teaming efforts, we iterated during the model development based on feedback from these institutions to\\nimprove the RAI adherence of our models. We leverage red-teaming firms including ActiveFence to conduct testing\\nin areas such as hate speech, political misinformation, extremism and other RAI dimensions. We also work with\\nspecialized third parties to red team our models for Chemical, Biological, Radiological and Nuclear (CBRN) capabilities.\\nOur work with Deloitte Consulting, tests our AI models’ capabilities in Biological risks and harms. Our work with\\nNemesys Insights LLC tests our AI models’ capabilities in the Radiological and Nuclear domains. We also work with\\nthe Gomes Group at Carnegie Mellon University to test our models’ capabilities in Chemistry and chemical compounds.\\nEach of these partners was carefully selected based on their industry leadership, previous/parallel red teaming work\\nwith other AI model developers, and their contributions to evolving government and industry standards around CBRN\\nand overall AI safety. We provide a brief summary of expertise of each of these vendors and their testing methodology\\nbelow.\\nActiveFence: ActiveFence is a team of over 150 subject matter experts providing AI Safety and Content Moderation\\nsolutions. The team produced over 9,700 adversarial prompts, distributed over 20 categories, including content-targeted\\nred teaming (evaluating the model’s ability to generate harmful or inappropriate content), and security-targeted red\\nteaming (assessing the model’s resilience against malicious attempts to manipulate its behavior or extract sensitive\\ninformation).\\nDeloitte\\n            : The evaluation team at Deloitte Consulting LLP (formerly known as Gryphon Scientific) has unique experience\\nat the intersection of artificial intelligence and biology. The primary thrust of this effort involved evaluating the model\\nagainst a panel of 30 questions developed to test an LLM’s scientific knowledge and reasoning capabilities that could\\nfacilitate the development or use of biological weapons. The model’s responses to these questions were evaluated\\nfor their scientific accuracy and utility to someone seeking to do harm with biology. After completing the initial\\n                                                                                   20',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n|Cipher-based:|3|\\n|---|---|\\n|Text based obfuscation:|5|\\n|Obfuscation:|8|\\n|Code injection:|9|\\n|Recursive injection:|1|\\n|Virtualization:|9|\\n|Defined dictionary attack:|1|\\n|Payload splitting:|6|\\n|Token smuggling:|6|\\n|Prompt injections (direct and indirect):|40|\\n|one-shot:|12|\\n|many-shot:|12|\\n|Jailbreak:|24|\\n|Multilingual prompting:|14|\\n|Translation requests:|8|\\n|Multiple languages:|33|\\n|Mixed language requests:|11|\\n|In-Context Learning:|14|\\n|Context-based:|32|\\n|Syntactic separators:|8|\\n|Context switching:|18|\\n|Semantic separators:|10|\\n|Instruction repetition:|19|\\n|Attacks:|307|\\n|Completion Compliance:|13|\\n|Affirmative Suffixes:|13|\\n|Persuasion:|119|\\n|One-sided arguments:|12|\\n|Refusal suppression:|10|\\n|Chain of utterances:|10|\\n|Socratic Questioning Technique:|15|\\n|Personification:|10|\\n|Obfuscations:|59|\\n|Task constraints:|17|\\n|Veiled Expressions:|16|\\n|Output constraints:|20|\\n|Euphemisms via Ciphers:|11|\\n|Decoding Manipulation:|11|\\n|Macaronic prompt:|1|\\n\\nFigure 4: Broad taxonomy and count of attack techniques we use for our red teaming exercises\\n\\nAfter each round of red teaming, we gathered feedback from the team regarding failure patterns which guided the next stage of the model development.\\n\\n# 5.4.2 External Red Teaming\\n\\nIn accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Artificial Intelligence, we partner with a variety of third parties to conduct red teaming against our AI models. These initiatives are in addition to our extensive in-house efforts, which includes all aspects of Cybersecurity red teaming. Just like with our internal red teaming efforts, we iterated during the model development based on feedback from these institutions to improve the RAI adherence of our models. We leverage red-teaming firms including ActiveFence to conduct testing in areas such as hate speech, political misinformation, extremism and other RAI dimensions. We also work with specialized third parties to red team our models for Chemical, Biological, Radiological and Nuclear (CBRN) capabilities. Our work with Deloitte Consulting, tests our AI models’ capabilities in Biological risks and harms. Our work with Nemesys Insights LLC tests our AI models’ capabilities in the Radiological and Nuclear domains. We also work with the Gomes Group at Carnegie Mellon University to test our models’ capabilities in Chemistry and chemical compounds. Each of these partners was carefully selected based on their industry leadership, previous/parallel red teaming work with other AI model developers, and their contributions to evolving government and industry standards around CBRN and overall AI safety. We provide a brief summary of expertise of each of these vendors and their testing methodology below.\\n\\n# ActiveFence\\n\\nActiveFence is a team of over 150 subject matter experts providing AI Safety and Content Moderation solutions. The team produced over 9,700 adversarial prompts, distributed over 20 categories, including content-targeted red teaming (evaluating the model’s ability to generate harmful or inappropriate content), and security-targeted red teaming (assessing the model’s resilience against malicious attempts to manipulate its behavior or extract sensitive information).\\n\\n# Deloitte\\n\\nThe evaluation team at Deloitte Consulting LLP (formerly known as Gryphon Scientific) has unique experience at the intersection of artificial intelligence and biology. The primary thrust of this effort involved evaluating the model against a panel of 30 questions developed to test an LLM’s scientific knowledge and reasoning capabilities that could facilitate the development or use of biological weapons. The model’s responses to these questions were evaluated for their scientific accuracy and utility to someone seeking to do harm with biology. After completing the initial',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['Cipher-based:', '3'],\n",
              "      ['Text based obfuscation:', '5'],\n",
              "      ['Obfuscation:', '8'],\n",
              "      ['Code injection:', '9'],\n",
              "      ['Recursive injection:', '1'],\n",
              "      ['Virtualization:', '9'],\n",
              "      ['Defined dictionary attack:', '1'],\n",
              "      ['Payload splitting:', '6'],\n",
              "      ['Token smuggling:', '6'],\n",
              "      ['Prompt injections (direct and indirect):', '40'],\n",
              "      ['one-shot:', '12'],\n",
              "      ['many-shot:', '12'],\n",
              "      ['Jailbreak:', '24'],\n",
              "      ['Multilingual prompting:', '14'],\n",
              "      ['Translation requests:', '8'],\n",
              "      ['Multiple languages:', '33'],\n",
              "      ['Mixed language requests:', '11'],\n",
              "      ['In-Context Learning:', '14'],\n",
              "      ['Context-based:', '32'],\n",
              "      ['Syntactic separators:', '8'],\n",
              "      ['Context switching:', '18'],\n",
              "      ['Semantic separators:', '10'],\n",
              "      ['Instruction repetition:', '19'],\n",
              "      ['Attacks:', '307'],\n",
              "      ['Completion Compliance:', '13'],\n",
              "      ['Affirmative Suffixes:', '13'],\n",
              "      ['Persuasion:', '119'],\n",
              "      ['One-sided arguments:', '12'],\n",
              "      ['Refusal suppression:', '10'],\n",
              "      ['Chain of utterances:', '10'],\n",
              "      ['Socratic Questioning Technique:', '15'],\n",
              "      ['Personification:', '10'],\n",
              "      ['Obfuscations:', '59'],\n",
              "      ['Task constraints:', '17'],\n",
              "      ['Veiled Expressions:', '16'],\n",
              "      ['Output constraints:', '20'],\n",
              "      ['Euphemisms via Ciphers:', '11'],\n",
              "      ['Decoding Manipulation:', '11'],\n",
              "      ['Macaronic prompt:', '1']],\n",
              "     'md': '|Cipher-based:|3|\\n|Text based obfuscation:|5|\\n|Obfuscation:|8|\\n|Code injection:|9|\\n|Recursive injection:|1|\\n|Virtualization:|9|\\n|Defined dictionary attack:|1|\\n|Payload splitting:|6|\\n|Token smuggling:|6|\\n|Prompt injections (direct and indirect):|40|\\n|one-shot:|12|\\n|many-shot:|12|\\n|Jailbreak:|24|\\n|Multilingual prompting:|14|\\n|Translation requests:|8|\\n|Multiple languages:|33|\\n|Mixed language requests:|11|\\n|In-Context Learning:|14|\\n|Context-based:|32|\\n|Syntactic separators:|8|\\n|Context switching:|18|\\n|Semantic separators:|10|\\n|Instruction repetition:|19|\\n|Attacks:|307|\\n|Completion Compliance:|13|\\n|Affirmative Suffixes:|13|\\n|Persuasion:|119|\\n|One-sided arguments:|12|\\n|Refusal suppression:|10|\\n|Chain of utterances:|10|\\n|Socratic Questioning Technique:|15|\\n|Personification:|10|\\n|Obfuscations:|59|\\n|Task constraints:|17|\\n|Veiled Expressions:|16|\\n|Output constraints:|20|\\n|Euphemisms via Ciphers:|11|\\n|Decoding Manipulation:|11|\\n|Macaronic prompt:|1|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"Cipher-based:\",\"3\"\\n\"Text based obfuscation:\",\"5\"\\n\"Obfuscation:\",\"8\"\\n\"Code injection:\",\"9\"\\n\"Recursive injection:\",\"1\"\\n\"Virtualization:\",\"9\"\\n\"Defined dictionary attack:\",\"1\"\\n\"Payload splitting:\",\"6\"\\n\"Token smuggling:\",\"6\"\\n\"Prompt injections (direct and indirect):\",\"40\"\\n\"one-shot:\",\"12\"\\n\"many-shot:\",\"12\"\\n\"Jailbreak:\",\"24\"\\n\"Multilingual prompting:\",\"14\"\\n\"Translation requests:\",\"8\"\\n\"Multiple languages:\",\"33\"\\n\"Mixed language requests:\",\"11\"\\n\"In-Context Learning:\",\"14\"\\n\"Context-based:\",\"32\"\\n\"Syntactic separators:\",\"8\"\\n\"Context switching:\",\"18\"\\n\"Semantic separators:\",\"10\"\\n\"Instruction repetition:\",\"19\"\\n\"Attacks:\",\"307\"\\n\"Completion Compliance:\",\"13\"\\n\"Affirmative Suffixes:\",\"13\"\\n\"Persuasion:\",\"119\"\\n\"One-sided arguments:\",\"12\"\\n\"Refusal suppression:\",\"10\"\\n\"Chain of utterances:\",\"10\"\\n\"Socratic Questioning Technique:\",\"15\"\\n\"Personification:\",\"10\"\\n\"Obfuscations:\",\"59\"\\n\"Task constraints:\",\"17\"\\n\"Veiled Expressions:\",\"16\"\\n\"Output constraints:\",\"20\"\\n\"Euphemisms via Ciphers:\",\"11\"\\n\"Decoding Manipulation:\",\"11\"\\n\"Macaronic prompt:\",\"1\"',\n",
              "     'bBox': {'x': 71, 'y': 73.32, 'w': 469, 'h': 676.72}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Figure 4: Broad taxonomy and count of attack techniques we use for our red teaming exercises\\n\\nAfter each round of red teaming, we gathered feedback from the team regarding failure patterns which guided the next stage of the model development.',\n",
              "     'md': 'Figure 4: Broad taxonomy and count of attack techniques we use for our red teaming exercises\\n\\nAfter each round of red teaming, we gathered feedback from the team regarding failure patterns which guided the next stage of the model development.',\n",
              "     'bBox': {'x': 71, 'y': 349.04, 'w': 468, 'h': 59}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '5.4.2 External Red Teaming',\n",
              "     'md': '# 5.4.2 External Red Teaming',\n",
              "     'bBox': {'x': 72, 'y': 428.04, 'w': 125, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'In accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Artificial Intelligence, we partner with a variety of third parties to conduct red teaming against our AI models. These initiatives are in addition to our extensive in-house efforts, which includes all aspects of Cybersecurity red teaming. Just like with our internal red teaming efforts, we iterated during the model development based on feedback from these institutions to improve the RAI adherence of our models. We leverage red-teaming firms including ActiveFence to conduct testing in areas such as hate speech, political misinformation, extremism and other RAI dimensions. We also work with specialized third parties to red team our models for Chemical, Biological, Radiological and Nuclear (CBRN) capabilities. Our work with Deloitte Consulting, tests our AI models’ capabilities in Biological risks and harms. Our work with Nemesys Insights LLC tests our AI models’ capabilities in the Radiological and Nuclear domains. We also work with the Gomes Group at Carnegie Mellon University to test our models’ capabilities in Chemistry and chemical compounds. Each of these partners was carefully selected based on their industry leadership, previous/parallel red teaming work with other AI model developers, and their contributions to evolving government and industry standards around CBRN and overall AI safety. We provide a brief summary of expertise of each of these vendors and their testing methodology below.',\n",
              "     'md': 'In accordance with our commitment to the US White House on ensuring Safe, Secure, and Trustworthy Artificial Intelligence, we partner with a variety of third parties to conduct red teaming against our AI models. These initiatives are in addition to our extensive in-house efforts, which includes all aspects of Cybersecurity red teaming. Just like with our internal red teaming efforts, we iterated during the model development based on feedback from these institutions to improve the RAI adherence of our models. We leverage red-teaming firms including ActiveFence to conduct testing in areas such as hate speech, political misinformation, extremism and other RAI dimensions. We also work with specialized third parties to red team our models for Chemical, Biological, Radiological and Nuclear (CBRN) capabilities. Our work with Deloitte Consulting, tests our AI models’ capabilities in Biological risks and harms. Our work with Nemesys Insights LLC tests our AI models’ capabilities in the Radiological and Nuclear domains. We also work with the Gomes Group at Carnegie Mellon University to test our models’ capabilities in Chemistry and chemical compounds. Each of these partners was carefully selected based on their industry leadership, previous/parallel red teaming work with other AI model developers, and their contributions to evolving government and industry standards around CBRN and overall AI safety. We provide a brief summary of expertise of each of these vendors and their testing methodology below.',\n",
              "     'bBox': {'x': 71, 'y': 449.04, 'w': 471, 'h': 228}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'ActiveFence',\n",
              "     'md': '# ActiveFence',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'ActiveFence is a team of over 150 subject matter experts providing AI Safety and Content Moderation solutions. The team produced over 9,700 adversarial prompts, distributed over 20 categories, including content-targeted red teaming (evaluating the model’s ability to generate harmful or inappropriate content), and security-targeted red teaming (assessing the model’s resilience against malicious attempts to manipulate its behavior or extract sensitive information).',\n",
              "     'md': 'ActiveFence is a team of over 150 subject matter experts providing AI Safety and Content Moderation solutions. The team produced over 9,700 adversarial prompts, distributed over 20 categories, including content-targeted red teaming (evaluating the model’s ability to generate harmful or inappropriate content), and security-targeted red teaming (assessing the model’s resilience against malicious attempts to manipulate its behavior or extract sensitive information).',\n",
              "     'bBox': {'x': 72, 'y': 618.04, 'w': 468, 'h': 132}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Deloitte',\n",
              "     'md': '# Deloitte',\n",
              "     'bBox': {'x': 72, 'y': 667.04, 'w': 33, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'The evaluation team at Deloitte Consulting LLP (formerly known as Gryphon Scientific) has unique experience at the intersection of artificial intelligence and biology. The primary thrust of this effort involved evaluating the model against a panel of 30 questions developed to test an LLM’s scientific knowledge and reasoning capabilities that could facilitate the development or use of biological weapons. The model’s responses to these questions were evaluated for their scientific accuracy and utility to someone seeking to do harm with biology. After completing the initial',\n",
              "     'md': 'The evaluation team at Deloitte Consulting LLP (formerly known as Gryphon Scientific) has unique experience at the intersection of artificial intelligence and biology. The primary thrust of this effort involved evaluating the model against a panel of 30 questions developed to test an LLM’s scientific knowledge and reasoning capabilities that could facilitate the development or use of biological weapons. The model’s responses to these questions were evaluated for their scientific accuracy and utility to someone seeking to do harm with biology. After completing the initial',\n",
              "     'bBox': {'x': 72, 'y': 667.04, 'w': 468, 'h': 53}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 21,\n",
              "   'text': '                                      The Amazon Nova Family of Models\\nevaluations, the Deloitte team probed more deeply into the questions the LLM originally replied with potentially\\nconcerning information.\\nGomes Group: The Gomes Group at Carnegie Mellon University is at the forefront of integrating advanced artificial\\nintelligence into chemical research. Their evaluation framework consisted of both automated and non-automated\\nassessments. Two non-automated evaluations explored aggregation attack vulnerabilities through purchasing and remote\\nchemical mixing scenarios. The automated evaluations utilized two distinct datasets: one containing 39 hazardous\\nchemicals (including DEA Schedule I, II, and chemical warfare agents) and another with 362 common chemicals for\\nNFPA diamond classifications. Three primary automated evaluations were conducted using the hazardous chemicals\\ndataset. The NFPA diamond evaluation comprised 1,810 prompts, testing both single-turn and multi-turn approaches\\nwith consistent accuracy across both methods.\\nNemesys: Nemesys Insights LLC run uplift studies, red teaming exercises, and risk assessments for a variety of\\ntechnology companies and third-party research entities to assess national security related risks of large language models\\nand other generative AI tools. For their testing, they started with human red teaming exercises focused on non-state\\nacquisition or use of illicit radiological/nuclear (RN) materials, followed by prompt-response evaluation and uplift\\nstudies. The exercises comprised two different scenarios (a. violent non-state actor acquisition and use of Cobalt-60; b.\\nnon-state actor acquisition and international transport of HEU [highly enriched uranium]), and utilized 8 subject matter\\nexperts with operational and technological knowledge in a 2-team x 2-scenario design to construct and refine threat\\nplans across a 6-hour planning cycle.\\n5.4.3  Automated Red Teaming\\nFinally, to augment human based red teaming, we built an automated red teaming mechanism by adapting our (Feedback\\nLoop In-context Red Teaming) FLIRT [52] framework. This approach helped us scale red teaming and repeat red\\nteaming efficiently. FLIRT uses a list of seed prompts that have been identified by human evaluators as potentially\\nviolating one or more of our RAI dimensions. For every dimension, a subset of seeds is used to generate additional\\nprompts with a dedicated language model, called red-LM, through in-context-learning (ICL) [18] and a carefully\\ncrafted set of instructions. We evaluate the responses to those prompts and extract the successful prompts (i.e., the ones\\ntriggering a prohibited response) for the next round of generation. The above steps are repeated for a chosen number of\\niterations across all RAI categories. We use our automated red teaming mechanism to evaluate both RAI adherence\\nrobustness and false refusals. We use the mechanism to generate adversarial tests across multi-turn interactions, multiple\\nlanguages, and multiple input/output modalities to uncover and correct robustness issues in our models due to potential\\nadversarial content in such interactions and inputs.\\n6   Training Infrastructure\\nThe Nova family of models were trained on Amazon’s custom Trainium1 (TRN1) chips,10 NVidia A100 (P4d instances),\\nand H100 (P5 instances) accelerators. Working with AWS SageMaker, we stood up NVidia GPU and TRN1 clusters\\nand ran parallel trainings to ensure model performance parity, while optimizing training throughput on the different\\nstacks. All clusters utilize petabit-scale non-blocking EFA network fabric which is less prone to packet loss than other\\nnetwork transport protocols11 and provides the highest network bandwidth with H100 accelerators compared to any\\nother instance type available on AWS EC212. We conducted distributed training on AWS SageMaker-managed Elastic\\nKubernetes Service (EKS) clusters, and utilized AWS File System X (FSx) and Simple Storage Solution (S3) for\\ndata and checkpoint IO. While FSx offers performant and convenient storage for large scale training jobs, S3 allowed\\ncost-efficient scaling to large multimodal datasets and model checkpoints.\\nGoodput achieved weekly average values of up to 97% in pretraining runs through optimizations targeting lower job\\nfailure rate, minimizing checkpoint overhead, and overall reduction in the Mean Time to Restart (MTTR). This time\\nis inclusive of time from the last successful checkpoint before training interruption, time taken to restart components\\nof the system and resume training at steady state from checkpoint. Techniques such as fully distributed optimizer\\nstate and weight sharding and the elimination of all blocking overhead associated with checkpoint persistence resulted\\nin a reduction of checkpointing overhead to ~1 sec on H100 clusters, and ~0.1 sec on TRN1 clusters. We exceeded\\nour MTTR target of 9 minutes and achieved an average of 6.5 minutes on our TRN1 clusters by optimizing the\\n                 10https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin\\ng-are-now-available/\\n                11https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s\\ncalable-hpc\\n                12https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor\\ne-gpus-for-accelerating-generative-ai-and-hpc-applications/\\n                                                       21',\n",
              "   'md': \"The Amazon Nova Family of Models\\n\\nevaluations, the Deloitte team probed more deeply into the questions the LLM originally replied with potentially concerning information.\\n\\nGomes Group: The Gomes Group at Carnegie Mellon University is at the forefront of integrating advanced artificial intelligence into chemical research. Their evaluation framework consisted of both automated and non-automated assessments. Two non-automated evaluations explored aggregation attack vulnerabilities through purchasing and remote chemical mixing scenarios. The automated evaluations utilized two distinct datasets: one containing 39 hazardous chemicals (including DEA Schedule I, II, and chemical warfare agents) and another with 362 common chemicals for NFPA diamond classifications. Three primary automated evaluations were conducted using the hazardous chemicals dataset. The NFPA diamond evaluation comprised 1,810 prompts, testing both single-turn and multi-turn approaches with consistent accuracy across both methods.\\n\\nNemesys: Nemesys Insights LLC run uplift studies, red teaming exercises, and risk assessments for a variety of technology companies and third-party research entities to assess national security related risks of large language models and other generative AI tools. For their testing, they started with human red teaming exercises focused on non-state acquisition or use of illicit radiological/nuclear (RN) materials, followed by prompt-response evaluation and uplift studies. The exercises comprised two different scenarios (a. violent non-state actor acquisition and use of Cobalt-60; b. non-state actor acquisition and international transport of HEU [highly enriched uranium]), and utilized 8 subject matter experts with operational and technological knowledge in a 2-team x 2-scenario design to construct and refine threat plans across a 6-hour planning cycle.\\n\\n### 5.4.3 Automated Red Teaming\\n\\nFinally, to augment human based red teaming, we built an automated red teaming mechanism by adapting our (Feedback Loop In-context Red Teaming) FLIRT [52] framework. This approach helped us scale red teaming and repeat red teaming efficiently. FLIRT uses a list of seed prompts that have been identified by human evaluators as potentially violating one or more of our RAI dimensions. For every dimension, a subset of seeds is used to generate additional prompts with a dedicated language model, called red-LM, through in-context-learning (ICL) [18] and a carefully crafted set of instructions. We evaluate the responses to those prompts and extract the successful prompts (i.e., the ones triggering a prohibited response) for the next round of generation. The above steps are repeated for a chosen number of iterations across all RAI categories. We use our automated red teaming mechanism to evaluate both RAI adherence robustness and false refusals. We use the mechanism to generate adversarial tests across multi-turn interactions, multiple languages, and multiple input/output modalities to uncover and correct robustness issues in our models due to potential adversarial content in such interactions and inputs.\\n\\n## 6 Training Infrastructure\\n\\nThe Nova family of models were trained on Amazon's custom Trainium1 (TRN1) chips, NVidia A100 (P4d instances), and H100 (P5 instances) accelerators. Working with AWS SageMaker, we stood up NVidia GPU and TRN1 clusters and ran parallel trainings to ensure model performance parity, while optimizing training throughput on the different stacks. All clusters utilize petabit-scale non-blocking EFA network fabric which is less prone to packet loss than other network transport protocols and provides the highest network bandwidth with H100 accelerators compared to any other instance type available on AWS EC2. We conducted distributed training on AWS SageMaker-managed Elastic Kubernetes Service (EKS) clusters, and utilized AWS File System X (FSx) and Simple Storage Solution (S3) for data and checkpoint IO. While FSx offers performant and convenient storage for large scale training jobs, S3 allowed cost-efficient scaling to large multimodal datasets and model checkpoints.\\n\\nGoodput achieved weekly average values of up to 97% in pretraining runs through optimizations targeting lower job failure rate, minimizing checkpoint overhead, and overall reduction in the Mean Time to Restart (MTTR). This time is inclusive of time from the last successful checkpoint before training interruption, time taken to restart components of the system and resume training at steady state from checkpoint. Techniques such as fully distributed optimizer state and weight sharding and the elimination of all blocking overhead associated with checkpoint persistence resulted in a reduction of checkpointing overhead to ~1 sec on H100 clusters, and ~0.1 sec on TRN1 clusters. We exceeded our MTTR target of 9 minutes and achieved an average of 6.5 minutes on our TRN1 clusters by optimizing the\\n\\n[Footnotes]\\n10. https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin g-are-now-available/\\n11. https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s calable-hpc\\n12. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor e-gpus-for-accelerating-generative-ai-and-hpc-applications/\",\n",
              "   'images': [{'name': 'page_21.jpg',\n",
              "     'height': 792,\n",
              "     'width': 612,\n",
              "     'x': 0,\n",
              "     'y': 0,\n",
              "     'original_width': 1236,\n",
              "     'original_height': 1600,\n",
              "     'type': 'full_page_screenshot'}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'text',\n",
              "     'value': 'The Amazon Nova Family of Models\\n\\nevaluations, the Deloitte team probed more deeply into the questions the LLM originally replied with potentially concerning information.\\n\\nGomes Group: The Gomes Group at Carnegie Mellon University is at the forefront of integrating advanced artificial intelligence into chemical research. Their evaluation framework consisted of both automated and non-automated assessments. Two non-automated evaluations explored aggregation attack vulnerabilities through purchasing and remote chemical mixing scenarios. The automated evaluations utilized two distinct datasets: one containing 39 hazardous chemicals (including DEA Schedule I, II, and chemical warfare agents) and another with 362 common chemicals for NFPA diamond classifications. Three primary automated evaluations were conducted using the hazardous chemicals dataset. The NFPA diamond evaluation comprised 1,810 prompts, testing both single-turn and multi-turn approaches with consistent accuracy across both methods.\\n\\nNemesys: Nemesys Insights LLC run uplift studies, red teaming exercises, and risk assessments for a variety of technology companies and third-party research entities to assess national security related risks of large language models and other generative AI tools. For their testing, they started with human red teaming exercises focused on non-state acquisition or use of illicit radiological/nuclear (RN) materials, followed by prompt-response evaluation and uplift studies. The exercises comprised two different scenarios (a. violent non-state actor acquisition and use of Cobalt-60; b. non-state actor acquisition and international transport of HEU [highly enriched uranium]), and utilized 8 subject matter experts with operational and technological knowledge in a 2-team x 2-scenario design to construct and refine threat plans across a 6-hour planning cycle.',\n",
              "     'md': 'The Amazon Nova Family of Models\\n\\nevaluations, the Deloitte team probed more deeply into the questions the LLM originally replied with potentially concerning information.\\n\\nGomes Group: The Gomes Group at Carnegie Mellon University is at the forefront of integrating advanced artificial intelligence into chemical research. Their evaluation framework consisted of both automated and non-automated assessments. Two non-automated evaluations explored aggregation attack vulnerabilities through purchasing and remote chemical mixing scenarios. The automated evaluations utilized two distinct datasets: one containing 39 hazardous chemicals (including DEA Schedule I, II, and chemical warfare agents) and another with 362 common chemicals for NFPA diamond classifications. Three primary automated evaluations were conducted using the hazardous chemicals dataset. The NFPA diamond evaluation comprised 1,810 prompts, testing both single-turn and multi-turn approaches with consistent accuracy across both methods.\\n\\nNemesys: Nemesys Insights LLC run uplift studies, red teaming exercises, and risk assessments for a variety of technology companies and third-party research entities to assess national security related risks of large language models and other generative AI tools. For their testing, they started with human red teaming exercises focused on non-state acquisition or use of illicit radiological/nuclear (RN) materials, followed by prompt-response evaluation and uplift studies. The exercises comprised two different scenarios (a. violent non-state actor acquisition and use of Cobalt-60; b. non-state actor acquisition and international transport of HEU [highly enriched uranium]), and utilized 8 subject matter experts with operational and technological knowledge in a 2-team x 2-scenario design to construct and refine threat plans across a 6-hour planning cycle.',\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 424.01}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 3,\n",
              "     'value': '5.4.3 Automated Red Teaming',\n",
              "     'md': '### 5.4.3 Automated Red Teaming',\n",
              "     'bBox': {'x': 72, 'y': 292.04, 'w': 135, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Finally, to augment human based red teaming, we built an automated red teaming mechanism by adapting our (Feedback Loop In-context Red Teaming) FLIRT [52] framework. This approach helped us scale red teaming and repeat red teaming efficiently. FLIRT uses a list of seed prompts that have been identified by human evaluators as potentially violating one or more of our RAI dimensions. For every dimension, a subset of seeds is used to generate additional prompts with a dedicated language model, called red-LM, through in-context-learning (ICL) [18] and a carefully crafted set of instructions. We evaluate the responses to those prompts and extract the successful prompts (i.e., the ones triggering a prohibited response) for the next round of generation. The above steps are repeated for a chosen number of iterations across all RAI categories. We use our automated red teaming mechanism to evaluate both RAI adherence robustness and false refusals. We use the mechanism to generate adversarial tests across multi-turn interactions, multiple languages, and multiple input/output modalities to uncover and correct robustness issues in our models due to potential adversarial content in such interactions and inputs.',\n",
              "     'md': 'Finally, to augment human based red teaming, we built an automated red teaming mechanism by adapting our (Feedback Loop In-context Red Teaming) FLIRT [52] framework. This approach helped us scale red teaming and repeat red teaming efficiently. FLIRT uses a list of seed prompts that have been identified by human evaluators as potentially violating one or more of our RAI dimensions. For every dimension, a subset of seeds is used to generate additional prompts with a dedicated language model, called red-LM, through in-context-learning (ICL) [18] and a carefully crafted set of instructions. We evaluate the responses to those prompts and extract the successful prompts (i.e., the ones triggering a prohibited response) for the next round of generation. The above steps are repeated for a chosen number of iterations across all RAI categories. We use our automated red teaming mechanism to evaluate both RAI adherence robustness and false refusals. We use the mechanism to generate adversarial tests across multi-turn interactions, multiple languages, and multiple input/output modalities to uncover and correct robustness issues in our models due to potential adversarial content in such interactions and inputs.',\n",
              "     'bBox': {'x': 71, 'y': 292.04, 'w': 469, 'h': 138}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 2,\n",
              "     'value': '6 Training Infrastructure',\n",
              "     'md': '## 6 Training Infrastructure',\n",
              "     'bBox': {'x': 72, 'y': 446.04, 'w': 138, 'h': 12}},\n",
              "    {'type': 'text',\n",
              "     'value': \"The Nova family of models were trained on Amazon's custom Trainium1 (TRN1) chips, NVidia A100 (P4d instances), and H100 (P5 instances) accelerators. Working with AWS SageMaker, we stood up NVidia GPU and TRN1 clusters and ran parallel trainings to ensure model performance parity, while optimizing training throughput on the different stacks. All clusters utilize petabit-scale non-blocking EFA network fabric which is less prone to packet loss than other network transport protocols and provides the highest network bandwidth with H100 accelerators compared to any other instance type available on AWS EC2. We conducted distributed training on AWS SageMaker-managed Elastic Kubernetes Service (EKS) clusters, and utilized AWS File System X (FSx) and Simple Storage Solution (S3) for data and checkpoint IO. While FSx offers performant and convenient storage for large scale training jobs, S3 allowed cost-efficient scaling to large multimodal datasets and model checkpoints.\\n\\nGoodput achieved weekly average values of up to 97% in pretraining runs through optimizations targeting lower job failure rate, minimizing checkpoint overhead, and overall reduction in the Mean Time to Restart (MTTR). This time is inclusive of time from the last successful checkpoint before training interruption, time taken to restart components of the system and resume training at steady state from checkpoint. Techniques such as fully distributed optimizer state and weight sharding and the elimination of all blocking overhead associated with checkpoint persistence resulted in a reduction of checkpointing overhead to ~1 sec on H100 clusters, and ~0.1 sec on TRN1 clusters. We exceeded our MTTR target of 9 minutes and achieved an average of 6.5 minutes on our TRN1 clusters by optimizing the\\n\\n[Footnotes]\\n10. https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin g-are-now-available/\\n11. https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s calable-hpc\\n12. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor e-gpus-for-accelerating-generative-ai-and-hpc-applications/\",\n",
              "     'md': \"The Nova family of models were trained on Amazon's custom Trainium1 (TRN1) chips, NVidia A100 (P4d instances), and H100 (P5 instances) accelerators. Working with AWS SageMaker, we stood up NVidia GPU and TRN1 clusters and ran parallel trainings to ensure model performance parity, while optimizing training throughput on the different stacks. All clusters utilize petabit-scale non-blocking EFA network fabric which is less prone to packet loss than other network transport protocols and provides the highest network bandwidth with H100 accelerators compared to any other instance type available on AWS EC2. We conducted distributed training on AWS SageMaker-managed Elastic Kubernetes Service (EKS) clusters, and utilized AWS File System X (FSx) and Simple Storage Solution (S3) for data and checkpoint IO. While FSx offers performant and convenient storage for large scale training jobs, S3 allowed cost-efficient scaling to large multimodal datasets and model checkpoints.\\n\\nGoodput achieved weekly average values of up to 97% in pretraining runs through optimizations targeting lower job failure rate, minimizing checkpoint overhead, and overall reduction in the Mean Time to Restart (MTTR). This time is inclusive of time from the last successful checkpoint before training interruption, time taken to restart components of the system and resume training at steady state from checkpoint. Techniques such as fully distributed optimizer state and weight sharding and the elimination of all blocking overhead associated with checkpoint persistence resulted in a reduction of checkpointing overhead to ~1 sec on H100 clusters, and ~0.1 sec on TRN1 clusters. We exceeded our MTTR target of 9 minutes and achieved an average of 6.5 minutes on our TRN1 clusters by optimizing the\\n\\n[Footnotes]\\n10. https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin g-are-now-available/\\n11. https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s calable-hpc\\n12. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor e-gpus-for-accelerating-generative-ai-and-hpc-applications/\",\n",
              "     'bBox': {'x': 72, 'y': 446.04, 'w': 468, 'h': 273.99}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-training-are-now-available/',\n",
              "     'text': 'https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-trainin g-are-now-available/'},\n",
              "    {'url': 'https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-training-are-now-available/',\n",
              "     'text': 'g-are-now-available/'},\n",
              "    {'url': 'https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-scalable-hpc',\n",
              "     'text': 'https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-s calable-hpc'},\n",
              "    {'url': 'https://www.amazon.science/publications/a-cloud-optimized-transport-protocol-for-elastic-and-scalable-hpc',\n",
              "     'text': 'calable-hpc'},\n",
              "    {'url': 'https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/',\n",
              "     'text': 'https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-cor e-gpus-for-accelerating-generative-ai-and-hpc-applications/'},\n",
              "    {'url': 'https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/',\n",
              "     'text': 'e-gpus-for-accelerating-generative-ai-and-hpc-applications/'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': True,\n",
              "   'parsingMode': 'premium',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 22,\n",
              "   'text': '                                       The Amazon Nova Family of Models\\nnode communication initialization in the training startup process and reduced time to load checkpoints through an\\nasynchronous observer process. This process maps each latest checkpoint file to its corresponding node in the cluster.\\nWhen resuming from the checkpoint, each node only loads the checkpoint files for its corresponding rank, reducing\\nthe time taken to discover the latest checkpoint from 3 minutes to 5 seconds. We also cache and reuse data indices to\\noptimize training data loading initialization time. These improvements reduced data loading initialization to 205ms per\\nrestart.\\nTo increase training efficiency we developed a new activation checkpointing scheme called Super-Selective Activation\\nCheckpointing (SSC). SSC minimizes activation re-computation in memory-constrained environments, reducing mem-\\nory consumption by ~50% while adding ~2% re-computation overhead compared to NVidia’s Selective Checkpointing.\\nWe also found optimizations in default gradient reduction behavior and the default PyTorch memory allocator behavior.\\nThe default gradient reduction behavior leads to suboptimal communication overlap and we found the synchronous\\nnature of the default PyTorch allocation led to stragglers in collectives resulting in multiple stalled workers. We adjusted\\nthe gradient reduction order and frequency, allowing us to overlap the majority of data parallelism communication.\\n                                                        22',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\nNode communication initialization in the training startup process and reduced time to load checkpoints through an asynchronous observer process. This process maps each latest checkpoint file to its corresponding node in the cluster. When resuming from the checkpoint, each node only loads the checkpoint files for its corresponding rank, reducing the time taken to discover the latest checkpoint from 3 minutes to 5 seconds. We also cache and reuse data indices to optimize training data loading initialization time. These improvements reduced data loading initialization to 205ms per restart.\\n\\nTo increase training efficiency we developed a new activation checkpointing scheme called Super-Selective Activation Checkpointing (SSC). SSC minimizes activation re-computation in memory-constrained environments, reducing memory consumption by ~50% while adding ~2% re-computation overhead compared to NVidia’s Selective Checkpointing. We also found optimizations in default gradient reduction behavior and the default PyTorch memory allocator behavior. The default gradient reduction behavior leads to suboptimal communication overlap and we found the synchronous nature of the default PyTorch allocation led to stragglers in collectives resulting in multiple stalled workers. We adjusted the gradient reduction order and frequency, allowing us to overlap the majority of data parallelism communication.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Node communication initialization in the training startup process and reduced time to load checkpoints through an asynchronous observer process. This process maps each latest checkpoint file to its corresponding node in the cluster. When resuming from the checkpoint, each node only loads the checkpoint files for its corresponding rank, reducing the time taken to discover the latest checkpoint from 3 minutes to 5 seconds. We also cache and reuse data indices to optimize training data loading initialization time. These improvements reduced data loading initialization to 205ms per restart.\\n\\nTo increase training efficiency we developed a new activation checkpointing scheme called Super-Selective Activation Checkpointing (SSC). SSC minimizes activation re-computation in memory-constrained environments, reducing memory consumption by ~50% while adding ~2% re-computation overhead compared to NVidia’s Selective Checkpointing. We also found optimizations in default gradient reduction behavior and the default PyTorch memory allocator behavior. The default gradient reduction behavior leads to suboptimal communication overlap and we found the synchronous nature of the default PyTorch allocation led to stragglers in collectives resulting in multiple stalled workers. We adjusted the gradient reduction order and frequency, allowing us to overlap the majority of data parallelism communication.',\n",
              "     'md': 'Node communication initialization in the training startup process and reduced time to load checkpoints through an asynchronous observer process. This process maps each latest checkpoint file to its corresponding node in the cluster. When resuming from the checkpoint, each node only loads the checkpoint files for its corresponding rank, reducing the time taken to discover the latest checkpoint from 3 minutes to 5 seconds. We also cache and reuse data indices to optimize training data loading initialization time. These improvements reduced data loading initialization to 205ms per restart.\\n\\nTo increase training efficiency we developed a new activation checkpointing scheme called Super-Selective Activation Checkpointing (SSC). SSC minimizes activation re-computation in memory-constrained environments, reducing memory consumption by ~50% while adding ~2% re-computation overhead compared to NVidia’s Selective Checkpointing. We also found optimizations in default gradient reduction behavior and the default PyTorch memory allocator behavior. The default gradient reduction behavior leads to suboptimal communication overlap and we found the synchronous nature of the default PyTorch allocation led to stragglers in collectives resulting in multiple stalled workers. We adjusted the gradient reduction order and frequency, allowing us to overlap the majority of data parallelism communication.',\n",
              "     'bBox': {'x': 71, 'y': 72.04, 'w': 471, 'h': 147}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 23,\n",
              "   'text': '                                         The Amazon Nova Family of Models\\n     References\\n      [1] Efficient Batch Computing – AWS Batch - AWS, 2024. URL https://aws.amazon.com/batch/.\\n      [2] Big Data Platform - Amazon EMR - AWS, 2024. URL https://aws.amazon.com/emr/.\\n      [3]  AgentStudio. Gemini flash. https://computer-agents.github.io/agent-studio/, 2024. Accessed:\\n           2024-11-29.\\n      [4]  P. Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. D. Monicault, S. Garg,\\n           T. Gervet, S. Ghosh, A. Héliou, P. Jacob, A. Q. Jiang, K. Khandelwal, T. Lacroix, G. Lample, D. L. Casas,\\n           T. Lavril, T. L. Scao, A. Lo, W. Marshall, L. Martin, A. Mensch, P. Muddireddy, V. Nemychnikova, M. Pellat,\\n           P. V. Platen, N. Raghuraman, B. Rozière, A. Sablayrolles, L. Saulnier, R. Sauvestre, W. Shang, R. Soletskyi,\\n           L. Stewart, P. Stock, J. Studnia, S. Subramanian, S. Vaze, T. Wang, and S. Yang. Pixtral 12B, 2024. URL\\n           https://arxiv.org/abs/2410.07073.\\n      [5]  Amazon. Amazon joins Partnership on AI. https://www.aboutamazon.com/news/amazon-ai/amazon-j\\n           oins-partnership-on-ai, 2016. Accessed: 2024-11-20.\\n      [6]  Amazon. Our commitment to the responsible use of AI. https://www.aboutamazon.com/news/company-n\\n           ews/amazon-responsible-ai, 2023. Accessed: 2024-11-20.\\n      [7]  Amazon. Amazon joins US Artificial Intelligence safety institute to advance responsible AI. https://www.abou\\n           tamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-i\\n           nstitute-to-advance-responsible-ai, 2024. Accessed: 2024-11-20.\\n      [8]  Amazon. Amazon SageMaker Clarify. https://aws.amazon.com/sagemaker/clarify/, 2024. Accessed:\\n           2024-11-20.\\n      [9]  Amazon. Data protection & privacy at AWS. https://aws.amazon.com/compliance/data-protection/,\\n           2024. Accessed: 2024-11-20.\\n     [10] Amazon.   Building AI responsibly at AWS.               https://aws.amazon.com/ai/responsible-ai/, 2024.\\n           Accessed: 2024-11-20.\\n     [11] Anthropic.  The Claude 3 model family: Opus, Sonnet, Haiku.     Technical report, Anthropic, 2023.   URL\\n           https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Cla\\n           ude_3.pdf.\\n     [12] Anthropic. Claude Sonnet. https://www.anthropic.com/claude/sonnet, 2024. Accessed: 2024-11-20.\\n     [13] Anthropic AI. Claude 3.5 Sonnet model card addendum. Technical report, 2024.\\n           [14] Anthropic AI Team. Claude 3.5 Haiku and upgraded Claude 3.5 Sonnet, 2024. URL https://assets.anthr\\n           opic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf.\\n     [15] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.\\n     [16]  J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image\\n           generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\\n     [17] Black Forest Labs. Flux models. 2024. URL https://github.com/black-forest-labs/flux.\\n     [18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\\n           A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n     [19]  M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman,\\n           et al. Evaluating large language models trained on code, 2021.\\n[20] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. N. Moussa, M. I. Beane, T.-H. K. Huang, B. R.\\n           Routledge, and W. Y. Wang. FinQA: A dataset of numerical reasoning over financial data. ArXiv, abs/2109.00122,\\n           2021. URL https://api.semanticscholar.org/CorpusID:235399966.\\n     [21] J. Cho, A. Zala, and M. Bansal. DALL-eval: Probing the reasoning skills and social biases of text-to-image\\n           generation models. In    Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\\n           3043–3054, 2023.\\n                                                          23',\n",
              "   'md': 'The Amazon Nova Family of Models\\n\\n## References\\n\\n[1] Efficient Batch Computing – AWS Batch - AWS, 2024. URL https://aws.amazon.com/batch/.\\n\\n[2] Big Data Platform - Amazon EMR - AWS, 2024. URL https://aws.amazon.com/emr/.\\n\\n[3] AgentStudio. Gemini flash. https://computer-agents.github.io/agent-studio/, 2024. Accessed: 2024-11-29.\\n\\n[4] P. Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. D. Monicault, S. Garg, T. Gervet, S. Ghosh, A. Héliou, P. Jacob, A. Q. Jiang, K. Khandelwal, T. Lacroix, G. Lample, D. L. Casas, T. Lavril, T. L. Scao, A. Lo, W. Marshall, L. Martin, A. Mensch, P. Muddireddy, V. Nemychnikova, M. Pellat, P. V. Platen, N. Raghuraman, B. Rozière, A. Sablayrolles, L. Saulnier, R. Sauvestre, W. Shang, R. Soletskyi, L. Stewart, P. Stock, J. Studnia, S. Subramanian, S. Vaze, T. Wang, and S. Yang. Pixtral 12B, 2024. URL https://arxiv.org/abs/2410.07073.\\n\\n[5] Amazon. Amazon joins Partnership on AI. https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai, 2016. Accessed: 2024-11-20.\\n\\n[6] Amazon. Our commitment to the responsible use of AI. https://www.aboutamazon.com/news/company-news/amazon-responsible-ai, 2023. Accessed: 2024-11-20.\\n\\n[7] Amazon. Amazon joins US Artificial Intelligence safety institute to advance responsible AI. https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai, 2024. Accessed: 2024-11-20.\\n\\n[8] Amazon. Amazon SageMaker Clarify. https://aws.amazon.com/sagemaker/clarify/, 2024. Accessed: 2024-11-20.\\n\\n[9] Amazon. Data protection & privacy at AWS. https://aws.amazon.com/compliance/data-protection/, 2024. Accessed: 2024-11-20.\\n\\n[10] Amazon. Building AI responsibly at AWS. https://aws.amazon.com/ai/responsible-ai/, 2024. Accessed: 2024-11-20.\\n\\n[11] Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Technical report, Anthropic, 2023. URL https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n\\n[12] Anthropic. Claude Sonnet. https://www.anthropic.com/claude/sonnet, 2024. Accessed: 2024-11-20.\\n\\n[13] Anthropic AI. Claude 3.5 Sonnet model card addendum. Technical report, 2024.\\n\\n[14] Anthropic AI Team. Claude 3.5 Haiku and upgraded Claude 3.5 Sonnet, 2024. URL https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf.\\n\\n[15] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.\\n\\n[16] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\\n\\n[17] Black Forest Labs. Flux models. 2024. URL https://github.com/black-forest-labs/flux.\\n\\n[18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\n[19] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code, 2021.\\n\\n[20] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. N. Moussa, M. I. Beane, T.-H. K. Huang, B. R. Routledge, and W. Y. Wang. FinQA: A dataset of numerical reasoning over financial data. ArXiv, abs/2109.00122, 2021. URL https://api.semanticscholar.org/CorpusID:235399966.\\n\\n[21] J. Cho, A. Zala, and M. Bansal. DALL-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3043–3054, 2023.',\n",
              "   'images': [{'name': 'page_23.jpg',\n",
              "     'height': 792,\n",
              "     'width': 612,\n",
              "     'x': 0,\n",
              "     'y': 0,\n",
              "     'original_width': 1236,\n",
              "     'original_height': 1600,\n",
              "     'type': 'full_page_screenshot'}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'text',\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': 'The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 2,\n",
              "     'value': 'References',\n",
              "     'md': '## References',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 56, 'h': 12}},\n",
              "    {'type': 'text',\n",
              "     'value': '[1] Efficient Batch Computing – AWS Batch - AWS, 2024. URL https://aws.amazon.com/batch/.\\n\\n[2] Big Data Platform - Amazon EMR - AWS, 2024. URL https://aws.amazon.com/emr/.\\n\\n[3] AgentStudio. Gemini flash. https://computer-agents.github.io/agent-studio/, 2024. Accessed: 2024-11-29.\\n\\n[4] P. Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. D. Monicault, S. Garg, T. Gervet, S. Ghosh, A. Héliou, P. Jacob, A. Q. Jiang, K. Khandelwal, T. Lacroix, G. Lample, D. L. Casas, T. Lavril, T. L. Scao, A. Lo, W. Marshall, L. Martin, A. Mensch, P. Muddireddy, V. Nemychnikova, M. Pellat, P. V. Platen, N. Raghuraman, B. Rozière, A. Sablayrolles, L. Saulnier, R. Sauvestre, W. Shang, R. Soletskyi, L. Stewart, P. Stock, J. Studnia, S. Subramanian, S. Vaze, T. Wang, and S. Yang. Pixtral 12B, 2024. URL https://arxiv.org/abs/2410.07073.\\n\\n[5] Amazon. Amazon joins Partnership on AI. https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai, 2016. Accessed: 2024-11-20.\\n\\n[6] Amazon. Our commitment to the responsible use of AI. https://www.aboutamazon.com/news/company-news/amazon-responsible-ai, 2023. Accessed: 2024-11-20.\\n\\n[7] Amazon. Amazon joins US Artificial Intelligence safety institute to advance responsible AI. https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai, 2024. Accessed: 2024-11-20.\\n\\n[8] Amazon. Amazon SageMaker Clarify. https://aws.amazon.com/sagemaker/clarify/, 2024. Accessed: 2024-11-20.\\n\\n[9] Amazon. Data protection & privacy at AWS. https://aws.amazon.com/compliance/data-protection/, 2024. Accessed: 2024-11-20.\\n\\n[10] Amazon. Building AI responsibly at AWS. https://aws.amazon.com/ai/responsible-ai/, 2024. Accessed: 2024-11-20.\\n\\n[11] Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Technical report, Anthropic, 2023. URL https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n\\n[12] Anthropic. Claude Sonnet. https://www.anthropic.com/claude/sonnet, 2024. Accessed: 2024-11-20.\\n\\n[13] Anthropic AI. Claude 3.5 Sonnet model card addendum. Technical report, 2024.\\n\\n[14] Anthropic AI Team. Claude 3.5 Haiku and upgraded Claude 3.5 Sonnet, 2024. URL https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf.\\n\\n[15] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.\\n\\n[16] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\\n\\n[17] Black Forest Labs. Flux models. 2024. URL https://github.com/black-forest-labs/flux.\\n\\n[18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\n[19] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code, 2021.\\n\\n[20] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. N. Moussa, M. I. Beane, T.-H. K. Huang, B. R. Routledge, and W. Y. Wang. FinQA: A dataset of numerical reasoning over financial data. ArXiv, abs/2109.00122, 2021. URL https://api.semanticscholar.org/CorpusID:235399966.\\n\\n[21] J. Cho, A. Zala, and M. Bansal. DALL-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3043–3054, 2023.',\n",
              "     'md': '[1] Efficient Batch Computing – AWS Batch - AWS, 2024. URL https://aws.amazon.com/batch/.\\n\\n[2] Big Data Platform - Amazon EMR - AWS, 2024. URL https://aws.amazon.com/emr/.\\n\\n[3] AgentStudio. Gemini flash. https://computer-agents.github.io/agent-studio/, 2024. Accessed: 2024-11-29.\\n\\n[4] P. Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. D. Monicault, S. Garg, T. Gervet, S. Ghosh, A. Héliou, P. Jacob, A. Q. Jiang, K. Khandelwal, T. Lacroix, G. Lample, D. L. Casas, T. Lavril, T. L. Scao, A. Lo, W. Marshall, L. Martin, A. Mensch, P. Muddireddy, V. Nemychnikova, M. Pellat, P. V. Platen, N. Raghuraman, B. Rozière, A. Sablayrolles, L. Saulnier, R. Sauvestre, W. Shang, R. Soletskyi, L. Stewart, P. Stock, J. Studnia, S. Subramanian, S. Vaze, T. Wang, and S. Yang. Pixtral 12B, 2024. URL https://arxiv.org/abs/2410.07073.\\n\\n[5] Amazon. Amazon joins Partnership on AI. https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai, 2016. Accessed: 2024-11-20.\\n\\n[6] Amazon. Our commitment to the responsible use of AI. https://www.aboutamazon.com/news/company-news/amazon-responsible-ai, 2023. Accessed: 2024-11-20.\\n\\n[7] Amazon. Amazon joins US Artificial Intelligence safety institute to advance responsible AI. https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai, 2024. Accessed: 2024-11-20.\\n\\n[8] Amazon. Amazon SageMaker Clarify. https://aws.amazon.com/sagemaker/clarify/, 2024. Accessed: 2024-11-20.\\n\\n[9] Amazon. Data protection & privacy at AWS. https://aws.amazon.com/compliance/data-protection/, 2024. Accessed: 2024-11-20.\\n\\n[10] Amazon. Building AI responsibly at AWS. https://aws.amazon.com/ai/responsible-ai/, 2024. Accessed: 2024-11-20.\\n\\n[11] Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Technical report, Anthropic, 2023. URL https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n\\n[12] Anthropic. Claude Sonnet. https://www.anthropic.com/claude/sonnet, 2024. Accessed: 2024-11-20.\\n\\n[13] Anthropic AI. Claude 3.5 Sonnet model card addendum. Technical report, 2024.\\n\\n[14] Anthropic AI Team. Claude 3.5 Haiku and upgraded Claude 3.5 Sonnet, 2024. URL https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf.\\n\\n[15] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.\\n\\n[16] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\\n\\n[17] Black Forest Labs. Flux models. 2024. URL https://github.com/black-forest-labs/flux.\\n\\n[18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\n[19] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code, 2021.\\n\\n[20] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. N. Moussa, M. I. Beane, T.-H. K. Huang, B. R. Routledge, and W. Y. Wang. FinQA: A dataset of numerical reasoning over financial data. ArXiv, abs/2109.00122, 2021. URL https://api.semanticscholar.org/CorpusID:235399966.\\n\\n[21] J. Cho, A. Zala, and M. Bansal. DALL-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3043–3054, 2023.',\n",
              "     'bBox': {'x': 71, 'y': 90.04, 'w': 470, 'h': 660}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://aws.amazon.com/batch/',\n",
              "     'text': 'https://aws.amazon.com/batch/ .'},\n",
              "    {'url': 'https://aws.amazon.com/emr/',\n",
              "     'text': 'https://aws.amazon.com/emr/ .'},\n",
              "    {'url': 'https://computer-agents.github.io/agent-studio/',\n",
              "     'text': 'https://computer-agents.github.io/agent-studio/ , 2024. Accessed:'},\n",
              "    {'url': 'https://arxiv.org/abs/2410.07073',\n",
              "     'text': 'https://arxiv.org/abs/2410.07073 .'},\n",
              "    {'url': 'https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai',\n",
              "     'text': 'https://www.aboutamazon.com/news/amazon-ai/amazon-j , 2016. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://www.aboutamazon.com/news/amazon-ai/amazon-joins-partnership-on-ai',\n",
              "     'text': 'oins-partnership-on-ai , 2016. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://www.aboutamazon.com/news/company-news/amazon-responsible-ai',\n",
              "     'text': 'https://www.aboutamazon.com/news/company-n , 2023. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://www.aboutamazon.com/news/company-news/amazon-responsible-ai',\n",
              "     'text': 'ews/amazon-responsible-ai , 2023. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai',\n",
              "     'text': 'https://www.abou tamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-i'},\n",
              "    {'url': 'https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai',\n",
              "     'text': 'tamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-i'},\n",
              "    {'url': 'https://www.aboutamazon.com/news/policy-news-views/amazon-joins-us-artificial-intelligence-safety-institute-to-advance-responsible-ai',\n",
              "     'text': 'nstitute-to-advance-responsible-ai , 2024. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://aws.amazon.com/sagemaker/clarify/',\n",
              "     'text': 'https://aws.amazon.com/sagemaker/clarify/ , 2024. Accessed:'},\n",
              "    {'url': 'https://aws.amazon.com/compliance/data-protection/',\n",
              "     'text': 'https://aws.amazon.com/compliance/data-protection/ ,'},\n",
              "    {'url': 'https://aws.amazon.com/ai/responsible-ai/',\n",
              "     'text': 'https://aws.amazon.com/ai/responsible-ai/ , 2024.'},\n",
              "    {'url': 'https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf',\n",
              "     'text': 'https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Cla'},\n",
              "    {'url': 'https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf',\n",
              "     'text': 'ude_3.pdf .'},\n",
              "    {'url': 'https://www.anthropic.com/claude/sonnet',\n",
              "     'text': 'https://www.anthropic.com/claude/sonnet , 2024. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf',\n",
              "     'text': 'https://assets.anthr opic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf'},\n",
              "    {'url': 'https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf',\n",
              "     'text': 'opic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf .'},\n",
              "    {'url': 'https://github.com/black-forest-labs/flux',\n",
              "     'text': 'https://github.com/black-forest-labs/flux .'},\n",
              "    {'url': 'https://api.semanticscholar.org/CorpusID:235399966',\n",
              "     'text': 'https://api.semanticscholar.org/CorpusID:235399966 .'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': True,\n",
              "   'parsingMode': 'premium',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 24,\n",
              "   'text': '                                         The Amazon Nova Family of Models\\n  [22] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved\\n          question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018.\\n    [23] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,\\n          C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,\\n          2021.\\n    [24] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a generalist\\n          agent for the web. In NeurIPS, 2023.\\n     [25] J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun, K.-W. Chang, and R. Gupta. BOLD: Dataset and\\n          metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on\\n         Fairness, Accountability, and Transparency, FAccT ’21, page 862–872, New York, NY, USA, 2021. Association\\n          for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/\\n          10.1145/3442188.3445924.\\n  [26] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark\\n          requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.\\n    [27]  P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al.\\n          Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on\\n          Machine Learning, 2024. URL https://huggingface.co/stabilityai/stable-diffusion-3-medium.\\n    [28]  P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al.\\n          Stable Diffusion 3.5. 2024. URL https://stability.ai/news/introducing-stable-diffusion-3-5.\\n    [29] Frontier Model Forum.   Amazon and Meta join the Frontier Model Forum to promote AI safety.       https:\\n          //www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-t\\n          o-promote-ai-safety/, 2024. Accessed: 2024-11-20.\\n   [30] G7 Hiroshima Summit. Hiroshima process international code of conduct for organizations developing advanced\\n          AI systems. https://www.mofa.go.jp/files/100573473.pdf, 2023. Accessed: 2024-11-20.\\n     [31] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic\\n          degeneration in language models. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for\\n          Computational Linguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational\\n          Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings\\n        -emnlp.301.\\n    [32]  Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL\\n          https://arxiv.org/abs/2403.05530.\\n    [33]  Google Deepmind. Gemini Flash. https://deepmind.google/technologies/gemini/flash/, 2024.\\n          Accessed: 2024-11-20.\\n    [34] N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzmán, and A. Fan.\\n          The FLORES-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.\\n[35] F. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. Two new evaluation\\n          datasets for low-resource machine translation: Nepali-english and sinhala-english. 2019.\\n    [36] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt.      Measuring massive\\n          multitask language understanding. In International Conference on Learning Representations, 2021.   URL\\n          https://openreview.net/forum?id=d7KBjmI3GmQ.\\n    [37]  D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring\\n          mathematical problem solving with the MATH dataset. NeurIPS, 2021.\\n    [38]  Y. Hu, B. Liu, J. Kasai, Y. Wang, M. Ostendorf, R. Krishna, and N. A. Smith. TIFA: Accurate and interpretable\\n          text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International\\n          Conference on Computer Vision, pages 20406–20417, 2023.\\n    [39] R. Islam and O. M. Moushi. GPT-4o: The cutting-edge advancement in multimodal LLM. Technical report, 2024.\\n                                                         24',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n1. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018.\\n2. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n3. X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a generalist agent for the web. In NeurIPS, 2023.\\n4. J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun, K.-W. Chang, and R. Gupta. BOLD: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 862–872, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/10.1145/3442188.3445924.\\n5. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.\\n6. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. URL https://huggingface.co/stabilityai/stable-diffusion-3-medium.\\n7. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Stable Diffusion 3.5. 2024. URL https://stability.ai/news/introducing-stable-diffusion-3-5.\\n8. Frontier Model Forum. Amazon and Meta join the Frontier Model Forum to promote AI safety. https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/, 2024. Accessed: 2024-11-20.\\n9. G7 Hiroshima Summit. Hiroshima process international code of conduct for organizations developing advanced AI systems. https://www.mofa.go.jp/files/100573473.pdf, 2023. Accessed: 2024-11-20.\\n10. S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.\\n11. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530.\\n12. Google Deepmind. Gemini Flash. https://deepmind.google/technologies/gemini/flash/, 2024. Accessed: 2024-11-20.\\n13. N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzmán, and A. Fan. The FLORES-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.\\n14. F. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. Two new evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english. 2019.\\n15. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\\n16. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. NeurIPS, 2021.\\n17. Y. Hu, B. Liu, J. Kasai, Y. Wang, M. Ostendorf, R. Krishna, and N. A. Smith. TIFA: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20406–20417, 2023.\\n18. R. Islam and O. M. Moushi. GPT-4o: The cutting-edge advancement in multimodal LLM. Technical report, 2024.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': '1. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018.\\n2. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n3. X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a generalist agent for the web. In NeurIPS, 2023.\\n4. J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun, K.-W. Chang, and R. Gupta. BOLD: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 862–872, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/10.1145/3442188.3445924.\\n5. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.\\n6. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. URL https://huggingface.co/stabilityai/stable-diffusion-3-medium.\\n7. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Stable Diffusion 3.5. 2024. URL https://stability.ai/news/introducing-stable-diffusion-3-5.\\n8. Frontier Model Forum. Amazon and Meta join the Frontier Model Forum to promote AI safety. https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/, 2024. Accessed: 2024-11-20.\\n9. G7 Hiroshima Summit. Hiroshima process international code of conduct for organizations developing advanced AI systems. https://www.mofa.go.jp/files/100573473.pdf, 2023. Accessed: 2024-11-20.\\n10. S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.\\n11. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530.\\n12. Google Deepmind. Gemini Flash. https://deepmind.google/technologies/gemini/flash/, 2024. Accessed: 2024-11-20.\\n13. N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzmán, and A. Fan. The FLORES-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.\\n14. F. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. Two new evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english. 2019.\\n15. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\\n16. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. NeurIPS, 2021.\\n17. Y. Hu, B. Liu, J. Kasai, Y. Wang, M. Ostendorf, R. Krishna, and N. A. Smith. TIFA: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20406–20417, 2023.\\n18. R. Islam and O. M. Moushi. GPT-4o: The cutting-edge advancement in multimodal LLM. Technical report, 2024.',\n",
              "     'md': '1. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv:1803.05457v1, 2018.\\n2. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n3. X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a generalist agent for the web. In NeurIPS, 2023.\\n4. J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun, K.-W. Chang, and R. Gupta. BOLD: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 862–872, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445924. URL https://doi.org/10.1145/3442188.3445924.\\n5. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.\\n6. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. URL https://huggingface.co/stabilityai/stable-diffusion-3-medium.\\n7. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Stable Diffusion 3.5. 2024. URL https://stability.ai/news/introducing-stable-diffusion-3-5.\\n8. Frontier Model Forum. Amazon and Meta join the Frontier Model Forum to promote AI safety. https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/, 2024. Accessed: 2024-11-20.\\n9. G7 Hiroshima Summit. Hiroshima process international code of conduct for organizations developing advanced AI systems. https://www.mofa.go.jp/files/100573473.pdf, 2023. Accessed: 2024-11-20.\\n10. S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.\\n11. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530.\\n12. Google Deepmind. Gemini Flash. https://deepmind.google/technologies/gemini/flash/, 2024. Accessed: 2024-11-20.\\n13. N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzmán, and A. Fan. The FLORES-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.\\n14. F. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. Two new evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english. 2019.\\n15. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\\n16. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. NeurIPS, 2021.\\n17. Y. Hu, B. Liu, J. Kasai, Y. Wang, M. Ostendorf, R. Krishna, and N. A. Smith. TIFA: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20406–20417, 2023.\\n18. R. Islam and O. M. Moushi. GPT-4o: The cutting-edge advancement in multimodal LLM. Technical report, 2024.',\n",
              "     'bBox': {'x': 90, 'y': 83.04, 'w': 452, 'h': 667}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://doi.org/10.1145/3442188.3445924',\n",
              "     'text': 'https://doi.org/'},\n",
              "    {'url': 'https://doi.org/10.1145/3442188.3445924',\n",
              "     'text': '10.1145/3442188.3445924 .'},\n",
              "    {'url': 'https://huggingface.co/stabilityai/stable-diffusion-3-medium',\n",
              "     'text': 'https://huggingface.co/stabilityai/stable-diffusion-3-medium .'},\n",
              "    {'url': 'https://stability.ai/news/introducing-stable-diffusion-3-5',\n",
              "     'text': 'https://stability.ai/news/introducing-stable-diffusion-3-5 .'},\n",
              "    {'url': 'https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/',\n",
              "     'text': 'https: //www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-t'},\n",
              "    {'url': 'https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/',\n",
              "     'text': '//www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-t'},\n",
              "    {'url': 'https://www.frontiermodelforum.org/updates/amazon-and-meta-join-the-frontier-model-forum-to-promote-ai-safety/',\n",
              "     'text': 'o-promote-ai-safety/ , 2024. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://www.mofa.go.jp/files/100573473.pdf',\n",
              "     'text': 'https://www.mofa.go.jp/files/100573473.pdf , 2023. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://aclanthology.org/2020.findings-emnlp.301',\n",
              "     'text': 'https://aclanthology.org/2020.findings'},\n",
              "    {'url': 'https://aclanthology.org/2020.findings-emnlp.301', 'text': '.'},\n",
              "    {'url': 'https://arxiv.org/abs/2403.05530',\n",
              "     'text': 'https://arxiv.org/abs/2403.05530 .'},\n",
              "    {'url': 'https://deepmind.google/technologies/gemini/flash/',\n",
              "     'text': 'https://deepmind.google/technologies/gemini/flash/ , 2024.'},\n",
              "    {'url': 'https://openreview.net/forum?id=d7KBjmI3GmQ',\n",
              "     'text': 'https://openreview.net/forum?id=d7KBjmI3GmQ .'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 25,\n",
              "   'text': '                                     The Amazon Nova Family of Models\\n[40] G. Kamradt. LLMTest NeedleInAHaystack, 2023. URL https://github.com/gkamradt/LLMTestNeedleI\\n      nAHaystack/blob/main/README.md.\\n[41] D. P. Kingma. Auto-encoding variational Bayes.  2nd International Conference on Learning Representations,\\n      ICLR, 2014.\\n[42] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO:\\n      Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,\\n      September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\\n[43] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. VisualWebBench: How far have multimodal\\n      llms evolved in web page understanding and grounding?, 2024.\\n[44] X. Liu, Y. Zhu, J. Gu, Y. Lan, C. Yang, and Y. Qiao. MM-SafetyBench: A benchmark for safety evaluation of\\n      multimodal large language models. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol,\\n      editors, Computer Vision – ECCV 2024, pages 386–403, Cham, 2025. Springer Nature Switzerland.       ISBN\\n      978-3-031-72992-8.\\n[45] Llama Team, AI Meta. The Llama 3 herd of models, 2024. URL https://github.com/meta-llama/llama\\n    -models/blob/main/models/llama3_1/MODEL_CARD.md.\\n[46] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play\\n      compositional reasoning with large language models. In The 37th Conference on Neural Information Processing\\n      Systems (NeurIPS), 2023.\\n[47] Luma Labs, 2024. URL https://lumalabs.ai/dream-machine.\\n[48]  L. Madaan, A. K. Singh, R. Schaeffer, A. Poulton, S. Koyejo, P. Stenetorp, S. Narang, and D. Hupkes. Quantifying\\n      variance in evaluation benchmarks, 2024. URL https://arxiv.org/abs/2406.10229.\\n[49] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: A diagnostic benchmark for very long-form video\\n      language understanding. In NeurIPS, 2023.\\n[50] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question answering about\\n      charts with visual and logical reasoning. In ACL Findings, 2022.\\n[51] M. Mathew, D. Karatzas, and C. Jawahar. DocVQA: A dataset for VQA on document images. In WACV, 2021.\\n[52]  N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta. FLIRT:\\n      Feedback loop in-context red teaming. In EMNLP 2024, 2024. URL https://www.amazon.science/publi\\n      cations/flirt-feedback-loop-in-context-red-teaming.\\n[53]  Meta. Llama 3.2 Github model card vision.                https://github.com/meta-llama/llama-models/blob\\n      /main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models, 2024.                     Accessed:\\n      2024-11-20.\\n[54]  Y. Onoe, S. Rane, Z. Berger, Y. Bitton, J. Cho, R. Garg, A. Ku, Z. Parekh, J. Pont-Tuset, G. Tanzer, et al. DOCCI:\\n      Descriptions of connected and contrasting images. URL https://arxiv. org/abs/2404.19753.\\n[55] OpenAI. GPT 4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-int\\n      elligence, 2024. Accessed: 2024-11-20.\\n[56] OpenAI. Hello GPT 4o. https://openai.com/index/hello-gpt-4o, 2024. Accessed: 2024-11-20.\\n[57] OpenAI Team. simple evals GPT4, 2024. URL https://github.com/openai/simple-evals.\\n[58] OpenAI Team. o1 mini system card, 2024. URL https://cdn.openai.com/o1-system-card-20240917.\\n      pdf.\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\\n      J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike,\\n      and R. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural\\n      Information Processing Systems, volume 35, pages 27730–27744, 2022.\\n                                                     25',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n[40] G. Kamradt. LLMTest NeedleInAHaystack, 2023. URL https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md.\\n\\n[41] D. P. Kingma. Auto-encoding variational Bayes. 2nd International Conference on Learning Representations, ICLR, 2014.\\n\\n[42] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\\n\\n[43] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. VisualWebBench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.\\n\\n[44] X. Liu, Y. Zhu, J. Gu, Y. Lan, C. Yang, and Y. Qiao. MM-SafetyBench: A benchmark for safety evaluation of multimodal large language models. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol, editors, Computer Vision – ECCV 2024, pages 386–403, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-72992-8.\\n\\n[45] Llama Team, AI Meta. The Llama 3 herd of models, 2024. URL https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md.\\n\\n[46] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In The 37th Conference on Neural Information Processing Systems (NeurIPS), 2023.\\n\\n[47] Luma Labs, 2024. URL https://lumalabs.ai/dream-machine.\\n\\n[48] L. Madaan, A. K. Singh, R. Schaeffer, A. Poulton, S. Koyejo, P. Stenetorp, S. Narang, and D. Hupkes. Quantifying variance in evaluation benchmarks, 2024. URL https://arxiv.org/abs/2406.10229.\\n\\n[49] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: A diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023.\\n\\n[50] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In ACL Findings, 2022.\\n\\n[51] M. Mathew, D. Karatzas, and C. Jawahar. DocVQA: A dataset for VQA on document images. In WACV, 2021.\\n\\n[52] N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta. FLIRT: Feedback loop in-context red teaming. In EMNLP 2024, 2024. URL https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming.\\n\\n[53] Meta. Llama 3.2 Github model card vision. https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models, 2024. Accessed: 2024-11-20.\\n\\n[54] Y. Onoe, S. Rane, Z. Berger, Y. Bitton, J. Cho, R. Garg, A. Ku, Z. Parekh, J. Pont-Tuset, G. Tanzer, et al. DOCCI: Descriptions of connected and contrasting images. URL https://arxiv.org/abs/2404.19753.\\n\\n[55] OpenAI. GPT 4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence, 2024. Accessed: 2024-11-20.\\n\\n[56] OpenAI. Hello GPT 4o. https://openai.com/index/hello-gpt-4o, 2024. Accessed: 2024-11-20.\\n\\n[57] OpenAI Team. simple evals GPT4, 2024. URL https://github.com/openai/simple-evals.\\n\\n[58] OpenAI Team. o1 mini system card, 2024. URL https://cdn.openai.com/o1-system-card-20240917.pdf.\\n\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744, 2022.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': '[40] G. Kamradt. LLMTest NeedleInAHaystack, 2023. URL https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md.\\n\\n[41] D. P. Kingma. Auto-encoding variational Bayes. 2nd International Conference on Learning Representations, ICLR, 2014.\\n\\n[42] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\\n\\n[43] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. VisualWebBench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.\\n\\n[44] X. Liu, Y. Zhu, J. Gu, Y. Lan, C. Yang, and Y. Qiao. MM-SafetyBench: A benchmark for safety evaluation of multimodal large language models. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol, editors, Computer Vision – ECCV 2024, pages 386–403, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-72992-8.\\n\\n[45] Llama Team, AI Meta. The Llama 3 herd of models, 2024. URL https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md.\\n\\n[46] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In The 37th Conference on Neural Information Processing Systems (NeurIPS), 2023.\\n\\n[47] Luma Labs, 2024. URL https://lumalabs.ai/dream-machine.\\n\\n[48] L. Madaan, A. K. Singh, R. Schaeffer, A. Poulton, S. Koyejo, P. Stenetorp, S. Narang, and D. Hupkes. Quantifying variance in evaluation benchmarks, 2024. URL https://arxiv.org/abs/2406.10229.\\n\\n[49] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: A diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023.\\n\\n[50] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In ACL Findings, 2022.\\n\\n[51] M. Mathew, D. Karatzas, and C. Jawahar. DocVQA: A dataset for VQA on document images. In WACV, 2021.\\n\\n[52] N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta. FLIRT: Feedback loop in-context red teaming. In EMNLP 2024, 2024. URL https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming.\\n\\n[53] Meta. Llama 3.2 Github model card vision. https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models, 2024. Accessed: 2024-11-20.\\n\\n[54] Y. Onoe, S. Rane, Z. Berger, Y. Bitton, J. Cho, R. Garg, A. Ku, Z. Parekh, J. Pont-Tuset, G. Tanzer, et al. DOCCI: Descriptions of connected and contrasting images. URL https://arxiv.org/abs/2404.19753.\\n\\n[55] OpenAI. GPT 4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence, 2024. Accessed: 2024-11-20.\\n\\n[56] OpenAI. Hello GPT 4o. https://openai.com/index/hello-gpt-4o, 2024. Accessed: 2024-11-20.\\n\\n[57] OpenAI Team. simple evals GPT4, 2024. URL https://github.com/openai/simple-evals.\\n\\n[58] OpenAI Team. o1 mini system card, 2024. URL https://cdn.openai.com/o1-system-card-20240917.pdf.\\n\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744, 2022.',\n",
              "     'md': '[40] G. Kamradt. LLMTest NeedleInAHaystack, 2023. URL https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md.\\n\\n[41] D. P. Kingma. Auto-encoding variational Bayes. 2nd International Conference on Learning Representations, ICLR, 2014.\\n\\n[42] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\\n\\n[43] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. VisualWebBench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.\\n\\n[44] X. Liu, Y. Zhu, J. Gu, Y. Lan, C. Yang, and Y. Qiao. MM-SafetyBench: A benchmark for safety evaluation of multimodal large language models. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol, editors, Computer Vision – ECCV 2024, pages 386–403, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-72992-8.\\n\\n[45] Llama Team, AI Meta. The Llama 3 herd of models, 2024. URL https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md.\\n\\n[46] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In The 37th Conference on Neural Information Processing Systems (NeurIPS), 2023.\\n\\n[47] Luma Labs, 2024. URL https://lumalabs.ai/dream-machine.\\n\\n[48] L. Madaan, A. K. Singh, R. Schaeffer, A. Poulton, S. Koyejo, P. Stenetorp, S. Narang, and D. Hupkes. Quantifying variance in evaluation benchmarks, 2024. URL https://arxiv.org/abs/2406.10229.\\n\\n[49] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: A diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023.\\n\\n[50] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In ACL Findings, 2022.\\n\\n[51] M. Mathew, D. Karatzas, and C. Jawahar. DocVQA: A dataset for VQA on document images. In WACV, 2021.\\n\\n[52] N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta. FLIRT: Feedback loop in-context red teaming. In EMNLP 2024, 2024. URL https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming.\\n\\n[53] Meta. Llama 3.2 Github model card vision. https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models, 2024. Accessed: 2024-11-20.\\n\\n[54] Y. Onoe, S. Rane, Z. Berger, Y. Bitton, J. Cho, R. Garg, A. Ku, Z. Parekh, J. Pont-Tuset, G. Tanzer, et al. DOCCI: Descriptions of connected and contrasting images. URL https://arxiv.org/abs/2404.19753.\\n\\n[55] OpenAI. GPT 4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence, 2024. Accessed: 2024-11-20.\\n\\n[56] OpenAI. Hello GPT 4o. https://openai.com/index/hello-gpt-4o, 2024. Accessed: 2024-11-20.\\n\\n[57] OpenAI Team. simple evals GPT4, 2024. URL https://github.com/openai/simple-evals.\\n\\n[58] OpenAI Team. o1 mini system card, 2024. URL https://cdn.openai.com/o1-system-card-20240917.pdf.\\n\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744, 2022.',\n",
              "     'bBox': {'x': 71, 'y': 72.04, 'w': 472, 'h': 678}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md',\n",
              "     'text': 'https://github.com/gkamradt/LLMTestNeedleI'},\n",
              "    {'url': 'https://github.com/gkamradt/LLMTestNeedleInAHaystack/blob/main/README.md',\n",
              "     'text': 'nAHaystack/blob/main/README.md .'},\n",
              "    {'url': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md',\n",
              "     'text': 'https://github.com/meta-llama/llama'},\n",
              "    {'url': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md',\n",
              "     'text': '.'},\n",
              "    {'url': 'https://lumalabs.ai/dream-machine',\n",
              "     'text': 'https://lumalabs.ai/dream-machine .'},\n",
              "    {'url': 'https://arxiv.org/abs/2406.10229',\n",
              "     'text': 'https://arxiv.org/abs/2406.10229 .'},\n",
              "    {'url': 'https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming',\n",
              "     'text': 'https://www.amazon.science/publi'},\n",
              "    {'url': 'https://www.amazon.science/publications/flirt-feedback-loop-in-context-red-teaming',\n",
              "     'text': 'cations/flirt-feedback-loop-in-context-red-teaming .'},\n",
              "    {'url': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models',\n",
              "     'text': 'https://github.com/meta-llama/llama-models/blob /main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models'},\n",
              "    {'url': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models',\n",
              "     'text': '/main/models/llama3_2/MODEL_CARD_VISION.md#instruction-tuned-models , 2024.'},\n",
              "    {'url': 'https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence',\n",
              "     'text': 'https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-int , 2024. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence',\n",
              "     'text': 'elligence , 2024. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://openai.com/index/hello-gpt-4o',\n",
              "     'text': 'https://openai.com/index/hello-gpt-4o , 2024. Accessed: 2024-11-20.'},\n",
              "    {'url': 'https://github.com/openai/simple-evals',\n",
              "     'text': 'https://github.com/openai/simple-evals .'},\n",
              "    {'url': 'https://cdn.openai.com/o1-system-card-20240917.pdf',\n",
              "     'text': 'https://cdn.openai.com/o1-system-card-20240917.'},\n",
              "    {'url': 'https://cdn.openai.com/o1-system-card-20240917.pdf',\n",
              "     'text': 'pdf .'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 26,\n",
              "   'text': '                                     The Amazon Nova Family of Models\\n [60] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive APIs,\\n      2023. URL https://arxiv.org/abs/2305.15334.\\n[61] W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, 2023.\\n[62]  R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization:\\n      Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing\\n      Systems, 2023.\\n  [63] R. Rei, J. G. C. de Souza, D. Alves, C. Zerva, A. C. Farinha, T. Glushkova, A. Lavie, L. Coheur, and A. F. T.\\n      Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task.          In P. Koehn, L. Barrault,\\n      O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussà, C. Federmann, M. Fishel, A. Fraser, M. Freitag,\\n      Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno Yepes, T. Kocmi, A. Martins,\\n      M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Névéol, M. Neves, M. Popel, M. Turchi, and\\n      M. Zampieri, editors, Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578–585,\\n      Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics.            URL\\n      https://aclanthology.org/2022.wmt-1.52.\\n    [64] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A\\n      graduate-level google-proof Q&A benchmark, 2023. URL https://arxiv.org/abs/2311.12022.\\n[65] Runway Research, 2024. URL https://runwayml.com/research/introducing-gen-3-alpha.\\n[66] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,\\n      B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language un-\\n      derstanding. Advances in neural information processing systems, 35:36479–36494, 2022.\\n[67] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and\\n      T. Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on\\n      Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.\\n [68] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.\\n[69] U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark for long text\\n      understanding. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational\\n      Linguistics: EMNLP 2023, pages 7977–7989, Singapore, Dec. 2023. Association for Computational Linguistics.\\n      doi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.findings-emnlp.5\\n      36.\\n[70] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models\\n      that can read. In CVPR, 2019.\\n        [71] K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu. T2V-CompBench: A comprehensive benchmark for\\n      compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024.\\n         [72] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi,\\n      D. Zhou, , and J. Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint\\n      arXiv:2210.09261, 2022.\\n   [73] N. Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam,\\n      D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez,\\n      P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan,\\n      S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem,\\n      H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. 2022.\\n[74] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention\\n      is all you need, 2023. URL https://arxiv.org/abs/1706.03762.\\n       [75] R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In CVPR,\\n      2015.\\n[76] A. Wang, R. Y. Pang, A. Chen, J. Phang, and S. R. Bowman. SQuALITY: Building a long-document summarization\\n      dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\n      Processing, pages 1139–1156, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational\\n      Linguistics. URL https://aclanthology.org/2022.emnlp-main.75.\\n                                                     26',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n1. S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive APIs, 2023. URL https://arxiv.org/abs/2305.15334.\\n2. W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, 2023.\\n3. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n4. R. Rei, J. G. C. de Souza, D. Alves, C. Zerva, A. C. Farinha, T. Glushkova, A. Lavie, L. Coheur, and A. F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussà, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno Yepes, T. Kocmi, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Névéol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, editors, Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.52.\\n5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof Q&A benchmark, 2023. URL https://arxiv.org/abs/2311.12022.\\n6. Runway Research, 2024. URL https://runwayml.com/research/introducing-gen-3-alpha.\\n7. C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479–36494, 2022.\\n8. T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.\\n9. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.\\n10. U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977–7989, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.findings-emnlp.536.\\n11. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In CVPR, 2019.\\n12. K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu. T2V-CompBench: A comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024.\\n13. M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n14. N. Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. 2022.\\n15. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762.\\n16. R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In CVPR, 2015.\\n17. A. Wang, R. Y. Pang, A. Chen, J. Phang, and S. R. Bowman. SQuALITY: Building a long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139–1156, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.75.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': '1. S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive APIs, 2023. URL https://arxiv.org/abs/2305.15334.\\n2. W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, 2023.\\n3. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n4. R. Rei, J. G. C. de Souza, D. Alves, C. Zerva, A. C. Farinha, T. Glushkova, A. Lavie, L. Coheur, and A. F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussà, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno Yepes, T. Kocmi, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Névéol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, editors, Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.52.\\n5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof Q&A benchmark, 2023. URL https://arxiv.org/abs/2311.12022.\\n6. Runway Research, 2024. URL https://runwayml.com/research/introducing-gen-3-alpha.\\n7. C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479–36494, 2022.\\n8. T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.\\n9. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.\\n10. U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977–7989, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.findings-emnlp.536.\\n11. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In CVPR, 2019.\\n12. K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu. T2V-CompBench: A comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024.\\n13. M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n14. N. Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. 2022.\\n15. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762.\\n16. R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In CVPR, 2015.\\n17. A. Wang, R. Y. Pang, A. Chen, J. Phang, and S. R. Bowman. SQuALITY: Building a long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139–1156, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.75.',\n",
              "     'md': '1. S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive APIs, 2023. URL https://arxiv.org/abs/2305.15334.\\n2. W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, 2023.\\n3. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n4. R. Rei, J. G. C. de Souza, D. Alves, C. Zerva, A. C. Farinha, T. Glushkova, A. Lavie, L. Coheur, and A. F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussà, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno Yepes, T. Kocmi, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Névéol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, editors, Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.52.\\n5. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof Q&A benchmark, 2023. URL https://arxiv.org/abs/2311.12022.\\n6. Runway Research, 2024. URL https://runwayml.com/research/introducing-gen-3-alpha.\\n7. C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479–36494, 2022.\\n8. T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.\\n9. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.\\n10. U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977–7989, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.findings-emnlp.536.\\n11. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In CVPR, 2019.\\n12. K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu. T2V-CompBench: A comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024.\\n13. M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n14. N. Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. 2022.\\n15. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762.\\n16. R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In CVPR, 2015.\\n17. A. Wang, R. Y. Pang, A. Chen, J. Phang, and S. R. Bowman. SQuALITY: Building a long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139–1156, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.75.',\n",
              "     'bBox': {'x': 93, 'y': 83.04, 'w': 448, 'h': 667}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://arxiv.org/abs/2305.15334',\n",
              "     'text': 'https://arxiv.org/abs/2305.15334 .'},\n",
              "    {'url': 'https://aclanthology.org/2022.wmt-1.52',\n",
              "     'text': 'https://aclanthology.org/2022.wmt-1.52 .'},\n",
              "    {'url': 'https://arxiv.org/abs/2311.12022',\n",
              "     'text': 'https://arxiv.org/abs/2311.12022 .'},\n",
              "    {'url': 'https://runwayml.com/research/introducing-gen-3-alpha',\n",
              "     'text': 'https://runwayml.com/research/introducing-gen-3-alpha .'},\n",
              "    {'url': 'https://openreview.net/forum?id=Yacmpz84TH',\n",
              "     'text': 'https://openreview.net/forum?id=Yacmpz84TH .'},\n",
              "    {'url': 'https://aclanthology.org/2023.findings-emnlp.536',\n",
              "     'text': 'https://aclanthology.org/2023.findings-emnlp.5'},\n",
              "    {'url': 'https://aclanthology.org/2023.findings-emnlp.536',\n",
              "     'text': '36 .'},\n",
              "    {'url': 'https://arxiv.org/abs/1706.03762',\n",
              "     'text': 'https://arxiv.org/abs/1706.03762 .'},\n",
              "    {'url': 'https://aclanthology.org/2022.emnlp-main.75',\n",
              "     'text': 'https://aclanthology.org/2022.emnlp-main.75 .'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 27,\n",
              "   'text': '                                     The Amazon Nova Family of Models\\n[77] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. LVBench: An\\n      extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024.\\n[78] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. VATEX: A large-scale, high-quality multilingual\\n      dataset for video-and-language research. In ICCV, 2019.\\n[79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought\\n      prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on\\n      Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN\\n      9781713871088.\\n[80] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. ImageReward: Learning and evaluating\\n      human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.\\n[81] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling\\n      leaderboard. 2024.\\n[82] X. Yang, K. Sun, H. Xin, Y. Sun, N. Bhalla, X. Chen, S. Choudhary, R. D. Gui, Z. W. Jiang, Z. Jiang, L. Kong,\\n      B. Moran, J. Wang, Y. E. Xu, A. Yan, C. Yang, E. Yuan, H. Zha, N. Tang, L. Chen, N. Scheffer, Y. Liu, N. Shah,\\n      R. Wanga, A. Kumar, W. tau Yih, and X. L. Dong.    Crag – comprehensive rag benchmark.       arXiv preprint\\n      arXiv:2406.04744, 2024. URL https://arxiv.org/abs/2406.04744.\\n[83] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting\\n      in language models. In International Conference on Learning Representations (ICLR), 2023.\\n[84] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling\\n      autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\\n[85] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu,\\n      R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive\\n      multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024.\\n[86]  B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded. In ICML, 2024.\\n[87] L. Zheng, Z. Huang, Z. Xue, X. Wang, B. An, and S. Yan. AgentStudio: A toolkit for building general virtual\\n      agents. arXiv preprint arXiv:2403.17918, 2024.\\n[88]  M. Zhong, A. Zhang, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang,\\n      M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten. Law of the weakest link: Cross capabilities\\n      of large language models. arXiv preprint arXiv:2409.19951, 2024.\\n[89]  J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for\\n      large language models, 2023. URL https://arxiv.org/abs/2311.07911.\\n                                                     27',\n",
              "   'md': \"The Amazon Nova Family of Models\\n\\n[77] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. LVBench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024.\\n\\n[78] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In ICCV, 2019.\\n\\n[79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.\\n\\n[80] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. ImageReward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[81] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling leaderboard. 2024.\\n\\n[82] X. Yang, K. Sun, H. Xin, Y. Sun, N. Bhalla, X. Chen, S. Choudhary, R. D. Gui, Z. W. Jiang, Z. Jiang, L. Kong, B. Moran, J. Wang, Y. E. Xu, A. Yan, C. Yang, E. Yuan, H. Zha, N. Tang, L. Chen, N. Scheffer, Y. Liu, N. Shah, R. Wanga, A. Kumar, W. tau Yih, and X. L. Dong. Crag – comprehensive rag benchmark. arXiv preprint arXiv:2406.04744, 2024. URL https://arxiv.org/abs/2406.04744.\\n\\n[83] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[84] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\\n\\n[85] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024.\\n\\n[86] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded. In ICML, 2024.\\n\\n[87] L. Zheng, Z. Huang, Z. Xue, X. Wang, B. An, and S. Yan. AgentStudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024.\\n\\n[88] M. Zhong, A. Zhang, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang, M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten. Law of the weakest link: Cross capabilities of large language models. arXiv preprint arXiv:2409.19951, 2024.\\n\\n[89] J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911.\",\n",
              "   'images': [{'name': 'page_27.jpg',\n",
              "     'height': 792,\n",
              "     'width': 612,\n",
              "     'x': 0,\n",
              "     'y': 0,\n",
              "     'original_width': 1236,\n",
              "     'original_height': 1600,\n",
              "     'type': 'full_page_screenshot'}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'text',\n",
              "     'value': \"The Amazon Nova Family of Models\\n\\n[77] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. LVBench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024.\\n\\n[78] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In ICCV, 2019.\\n\\n[79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.\\n\\n[80] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. ImageReward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[81] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling leaderboard. 2024.\\n\\n[82] X. Yang, K. Sun, H. Xin, Y. Sun, N. Bhalla, X. Chen, S. Choudhary, R. D. Gui, Z. W. Jiang, Z. Jiang, L. Kong, B. Moran, J. Wang, Y. E. Xu, A. Yan, C. Yang, E. Yuan, H. Zha, N. Tang, L. Chen, N. Scheffer, Y. Liu, N. Shah, R. Wanga, A. Kumar, W. tau Yih, and X. L. Dong. Crag – comprehensive rag benchmark. arXiv preprint arXiv:2406.04744, 2024. URL https://arxiv.org/abs/2406.04744.\\n\\n[83] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[84] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\\n\\n[85] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024.\\n\\n[86] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded. In ICML, 2024.\\n\\n[87] L. Zheng, Z. Huang, Z. Xue, X. Wang, B. An, and S. Yan. AgentStudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024.\\n\\n[88] M. Zhong, A. Zhang, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang, M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten. Law of the weakest link: Cross capabilities of large language models. arXiv preprint arXiv:2409.19951, 2024.\\n\\n[89] J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911.\",\n",
              "     'md': \"The Amazon Nova Family of Models\\n\\n[77] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, X. Gu, S. Huang, B. Xu, Y. Dong, et al. LVBench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024.\\n\\n[78] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In ICCV, 2019.\\n\\n[79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.\\n\\n[80] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. ImageReward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[81] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling leaderboard. 2024.\\n\\n[82] X. Yang, K. Sun, H. Xin, Y. Sun, N. Bhalla, X. Chen, S. Choudhary, R. D. Gui, Z. W. Jiang, Z. Jiang, L. Kong, B. Moran, J. Wang, Y. E. Xu, A. Yan, C. Yang, E. Yuan, H. Zha, N. Tang, L. Chen, N. Scheffer, Y. Liu, N. Shah, R. Wanga, A. Kumar, W. tau Yih, and X. L. Dong. Crag – comprehensive rag benchmark. arXiv preprint arXiv:2406.04744, 2024. URL https://arxiv.org/abs/2406.04744.\\n\\n[83] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[84] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\\n\\n[85] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024.\\n\\n[86] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded. In ICML, 2024.\\n\\n[87] L. Zheng, Z. Huang, Z. Xue, X. Wang, B. An, and S. Yan. AgentStudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024.\\n\\n[88] M. Zhong, A. Zhang, X. Wang, R. Hou, W. Xiong, C. Zhu, Z. Chen, L. Tan, C. Bi, M. Lewis, S. Popuri, S. Narang, M. Kambadur, D. Mahajan, S. Edunov, J. Han, and L. van der Maaten. Law of the weakest link: Cross capabilities of large language models. arXiv preprint arXiv:2409.19951, 2024.\\n\\n[89] J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911.\",\n",
              "     'bBox': {'x': 71, 'y': 34.04, 'w': 471, 'h': 471}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://arxiv.org/abs/2406.04744',\n",
              "     'text': 'https://arxiv.org/abs/2406.04744 .'},\n",
              "    {'url': 'https://arxiv.org/abs/2311.07911',\n",
              "     'text': 'https://arxiv.org/abs/2311.07911 .'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': True,\n",
              "   'parsingMode': 'premium',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 28,\n",
              "   'text': '                                      The Amazon Nova Family of Models\\nA    Amazon Nova Canvas Capabilities\\nOur Nova Canvas model offers the following functionalities, with examples given in Figure 5.\\n       •  Text-to-image generation allows customers to create images with various resolutions (from 512×512 up to\\n          2K×2K resolution).\\n       •  Editing allows developers to edit images using a combination of text prompt or mask image. Amazon Nova\\n          Canvas supports text-to-image editing and image-to-image editing, including inpainting, outpainting and object\\n          removal.\\n       •  Image variation allows customers to output images with similar contents but with variations from the user\\n          provided ones.\\n       •  Image conditioning provide a reference image along with a text prompt, resulting in outputs that follow the\\n          layout and structure of the user-supplied reference.\\n       •  Image guidance with color palette allows customers to precisely control the color palette of generated images\\n          by providing a list of hex codes along with the text prompt.\\n       •  Background removal automatically removes background from images containing multiple objects.\\n                                                      28',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# Amazon Nova Canvas Capabilities\\n\\nOur Nova Canvas model offers the following functionalities, with examples given in Figure 5.\\n\\n- Text-to-image generation allows customers to create images with various resolutions (from 512×512 up to 2K×2K resolution).\\n- Editing allows developers to edit images using a combination of text prompt or mask image. Amazon Nova Canvas supports text-to-image editing and image-to-image editing, including inpainting, outpainting and object removal.\\n- Image variation allows customers to output images with similar contents but with variations from the user provided ones.\\n- Image conditioning provide a reference image along with a text prompt, resulting in outputs that follow the layout and structure of the user-supplied reference.\\n- Image guidance with color palette allows customers to precisely control the color palette of generated images by providing a list of hex codes along with the text prompt.\\n- Background removal automatically removes background from images containing multiple objects.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 72, 'y': 34.04, 'w': 306, 'h': 48.01}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Amazon Nova Canvas Capabilities',\n",
              "     'md': '# Amazon Nova Canvas Capabilities',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 196, 'h': 12}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Our Nova Canvas model offers the following functionalities, with examples given in Figure 5.\\n\\n- Text-to-image generation allows customers to create images with various resolutions (from 512×512 up to 2K×2K resolution).\\n- Editing allows developers to edit images using a combination of text prompt or mask image. Amazon Nova Canvas supports text-to-image editing and image-to-image editing, including inpainting, outpainting and object removal.\\n- Image variation allows customers to output images with similar contents but with variations from the user provided ones.\\n- Image conditioning provide a reference image along with a text prompt, resulting in outputs that follow the layout and structure of the user-supplied reference.\\n- Image guidance with color palette allows customers to precisely control the color palette of generated images by providing a list of hex codes along with the text prompt.\\n- Background removal automatically removes background from images containing multiple objects.',\n",
              "     'md': 'Our Nova Canvas model offers the following functionalities, with examples given in Figure 5.\\n\\n- Text-to-image generation allows customers to create images with various resolutions (from 512×512 up to 2K×2K resolution).\\n- Editing allows developers to edit images using a combination of text prompt or mask image. Amazon Nova Canvas supports text-to-image editing and image-to-image editing, including inpainting, outpainting and object removal.\\n- Image variation allows customers to output images with similar contents but with variations from the user provided ones.\\n- Image conditioning provide a reference image along with a text prompt, resulting in outputs that follow the layout and structure of the user-supplied reference.\\n- Image guidance with color palette allows customers to precisely control the color palette of generated images by providing a list of hex codes along with the text prompt.\\n- Background removal automatically removes background from images containing multiple objects.',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 468, 'h': 195.99}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 29,\n",
              "   'text': '                                   The Amazon Nova Family of Models\\n A dinosaur sitting\\n    in a tea cup\\n       (a) Image generation from a text prompt                     (b) Inpainting the image with swans\\n                                                                  amazon                    amazon\\nchange flowers to orange color\\n                   (c) Image editing                                (d) Outpainting a new background\\na hamster eats apple slice                               A wooden boat in summer\\n                   (e) Style transfer                                     (f) Guided generation\\n      A jar of salad dressing\\n   in a rustic kitchen\\n   surrounded by fresh vegetables\\n   with studio lighting\\n            (g) Controlling the color palette                            (h) Background Removal\\n   Figure 5: Example capabilities of Amazon Nova Canvas, our content generation model for images.\\n                                                      29',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\nA dinosaur sitting in a tea cup\\n\\n|(a) Image generation from a text prompt|(b) Inpainting the image with swans|\\n|---|---|\\n|amazon|amazon|\\n|change flowers to orange color| |\\n|(c) Image editing|(d) Outpainting a new background|\\n|a hamster eats apple slice|A wooden boat in summer|\\n|(e) Style transfer|(f) Guided generation|\\n|A jar of salad dressing in a rustic kitchen surrounded by fresh vegetables with studio lighting| |\\n|(g) Controlling the color palette|(h) Background Removal|\\n\\nFigure 5: Example capabilities of Amazon Nova Canvas, our content generation model for images.\\n\\n29',\n",
              "   'images': [{'name': 'img_p28_1.png',\n",
              "     'height': 324,\n",
              "     'width': 324,\n",
              "     'x': 191.415,\n",
              "     'y': 126.615,\n",
              "     'original_width': 324,\n",
              "     'original_height': 324},\n",
              "    {'name': 'img_p28_2.png',\n",
              "     'height': 222,\n",
              "     'width': 394,\n",
              "     'x': 307.245,\n",
              "     'y': 167.99055999999996,\n",
              "     'original_width': 394,\n",
              "     'original_height': 222},\n",
              "    {'name': 'img_p28_3.png',\n",
              "     'height': 221,\n",
              "     'width': 392,\n",
              "     'x': 404.505,\n",
              "     'y': 167.96145999999996,\n",
              "     'original_width': 392,\n",
              "     'original_height': 221},\n",
              "    {'name': 'img_p28_4.png',\n",
              "     'height': 1365,\n",
              "     'width': 1365,\n",
              "     'x': 94.156,\n",
              "     'y': 250.27924876000003,\n",
              "     'original_width': 1365,\n",
              "     'original_height': 1365},\n",
              "    {'name': 'img_p28_5.png',\n",
              "     'height': 1365,\n",
              "     'width': 1365,\n",
              "     'x': 191.41500000000002,\n",
              "     'y': 260.56224876000005,\n",
              "     'original_width': 1365,\n",
              "     'original_height': 1365},\n",
              "    {'name': 'img_p28_6.png',\n",
              "     'height': 316,\n",
              "     'width': 315,\n",
              "     'x': 307.245,\n",
              "     'y': 269.22856,\n",
              "     'original_width': 315,\n",
              "     'original_height': 316,\n",
              "     'ocr': [{'x': 116,\n",
              "       'y': 140,\n",
              "       'w': 74,\n",
              "       'h': 28,\n",
              "       'confidence': '0.9992311411385467',\n",
              "       'text': 'amazon'}]},\n",
              "    {'name': 'img_p28_7.png',\n",
              "     'height': 315,\n",
              "     'width': 316,\n",
              "     'x': 404.505,\n",
              "     'y': 269.8255,\n",
              "     'original_width': 316,\n",
              "     'original_height': 315,\n",
              "     'ocr': [{'x': 116,\n",
              "       'y': 140,\n",
              "       'w': 74,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9997271158260564',\n",
              "       'text': 'amazon'}]},\n",
              "    {'name': 'img_p28_8.png',\n",
              "     'height': 262,\n",
              "     'width': 262,\n",
              "     'x': 94.156,\n",
              "     'y': 393.37298000000004,\n",
              "     'original_width': 262,\n",
              "     'original_height': 262},\n",
              "    {'name': 'img_p28_9.png',\n",
              "     'height': 262,\n",
              "     'width': 263,\n",
              "     'x': 191.41500000000002,\n",
              "     'y': 404.52492,\n",
              "     'original_width': 263,\n",
              "     'original_height': 262},\n",
              "    {'name': 'img_p28_10.png',\n",
              "     'height': 306,\n",
              "     'width': 307,\n",
              "     'x': 307.245,\n",
              "     'y': 393.18386,\n",
              "     'original_width': 307,\n",
              "     'original_height': 306},\n",
              "    {'name': 'img_p28_11.png',\n",
              "     'height': 306,\n",
              "     'width': 307,\n",
              "     'x': 404.505,\n",
              "     'y': 404.47486,\n",
              "     'original_width': 307,\n",
              "     'original_height': 306},\n",
              "    {'name': 'img_p28_12.png',\n",
              "     'height': 36,\n",
              "     'width': 126,\n",
              "     'x': 94.156,\n",
              "     'y': 562.7536,\n",
              "     'original_width': 126,\n",
              "     'original_height': 36},\n",
              "    {'name': 'img_p28_13.png',\n",
              "     'height': 283,\n",
              "     'width': 284,\n",
              "     'x': 191.41500000000002,\n",
              "     'y': 537.1229000000001,\n",
              "     'original_width': 284,\n",
              "     'original_height': 283},\n",
              "    {'name': 'img_p28_14.png',\n",
              "     'height': 322,\n",
              "     'width': 322,\n",
              "     'x': 307.245,\n",
              "     'y': 536.7921799999999,\n",
              "     'original_width': 322,\n",
              "     'original_height': 322},\n",
              "    {'name': 'img_p28_15.png',\n",
              "     'height': 320,\n",
              "     'width': 320,\n",
              "     'x': 404.505,\n",
              "     'y': 536.7951999999999,\n",
              "     'original_width': 320,\n",
              "     'original_height': 320}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 232, 'h': 285.96}},\n",
              "    {'type': 'text',\n",
              "     'value': 'A dinosaur sitting in a tea cup',\n",
              "     'md': 'A dinosaur sitting in a tea cup',\n",
              "     'bBox': {'x': 103, 'y': 168.98, 'w': 76, 'h': 17}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      ['(a) Image generation from a text prompt',\n",
              "       '(b) Inpainting the image with swans'],\n",
              "      ['---', '---'],\n",
              "      ['amazon', 'amazon'],\n",
              "      ['change flowers to orange color', ''],\n",
              "      ['(c) Image editing', '(d) Outpainting a new background'],\n",
              "      ['a hamster eats apple slice', 'A wooden boat in summer'],\n",
              "      ['(e) Style transfer', '(f) Guided generation'],\n",
              "      ['A jar of salad dressing in a rustic kitchen surrounded by fresh vegetables with studio lighting',\n",
              "       ''],\n",
              "      ['(g) Controlling the color palette', '(h) Background Removal']],\n",
              "     'md': 'A dinosaur sitting in a tea cup\\n|(a) Image generation from a text prompt|(b) Inpainting the image with swans|\\n|---|---|\\n|amazon|amazon|\\n|change flowers to orange color| |\\n|(c) Image editing|(d) Outpainting a new background|\\n|a hamster eats apple slice|A wooden boat in summer|\\n|(e) Style transfer|(f) Guided generation|\\n|A jar of salad dressing in a rustic kitchen surrounded by fresh vegetables with studio lighting| |\\n|(g) Controlling the color palette|(h) Background Removal|',\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\"(a) Image generation from a text prompt\",\"(b) Inpainting the image with swans\"\\n\"---\",\"---\"\\n\"amazon\",\"amazon\"\\n\"change flowers to orange color\",\"\"\\n\"(c) Image editing\",\"(d) Outpainting a new background\"\\n\"a hamster eats apple slice\",\"A wooden boat in summer\"\\n\"(e) Style transfer\",\"(f) Guided generation\"\\n\"A jar of salad dressing in a rustic kitchen surrounded by fresh vegetables with studio lighting\",\"\"\\n\"(g) Controlling the color palette\",\"(h) Background Removal\"',\n",
              "     'bBox': {'x': 94, 'y': 34.04, 'w': 409, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Figure 5: Example capabilities of Amazon Nova Canvas, our content generation model for images.\\n\\n29',\n",
              "     'md': 'Figure 5: Example capabilities of Amazon Nova Canvas, our content generation model for images.\\n\\n29',\n",
              "     'bBox': {'x': 109, 'y': 311, 'w': 394, 'h': 439.04}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 30,\n",
              "   'text': '                              The Amazon Nova Family of Models\\nB  Prompts and Scoring\\nPrompt templates used for Amazon Nova evaluations are given below, along with those used for select other public\\nmodels where noted. Additional materials and evaluation results from this report can be found at:\\n                            https://huggingface.co/amazon-agi\\nB.1 Text evaluation\\nB.1.1 Language Understanding\\nFor MMLU:\\nWhat is the correct answer to this question: <question>\\nChoices: <choices>. Let’s think step by step:\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"\\nThe correct answer is (insert answer here).\"\\nFor ARC-C:\\nGiven the following question and four candidate answers (A, B, C and D), choose the best\\nanswer.\\nQuestion: <question>\\nYour response should end with \"The best answer is [the_answer_letter]\" where the [\\nthe_answer_letter] is one of A, B, C or D.\\nFor DROP:\\nWe use the following 6 shots:\\n - answer: >-\\n     According to the passage, the European Coal and Steel Community was\\n     established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the\\n     answer is 7\\n   passage: >-\\n     Since the 1970s, U.S. governments have negotiated managed-trade\\n     agreements, such as the North American Free Trade Agreement in the 1990s,\\n     the Dominican Republic-Central America Free Trade Agreement in 2006, and a\\n     number of bilateral agreements. In Europe, six countries formed the\\n     European Coal and Steel Community in 1951 which became the European\\n     Economic Community in 1958. Two core objectives of the EEC were the\\n     development of a common market, subsequently renamed the single market,\\n     and establishing a customs union between its member states.\\n   question: How many years did the European Coal and Steel Community exist?\\n - answer: >-\\n     According to the passage, 23.5% are under the age of 18, and 8.7% are from\\n     ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8\\n   passage: >-\\n     In the county, the population was spread out with 23.50% under the age of\\n     18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and\\n     13.30% who were 65 years of age or older.\\n   question: >-\\n     How many more percent are under the age of 18 compared to the 18 to 24\\n     group?\\n - answer: >-\\n     According to the passage, Stafford threw 5 TD passes, 3 of which were to\\n     Johnson. 5 - 3 = 2. So the answer is 2\\n   passage: >-\\n     Playing in their second straight Thanksgiving game, the Eagles struggled\\n     especially on defense, where they were unable to stop the much-hyped Lions\\n     offense. The worst of it all was how unproven rookie Eric Rowe was tasked\\n                                           30',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# Prompts and Scoring\\n\\nPrompt templates used for Amazon Nova evaluations are given below, along with those used for select other public models where noted. Additional materials and evaluation results from this report can be found at: https://huggingface.co/amazon-agi\\n\\n# B.1 Text evaluation\\n\\n# B.1.1 Language Understanding\\n\\n# For MMLU:\\n\\nWhat is the correct answer to this question: &lt;question&gt;\\n\\nChoices: &lt;choices&gt;. Let’s think step by step:\\n\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"The correct answer is (insert answer here).\"\\n\\n# For ARC-C:\\n\\nGiven the following question and four candidate answers (A, B, C and D), choose the best answer.\\n\\nQuestion: &lt;question&gt;\\n\\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.\\n\\n# For DROP:\\n\\nWe use the following 6 shots:\\n\\n- answer: &gt;-\\n\\nAccording to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the answer is 7\\n\\npassage: &gt;-\\n\\nSince the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states.\\n\\nquestion: How many years did the European Coal and Steel Community exist?\\n- answer: &gt;-\\n\\nAccording to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8\\n\\npassage: &gt;-\\n\\nIn the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older.\\n\\nquestion: &gt;-\\n\\nHow many more percent are under the age of 18 compared to the 18 to 24 group?\\n- answer: &gt;-\\n\\nAccording to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - 3 = 2. So the answer is 2\\n\\npassage: &gt;-\\n\\nPlaying in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Prompts and Scoring',\n",
              "     'md': '# Prompts and Scoring',\n",
              "     'bBox': {'x': 91, 'y': 70.04, 'w': 108, 'h': 12}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Prompt templates used for Amazon Nova evaluations are given below, along with those used for select other public models where noted. Additional materials and evaluation results from this report can be found at: https://huggingface.co/amazon-agi',\n",
              "     'md': 'Prompt templates used for Amazon Nova evaluations are given below, along with those used for select other public models where noted. Additional materials and evaluation results from this report can be found at: https://huggingface.co/amazon-agi',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 468, 'h': 67.99}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.1 Text evaluation',\n",
              "     'md': '# B.1 Text evaluation',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 89, 'h': 91.99}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.1.1 Language Understanding',\n",
              "     'md': '# B.1.1 Language Understanding',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 139, 'h': 111.99}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'For MMLU:',\n",
              "     'md': '# For MMLU:',\n",
              "     'bBox': {'x': 72, 'y': 191.04, 'w': 50, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'What is the correct answer to this question: &lt;question&gt;\\n\\nChoices: &lt;choices&gt;. Let’s think step by step:\\n\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"The correct answer is (insert answer here).\"',\n",
              "     'md': 'What is the correct answer to this question: &lt;question&gt;\\n\\nChoices: &lt;choices&gt;. Let’s think step by step:\\n\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"The correct answer is (insert answer here).\"',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 465, 'h': 181.99}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'For ARC-C:',\n",
              "     'md': '# For ARC-C:',\n",
              "     'bBox': {'x': 72, 'y': 267.04, 'w': 49, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Given the following question and four candidate answers (A, B, C and D), choose the best answer.\\n\\nQuestion: &lt;question&gt;\\n\\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.',\n",
              "     'md': 'Given the following question and four candidate answers (A, B, C and D), choose the best answer.\\n\\nQuestion: &lt;question&gt;\\n\\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 460, 'h': 268.99}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'For DROP:',\n",
              "     'md': '# For DROP:',\n",
              "     'bBox': {'x': 72, 'y': 354.04, 'w': 45, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We use the following 6 shots:\\n\\n- answer: &gt;-\\n\\nAccording to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the answer is 7\\n\\npassage: &gt;-\\n\\nSince the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states.\\n\\nquestion: How many years did the European Coal and Steel Community exist?\\n- answer: &gt;-\\n\\nAccording to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8\\n\\npassage: &gt;-\\n\\nIn the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older.\\n\\nquestion: &gt;-\\n\\nHow many more percent are under the age of 18 compared to the 18 to 24 group?\\n- answer: &gt;-\\n\\nAccording to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - 3 = 2. So the answer is 2\\n\\npassage: &gt;-\\n\\nPlaying in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked',\n",
              "     'md': 'We use the following 6 shots:\\n\\n- answer: &gt;-\\n\\nAccording to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the answer is 7\\n\\npassage: &gt;-\\n\\nSince the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states.\\n\\nquestion: How many years did the European Coal and Steel Community exist?\\n- answer: &gt;-\\n\\nAccording to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8\\n\\npassage: &gt;-\\n\\nIn the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older.\\n\\nquestion: &gt;-\\n\\nHow many more percent are under the age of 18 compared to the 18 to 24 group?\\n- answer: &gt;-\\n\\nAccording to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - 3 = 2. So the answer is 2\\n\\npassage: &gt;-\\n\\nPlaying in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked',\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 416, 'h': 679.99}}],\n",
              "   'status': 'OK',\n",
              "   'links': [{'url': 'https://huggingface.co/amazon-agi',\n",
              "     'text': 'https://huggingface.co/amazon-agi'}],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 31,\n",
              "   'text': '                              The Amazon Nova Family of Models\\n     with covering wide receiver Calvin Johnson, leading to Johnson catching 3\\n     touchdowns. Staffords five passing touchdowns, including three of them to\\n     Johnson was too much for the Eagles to overcome and for the second\\n     consecutive time this season, the Eagles gave up 45 points in a game. With\\n     the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on\\n     Thanksgiving.\\n   question: How many TD passes did Stafford throw other than to Johnson?\\n - answer: >-\\n     All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown\\n     run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So\\n     the shortest touchdown run was 9 yards. All the touchdown passes are: a\\n     12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the\\n     shortest touchdown run and the longest touchdown pass combine for 9 + 12 =\\n     21 yards. So the answer is 21\\n   passage: >-\\n     The Seahawks played the San Francisco 49ers. In the first quarter, the\\n     Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill\\n     returning a fumble 9 yards for a touchdown. In the third quarter, the\\n     49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas\\n     Nelly, along with Mare kicking a 32-yard field goal. In the final quarter,\\n     Julius Jones got another 11-yard TD.\\n   question: >-\\n     How many yards do the shortest touchdown run and the longest touchdown\\n     pass combine for?\\n - answer: >-\\n     The Ravens kicker Billy Cundiff got a 45-yard field goal in the second\\n     quarter, concluding the first half with a 10-7 lead. So the Ravens had 10\\n     points at halftime. So the answer is 10\\n   passage: >-\\n     The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh\\n     would deliver the opening punch in the first quarter with a 1-yard\\n     touchdown from running back Rashard Mendenhall. The Ravens would make it\\n     even as running back Willis McGahee got a 9-yard TD. The Ravens kicker\\n     Billy Cundiff got a 45-yard field goal in the second quarter, concluding\\n     the first half with a 10-7 lead. The Steelers brought the game into\\n     overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy\\n     Cundiff pulled off a winning 33-yard field goal in overtime.\\n   question: How many points did the Ravens have at halftime?\\n - answer: >-\\n     The first and third quarters were the scoreless quarters. So there are 2\\n     scoreless quarters. So the answer is 2\\n   passage: >-\\n     The Vikings flew to Bank of America Stadium to face the Carolina Panthers.\\n     After a scoreless first quarter, Carolina got on the board with\\n     quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass.\\n     After yet another scoreless quarter, Carolina sealed the game as Matt\\n     Moore completed a 42-yard touchdown pass to wide receiver Steve Smith.\\n   question: How many scoreless quarters were there?\\nFor each shot we provide the following instruction:\\nConclude your answer with: \"So the answer is {final answer}\". Make sure the final answer\\nis in plain text format\\nAnd we create each user prompt as follows:\\n<passage>\\n<question>\\n<instruction>\\n                                           31',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\nwith covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Staffords five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving.\\n\\n# Question\\n\\nHow many TD passes did Stafford throw other than to Johnson?\\n\\n# Answer\\n\\nAll the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21.\\n\\n# Passage\\n\\nThe Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD.\\n\\n# Question\\n\\nHow many yards do the shortest touchdown run and the longest touchdown pass combine for?\\n\\n# Answer\\n\\nThe Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10.\\n\\n# Passage\\n\\nThe Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime.\\n\\n# Question\\n\\nHow many points did the Ravens have at halftime?\\n\\n# Answer\\n\\nThe first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2.\\n\\n# Passage\\n\\nThe Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith.\\n\\n# Question\\n\\nHow many scoreless quarters were there?',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'with covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Staffords five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving.',\n",
              "     'md': 'with covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Staffords five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving.',\n",
              "     'bBox': {'x': 100, 'y': 72.04, 'w': 387, 'h': 65}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Question',\n",
              "     'md': '# Question',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'How many TD passes did Stafford throw other than to Johnson?',\n",
              "     'md': 'How many TD passes did Stafford throw other than to Johnson?',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Answer',\n",
              "     'md': '# Answer',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21.',\n",
              "     'md': 'All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21.',\n",
              "     'bBox': {'x': 100, 'y': 160.04, 'w': 387, 'h': 64}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Passage',\n",
              "     'md': '# Passage',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD.',\n",
              "     'md': 'The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD.',\n",
              "     'bBox': {'x': 100, 'y': 236.04, 'w': 387, 'h': 65}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Question',\n",
              "     'md': '# Question',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'How many yards do the shortest touchdown run and the longest touchdown pass combine for?',\n",
              "     'md': 'How many yards do the shortest touchdown run and the longest touchdown pass combine for?',\n",
              "     'bBox': {'x': 100, 'y': 312.04, 'w': 366, 'h': 21}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Answer',\n",
              "     'md': '# Answer',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10.',\n",
              "     'md': 'The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10.',\n",
              "     'bBox': {'x': 100, 'y': 345.04, 'w': 382, 'h': 97}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Passage',\n",
              "     'md': '# Passage',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime.',\n",
              "     'md': 'The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime.',\n",
              "     'bBox': {'x': 100, 'y': 345.04, 'w': 376, 'h': 130}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Question',\n",
              "     'md': '# Question',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'How many points did the Ravens have at halftime?',\n",
              "     'md': 'How many points did the Ravens have at halftime?',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Answer',\n",
              "     'md': '# Answer',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2.',\n",
              "     'md': 'The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2.',\n",
              "     'bBox': {'x': 100, 'y': 498.04, 'w': 376, 'h': 21}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Passage',\n",
              "     'md': '# Passage',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith.',\n",
              "     'md': 'The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith.',\n",
              "     'bBox': {'x': 100, 'y': 531.04, 'w': 387, 'h': 53}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Question',\n",
              "     'md': '# Question',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'How many scoreless quarters were there?',\n",
              "     'md': 'How many scoreless quarters were there?',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 32,\n",
              "   'text': '                              The Amazon Nova Family of Models\\nFor IFEval:\\nNo particular prompt was added (query was inputted to the model).\\nFor BBH:\\nWe use a preamble that describes the task, for example:\\nEvaluate the result of a random Boolean expression.\\nWe then provide few shot examples in the following format:\\n<preamble>\\nQuestion: <question>\\n<instruction>\\nLet’s think step by step.\\n<ground truth chain of thought>. So the answer is <answer>\\nAnd we follow this by the query:\\n<preamble>\\nQuestion: <question>\\n<instruction>\\nLet’s think step by step.\\nFor each subject, We provide the subject-specific instructions as below:\\n- subject: boolean_expressions\\n  instruction: Conclude your answer with: \"So the answer is True or False.\".\\n- subject: causal_judgement\\n  instruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: date_understanding\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: disambiguation_qa\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: dyck_languages\\n  instruction: Correctly close a Dyck-n word. Conclude your answer with: \"So the answer\\n  is {final answer}.\". Make sure the final answer is in plain text format\\n- subject: formal_fallacies\\n  instruction: Conclude your answer with: \"So the answer is valid or invalid.\".\\n- subject: geometric_shapes\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: hyperbaton\\n  instruction: Conclude your answer with: \\\\\"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: logical_deduction_five_objects\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: logical_deduction_seven_objects\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: logical_deduction_three_objects\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: movie_recommendation\\n  instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n  answer_letter is A, or B, or ...\\n- subject: multistep_arithmetic_two\\n  instruction: Conclude your answer with: \"So the answer is {final answer}.\". Make sure\\n  the final answer is in plain text format\\n                                           32',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# For IFEval:\\n\\nNo particular prompt was added (query was inputted to the model).\\n\\n# For BBH:\\n\\nWe use a preamble that describes the task, for example:\\n\\nEvaluate the result of a random Boolean expression.\\n\\nWe then provide few shot examples in the following format:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.\\n\\n<ground truth chain of thought>. So the answer is <answer>\\n\\nAnd we follow this by the query:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.\\n\\n# For each subject, We provide the subject-specific instructions as below:\\n\\n- subject: boolean_expressions\\n\\ninstruction: Conclude your answer with: \"So the answer is True or False.\".\\n- subject: causal_judgement\\n\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: date_understanding\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: disambiguation_qa\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: dyck_languages\\n\\ninstruction: Correctly close a Dyck-n word. Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.\\n- subject: formal_fallacies\\n\\ninstruction: Conclude your answer with: \"So the answer is valid or invalid.\".\\n- subject: geometric_shapes\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: hyperbaton\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_five_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_seven_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_three_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: movie_recommendation\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: multistep_arithmetic_two\\n\\ninstruction: Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'For IFEval:',\n",
              "     'md': '# For IFEval:',\n",
              "     'bBox': {'x': 72, 'y': 72.04, 'w': 46, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'No particular prompt was added (query was inputted to the model).',\n",
              "     'md': 'No particular prompt was added (query was inputted to the model).',\n",
              "     'bBox': {'x': 72, 'y': 83.04, 'w': 268, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'For BBH:',\n",
              "     'md': '# For BBH:',\n",
              "     'bBox': {'x': 72, 'y': 100.04, 'w': 39, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'We use a preamble that describes the task, for example:\\n\\nEvaluate the result of a random Boolean expression.\\n\\nWe then provide few shot examples in the following format:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.\\n\\n<ground truth chain of thought>. So the answer is <answer>\\n\\nAnd we follow this by the query:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.',\n",
              "     'md': 'We use a preamble that describes the task, for example:\\n\\nEvaluate the result of a random Boolean expression.\\n\\nWe then provide few shot examples in the following format:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.\\n\\n<ground truth chain of thought>. So the answer is <answer>\\n\\nAnd we follow this by the query:\\n\\n</preamble>\\n\\nQuestion: <question>\\n\\n<instruction>\\n\\nLet’s think step by step.',\n",
              "     'bBox': {'x': 71, 'y': 111.04, 'w': 304, 'h': 186}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'For each subject, We provide the subject-specific instructions as below:',\n",
              "     'md': '# For each subject, We provide the subject-specific instructions as below:',\n",
              "     'bBox': {'x': 72, 'y': 312.04, 'w': 284, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': '- subject: boolean_expressions\\n\\ninstruction: Conclude your answer with: \"So the answer is True or False.\".\\n- subject: causal_judgement\\n\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: date_understanding\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: disambiguation_qa\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: dyck_languages\\n\\ninstruction: Correctly close a Dyck-n word. Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.\\n- subject: formal_fallacies\\n\\ninstruction: Conclude your answer with: \"So the answer is valid or invalid.\".\\n- subject: geometric_shapes\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: hyperbaton\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_five_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_seven_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_three_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: movie_recommendation\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: multistep_arithmetic_two\\n\\ninstruction: Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.',\n",
              "     'md': '- subject: boolean_expressions\\n\\ninstruction: Conclude your answer with: \"So the answer is True or False.\".\\n- subject: causal_judgement\\n\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: date_understanding\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: disambiguation_qa\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: dyck_languages\\n\\ninstruction: Correctly close a Dyck-n word. Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.\\n- subject: formal_fallacies\\n\\ninstruction: Conclude your answer with: \"So the answer is valid or invalid.\".\\n- subject: geometric_shapes\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: hyperbaton\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_five_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_seven_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: logical_deduction_three_objects\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: movie_recommendation\\n\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: multistep_arithmetic_two\\n\\ninstruction: Conclude your answer with: \"So the answer is {final answer}.\". Make sure the final answer is in plain text format.',\n",
              "     'bBox': {'x': 72, 'y': 329.04, 'w': 453, 'h': 391}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 33,\n",
              "   'text': '                              The Amazon Nova Family of Models\\n- subject: navigate\\n instruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: object_counting\\n instruction: Conclude your answer with: \"So the answer is <ANSWER>.\". Where <ANSWER> is\\n  an integer\\n- subject: penguins_in_a_table\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: reasoning_about_colored_objects\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: ruin_names\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: salient_translation_error_detection\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: snarks\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: sports_understanding\\n instruction: Conclude your answer with: \"So the answer is yes or no.\".\\n- subject: temporal_sequences\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_five_objects\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_seven_objects\\n instruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_three_objects\\n instruction: \"Conclude your answer with: \"So the answer is (answer_letter).\". Where\\n answer_letter is A, or B, or ...\\n- subject: web_of_lies\\n instruction: Conclude your answer with: \"So the answer is Yes or No.\".\\n- subject: word_sorting\\n instruction: Conclude your answer with: \"So the answer is word_1 word_2 ... word_n.\".\"\\nFor GPQA:\\nWhat is the correct answer to this question: <question>\\nChoices: <choices>. Let’s think step by step:\\nBased on the above, what is the single, most likely answer choice? Answer in the format \"\\nThe correct answer is (insert answer here).\"\\nB.1.2 Mathematical Reasoning\\nFor MATH, GSM8K:\\nSolve the following math problem step by step.\\n<problem>\\nRemember to put your answer inside \\\\boxed{}\\n                                           33',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n- subject: navigate\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: object_counting\\ninstruction: Conclude your answer with: \"So the answer is <ANSWER>.\". Where <ANSWER> is an integer\\n- subject: penguins_in_a_table\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: reasoning_about_colored_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: ruin_names\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: salient_translation_error_detection\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: snarks\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: sports_understanding\\ninstruction: Conclude your answer with: \"So the answer is yes or no.\"\\n- subject: temporal_sequences\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_five_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_seven_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_three_objects\\ninstruction: \"Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: web_of_lies\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: word_sorting\\ninstruction: Conclude your answer with: \"So the answer is word_1 word_2 ... word_n.\"\\n\\n# B.1.2 Mathematical Reasoning\\n\\nFor MATH, GSM8K:\\n\\nSolve the following math problem step by step.\\n\\n&lt;problem&gt;\\n\\nRemember to put your answer inside \\\\boxed{}\\n\\n33',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': '- subject: navigate\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: object_counting\\ninstruction: Conclude your answer with: \"So the answer is <ANSWER>.\". Where <ANSWER> is an integer\\n- subject: penguins_in_a_table\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: reasoning_about_colored_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: ruin_names\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: salient_translation_error_detection\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: snarks\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: sports_understanding\\ninstruction: Conclude your answer with: \"So the answer is yes or no.\"\\n- subject: temporal_sequences\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_five_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_seven_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_three_objects\\ninstruction: \"Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: web_of_lies\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: word_sorting\\ninstruction: Conclude your answer with: \"So the answer is word_1 word_2 ... word_n.\"',\n",
              "     'md': '- subject: navigate\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: object_counting\\ninstruction: Conclude your answer with: \"So the answer is <ANSWER>.\". Where <ANSWER> is an integer\\n- subject: penguins_in_a_table\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: reasoning_about_colored_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: ruin_names\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: salient_translation_error_detection\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: snarks\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: sports_understanding\\ninstruction: Conclude your answer with: \"So the answer is yes or no.\"\\n- subject: temporal_sequences\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_five_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_seven_objects\\ninstruction: Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: tracking_shuffled_objects_three_objects\\ninstruction: \"Conclude your answer with: \"So the answer is (answer_letter).\". Where answer_letter is A, or B, or ...\\n- subject: web_of_lies\\ninstruction: Conclude your answer with: \"So the answer is Yes or No.\"\\n- subject: word_sorting\\ninstruction: Conclude your answer with: \"So the answer is word_1 word_2 ... word_n.\"',\n",
              "     'bBox': {'x': 72, 'y': 72.04, 'w': 464, 'h': 403}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.1.2 Mathematical Reasoning',\n",
              "     'md': '# B.1.2 Mathematical Reasoning',\n",
              "     'bBox': {'x': 72, 'y': 605.04, 'w': 137, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'For MATH, GSM8K:\\n\\nSolve the following math problem step by step.\\n\\n&lt;problem&gt;\\n\\nRemember to put your answer inside \\\\boxed{}\\n\\n33',\n",
              "     'md': 'For MATH, GSM8K:\\n\\nSolve the following math problem step by step.\\n\\n&lt;problem&gt;\\n\\nRemember to put your answer inside \\\\boxed{}\\n\\n33',\n",
              "     'bBox': {'x': 72, 'y': 626.04, 'w': 241, 'h': 124}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 34,\n",
              "   'text': '                              The Amazon Nova Family of Models\\nB.1.3  Translation\\nFor Flores:\\nNova and LLama:\\nTranslate the following text into {tgt_lang}. Please output only the translated text with\\n no prefix or introduction: {src}\\nGemini and GPT:\\nYour job is to translate a sentence from {src_lang} into {tgt_lang}. Please output ONLY\\nthe translation and nothing else: {src}\\nB.1.4  Long Context\\nFor SQuALITY (ZeroScrolls Benchmark), we use the standard prompt template for Amazon Nova and Gemini models\\nas in [69]:\\nYou are given a story and a question. Answer the question in a paragraph.\\nStory:\\n<story>\\nQuestion:\\n<question>\\nAnswer:\\nB.2 Multimodal evaluation\\nB.2.1 MMMU\\nFor multiple-choice questions:\\nWith the image, the following question, and the four possible answers (A, B, C and D),\\nselect the correct answer.\\n<question>\\n(A) <answer-a>\\n(B) <answer-b>\\n...\\n(X) <answer-x>\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n- For complex questions: Adopt this step-by-step method:\\n## Step 1: [Concise description]\\n[Brief explanation]\\n## Step 2: [Concise description]\\n[Brief explanation]\\nIn every scenario, conclude with: The best answer is [the_answer_letter]. where [\\nthe_answer_letter] is one of A, B, C or D. Let’s proceed with a systematic approach\\nFor open-ended questions:\\nWith the image and the following question, provide a correct answer.\\n<question>\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n- For complex questions: Adopt this step-by-step method:\\n                                           34',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# B.1.3 Translation\\n\\nFor Flores:\\n\\nNova and LLama:\\n\\nTranslate the following text into {tgt_lang}. Please output only the translated text with no prefix or introduction: {src}\\n\\nGemini and GPT:\\n\\nYour job is to translate a sentence from {src_lang} into {tgt_lang}. Please output ONLY the translation and nothing else: {src}\\n\\n# B.1.4 Long Context\\n\\nFor SQuALITY (ZeroScrolls Benchmark), we use the standard prompt template for Amazon Nova and Gemini models as in [69]:\\n\\nYou are given a story and a question. Answer the question in a paragraph.\\n\\nStory:\\n\\n&lt;story&gt;\\n\\nQuestion:\\n\\n&lt;question&gt;\\n\\nAnswer:\\n\\n# B.2 Multimodal evaluation\\n\\n# B.2.1 MMMU\\n\\nFor multiple-choice questions:\\n\\nWith the image, the following question, and the four possible answers (A, B, C and D), select the correct answer.\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n...\\n\\n(X) &lt;answer-x&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:\\n\\n## Step 1: [Concise description]\\n\\n[Brief explanation]\\n\\n## Step 2: [Concise description]\\n\\n[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_letter]. where [the_answer_letter] is one of A, B, C or D. Let’s proceed with a systematic approach.\\n\\nFor open-ended questions:\\n\\nWith the image and the following question, provide a correct answer.\\n\\n&lt;question&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.1.3 Translation',\n",
              "     'md': '# B.1.3 Translation',\n",
              "     'bBox': {'x': 72, 'y': 72.04, 'w': 80, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'For Flores:\\n\\nNova and LLama:\\n\\nTranslate the following text into {tgt_lang}. Please output only the translated text with no prefix or introduction: {src}\\n\\nGemini and GPT:\\n\\nYour job is to translate a sentence from {src_lang} into {tgt_lang}. Please output ONLY the translation and nothing else: {src}',\n",
              "     'md': 'For Flores:\\n\\nNova and LLama:\\n\\nTranslate the following text into {tgt_lang}. Please output only the translated text with no prefix or introduction: {src}\\n\\nGemini and GPT:\\n\\nYour job is to translate a sentence from {src_lang} into {tgt_lang}. Please output ONLY the translation and nothing else: {src}',\n",
              "     'bBox': {'x': 72, 'y': 72.04, 'w': 465, 'h': 125}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.1.4 Long Context',\n",
              "     'md': '# B.1.4 Long Context',\n",
              "     'bBox': {'x': 72, 'y': 220.04, 'w': 89, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'For SQuALITY (ZeroScrolls Benchmark), we use the standard prompt template for Amazon Nova and Gemini models as in [69]:\\n\\nYou are given a story and a question. Answer the question in a paragraph.\\n\\nStory:\\n\\n&lt;story&gt;\\n\\nQuestion:\\n\\n&lt;question&gt;\\n\\nAnswer:',\n",
              "     'md': 'For SQuALITY (ZeroScrolls Benchmark), we use the standard prompt template for Amazon Nova and Gemini models as in [69]:\\n\\nYou are given a story and a question. Answer the question in a paragraph.\\n\\nStory:\\n\\n&lt;story&gt;\\n\\nQuestion:\\n\\n&lt;question&gt;\\n\\nAnswer:',\n",
              "     'bBox': {'x': 72, 'y': 239.04, 'w': 468, 'h': 127}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.2 Multimodal evaluation',\n",
              "     'md': '# B.2 Multimodal evaluation',\n",
              "     'bBox': {'x': 72, 'y': 390.04, 'w': 121, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.2.1 MMMU',\n",
              "     'md': '# B.2.1 MMMU',\n",
              "     'bBox': {'x': 72, 'y': 390.04, 'w': 66, 'h': 31}},\n",
              "    {'type': 'text',\n",
              "     'value': 'For multiple-choice questions:\\n\\nWith the image, the following question, and the four possible answers (A, B, C and D), select the correct answer.\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n...\\n\\n(X) &lt;answer-x&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "     'md': 'For multiple-choice questions:\\n\\nWith the image, the following question, and the four possible answers (A, B, C and D), select the correct answer.\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n...\\n\\n(X) &lt;answer-x&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "     'bBox': {'x': 72, 'y': 429.04, 'w': 450, 'h': 291}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 2,\n",
              "     'value': 'Step 1: [Concise description]',\n",
              "     'md': '## Step 1: [Concise description]',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': '[Brief explanation]',\n",
              "     'md': '[Brief explanation]',\n",
              "     'bBox': {'x': 72, 'y': 568.04, 'w': 99, 'h': 32}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 2,\n",
              "     'value': 'Step 2: [Concise description]',\n",
              "     'md': '## Step 2: [Concise description]',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': '[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_letter]. where [the_answer_letter] is one of A, B, C or D. Let’s proceed with a systematic approach.\\n\\nFor open-ended questions:\\n\\nWith the image and the following question, provide a correct answer.\\n\\n&lt;question&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "     'md': '[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_letter]. where [the_answer_letter] is one of A, B, C or D. Let’s proceed with a systematic approach.\\n\\nFor open-ended questions:\\n\\nWith the image and the following question, provide a correct answer.\\n\\n&lt;question&gt;\\n\\n- For clear-cut questions: Give the answer directly with minimal elaboration.\\n\\n- For complex questions: Adopt this step-by-step method:',\n",
              "     'bBox': {'x': 72, 'y': 535.04, 'w': 434, 'h': 185}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 35,\n",
              "   'text': '                              The Amazon Nova Family of Models\\n## Step 1: [Concise description]\\n[Brief explanation]\\n## Step 2: [Concise description]\\n[Brief explanation]\\nIn every scenario, conclude with: The best answer is [the_answer_phrase]. where [\\nthe_answer_phrase] is a concise and direct answer to the question Let’s proceed with a\\nsystematic approach.\\nB.2.2  ChartQA, DocVQA, and TextVQA\\n<question>\\nAnswer the question using a single word or phrase.\\nB.2.3  VATEX\\nRender a clear and concise one-sentence summary of the video. The summary should be at\\nleast 10 words but no more than 20 words. Analyze the video first before summarizing it.\\nDo not hallucinate objects.\\nB.2.4  EgoSchema\\nYou will be given a question about a video and three possible answer options. You will be\\n provided frames from the video, sampled evenly across the video\\n<question>\\n(A) <answer-a>\\n(B) <answer-b>\\n(C) <answer-c>\\nAnswer with the option’s letter from the given choices directly.\\nAnswer with the option letter from the given choices directly.\\nB.2.5  VisualWebBench\\nFor the web captioning task:\\n\"You are given a screenshot of a webpage. Please generate the meta web description\\ninformation of this webpage, i.e., content attribute in <meta name=\"description\" content\\n=\"\"> HTML element.\\nYou should use this format, and do not output any explanation or any other contents:\\n<meta name=\"description\" content=\"YOUR ANSWER\">\\nFor the heading OCR task:\\nYou are given a screenshot of a webpage. Please generate the main text within the\\nscreenshot, which can be regarded as the heading of the webpage.\\nYou should directly tell me the first sentence of the main content, and do not output any\\n explanation or any other contents.\\nFor the web QA task:\\n<question>\\nYou should directly tell me your answer in the fewest words possible, and do not output\\nany explanation or any other contents.\\nFor the element OCR task:\\n                                           35',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# Step 1: [Concise description]\\n\\n[Brief explanation]\\n\\n# Step 2: [Concise description]\\n\\n[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_phrase]. where [the_answer_phrase] is a concise and direct answer to the question Let’s proceed with a systematic approach.\\n\\n# B.2.2  ChartQA, DocVQA, and TextVQA\\n\\n&lt;question&gt;\\n\\nAnswer the question using a single word or phrase.\\n\\n# B.2.3  VATEX\\n\\nRender a clear and concise one-sentence summary of the video. The summary should be at least 10 words but no more than 20 words. Analyze the video first before summarizing it. Do not hallucinate objects.\\n\\n# B.2.4  EgoSchema\\n\\nYou will be given a question about a video and three possible answer options. You will be provided frames from the video, sampled evenly across the video\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n(C) &lt;answer-c&gt;\\n\\nAnswer with the option’s letter from the given choices directly.\\n\\nAnswer with the option letter from the given choices directly.\\n\\n# B.2.5  VisualWebBench\\n\\nFor the web captioning task:\\n\\n\"You are given a screenshot of a webpage. Please generate the meta web description information of this webpage, i.e., content attribute in &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt; HTML element.\\n\\nYou should use this format, and do not output any explanation or any other contents:\\n\\n&lt;meta name=&quot;description&quot; content=&quot;YOUR ANSWER&quot;&gt;\\n\\nFor the heading OCR task:\\n\\nYou are given a screenshot of a webpage. Please generate the main text within the screenshot, which can be regarded as the heading of the webpage.\\n\\nYou should directly tell me the first sentence of the main content, and do not output any explanation or any other contents.\\n\\nFor the web QA task:\\n\\n&lt;question&gt;\\n\\nYou should directly tell me your answer in the fewest words possible, and do not output any explanation or any other contents.\\n\\nFor the element OCR task:',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Step 1: [Concise description]',\n",
              "     'md': '# Step 1: [Concise description]',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': '[Brief explanation]',\n",
              "     'md': '[Brief explanation]',\n",
              "     'bBox': {'x': 72, 'y': 83.04, 'w': 99, 'h': 32}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Step 2: [Concise description]',\n",
              "     'md': '# Step 2: [Concise description]',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': '[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_phrase]. where [the_answer_phrase] is a concise and direct answer to the question Let’s proceed with a systematic approach.',\n",
              "     'md': '[Brief explanation]\\n\\nIn every scenario, conclude with: The best answer is [the_answer_phrase]. where [the_answer_phrase] is a concise and direct answer to the question Let’s proceed with a systematic approach.',\n",
              "     'bBox': {'x': 72, 'y': 83.04, 'w': 450, 'h': 76}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.2.2  ChartQA, DocVQA, and TextVQA',\n",
              "     'md': '# B.2.2  ChartQA, DocVQA, and TextVQA',\n",
              "     'bBox': {'x': 72, 'y': 180.04, 'w': 179, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': '&lt;question&gt;\\n\\nAnswer the question using a single word or phrase.',\n",
              "     'md': '&lt;question&gt;\\n\\nAnswer the question using a single word or phrase.',\n",
              "     'bBox': {'x': 72, 'y': 211.04, 'w': 261, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.2.3  VATEX',\n",
              "     'md': '# B.2.3  VATEX',\n",
              "     'bBox': {'x': 72, 'y': 243.04, 'w': 64, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Render a clear and concise one-sentence summary of the video. The summary should be at least 10 words but no more than 20 words. Analyze the video first before summarizing it. Do not hallucinate objects.',\n",
              "     'md': 'Render a clear and concise one-sentence summary of the video. The summary should be at least 10 words but no more than 20 words. Analyze the video first before summarizing it. Do not hallucinate objects.',\n",
              "     'bBox': {'x': 72, 'y': 263.04, 'w': 460, 'h': 31}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.2.4  EgoSchema',\n",
              "     'md': '# B.2.4  EgoSchema',\n",
              "     'bBox': {'x': 72, 'y': 316.04, 'w': 81, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'You will be given a question about a video and three possible answer options. You will be provided frames from the video, sampled evenly across the video\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n(C) &lt;answer-c&gt;\\n\\nAnswer with the option’s letter from the given choices directly.\\n\\nAnswer with the option letter from the given choices directly.',\n",
              "     'md': 'You will be given a question about a video and three possible answer options. You will be provided frames from the video, sampled evenly across the video\\n\\n&lt;question&gt;\\n\\n(A) &lt;answer-a&gt;\\n\\n(B) &lt;answer-b&gt;\\n\\n(C) &lt;answer-c&gt;\\n\\nAnswer with the option’s letter from the given choices directly.\\n\\nAnswer with the option letter from the given choices directly.',\n",
              "     'bBox': {'x': 72, 'y': 336.04, 'w': 465, 'h': 86}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.2.5  VisualWebBench',\n",
              "     'md': '# B.2.5  VisualWebBench',\n",
              "     'bBox': {'x': 72, 'y': 444.04, 'w': 104, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'For the web captioning task:\\n\\n\"You are given a screenshot of a webpage. Please generate the meta web description information of this webpage, i.e., content attribute in &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt; HTML element.\\n\\nYou should use this format, and do not output any explanation or any other contents:\\n\\n&lt;meta name=&quot;description&quot; content=&quot;YOUR ANSWER&quot;&gt;\\n\\nFor the heading OCR task:\\n\\nYou are given a screenshot of a webpage. Please generate the main text within the screenshot, which can be regarded as the heading of the webpage.\\n\\nYou should directly tell me the first sentence of the main content, and do not output any explanation or any other contents.\\n\\nFor the web QA task:\\n\\n&lt;question&gt;\\n\\nYou should directly tell me your answer in the fewest words possible, and do not output any explanation or any other contents.\\n\\nFor the element OCR task:',\n",
              "     'md': 'For the web captioning task:\\n\\n\"You are given a screenshot of a webpage. Please generate the meta web description information of this webpage, i.e., content attribute in &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt; HTML element.\\n\\nYou should use this format, and do not output any explanation or any other contents:\\n\\n&lt;meta name=&quot;description&quot; content=&quot;YOUR ANSWER&quot;&gt;\\n\\nFor the heading OCR task:\\n\\nYou are given a screenshot of a webpage. Please generate the main text within the screenshot, which can be regarded as the heading of the webpage.\\n\\nYou should directly tell me the first sentence of the main content, and do not output any explanation or any other contents.\\n\\nFor the web QA task:\\n\\n&lt;question&gt;\\n\\nYou should directly tell me your answer in the fewest words possible, and do not output any explanation or any other contents.\\n\\nFor the element OCR task:',\n",
              "     'bBox': {'x': 72, 'y': 462.04, 'w': 465, 'h': 258}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 36,\n",
              "   'text': '                              The Amazon Nova Family of Models\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease perform OCR in the bounding box and recognize the text content within the red\\nbounding box.\\nFor the action prediction task:\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease select the best webpage description that matches the new webpage after clicking\\nthe selected element in the bounding box:\\n<choices_text>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the element grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one best matches the description: <element_desc>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the action grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one I should click to complete the following task: <instruction>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nB.2.6 MM-Mind2Web\\nImagine that you are imitating humans doing web navigation for a task step by step. At\\neach stage, you can see the webpage like humans by a screenshot and know the previous\\nactions before the current step decided by yourself through recorded history. You need to\\n decide on the first following action to take. You can click on an element with the mouse\\n, select an option, type text or press Enter with the keyboard. (For your understanding,\\nthey are like the click(), select_option() type() functions in playwright respectively).\\nOne next step means one operation within the three.\\nYou are asked to complete the following task: <question>\\nPrevious Actions:\\n<previous_actions>\\nThe screenshot below shows the webpage you see.\\nFollow the following guidance to think step by step before outlining the next action step\\n at the current stage:\\n(Current Webpage Identification)\\nFirstly, think about what the current webpage is.\\n(Previous Action Analysis)\\n                                           36',\n",
              "   'md': '                              The Amazon Nova Family of Models\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease perform OCR in the bounding box and recognize the text content within the red\\nbounding box.\\nFor the action prediction task:\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease select the best webpage description that matches the new webpage after clicking\\nthe selected element in the bounding box:\\n<choices_text>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the element grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one best matches the description: <element_desc>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the action grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one I should click to complete the following task: <instruction>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nB.2.6 MM-Mind2Web\\nImagine that you are imitating humans doing web navigation for a task step by step. At\\neach stage, you can see the webpage like humans by a screenshot and know the previous\\nactions before the current step decided by yourself through recorded history. You need to\\n decide on the first following action to take. You can click on an element with the mouse\\n, select an option, type text or press Enter with the keyboard. (For your understanding,\\nthey are like the click(), select_option() type() functions in playwright respectively).\\nOne next step means one operation within the three.\\nYou are asked to complete the following task: <question>\\nPrevious Actions:\\n<previous_actions>\\nThe screenshot below shows the webpage you see.\\nFollow the following guidance to think step by step before outlining the next action step\\n at the current stage:\\n(Current Webpage Identification)\\nFirstly, think about what the current webpage is.\\n(Previous Action Analysis)\\n                                           36',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'text',\n",
              "     'value': 'The Amazon Nova Family of Models\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease perform OCR in the bounding box and recognize the text content within the red\\nbounding box.\\nFor the action prediction task:\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease select the best webpage description that matches the new webpage after clicking\\nthe selected element in the bounding box:\\n<choices_text>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the element grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one best matches the description: <element_desc>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the action grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one I should click to complete the following task: <instruction>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nB.2.6 MM-Mind2Web\\nImagine that you are imitating humans doing web navigation for a task step by step. At\\neach stage, you can see the webpage like humans by a screenshot and know the previous\\nactions before the current step decided by yourself through recorded history. You need to\\n decide on the first following action to take. You can click on an element with the mouse\\n, select an option, type text or press Enter with the keyboard. (For your understanding,\\nthey are like the click(), select_option() type() functions in playwright respectively).\\nOne next step means one operation within the three.\\nYou are asked to complete the following task: <question>\\nPrevious Actions:\\n<previous_actions>\\nThe screenshot below shows the webpage you see.\\nFollow the following guidance to think step by step before outlining the next action step\\n at the current stage:\\n(Current Webpage Identification)\\nFirstly, think about what the current webpage is.\\n(Previous Action Analysis)\\n                                           36',\n",
              "     'md': 'The Amazon Nova Family of Models\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease perform OCR in the bounding box and recognize the text content within the red\\nbounding box.\\nFor the action prediction task:\\nYou are given a screenshot of a webpage with a red rectangle bounding box. The [x1, y1,\\nx2, y2] coordinates of the bounding box is <bbox_coords>.\\nPlease select the best webpage description that matches the new webpage after clicking\\nthe selected element in the bounding box:\\n<choices_text>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the element grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one best matches the description: <element_desc>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nFor the action grounding task:\\nIn this website screenshot, I have labeled IDs for some HTML elements as candicates. Tell\\n me which one I should click to complete the following task: <instruction>\\nYou should directly tell me your choice in a single uppercase letter, and do not output\\nany explanation or any other contents.\\nB.2.6 MM-Mind2Web\\nImagine that you are imitating humans doing web navigation for a task step by step. At\\neach stage, you can see the webpage like humans by a screenshot and know the previous\\nactions before the current step decided by yourself through recorded history. You need to\\n decide on the first following action to take. You can click on an element with the mouse\\n, select an option, type text or press Enter with the keyboard. (For your understanding,\\nthey are like the click(), select_option() type() functions in playwright respectively).\\nOne next step means one operation within the three.\\nYou are asked to complete the following task: <question>\\nPrevious Actions:\\n<previous_actions>\\nThe screenshot below shows the webpage you see.\\nFollow the following guidance to think step by step before outlining the next action step\\n at the current stage:\\n(Current Webpage Identification)\\nFirstly, think about what the current webpage is.\\n(Previous Action Analysis)\\n                                           36',\n",
              "     'bBox': {'x': 72, 'y': 34.04, 'w': 465, 'h': 716}}],\n",
              "   'status': 'WARNING',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': True,\n",
              "   'noTextContent': False},\n",
              "  {'page': 37,\n",
              "   'text': '                              The Amazon Nova Family of Models\\nSecondly, combined with the screenshot, analyze each step of the previous action history\\nand their intention one by one. Particularly, pay more attention to the last step, which\\nmay be more related to what you should do now as the next step.\\n(Screenshot Details Analysis)\\nClosely examine the screenshot to check the status of every part of the webpage to\\nunderstand what you can operate with and what has been set or completed. You should\\nclosely examine the screenshot details to see what steps have been completed by previous\\nactions even though you are given the textual previous actions. Because the textual\\nhistory may not clearly and sufficiently record some effects of previous actions, you\\nshould closely evaluate the status of every part of the webpage to understand what you\\nhave done.\\n(Next Action Based on Webpage and Analysis)\\nThen, based on your analysis, in conjunction with human web browsing habits and the logic\\n of web design, decide on the following action. And clearly outline which element in the\\nwebpage users will operate with as the first next target element, its detailed location,\\nand the corresponding operation.\\nTo be successful, it is important to follow the following rules:\\n1. You should only issue a valid action given the current observation.\\n2. You should only issue one action at a time.\\n(Reiteration)\\nFirst, reiterate your next target element, its detailed location, and the corresponding\\noperation.\\n(Multichoice Question)\\nBelow is a multi-choice question, where the choices are elements in the webpage. From the\\n screenshot, find out where and what each one is on the webpage. Then, determine whether\\none matches your target element. Please examine the choices one by one. Choose the\\nmatching one. If multiple options match your answer, choose the most likely one by re-\\nexamining the screenshot, the choices, and your further reasoning.\\nIf none of these elements match your target element, please select, select <none_choice>.\\n None of the other options match the correct element.\\n<choices><none_choice>. None of the other options match the correct element.\\n(Final Answer)Finally, conclude your answer using the format below. Ensure your answer is\\n strictly adhering to the format provided below. Please do not leave any explanation in\\nyour answers of the final standardized format part, and this final part should be clear\\nand certain. The element choice, action, and value should be in three separate lines.\\nFormat:\\nELEMENT: The uppercase letter of your choice.\\nACTION: Choose an action from {CLICK, TYPE, SELECT, NONE}. Use NONE only if you choose\\noption F for the ELEMENT\\nVALUE: Provide additional input based on ACTION.\\nThe VALUE means:\\nIf ACTION == TYPE, specify the text to be typed.\\nIf ACTION == SELECT, specify the option to be chosen.\\nIf ACTION == CLICK, write \"None\".\\nB.2.7 GroundUI-1K\\n                                           37',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\nSecondly, combined with the screenshot, analyze each step of the previous action history and their intention one by one. Particularly, pay more attention to the last step, which may be more related to what you should do now as the next step.\\n\\n# (Screenshot Details Analysis)\\n\\nClosely examine the screenshot to check the status of every part of the webpage to understand what you can operate with and what has been set or completed. You should closely examine the screenshot details to see what steps have been completed by previous actions even though you are given the textual previous actions. Because the textual history may not clearly and sufficiently record some effects of previous actions, you should closely evaluate the status of every part of the webpage to understand what you have done.\\n\\n# (Next Action Based on Webpage and Analysis)\\n\\nThen, based on your analysis, in conjunction with human web browsing habits and the logic of web design, decide on the following action. And clearly outline which element in the webpage users will operate with as the first next target element, its detailed location, and the corresponding operation.\\n\\nTo be successful, it is important to follow the following rules:\\n\\n1. You should only issue a valid action given the current observation.\\n2. You should only issue one action at a time.\\n\\n# (Reiteration)\\n\\nFirst, reiterate your next target element, its detailed location, and the corresponding operation.\\n\\n# (Multichoice Question)\\n\\nBelow is a multi-choice question, where the choices are elements in the webpage. From the screenshot, find out where and what each one is on the webpage. Then, determine whether one matches your target element. Please examine the choices one by one. Choose the matching one. If multiple options match your answer, choose the most likely one by re-examining the screenshot, the choices, and your further reasoning.\\n\\nIf none of these elements match your target element, please select, select <none_choice>. None of the other options match the correct element.\\n\\n# (Final Answer)\\n\\nFinally, conclude your answer using the format below. Ensure your answer is strictly adhering to the format provided below. Please do not leave any explanation in your answers of the final standardized format part, and this final part should be clear and certain. The element choice, action, and value should be in three separate lines.\\n\\nFormat:\\n\\nELEMENT: The uppercase letter of your choice.\\n\\nACTION: Choose an action from {CLICK, TYPE, SELECT, NONE}. Use NONE only if you choose option F for the ELEMENT\\n\\nVALUE: Provide additional input based on ACTION.\\n\\nThe VALUE means:\\n\\n- If ACTION == TYPE, specify the text to be typed.\\n- If ACTION == SELECT, specify the option to be chosen.\\n- If ACTION == CLICK, write \"None\".\\n\\nB.2.7 GroundUI-1K',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Secondly, combined with the screenshot, analyze each step of the previous action history and their intention one by one. Particularly, pay more attention to the last step, which may be more related to what you should do now as the next step.',\n",
              "     'md': 'Secondly, combined with the screenshot, analyze each step of the previous action history and their intention one by one. Particularly, pay more attention to the last step, which may be more related to what you should do now as the next step.',\n",
              "     'bBox': {'x': 72, 'y': 72.04, 'w': 460, 'h': 32}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '(Screenshot Details Analysis)',\n",
              "     'md': '# (Screenshot Details Analysis)',\n",
              "     'bBox': {'x': 72, 'y': 116.04, 'w': 152, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Closely examine the screenshot to check the status of every part of the webpage to understand what you can operate with and what has been set or completed. You should closely examine the screenshot details to see what steps have been completed by previous actions even though you are given the textual previous actions. Because the textual history may not clearly and sufficiently record some effects of previous actions, you should closely evaluate the status of every part of the webpage to understand what you have done.',\n",
              "     'md': 'Closely examine the screenshot to check the status of every part of the webpage to understand what you can operate with and what has been set or completed. You should closely examine the screenshot details to see what steps have been completed by previous actions even though you are given the textual previous actions. Because the textual history may not clearly and sufficiently record some effects of previous actions, you should closely evaluate the status of every part of the webpage to understand what you have done.',\n",
              "     'bBox': {'x': 72, 'y': 127.04, 'w': 460, 'h': 75}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '(Next Action Based on Webpage and Analysis)',\n",
              "     'md': '# (Next Action Based on Webpage and Analysis)',\n",
              "     'bBox': {'x': 72, 'y': 214.04, 'w': 225, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Then, based on your analysis, in conjunction with human web browsing habits and the logic of web design, decide on the following action. And clearly outline which element in the webpage users will operate with as the first next target element, its detailed location, and the corresponding operation.\\n\\nTo be successful, it is important to follow the following rules:\\n\\n1. You should only issue a valid action given the current observation.\\n2. You should only issue one action at a time.',\n",
              "     'md': 'Then, based on your analysis, in conjunction with human web browsing habits and the logic of web design, decide on the following action. And clearly outline which element in the webpage users will operate with as the first next target element, its detailed location, and the corresponding operation.\\n\\nTo be successful, it is important to follow the following rules:\\n\\n1. You should only issue a valid action given the current observation.\\n2. You should only issue one action at a time.',\n",
              "     'bBox': {'x': 72, 'y': 225.04, 'w': 465, 'h': 130}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '(Reiteration)',\n",
              "     'md': '# (Reiteration)',\n",
              "     'bBox': {'x': 72, 'y': 323.04, 'w': 68, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'First, reiterate your next target element, its detailed location, and the corresponding operation.',\n",
              "     'md': 'First, reiterate your next target element, its detailed location, and the corresponding operation.',\n",
              "     'bBox': {'x': 72, 'y': 258.04, 'w': 455, 'h': 97}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '(Multichoice Question)',\n",
              "     'md': '# (Multichoice Question)',\n",
              "     'bBox': {'x': 72, 'y': 367.04, 'w': 115, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Below is a multi-choice question, where the choices are elements in the webpage. From the screenshot, find out where and what each one is on the webpage. Then, determine whether one matches your target element. Please examine the choices one by one. Choose the matching one. If multiple options match your answer, choose the most likely one by re-examining the screenshot, the choices, and your further reasoning.\\n\\nIf none of these elements match your target element, please select, select <none_choice>. None of the other options match the correct element.',\n",
              "     'md': 'Below is a multi-choice question, where the choices are elements in the webpage. From the screenshot, find out where and what each one is on the webpage. Then, determine whether one matches your target element. Please examine the choices one by one. Choose the matching one. If multiple options match your answer, choose the most likely one by re-examining the screenshot, the choices, and your further reasoning.\\n\\nIf none of these elements match your target element, please select, select <none_choice>. None of the other options match the correct element.',\n",
              "     'bBox': {'x': 72, 'y': 378.04, 'w': 465, 'h': 86}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': '(Final Answer)',\n",
              "     'md': '# (Final Answer)',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Finally, conclude your answer using the format below. Ensure your answer is strictly adhering to the format provided below. Please do not leave any explanation in your answers of the final standardized format part, and this final part should be clear and certain. The element choice, action, and value should be in three separate lines.\\n\\nFormat:\\n\\nELEMENT: The uppercase letter of your choice.\\n\\nACTION: Choose an action from {CLICK, TYPE, SELECT, NONE}. Use NONE only if you choose option F for the ELEMENT\\n\\nVALUE: Provide additional input based on ACTION.\\n\\nThe VALUE means:\\n\\n- If ACTION == TYPE, specify the text to be typed.\\n- If ACTION == SELECT, specify the option to be chosen.\\n- If ACTION == CLICK, write \"None\".\\n\\nB.2.7 GroundUI-1K',\n",
              "     'md': 'Finally, conclude your answer using the format below. Ensure your answer is strictly adhering to the format provided below. Please do not leave any explanation in your answers of the final standardized format part, and this final part should be clear and certain. The element choice, action, and value should be in three separate lines.\\n\\nFormat:\\n\\nELEMENT: The uppercase letter of your choice.\\n\\nACTION: Choose an action from {CLICK, TYPE, SELECT, NONE}. Use NONE only if you choose option F for the ELEMENT\\n\\nVALUE: Provide additional input based on ACTION.\\n\\nThe VALUE means:\\n\\n- If ACTION == TYPE, specify the text to be typed.\\n- If ACTION == SELECT, specify the option to be chosen.\\n- If ACTION == CLICK, write \"None\".\\n\\nB.2.7 GroundUI-1K',\n",
              "     'bBox': {'x': 72, 'y': 498.04, 'w': 455, 'h': 220}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 38,\n",
              "   'text': '                              The Amazon Nova Family of Models\\nWhich action should I do if I want to Click on <element> and where is the action? Express\\n the location coordinates using the (x1, y1, x2, y2) format, scaled between 0 and 1000.\\nB.3 Functional Capabilities\\nB.3.1 FinQA\\nGiven the following finance question, analyze the question in details step-by-step before\\n giving the final answer. Your answer should begin with \"Lets think step-by-step\". Your\\nresponse should end with \"The answer is [the_final_answer]\", where [the_final_answer]\\nshould be the most concise answer without any explanation.\\n### Input\\nSupporting Facts:\\n<pre-text>\\n<table>\\n<post-text>\\nQuestion:\\n<question>\\nWe use regex “The answer is (.*)” to extract the answer. We convert answers with percent signs and magnitude terms to\\ndecimal numerical representation (e.g. convert “1.3%” to 0.013 and “5.2 millions” to 5,200,000). An answer is correct\\nif it is identical to the ground truth when rounded to the same decimal places.\\nB.3.2 RAG\\nYou are a teacher grading a quiz.\\nYou are given a question, the student’s answer, and the true answer, and are asked to\\nscore the student answer as either Correct or Incorrect.\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student’s answer here\\nTRUE ANSWER: true answer here\\nGRADE: Correct or Incorrect here\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in\\npunctuation and phrasing between the student answer and true answer. It is OK if the\\nstudent answer contains more information than the true answer, as long as it does not\\ncontain any conflicting statements. Begin!\\nQUESTION: {query}\\nSTUDENT ANSWER: {answer}\\nTRUE ANSWER: {expected_answer}\\nGRADE:\\nYour response should be in json format as follows:\\n{{\\n \"justification\": (Without mentioning the student/teacher framing of this prompt, explain\\n  why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep\\n the answer as concise as possible.),\\n \"grade\": (correct or incorrect)\\n}}\\n                                           38',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\nWhich action should I do if I want to Click on &lt;element&gt; and where is the action? Express the location coordinates using the (x1, y1, x2, y2) format, scaled between 0 and 1000.\\n\\n# B.3 Functional Capabilities\\n\\n# B.3.1 FinQA\\n\\nGiven the following finance question, analyze the question in details step-by-step before giving the final answer. Your answer should begin with \"Lets think step-by-step\". Your response should end with \"The answer is [the_final_answer]\", where [the_final_answer] should be the most concise answer without any explanation.\\n\\n# Input\\n\\nSupporting Facts:\\n\\nQuestion:\\n\\n&lt;question&gt;\\nWe use regex “The answer is (.*)” to extract the answer. We convert answers with percent signs and magnitude terms to decimal numerical representation (e.g. convert “1.3%” to 0.013 and “5.2 millions” to 5,200,000). An answer is correct if it is identical to the ground truth when rounded to the same decimal places.\\n\\n# B.3.2 RAG\\n\\nYou are a teacher grading a quiz. You are given a question, the student’s answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\\n\\n# Example Format:\\n\\nQUESTION: question here\\n\\nSTUDENT ANSWER: student’s answer here\\n\\nTRUE ANSWER: true answer here\\n\\nGRADE: Correct or Incorrect here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\n\\nSTUDENT ANSWER: {answer}\\n\\nTRUE ANSWER: {expected_answer}\\n\\nGRADE:\\n\\nYour response should be in json format as follows:\\n\\n{\\n\"justification\": (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.),\\n\"grade\": (correct or incorrect)\\n}',\n",
              "   'images': [],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Which action should I do if I want to Click on &lt;element&gt; and where is the action? Express the location coordinates using the (x1, y1, x2, y2) format, scaled between 0 and 1000.',\n",
              "     'md': 'Which action should I do if I want to Click on &lt;element&gt; and where is the action? Express the location coordinates using the (x1, y1, x2, y2) format, scaled between 0 and 1000.',\n",
              "     'bBox': {'x': 77, 'y': 91.04, 'w': 450, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.3 Functional Capabilities',\n",
              "     'md': '# B.3 Functional Capabilities',\n",
              "     'bBox': {'x': 72, 'y': 125.04, 'w': 123, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.3.1 FinQA',\n",
              "     'md': '# B.3.1 FinQA',\n",
              "     'bBox': {'x': 72, 'y': 125.04, 'w': 60, 'h': 30}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Given the following finance question, analyze the question in details step-by-step before giving the final answer. Your answer should begin with \"Lets think step-by-step\". Your response should end with \"The answer is [the_final_answer]\", where [the_final_answer] should be the most concise answer without any explanation.',\n",
              "     'md': 'Given the following finance question, analyze the question in details step-by-step before giving the final answer. Your answer should begin with \"Lets think step-by-step\". Your response should end with \"The answer is [the_final_answer]\", where [the_final_answer] should be the most concise answer without any explanation.',\n",
              "     'bBox': {'x': 72, 'y': 165.04, 'w': 465, 'h': 43}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Input',\n",
              "     'md': '# Input',\n",
              "     'bBox': {'x': 0, 'y': 0, 'w': 612, 'h': 792}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Supporting Facts:\\n\\nQuestion:\\n\\n&lt;question&gt;\\nWe use regex “The answer is (.*)” to extract the answer. We convert answers with percent signs and magnitude terms to decimal numerical representation (e.g. convert “1.3%” to 0.013 and “5.2 millions” to 5,200,000). An answer is correct if it is identical to the ground truth when rounded to the same decimal places.',\n",
              "     'md': 'Supporting Facts:\\n\\nQuestion:\\n\\n&lt;question&gt;\\nWe use regex “The answer is (.*)” to extract the answer. We convert answers with percent signs and magnitude terms to decimal numerical representation (e.g. convert “1.3%” to 0.013 and “5.2 millions” to 5,200,000). An answer is correct if it is identical to the ground truth when rounded to the same decimal places.',\n",
              "     'bBox': {'x': 71, 'y': 230.04, 'w': 469, 'h': 123}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'B.3.2 RAG',\n",
              "     'md': '# B.3.2 RAG',\n",
              "     'bBox': {'x': 72, 'y': 125.04, 'w': 53, 'h': 251}},\n",
              "    {'type': 'text',\n",
              "     'value': 'You are a teacher grading a quiz. You are given a question, the student’s answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.',\n",
              "     'md': 'You are a teacher grading a quiz. You are given a question, the student’s answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.',\n",
              "     'bBox': {'x': 72, 'y': 387.04, 'w': 444, 'h': 31}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Example Format:',\n",
              "     'md': '# Example Format:',\n",
              "     'bBox': {'x': 72, 'y': 419.04, 'w': 78, 'h': 10}},\n",
              "    {'type': 'text',\n",
              "     'value': 'QUESTION: question here\\n\\nSTUDENT ANSWER: student’s answer here\\n\\nTRUE ANSWER: true answer here\\n\\nGRADE: Correct or Incorrect here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\n\\nSTUDENT ANSWER: {answer}\\n\\nTRUE ANSWER: {expected_answer}\\n\\nGRADE:\\n\\nYour response should be in json format as follows:\\n\\n{\\n\"justification\": (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.),\\n\"grade\": (correct or incorrect)\\n}',\n",
              "     'md': 'QUESTION: question here\\n\\nSTUDENT ANSWER: student’s answer here\\n\\nTRUE ANSWER: true answer here\\n\\nGRADE: Correct or Incorrect here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\\n\\nQUESTION: {query}\\n\\nSTUDENT ANSWER: {answer}\\n\\nTRUE ANSWER: {expected_answer}\\n\\nGRADE:\\n\\nYour response should be in json format as follows:\\n\\n{\\n\"justification\": (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.),\\n\"grade\": (correct or incorrect)\\n}',\n",
              "     'bBox': {'x': 72, 'y': 285.04, 'w': 464, 'h': 341}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 39,\n",
              "   'text': \"                                      The Amazon Nova Family of Models\\nC    Qualitative examples of multimodal intelligence\\n       User input                                                Response\\n  This looks like an amazing site. What is it and where      This is the Space Needle, a landmark in Seattle,\\n  is it located? Are there other fun things around this      Washington, USA. It is a well-known observation\\n  site to explore?                                           tower and asymbol of Seattle: It was built for the\\n                                                             1962 World's Fair and has since become a popular\\n                                                             tourist attraction. The Space Needle offers\\n                                                             panoramic views of thecity,the surrounding\\n                                                             mountains, and the Puget Sound.\\n                                                             The Space Needle is located in the Seattle Center;\\n                                                             which is a culturaland entertainment hub. The\\n                                                             Seattle Center is home to many other attractions,\\n                                                             including the Chihuly Garden and Glass, the\\n                                                             Museum of Pop Culture, and the Pacific Science\\n                                                             Center: The Seattle Center also hosts events like\\n                                                             concerts, festivals, and art exhibitions.\\n                                                             The Seattle Center is also close to other popular\\n                                                             attractions in Seattle, such as the Pike Place Market,\\n                                                             the Seattle Aquarium, and the Seattle Art Museum:\\n                                                             The city is known for its vibrant arts and culture\\n                                                             scene, outdoor activities, and beautiful natural\\n                                                             surroundings.\\n                         Figure 6: Created with Nova Pro. Photo taken by a team member.\\n                                                        39\",\n",
              "   'md': \"# The Amazon Nova Family of Models\\n\\n# Qualitative examples of multimodal intelligence\\n\\n|User input|Response|\\n|---|---|\\n|This looks like an amazing site. What is it and where is it located? Are there other fun things around this site to explore?|This is the Space Needle, a landmark in Seattle, Washington, USA. It is a well-known observation tower and a symbol of Seattle: It was built for the 1962 World's Fair and has since become a popular tourist attraction. The Space Needle offers panoramic views of the city, the surrounding mountains, and the Puget Sound. The Space Needle is located in the Seattle Center; which is a cultural and entertainment hub. The Seattle Center is home to many other attractions, including the Chihuly Garden and Glass, the Museum of Pop Culture, and the Pacific Science Center: The Seattle Center also hosts events like concerts, festivals, and art exhibitions. The Seattle Center is also close to other popular attractions in Seattle, such as the Pike Place Market, the Seattle Aquarium, and the Seattle Art Museum: The city is known for its vibrant arts and culture scene, outdoor activities, and beautiful natural surroundings.|\\n\\nFigure 6: Created with Nova Pro. Photo taken by a team member.\\n\\n39\",\n",
              "   'images': [{'name': 'img_p38_1.png',\n",
              "     'height': 1837,\n",
              "     'width': 3112,\n",
              "     'x': 72,\n",
              "     'y': 103.05993999999998,\n",
              "     'original_width': 3112,\n",
              "     'original_height': 1837,\n",
              "     'ocr': [{'x': 205,\n",
              "       'y': 62,\n",
              "       'w': 366,\n",
              "       'h': 90,\n",
              "       'confidence': '0.6628547286717683',\n",
              "       'text': 'User input'},\n",
              "      {'x': 1757,\n",
              "       'y': 61,\n",
              "       'w': 340,\n",
              "       'h': 101,\n",
              "       'confidence': '0.9999955324299241',\n",
              "       'text': 'Response'},\n",
              "      {'x': 68,\n",
              "       'y': 255,\n",
              "       'w': 1366,\n",
              "       'h': 86,\n",
              "       'confidence': '0.7810586773935181',\n",
              "       'text': 'This looks like an amazing site. What is it and where'},\n",
              "      {'x': 1651,\n",
              "       'y': 262,\n",
              "       'w': 1244,\n",
              "       'h': 85,\n",
              "       'confidence': '0.79727348576406',\n",
              "       'text': 'This is the Space Needle, a landmark in Seattle,'},\n",
              "      {'x': 68,\n",
              "       'y': 335,\n",
              "       'w': 1353,\n",
              "       'h': 86,\n",
              "       'confidence': '0.7041544994948097',\n",
              "       'text': 'is it located? Are there other fun things around this'},\n",
              "      {'x': 1654,\n",
              "       'y': 336,\n",
              "       'w': 1272,\n",
              "       'h': 86,\n",
              "       'confidence': '0.8550619505172774',\n",
              "       'text': 'Washington, USA. It is a well-known observation'},\n",
              "      {'x': 69,\n",
              "       'y': 417,\n",
              "       'w': 420,\n",
              "       'h': 80,\n",
              "       'confidence': '0.6981948604601093',\n",
              "       'text': 'site to explore?'},\n",
              "      {'x': 1650,\n",
              "       'y': 420,\n",
              "       'w': 324,\n",
              "       'h': 61,\n",
              "       'confidence': '0.8698726592883513',\n",
              "       'text': 'tower and a'},\n",
              "      {'x': 1973,\n",
              "       'y': 412,\n",
              "       'w': 980,\n",
              "       'h': 87,\n",
              "       'confidence': '0.7442992882417068',\n",
              "       'text': 'symbol of Seattle: It was built for the'},\n",
              "      {'x': 1653,\n",
              "       'y': 493,\n",
              "       'w': 1324,\n",
              "       'h': 86,\n",
              "       'confidence': '0.8500821333386501',\n",
              "       'text': \"1962 World's Fair and has since become a popular\"},\n",
              "      {'x': 1649,\n",
              "       'y': 572,\n",
              "       'w': 1118,\n",
              "       'h': 75,\n",
              "       'confidence': '0.9542317425274454',\n",
              "       'text': 'tourist attraction. The Space Needle offers'},\n",
              "      {'x': 1650,\n",
              "       'y': 649,\n",
              "       'w': 624,\n",
              "       'h': 77,\n",
              "       'confidence': '0.7315620401156177',\n",
              "       'text': 'panoramic views of the'},\n",
              "      {'x': 2388,\n",
              "       'y': 643,\n",
              "       'w': 439,\n",
              "       'h': 81,\n",
              "       'confidence': '0.983363656053505',\n",
              "       'text': 'the surrounding'},\n",
              "      {'x': 1654,\n",
              "       'y': 727,\n",
              "       'w': 880,\n",
              "       'h': 78,\n",
              "       'confidence': '0.4865689817306662',\n",
              "       'text': 'mountains, and the Puget Sound.'},\n",
              "      {'x': 1652,\n",
              "       'y': 802,\n",
              "       'w': 1302,\n",
              "       'h': 79,\n",
              "       'confidence': '0.8627259401501235',\n",
              "       'text': 'The Space Needle is located in the Seattle Center;'},\n",
              "      {'x': 1655,\n",
              "       'y': 882,\n",
              "       'w': 1228,\n",
              "       'h': 66,\n",
              "       'confidence': '0.8050970984365342',\n",
              "       'text': 'which is a culturaland entertainment hub. The'},\n",
              "      {'x': 1652,\n",
              "       'y': 954,\n",
              "       'w': 1301,\n",
              "       'h': 76,\n",
              "       'confidence': '0.8879288367708251',\n",
              "       'text': 'Seattle Center is home to many other attractions,'},\n",
              "      {'x': 1647,\n",
              "       'y': 1026,\n",
              "       'w': 1158,\n",
              "       'h': 85,\n",
              "       'confidence': '0.7586642731645438',\n",
              "       'text': 'including the Chihuly Garden and Glass, the'},\n",
              "      {'x': 1656,\n",
              "       'y': 1111,\n",
              "       'w': 1250,\n",
              "       'h': 74,\n",
              "       'confidence': '0.7087349628305936',\n",
              "       'text': 'Museum of Pop Culture, and the Pacific Science'},\n",
              "      {'x': 1653,\n",
              "       'y': 1188,\n",
              "       'w': 1254,\n",
              "       'h': 66,\n",
              "       'confidence': '0.7633825588016168',\n",
              "       'text': 'Center: The Seattle Center also hosts events like'},\n",
              "      {'x': 1652,\n",
              "       'y': 1263,\n",
              "       'w': 1008,\n",
              "       'h': 75,\n",
              "       'confidence': '0.6185691724891468',\n",
              "       'text': 'concerts, festivals, and art exhibitions.'},\n",
              "      {'x': 1652,\n",
              "       'y': 1340,\n",
              "       'w': 1266,\n",
              "       'h': 80,\n",
              "       'confidence': '0.9050881059450384',\n",
              "       'text': 'The Seattle Center is also close to other popular'},\n",
              "      {'x': 1649,\n",
              "       'y': 1416,\n",
              "       'w': 1375,\n",
              "       'h': 76,\n",
              "       'confidence': '0.7419498307335177',\n",
              "       'text': 'attractions in Seattle, such as the Pike Place Market,'},\n",
              "      {'x': 1649,\n",
              "       'y': 1495,\n",
              "       'w': 1350,\n",
              "       'h': 77,\n",
              "       'confidence': '0.8692300865639507',\n",
              "       'text': 'the Seattle Aquarium, and the Seattle Art Museum:'},\n",
              "      {'x': 1652,\n",
              "       'y': 1574,\n",
              "       'w': 1252,\n",
              "       'h': 73,\n",
              "       'confidence': '0.9698072830299177',\n",
              "       'text': 'The city is known for its vibrant arts and culture'},\n",
              "      {'x': 1649,\n",
              "       'y': 1649,\n",
              "       'w': 1235,\n",
              "       'h': 73,\n",
              "       'confidence': '0.6691461561841235',\n",
              "       'text': 'scene, outdoor activities, and beautiful natural'},\n",
              "      {'x': 1648,\n",
              "       'y': 1722,\n",
              "       'w': 372,\n",
              "       'h': 79,\n",
              "       'confidence': '0.6824997127742795',\n",
              "       'text': 'surroundings.'},\n",
              "      {'x': 2278,\n",
              "       'y': 646,\n",
              "       'w': 106,\n",
              "       'h': 80,\n",
              "       'confidence': '0.9200347122722271',\n",
              "       'text': 'city,'}]}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Qualitative examples of multimodal intelligence',\n",
              "     'md': '# Qualitative examples of multimodal intelligence',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 263, 'h': 12}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['User input', 'Response'],\n",
              "      ['This looks like an amazing site. What is it and where is it located? Are there other fun things around this site to explore?',\n",
              "       \"This is the Space Needle, a landmark in Seattle, Washington, USA. It is a well-known observation tower and a symbol of Seattle: It was built for the 1962 World's Fair and has since become a popular tourist attraction. The Space Needle offers panoramic views of the city, the surrounding mountains, and the Puget Sound. The Space Needle is located in the Seattle Center; which is a cultural and entertainment hub. The Seattle Center is home to many other attractions, including the Chihuly Garden and Glass, the Museum of Pop Culture, and the Pacific Science Center: The Seattle Center also hosts events like concerts, festivals, and art exhibitions. The Seattle Center is also close to other popular attractions in Seattle, such as the Pike Place Market, the Seattle Aquarium, and the Seattle Art Museum: The city is known for its vibrant arts and culture scene, outdoor activities, and beautiful natural surroundings.\"]],\n",
              "     'md': \"|User input|Response|\\n|This looks like an amazing site. What is it and where is it located? Are there other fun things around this site to explore?|This is the Space Needle, a landmark in Seattle, Washington, USA. It is a well-known observation tower and a symbol of Seattle: It was built for the 1962 World's Fair and has since become a popular tourist attraction. The Space Needle offers panoramic views of the city, the surrounding mountains, and the Puget Sound. The Space Needle is located in the Seattle Center; which is a cultural and entertainment hub. The Seattle Center is home to many other attractions, including the Chihuly Garden and Glass, the Museum of Pop Culture, and the Pacific Science Center: The Seattle Center also hosts events like concerts, festivals, and art exhibitions. The Seattle Center is also close to other popular attractions in Seattle, such as the Pike Place Market, the Seattle Aquarium, and the Seattle Art Museum: The city is known for its vibrant arts and culture scene, outdoor activities, and beautiful natural surroundings.|\",\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"User input\",\"Response\"\\n\"This looks like an amazing site. What is it and where is it located? Are there other fun things around this site to explore?\",\"This is the Space Needle, a landmark in Seattle, Washington, USA. It is a well-known observation tower and a symbol of Seattle: It was built for the 1962 World\\'s Fair and has since become a popular tourist attraction. The Space Needle offers panoramic views of the city, the surrounding mountains, and the Puget Sound. The Space Needle is located in the Seattle Center; which is a cultural and entertainment hub. The Seattle Center is home to many other attractions, including the Chihuly Garden and Glass, the Museum of Pop Culture, and the Pacific Science Center: The Seattle Center also hosts events like concerts, festivals, and art exhibitions. The Seattle Center is also close to other popular attractions in Seattle, such as the Pike Place Market, the Seattle Aquarium, and the Seattle Art Museum: The city is known for its vibrant arts and culture scene, outdoor activities, and beautiful natural surroundings.\"',\n",
              "     'bBox': {'x': 103, 'y': 112, 'w': 284, 'h': 15}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Figure 6: Created with Nova Pro. Photo taken by a team member.\\n\\n39',\n",
              "     'md': 'Figure 6: Created with Nova Pro. Photo taken by a team member.\\n\\n39',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 365, 'h': 679.99}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 40,\n",
              "   'text': '                                                                                                        The Amazon Nova Family of Models\\n                    User input:                                                           \"Search for Psych and buythe first episode\"\\n                    Agent step 1                                                                                                                                        Agent step 2\\n      amazon  Prime Video    Search Amazon                                                                         EN   Hello, sign in  Retums   10             amazon        Prime Video Psych                                                                                   EN -  Hello, sign in   Retums 10\\n                                                                                                                        Account & Lists   Orders                                            psych             Action: Click              \\\\\\'Psych\"                                       Account & Lists  & Orders\\n                                                                                                                                Black Friday Week=\\n   All Black Friday Deals          Action: Type               \\\\\\'Psych\"                ases  Music Customer ServiceAmazon Home                 shopnow             All  Black Friday Dcals                                                                                        Amazon Home   Black Friday Weekshopnow\\n                                                            1)                                                                                                                              psycho\\n    prime video     Home                                                                                                                Join Prime                  prime video    Home     psychological thrillers                                                                                      Join Prime\\n                                                                                                                                                                                            psychic\\n                                                                                                                                                                                            psychic kids\\n       prime                                                                                                                                                                                psych the movie\\n                                                                                                                                                                                            psychology\\n      amazonwish list                                                                                                                                                                       psychic investigators\\n    GAMES                                                                                                                                                           YELLO                   psycho-pass\\n                                                                                                                                                                    GOLDEN GLOBE@ winner    psychopath\\n            Episode                                                                                                                                                   BuyEpisode\\n            Watch now                                                                                                                                                 HD $2.99\\n       Freevee (with ads)                                                                                                                                             Available to buy                                                                                        TV-MA\\n    Popular movies     See more                                                                                                                                     Popular movies    See more\\n                                           IN     Tₚᵣₘₜₑ                BROTHERS                                                                                         APCCALTO                     NFANIT                            BROTHERS|\\n                                                                                  prime                        prime                                                                                                                              prime                       prime\\n    Top-~rated movies        See more                                                                                                                               Top-~rated movies       See more\\n      R                                    4                    Ihe                                                                                                 Re                                                         Iie\\n                                                                                                                            F78 E                                                                                              Godeler                                                     at8e\\n                                                      prime                       prime                        prime                                                                                                  prime                       ~prime                      prime\\n    Featured Originals and Exclusives                                                                                                                               Featured Originals and Exclusives\\n                                                                         Neeson                                                                                                                                                          Neeson\\n                                                              M E M & R Y                                                                                                                                                     M EM & R Y\\n       PIMPINERD                           FRoM                                                  RINGS                                        PFACHEP                 PIMPINERD                     FRoM                                                         RINGS                                  PFACHEP\\n          Agent step 3                                                                                                                                                   Agent step 4\\namazon         Prime Video   psych                                                                                 EN -   Hello, sign in  Retums           10,  amazon        Prime Video      psych                                                                                    Hello, sign in Retums   10\\n                                                                                                                          Accdunt Lists        Orders                                                                                                                             EN -  Account Lists    Orders\\n ZAl    Black Friday Dcals   Medical Care  Best SellersPrime  Amazon BasicsGroceries  New ReleasesMusicCustomer ServiceAmazon Home  Black Friday Week shop now  ZAUL  Black Friday Dcals    Medical Care  Best SellersPrime  Amazon BasicsGroceries  New ReleasesMusicCustomer ServiceAmazon Home  Black Friday Weekshopnow\\n   Home   Store Free with ads  Categories  My Stuff                                                                         Settings  Getting Started  Help         prime video    Home     Movies    TV shows    Sports    Live TV      88 Subscriptions                                                Join Prime\\n                  1-16 of 423 results for \"psych\\\\\\'                                                                   Soit Dy:Fejined\\n                  Department                   Results\\n                   Any Department\\n                     MoviesTV                                         psych  Psych Season\\n                  Prime Video                                                 Dna\\n                  Customer Reviews                                                        Action: Click \\\\\\'Psych Season 1\"\\n                               Up\\n                  Actor                                                      For 534.99to buy season         et al\\n                  James Roday                                                Or $0.00 withPrime membership\\n                  Dule Hill\\n                  Timothy Omundson\\n                  Maggie Lawson                                              Psych Season 3\\n                  Price                                               psych  2008   CC                                                                             prime\\n                  Frce\\n                  S0 to $1                                            Ua                  894\\n                  S1to $3                                                    Prime Video                     Starring: James Roday , Dule Hill                      Psych\\n                  53 to $5                                                       299                         and Timothy Omundson\\n                  S5 to $10                                                  For     [0buyepisode            Directed by: Mel DamskiSteve\\n                  510 to $15                                                 For 534.99 to buy season        Franks  Roday Rodriguez\\n                  515 to S20                                                 Or $0.00 withPrime membership   et al James                                            PRIMETIME EMMY? nominee\\n                  Condition                                                                                                                                                Season 1\\n                  New\\n                  Format                               peych                 Psych Season                                                                           James Roday plays the somewhat eccentric Shawn Spencer;     police\\n                    Prime Video                                              2013   CC                                                                              consultant who solves crimes with powers of observation so acute that\\n                  Genre                                                      Prime Video  1,326              Starring: James RodayShawn                             the police think he\\'$ psychic or S0 he lets them think:\\n                  ActionAdventure                                            For *299 to buy episode         Spencer,Dule Hill and Corbin                                       739 IMDb 8.42007        Action: Click BuyEpisode 1 HD $2.99\"\\n                  Animation                                                                                  Bernsen                                                Comedy    Suspense\\n                  Arts, CultureEntertainment                                 For 534.99 to buy season        Directed by: Mel DamskiSteve\\n                  Comedy                                                     Or $0.00 withPrime membership   FranksJames Roday Rodriguez\\n                  Documentary                                                                                et al.                                                   Watch with Prime                BuyEpisode          BuySeason          More purchase\\n                  Drama                                                                                                                                              Start your 30-day free trial     HD $2.99            HD $34.99          options                                     08\\n                  Fantasy\\n                  Horror                               amizonoRiginal        Bosch   Season 3\\n                  International                            BOScH             2017   TV-MA   CC                                                                        Join Prime\\n                  Music Videos  Concerts                                                  \\'782\\n                  Romance                                                    Prime Video                     Starring: Titus WelliverJamie                          Terms apply\\n                  Science Fiction                                                                            HectorAmy Aquino and\\n                    See more                                                     Watch now: Free with ads    Madison Lintz                                          Episodes   Related    Details\\n                  Video Length                                               Or $0.00 withPrime membership   Directed by: Adam Davidson ,\\n                     Up toMinutes                                                                            Alex Zakrzewski ,Sarah Pia\\n                  11t0 20 Minutes                                                                            Andersonet al.\\n                 21 t0 30 Minutes                                                                                                                                                                                                                                                                        Sort\\n                 31 to 60 Minutes                                            Hollywood Homicide\\n                 61 to 90 Minutes                                            2003   PG-13  CC                                                                                                  51 E1 - Psych\\n                    More Than 90 Minutes                                                  609\\n                  Decade                                                     Prime Video                     Starring: Harrison FordJosh                                                        July 6, 2006 1h 6 min13+\\n                      2020   Newer                                               399                         Hartnett .Isaiah Washington ,et                                                    When the Santa Barbara police department hit a wall on a high profile kidnapping case;\\n                    20102019                                                 For     t0 rent                 al:                                                                                thev reluctantlv hire Shawn Spencer the town\\'s newest Dsvchic detectiveto held solve the\\n                                                                                                   Figure 7: Example of a multimodal agent.\\n                                                                                                                                                      40',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\nUser input: \"Search for Psych and buy the first episode\"\\n\\n|Agent step 1|Agent step 2|\\n|---|---|\\n|amazon Prime Video Search Amazon|EN Hello, sign in Returns 10|\\n|amazon Prime Video Psych|EN - Hello, sign in Returns 10|\\n|Account & Lists Orders|psych Action: Click \\'Psych\\'|\\n|Black Friday Week= All Black Friday Deals|Action: Type \\'Psych\\'|\\n|amazon Home shop now|All Black Friday Deals|\\n|prime video Home|Join Prime|\\n|prime|psychological thrillers|\\n|psychic|psychic kids|\\n|psych the movie|psychology|\\n|psychic investigators|YELLO|\\n|psycho|GOLDEN GLOBE@ winner|\\n|Buy Episode|Watch now HD $2.99|\\n|Freevee (with ads)|Available to buy TV-MA|\\n|Popular movies See more|Top-rated movies See more|\\n|Featured Originals and Exclusives|Neeson|\\n|Agent step 3|Agent step 4|\\n|amazon Prime Video psych|EN - Hello, sign in Returns 10|\\n|Account Lists Orders|EN - Account Lists Orders|\\n|Black Friday Deals Medical Care Best Sellers Prime|Amazon Basics Groceries New Releases Music Customer Service Amazon Home Black Friday Week shop now|\\n|Home Store Free with ads Categories My Stuff|Settings Getting Started Help|\\n|prime video Home Movies TV shows Sports Live TV 88 Subscriptions|Join Prime|\\n|1-16 of 423 results for \"psych\"|Sort By: Featured|\\n|Department Results|Any Department Movies TV|\\n|psych Psych Season|Prime Video|\\n|Customer Reviews|Action: Click \\'Psych Season 1\\'|\\n|Actor|For $34.99 to buy season et al|\\n|James Roday|Or $0.00 with Prime membership|\\n|Dule Hill|Psych Season 3|\\n|Price|psych 2008 CC|\\n|Free|$0 to $1|\\n|$1 to $3|Prime Video|\\n|$3 to $5|Starring: James Roday, Dule Hill|\\n|$5 to $10|and Timothy Omundson|\\n|$10 to $15|For $34.99 to buy season|\\n|$15 to $20|Or $0.00 with Prime membership|\\n|Condition|New|\\n|Format|peych|\\n|Psych Season|2013 CC|\\n|Genre|Prime Video 1,326|\\n|Action Adventure|For $2.99 to buy episode|\\n|Animation|Comedy|\\n|Documentary|Drama|\\n|Fantasy|Horror|\\n|International|Music Videos Concerts|\\n|Romance|Science Fiction|\\n|See more|Watch now: Free with ads|\\n|Or $0.00 with Prime membership|Directed by: Adam Davidson, Alex Zakrzewski, Sarah Pia Anderson et al.|\\n|Video Length|Up to Minutes|\\n|11 to 20 Minutes|21 to 30 Minutes|\\n|31 to 60 Minutes|61 to 90 Minutes|\\n|More Than 90 Minutes|Decade|\\n|2020 Newer|2010-2019|\\n|Figure 7: Example of a multimodal agent.| |',\n",
              "   'images': [{'name': 'img_p39_1.png',\n",
              "     'height': 5536,\n",
              "     'width': 4817,\n",
              "     'x': 72,\n",
              "     'y': 118.15459999999996,\n",
              "     'original_width': 4817,\n",
              "     'original_height': 5536,\n",
              "     'ocr': [{'x': 211,\n",
              "       'y': 67,\n",
              "       'w': 400,\n",
              "       'h': 110,\n",
              "       'confidence': '0.917916340319179',\n",
              "       'text': 'User input:'},\n",
              "      {'x': 658,\n",
              "       'y': 53,\n",
              "       'w': 1013,\n",
              "       'h': 135,\n",
              "       'confidence': '0.7173904635202686',\n",
              "       'text': '\"Search for Psych and'},\n",
              "      {'x': 1865,\n",
              "       'y': 44,\n",
              "       'w': 824,\n",
              "       'h': 146,\n",
              "       'confidence': '0.9960798557273275',\n",
              "       'text': 'the first episode\"'},\n",
              "      {'x': 144,\n",
              "       'y': 360,\n",
              "       'w': 438,\n",
              "       'h': 92,\n",
              "       'confidence': '0.9254418084415431',\n",
              "       'text': 'Agent step 1'},\n",
              "      {'x': 2609,\n",
              "       'y': 360,\n",
              "       'w': 438,\n",
              "       'h': 92,\n",
              "       'confidence': '0.7972097171610071',\n",
              "       'text': 'Agent step 2'},\n",
              "      {'x': 15,\n",
              "       'y': 508,\n",
              "       'w': 198,\n",
              "       'h': 77,\n",
              "       'confidence': '0.9997983194279729',\n",
              "       'text': 'amazon'},\n",
              "      {'x': 235,\n",
              "       'y': 534,\n",
              "       'w': 134,\n",
              "       'h': 25,\n",
              "       'confidence': '0.9981976125871117',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 413,\n",
              "       'y': 529,\n",
              "       'w': 203,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8427519742649034',\n",
              "       'text': 'Search Amazon'},\n",
              "      {'x': 1870,\n",
              "       'y': 517,\n",
              "       'w': 138,\n",
              "       'h': 29,\n",
              "       'confidence': '0.8995546589959356',\n",
              "       'text': 'Hello, sign in'},\n",
              "      {'x': 2113,\n",
              "       'y': 522,\n",
              "       'w': 80,\n",
              "       'h': 19,\n",
              "       'confidence': '0.967562371594528',\n",
              "       'text': 'Retums'},\n",
              "      {'x': 2252,\n",
              "       'y': 509,\n",
              "       'w': 84,\n",
              "       'h': 62,\n",
              "       'confidence': '0.7054501210846282',\n",
              "       'text': '10'},\n",
              "      {'x': 2477,\n",
              "       'y': 509,\n",
              "       'w': 196,\n",
              "       'h': 66,\n",
              "       'confidence': '0.9998917146325323',\n",
              "       'text': 'amazon'},\n",
              "      {'x': 2701,\n",
              "       'y': 534,\n",
              "       'w': 129,\n",
              "       'h': 25,\n",
              "       'confidence': '0.9951375603568849',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 2878,\n",
              "       'y': 529,\n",
              "       'w': 77,\n",
              "       'h': 30,\n",
              "       'confidence': '0.6727589206489252',\n",
              "       'text': 'Psych'},\n",
              "      {'x': 4331,\n",
              "       'y': 517,\n",
              "       'w': 143,\n",
              "       'h': 29,\n",
              "       'confidence': '0.8490529989198822',\n",
              "       'text': 'Hello, sign in'},\n",
              "      {'x': 4574,\n",
              "       'y': 522,\n",
              "       'w': 80,\n",
              "       'h': 19,\n",
              "       'confidence': '0.6403322268615618',\n",
              "       'text': 'Retums'},\n",
              "      {'x': 4714,\n",
              "       'y': 506,\n",
              "       'w': 82,\n",
              "       'h': 55,\n",
              "       'confidence': '0.37215836309626016',\n",
              "       'text': '10'},\n",
              "      {'x': 1787,\n",
              "       'y': 537,\n",
              "       'w': 45,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9998894084607064',\n",
              "       'text': 'EN'},\n",
              "      {'x': 1870,\n",
              "       'y': 542,\n",
              "       'w': 199,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7941518116105512',\n",
              "       'text': 'Account & Lists'},\n",
              "      {'x': 2138,\n",
              "       'y': 542,\n",
              "       'w': 91,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9972698197387989',\n",
              "       'text': 'Orders'},\n",
              "      {'x': 4248,\n",
              "       'y': 537,\n",
              "       'w': 62,\n",
              "       'h': 36,\n",
              "       'confidence': '0.8833368416373905',\n",
              "       'text': 'EN -'},\n",
              "      {'x': 4335,\n",
              "       'y': 542,\n",
              "       'w': 195,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9237044086334374',\n",
              "       'text': 'Account & Lists'},\n",
              "      {'x': 4573,\n",
              "       'y': 542,\n",
              "       'w': 121,\n",
              "       'h': 30,\n",
              "       'confidence': '0.729798099310388',\n",
              "       'text': '& Orders'},\n",
              "      {'x': 62,\n",
              "       'y': 625,\n",
              "       'w': 43,\n",
              "       'h': 25,\n",
              "       'confidence': '0.8894435063933288',\n",
              "       'text': 'All'},\n",
              "      {'x': 127,\n",
              "       'y': 625,\n",
              "       'w': 233,\n",
              "       'h': 30,\n",
              "       'confidence': '0.5409348803293581',\n",
              "       'text': 'Black Friday Deals'},\n",
              "      {'x': 1351,\n",
              "       'y': 625,\n",
              "       'w': 60,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9976315498352051',\n",
              "       'text': 'ases'},\n",
              "      {'x': 1433,\n",
              "       'y': 625,\n",
              "       'w': 78,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999880581002243',\n",
              "       'text': 'Music'},\n",
              "      {'x': 1533,\n",
              "       'y': 625,\n",
              "       'w': 216,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9998624531978386',\n",
              "       'text': 'Customer Service'},\n",
              "      {'x': 1771,\n",
              "       'y': 625,\n",
              "       'w': 181,\n",
              "       'h': 30,\n",
              "       'confidence': '0.735600427666857',\n",
              "       'text': 'Amazon Home'},\n",
              "      {'x': 1986,\n",
              "       'y': 615,\n",
              "       'w': 235,\n",
              "       'h': 42,\n",
              "       'confidence': '0.9882184565070454',\n",
              "       'text': 'Black Friday Week='},\n",
              "      {'x': 2281,\n",
              "       'y': 625,\n",
              "       'w': 60,\n",
              "       'h': 25,\n",
              "       'confidence': '0.9801376369645807',\n",
              "       'text': 'now'},\n",
              "      {'x': 2523,\n",
              "       'y': 625,\n",
              "       'w': 48,\n",
              "       'h': 25,\n",
              "       'confidence': '0.6126816408990812',\n",
              "       'text': 'All'},\n",
              "      {'x': 2593,\n",
              "       'y': 625,\n",
              "       'w': 228,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7774366098608428',\n",
              "       'text': 'Black Friday Dcals'},\n",
              "      {'x': 2923,\n",
              "       'y': 596,\n",
              "       'w': 91,\n",
              "       'h': 42,\n",
              "       'confidence': '0.9995584583844068',\n",
              "       'text': 'psych'},\n",
              "      {'x': 3198,\n",
              "       'y': 573,\n",
              "       'w': 384,\n",
              "       'h': 77,\n",
              "       'confidence': '0.7123096345604704',\n",
              "       'text': 'Action: Click'},\n",
              "      {'x': 3609,\n",
              "       'y': 573,\n",
              "       'w': 211,\n",
              "       'h': 81,\n",
              "       'confidence': '0.9887209128477316',\n",
              "       'text': '\\\\\\'Psych\"'},\n",
              "      {'x': 4232,\n",
              "       'y': 625,\n",
              "       'w': 181,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9813658198515635',\n",
              "       'text': 'Amazon Home'},\n",
              "      {'x': 4451,\n",
              "       'y': 616,\n",
              "       'w': 231,\n",
              "       'h': 41,\n",
              "       'confidence': '0.9516638402020058',\n",
              "       'text': 'Black Friday Week'},\n",
              "      {'x': 4742,\n",
              "       'y': 625,\n",
              "       'w': 60,\n",
              "       'h': 25,\n",
              "       'confidence': '0.9153761731179411',\n",
              "       'text': 'now'},\n",
              "      {'x': 949,\n",
              "       'y': 642,\n",
              "       'w': 30,\n",
              "       'h': 30,\n",
              "       'confidence': '0.21406213480730973',\n",
              "       'text': '1)'},\n",
              "      {'x': 555,\n",
              "       'y': 637,\n",
              "       'w': 386,\n",
              "       'h': 87,\n",
              "       'confidence': '0.9997844124868857',\n",
              "       'text': 'Action: Type'},\n",
              "      {'x': 968,\n",
              "       'y': 631,\n",
              "       'w': 217,\n",
              "       'h': 93,\n",
              "       'confidence': '0.8367244481629066',\n",
              "       'text': '\\\\\\'Psych\"'},\n",
              "      {'x': 2925,\n",
              "       'y': 663,\n",
              "       'w': 105,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9998207558702209',\n",
              "       'text': 'psycho'},\n",
              "      {'x': 83,\n",
              "       'y': 710,\n",
              "       'w': 157,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9917668229322705',\n",
              "       'text': 'prime video'},\n",
              "      {'x': 318,\n",
              "       'y': 711,\n",
              "       'w': 81,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9615869522094727',\n",
              "       'text': 'Home'},\n",
              "      {'x': 2112,\n",
              "       'y': 711,\n",
              "       'w': 143,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9978273477166106',\n",
              "       'text': 'Join Prime'},\n",
              "      {'x': 2544,\n",
              "       'y': 710,\n",
              "       'w': 162,\n",
              "       'h': 36,\n",
              "       'confidence': '0.7017326399948782',\n",
              "       'text': 'prime video'},\n",
              "      {'x': 2778,\n",
              "       'y': 711,\n",
              "       'w': 87,\n",
              "       'h': 30,\n",
              "       'confidence': '0.4211942568797071',\n",
              "       'text': 'Home '},\n",
              "      {'x': 4578,\n",
              "       'y': 711,\n",
              "       'w': 142,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9930809543215755',\n",
              "       'text': 'Join Prime'},\n",
              "      {'x': 2925,\n",
              "       'y': 723,\n",
              "       'w': 313,\n",
              "       'h': 41,\n",
              "       'confidence': '0.9951432366839401',\n",
              "       'text': 'psychological thrillers'},\n",
              "      {'x': 2922,\n",
              "       'y': 784,\n",
              "       'w': 115,\n",
              "       'h': 49,\n",
              "       'confidence': '0.9996173533669184',\n",
              "       'text': 'psychic'},\n",
              "      {'x': 2922,\n",
              "       'y': 849,\n",
              "       'w': 178,\n",
              "       'h': 49,\n",
              "       'confidence': '0.7994014658019796',\n",
              "       'text': 'psychic kids'},\n",
              "      {'x': 75,\n",
              "       'y': 921,\n",
              "       'w': 130,\n",
              "       'h': 62,\n",
              "       'confidence': '0.9999660401783838',\n",
              "       'text': 'prime'},\n",
              "      {'x': 2923,\n",
              "       'y': 915,\n",
              "       'w': 234,\n",
              "       'h': 45,\n",
              "       'confidence': '0.8201774631623797',\n",
              "       'text': 'psych the movie'},\n",
              "      {'x': 2923,\n",
              "       'y': 977,\n",
              "       'w': 169,\n",
              "       'h': 48,\n",
              "       'confidence': '0.8603640747466503',\n",
              "       'text': 'psychology'},\n",
              "      {'x': 116,\n",
              "       'y': 1041,\n",
              "       'w': 390,\n",
              "       'h': 66,\n",
              "       'confidence': '0.8467535318909727',\n",
              "       'text': 'amazonwish list'},\n",
              "      {'x': 2925,\n",
              "       'y': 1047,\n",
              "       'w': 295,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9672519231614006',\n",
              "       'text': 'psychic investigators'},\n",
              "      {'x': 2925,\n",
              "       'y': 1112,\n",
              "       'w': 174,\n",
              "       'h': 37,\n",
              "       'confidence': '0.999543995190507',\n",
              "       'text': 'psycho-pass'},\n",
              "      {'x': 79,\n",
              "       'y': 1096,\n",
              "       'w': 463,\n",
              "       'h': 134,\n",
              "       'confidence': '0.9271827762124761',\n",
              "       'text': 'GAMES'},\n",
              "      {'x': 2540,\n",
              "       'y': 1104,\n",
              "       'w': 308,\n",
              "       'h': 88,\n",
              "       'confidence': '0.9982188715022721',\n",
              "       'text': 'YELLO'},\n",
              "      {'x': 2925,\n",
              "       'y': 1177,\n",
              "       'w': 166,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9999072998734861',\n",
              "       'text': 'psychopath'},\n",
              "      {'x': 2548,\n",
              "       'y': 1199,\n",
              "       'w': 305,\n",
              "       'h': 36,\n",
              "       'confidence': '0.4939987933599963',\n",
              "       'text': 'GOLDEN GLOBE@ winner'},\n",
              "      {'x': 195,\n",
              "       'y': 1290,\n",
              "       'w': 128,\n",
              "       'h': 40,\n",
              "       'confidence': '0.8988524286352164',\n",
              "       'text': 'Episode'},\n",
              "      {'x': 2648,\n",
              "       'y': 1290,\n",
              "       'w': 127,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999432278155246',\n",
              "       'text': 'Episode'},\n",
              "      {'x': 191,\n",
              "       'y': 1333,\n",
              "       'w': 183,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9899806313257733',\n",
              "       'text': 'Watch now'},\n",
              "      {'x': 2574,\n",
              "       'y': 1324,\n",
              "       'w': 158,\n",
              "       'h': 45,\n",
              "       'confidence': '0.7819594813342599',\n",
              "       'text': 'HD $2.99'},\n",
              "      {'x': 126,\n",
              "       'y': 1404,\n",
              "       'w': 223,\n",
              "       'h': 40,\n",
              "       'confidence': '0.998806670609146',\n",
              "       'text': 'Freevee (with ads)'},\n",
              "      {'x': 2587,\n",
              "       'y': 1406,\n",
              "       'w': 201,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9891956004758735',\n",
              "       'text': 'Available to buy'},\n",
              "      {'x': 4171,\n",
              "       'y': 1407,\n",
              "       'w': 91,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8352621589437896',\n",
              "       'text': 'TV-MA'},\n",
              "      {'x': 83,\n",
              "       'y': 1545,\n",
              "       'w': 253,\n",
              "       'h': 40,\n",
              "       'confidence': '0.8385811474590389',\n",
              "       'text': 'Popular movies'},\n",
              "      {'x': 368,\n",
              "       'y': 1541,\n",
              "       'w': 158,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9986442499686763',\n",
              "       'text': 'See more'},\n",
              "      {'x': 2548,\n",
              "       'y': 1545,\n",
              "       'w': 248,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999680439525179',\n",
              "       'text': 'Popular movies'},\n",
              "      {'x': 2829,\n",
              "       'y': 1545,\n",
              "       'w': 158,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9150156555142158',\n",
              "       'text': 'See more'},\n",
              "      {'x': 585,\n",
              "       'y': 1744,\n",
              "       'w': 71,\n",
              "       'h': 40,\n",
              "       'confidence': '0.38223549336090634',\n",
              "       'text': 'IN'},\n",
              "      {'x': 792,\n",
              "       'y': 1739,\n",
              "       'w': 41,\n",
              "       'h': 41,\n",
              "       'confidence': '0.8880757040141276',\n",
              "       'text': 'T'},\n",
              "      {'x': 1132,\n",
              "       'y': 1682,\n",
              "       'w': 234,\n",
              "       'h': 91,\n",
              "       'confidence': '0.48674205939742043',\n",
              "       'text': 'BROTHERS '},\n",
              "      {'x': 2629,\n",
              "       'y': 1708,\n",
              "       'w': 164,\n",
              "       'h': 51,\n",
              "       'confidence': '0.17719991165716037',\n",
              "       'text': 'APCCALTO'},\n",
              "      {'x': 3066,\n",
              "       'y': 1734,\n",
              "       'w': 233,\n",
              "       'h': 51,\n",
              "       'confidence': '0.19991481737953512',\n",
              "       'text': 'NFANIT'},\n",
              "      {'x': 3593,\n",
              "       'y': 1682,\n",
              "       'w': 234,\n",
              "       'h': 92,\n",
              "       'confidence': '0.7475934680822576',\n",
              "       'text': 'BROTHERS|'},\n",
              "      {'x': 846,\n",
              "       'y': 1780,\n",
              "       'w': 63,\n",
              "       'h': 20,\n",
              "       'confidence': '0.1731092374075883',\n",
              "       'text': 'prmte'},\n",
              "      {'x': 1286,\n",
              "       'y': 1779,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9970902099118429',\n",
              "       'text': 'prime'},\n",
              "      {'x': 1728,\n",
              "       'y': 1779,\n",
              "       'w': 64,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9985151302025602',\n",
              "       'text': 'prime'},\n",
              "      {'x': 3747,\n",
              "       'y': 1779,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9867869357255199',\n",
              "       'text': 'prime'},\n",
              "      {'x': 4188,\n",
              "       'y': 1779,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9871968984726133',\n",
              "       'text': 'prime'},\n",
              "      {'x': 144,\n",
              "       'y': 1904,\n",
              "       'w': 222,\n",
              "       'h': 36,\n",
              "       'confidence': '0.6950236292087872',\n",
              "       'text': '~rated movies'},\n",
              "      {'x': 399,\n",
              "       'y': 1900,\n",
              "       'w': 157,\n",
              "       'h': 40,\n",
              "       'confidence': '0.882151743133577',\n",
              "       'text': 'See more'},\n",
              "      {'x': 2604,\n",
              "       'y': 1904,\n",
              "       'w': 227,\n",
              "       'h': 36,\n",
              "       'confidence': '0.7445332049797466',\n",
              "       'text': '~rated movies'},\n",
              "      {'x': 2860,\n",
              "       'y': 1900,\n",
              "       'w': 157,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9997985261523906',\n",
              "       'text': 'See more'},\n",
              "      {'x': 998,\n",
              "       'y': 1971,\n",
              "       'w': 83,\n",
              "       'h': 66,\n",
              "       'confidence': '0.3739930204056462',\n",
              "       'text': 'Ihe'},\n",
              "      {'x': 3460,\n",
              "       'y': 1976,\n",
              "       'w': 77,\n",
              "       'h': 56,\n",
              "       'confidence': '0.6595128084053345',\n",
              "       'text': 'Iie'},\n",
              "      {'x': 101,\n",
              "       'y': 2022,\n",
              "       'w': 26,\n",
              "       'h': 38,\n",
              "       'confidence': '0.9747805501358222',\n",
              "       'text': 'R'},\n",
              "      {'x': 681,\n",
              "       'y': 1996,\n",
              "       'w': 116,\n",
              "       'h': 77,\n",
              "       'confidence': '0.12459114791059633',\n",
              "       'text': '4'},\n",
              "      {'x': 2557,\n",
              "       'y': 2016,\n",
              "       'w': 53,\n",
              "       'h': 45,\n",
              "       'confidence': '0.9261868900162286',\n",
              "       'text': 'Re'},\n",
              "      {'x': 3459,\n",
              "       'y': 2027,\n",
              "       'w': 179,\n",
              "       'h': 66,\n",
              "       'confidence': '0.1627831894621377',\n",
              "       'text': 'Godeler'},\n",
              "      {'x': 1923,\n",
              "       'y': 2083,\n",
              "       'w': 54,\n",
              "       'h': 19,\n",
              "       'confidence': '0.10930342113813432',\n",
              "       'text': 'F78 E'},\n",
              "      {'x': 4383,\n",
              "       'y': 2082,\n",
              "       'w': 60,\n",
              "       'h': 26,\n",
              "       'confidence': '0.15148858726024628',\n",
              "       'text': 'at8e'},\n",
              "      {'x': 844,\n",
              "       'y': 2131,\n",
              "       'w': 67,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9977921682164943',\n",
              "       'text': 'prime'},\n",
              "      {'x': 1286,\n",
              "       'y': 2134,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9934718228293573',\n",
              "       'text': 'prime'},\n",
              "      {'x': 1728,\n",
              "       'y': 2134,\n",
              "       'w': 64,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9989846118089108',\n",
              "       'text': 'prime'},\n",
              "      {'x': 3310,\n",
              "       'y': 2134,\n",
              "       'w': 65,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8773624812166075',\n",
              "       'text': 'prime'},\n",
              "      {'x': 3746,\n",
              "       'y': 2130,\n",
              "       'w': 67,\n",
              "       'h': 31,\n",
              "       'confidence': '0.2620499518470425',\n",
              "       'text': '~prime'},\n",
              "      {'x': 4188,\n",
              "       'y': 2134,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.949982069103508',\n",
              "       'text': 'prime'},\n",
              "      {'x': 83,\n",
              "       'y': 2254,\n",
              "       'w': 534,\n",
              "       'h': 45,\n",
              "       'confidence': '0.6732333477203712',\n",
              "       'text': 'Featured Originals and Exclusives'},\n",
              "      {'x': 2544,\n",
              "       'y': 2254,\n",
              "       'w': 534,\n",
              "       'h': 45,\n",
              "       'confidence': '0.848370909667181',\n",
              "       'text': 'Featured Originals and Exclusives'},\n",
              "      {'x': 1144,\n",
              "       'y': 2428,\n",
              "       'w': 138,\n",
              "       'h': 30,\n",
              "       'confidence': '0.2673903358681503',\n",
              "       'text': 'Neeson'},\n",
              "      {'x': 3609,\n",
              "       'y': 2428,\n",
              "       'w': 134,\n",
              "       'h': 30,\n",
              "       'confidence': '0.17985100582766964',\n",
              "       'text': 'Neeson'},\n",
              "      {'x': 981,\n",
              "       'y': 2451,\n",
              "       'w': 369,\n",
              "       'h': 62,\n",
              "       'confidence': '0.4454750748562714',\n",
              "       'text': 'M E M & R Y'},\n",
              "      {'x': 3442,\n",
              "       'y': 2451,\n",
              "       'w': 369,\n",
              "       'h': 62,\n",
              "       'confidence': '0.4195348774355594',\n",
              "       'text': 'M EM & R Y'},\n",
              "      {'x': 122,\n",
              "       'y': 2790,\n",
              "       'w': 326,\n",
              "       'h': 45,\n",
              "       'confidence': '0.27078450149156',\n",
              "       'text': 'PIMPINERD'},\n",
              "      {'x': 575,\n",
              "       'y': 2738,\n",
              "       'w': 302,\n",
              "       'h': 98,\n",
              "       'confidence': '0.8603183031082153',\n",
              "       'text': 'FRoM'},\n",
              "      {'x': 1517,\n",
              "       'y': 2745,\n",
              "       'w': 179,\n",
              "       'h': 66,\n",
              "       'confidence': '0.9997064056458465',\n",
              "       'text': 'RINGS'},\n",
              "      {'x': 1858,\n",
              "       'y': 2732,\n",
              "       'w': 379,\n",
              "       'h': 123,\n",
              "       'confidence': '0.7889320393138717',\n",
              "       'text': 'PFACHEP'},\n",
              "      {'x': 2583,\n",
              "       'y': 2790,\n",
              "       'w': 326,\n",
              "       'h': 45,\n",
              "       'confidence': '0.31063726699233973',\n",
              "       'text': 'PIMPINERD'},\n",
              "      {'x': 3036,\n",
              "       'y': 2738,\n",
              "       'w': 302,\n",
              "       'h': 98,\n",
              "       'confidence': '0.9141307473182678',\n",
              "       'text': 'FRoM'},\n",
              "      {'x': 3980,\n",
              "       'y': 2743,\n",
              "       'w': 179,\n",
              "       'h': 88,\n",
              "       'confidence': '0.9990941804283194',\n",
              "       'text': 'RINGS'},\n",
              "      {'x': 4324,\n",
              "       'y': 2737,\n",
              "       'w': 373,\n",
              "       'h': 117,\n",
              "       'confidence': '0.5193644770889223',\n",
              "       'text': 'PFACHEP'},\n",
              "      {'x': 160,\n",
              "       'y': 3054,\n",
              "       'w': 436,\n",
              "       'h': 98,\n",
              "       'confidence': '0.9995856884278198',\n",
              "       'text': 'Agent step 3'},\n",
              "      {'x': 2621,\n",
              "       'y': 3054,\n",
              "       'w': 440,\n",
              "       'h': 98,\n",
              "       'confidence': '0.9653420440883207',\n",
              "       'text': 'Agent step 4'},\n",
              "      {'x': 24,\n",
              "       'y': 3207,\n",
              "       'w': 198,\n",
              "       'h': 73,\n",
              "       'confidence': '0.9996660345894655',\n",
              "       'text': 'amazon'},\n",
              "      {'x': 248,\n",
              "       'y': 3233,\n",
              "       'w': 130,\n",
              "       'h': 25,\n",
              "       'confidence': '0.9992945627826011',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 426,\n",
              "       'y': 3228,\n",
              "       'w': 77,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9995702409457151',\n",
              "       'text': 'psych'},\n",
              "      {'x': 1879,\n",
              "       'y': 3215,\n",
              "       'w': 142,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8063767119203064',\n",
              "       'text': 'Hello, sign in'},\n",
              "      {'x': 2122,\n",
              "       'y': 3221,\n",
              "       'w': 80,\n",
              "       'h': 19,\n",
              "       'confidence': '0.8818010373665427',\n",
              "       'text': 'Retums'},\n",
              "      {'x': 2261,\n",
              "       'y': 3204,\n",
              "       'w': 83,\n",
              "       'h': 62,\n",
              "       'confidence': '0.45654509350812367',\n",
              "       'text': '10,'},\n",
              "      {'x': 2477,\n",
              "       'y': 3208,\n",
              "       'w': 196,\n",
              "       'h': 66,\n",
              "       'confidence': '0.9998943913812494',\n",
              "       'text': 'amazon'},\n",
              "      {'x': 2701,\n",
              "       'y': 3233,\n",
              "       'w': 129,\n",
              "       'h': 25,\n",
              "       'confidence': '0.9904803909534744',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 2878,\n",
              "       'y': 3228,\n",
              "       'w': 77,\n",
              "       'h': 30,\n",
              "       'confidence': '0.6786084458704948',\n",
              "       'text': 'psych'},\n",
              "      {'x': 4331,\n",
              "       'y': 3215,\n",
              "       'w': 143,\n",
              "       'h': 30,\n",
              "       'confidence': '0.765797291472659',\n",
              "       'text': 'Hello, sign in'},\n",
              "      {'x': 4573,\n",
              "       'y': 3220,\n",
              "       'w': 87,\n",
              "       'h': 25,\n",
              "       'confidence': '0.7068215433704742',\n",
              "       'text': 'Retums'},\n",
              "      {'x': 4713,\n",
              "       'y': 3204,\n",
              "       'w': 79,\n",
              "       'h': 62,\n",
              "       'confidence': '0.9968554407069007',\n",
              "       'text': '10'},\n",
              "      {'x': 1796,\n",
              "       'y': 3236,\n",
              "       'w': 62,\n",
              "       'h': 36,\n",
              "       'confidence': '0.7061376999481564',\n",
              "       'text': 'EN -'},\n",
              "      {'x': 1884,\n",
              "       'y': 3247,\n",
              "       'w': 102,\n",
              "       'h': 19,\n",
              "       'confidence': '0.5740531352993434',\n",
              "       'text': 'Accdunt'},\n",
              "      {'x': 2013,\n",
              "       'y': 3241,\n",
              "       'w': 65,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9997830173201773',\n",
              "       'text': 'Lists'},\n",
              "      {'x': 2147,\n",
              "       'y': 3241,\n",
              "       'w': 95,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9960772184229003',\n",
              "       'text': 'Orders'},\n",
              "      {'x': 4248,\n",
              "       'y': 3236,\n",
              "       'w': 62,\n",
              "       'h': 36,\n",
              "       'confidence': '0.8023887011671593',\n",
              "       'text': 'EN -'},\n",
              "      {'x': 4335,\n",
              "       'y': 3246,\n",
              "       'w': 108,\n",
              "       'h': 25,\n",
              "       'confidence': '0.9998392328926677',\n",
              "       'text': 'Account'},\n",
              "      {'x': 4465,\n",
              "       'y': 3241,\n",
              "       'w': 65,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9963450179021607',\n",
              "       'text': 'Lists'},\n",
              "      {'x': 4599,\n",
              "       'y': 3241,\n",
              "       'w': 95,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9978304804360096',\n",
              "       'text': 'Orders'},\n",
              "      {'x': 31,\n",
              "       'y': 3309,\n",
              "       'w': 88,\n",
              "       'h': 45,\n",
              "       'confidence': '0.26610803320015247',\n",
              "       'text': 'ZAl'},\n",
              "      {'x': 139,\n",
              "       'y': 3318,\n",
              "       'w': 231,\n",
              "       'h': 36,\n",
              "       'confidence': '0.8676475366326386',\n",
              "       'text': 'Black Friday Dcals'},\n",
              "      {'x': 390,\n",
              "       'y': 3318,\n",
              "       'w': 162,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9993112787009898',\n",
              "       'text': 'Medical Care'},\n",
              "      {'x': 593,\n",
              "       'y': 3318,\n",
              "       'w': 149,\n",
              "       'h': 36,\n",
              "       'confidence': '0.8836507467321031',\n",
              "       'text': 'Best Sellers'},\n",
              "      {'x': 763,\n",
              "       'y': 3323,\n",
              "       'w': 78,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9980259991648822',\n",
              "       'text': 'Prime'},\n",
              "      {'x': 886,\n",
              "       'y': 3322,\n",
              "       'w': 185,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9978911920596507',\n",
              "       'text': 'Amazon Basics'},\n",
              "      {'x': 1092,\n",
              "       'y': 3323,\n",
              "       'w': 116,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9603345200832023',\n",
              "       'text': 'Groceries'},\n",
              "      {'x': 1252,\n",
              "       'y': 3323,\n",
              "       'w': 172,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9987198815089615',\n",
              "       'text': 'New Releases'},\n",
              "      {'x': 1442,\n",
              "       'y': 3323,\n",
              "       'w': 78,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999940823642687',\n",
              "       'text': 'Music'},\n",
              "      {'x': 1542,\n",
              "       'y': 3323,\n",
              "       'w': 215,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9934419094029704',\n",
              "       'text': 'Customer Service'},\n",
              "      {'x': 1779,\n",
              "       'y': 3323,\n",
              "       'w': 182,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9963578755774053',\n",
              "       'text': 'Amazon Home'},\n",
              "      {'x': 1999,\n",
              "       'y': 3314,\n",
              "       'w': 352,\n",
              "       'h': 42,\n",
              "       'confidence': '0.7340397826886083',\n",
              "       'text': 'Black Friday Week shop now'},\n",
              "      {'x': 2483,\n",
              "       'y': 3309,\n",
              "       'w': 89,\n",
              "       'h': 45,\n",
              "       'confidence': '0.1351856142282486',\n",
              "       'text': 'ZAUL'},\n",
              "      {'x': 2593,\n",
              "       'y': 3323,\n",
              "       'w': 228,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7175983205750327',\n",
              "       'text': 'Black Friday Dcals'},\n",
              "      {'x': 2843,\n",
              "       'y': 3323,\n",
              "       'w': 160,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7168807469486497',\n",
              "       'text': 'Medical Care'},\n",
              "      {'x': 3047,\n",
              "       'y': 3323,\n",
              "       'w': 146,\n",
              "       'h': 30,\n",
              "       'confidence': '0.998392015251538',\n",
              "       'text': 'Best Sellers'},\n",
              "      {'x': 3213,\n",
              "       'y': 3319,\n",
              "       'w': 83,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9963950201220982',\n",
              "       'text': 'Prime'},\n",
              "      {'x': 3338,\n",
              "       'y': 3321,\n",
              "       'w': 185,\n",
              "       'h': 37,\n",
              "       'confidence': '0.8912872582901439',\n",
              "       'text': 'Amazon Basics'},\n",
              "      {'x': 3544,\n",
              "       'y': 3323,\n",
              "       'w': 116,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9998148991439115',\n",
              "       'text': 'Groceries'},\n",
              "      {'x': 3704,\n",
              "       'y': 3323,\n",
              "       'w': 173,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9471368496395136',\n",
              "       'text': 'New Releases'},\n",
              "      {'x': 3899,\n",
              "       'y': 3323,\n",
              "       'w': 77,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9737345790254467',\n",
              "       'text': 'Music'},\n",
              "      {'x': 3998,\n",
              "       'y': 3323,\n",
              "       'w': 212,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9956111847127106',\n",
              "       'text': 'Customer Service'},\n",
              "      {'x': 4232,\n",
              "       'y': 3323,\n",
              "       'w': 181,\n",
              "       'h': 30,\n",
              "       'confidence': '0.5957980788385012',\n",
              "       'text': 'Amazon Home'},\n",
              "      {'x': 4451,\n",
              "       'y': 3314,\n",
              "       'w': 231,\n",
              "       'h': 42,\n",
              "       'confidence': '0.9959123470892208',\n",
              "       'text': 'Black Friday Week'},\n",
              "      {'x': 4742,\n",
              "       'y': 3323,\n",
              "       'w': 60,\n",
              "       'h': 26,\n",
              "       'confidence': '0.5967769405863255',\n",
              "       'text': 'now'},\n",
              "      {'x': 62,\n",
              "       'y': 3388,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.999903678894043',\n",
              "       'text': 'Home'},\n",
              "      {'x': 163,\n",
              "       'y': 3389,\n",
              "       'w': 58,\n",
              "       'h': 20,\n",
              "       'confidence': '0.9994680360107827',\n",
              "       'text': 'Store'},\n",
              "      {'x': 257,\n",
              "       'y': 3388,\n",
              "       'w': 142,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999247470031682',\n",
              "       'text': 'Free with ads'},\n",
              "      {'x': 430,\n",
              "       'y': 3388,\n",
              "       'w': 116,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9094991597863141',\n",
              "       'text': 'Categories'},\n",
              "      {'x': 573,\n",
              "       'y': 3388,\n",
              "       'w': 95,\n",
              "       'h': 30,\n",
              "       'confidence': '0.805773094284618',\n",
              "       'text': 'My Stuff'},\n",
              "      {'x': 1957,\n",
              "       'y': 3388,\n",
              "       'w': 95,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9992413217937008',\n",
              "       'text': 'Settings'},\n",
              "      {'x': 2082,\n",
              "       'y': 3388,\n",
              "       'w': 164,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9843695935267467',\n",
              "       'text': 'Getting Started'},\n",
              "      {'x': 2277,\n",
              "       'y': 3388,\n",
              "       'w': 56,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9996105432510376',\n",
              "       'text': 'Help'},\n",
              "      {'x': 2544,\n",
              "       'y': 3409,\n",
              "       'w': 162,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9967215546035639',\n",
              "       'text': 'prime video'},\n",
              "      {'x': 2778,\n",
              "       'y': 3414,\n",
              "       'w': 87,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9994767308235168',\n",
              "       'text': 'Home'},\n",
              "      {'x': 2921,\n",
              "       'y': 3414,\n",
              "       'w': 95,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9811283838337265',\n",
              "       'text': 'Movies'},\n",
              "      {'x': 3077,\n",
              "       'y': 3410,\n",
              "       'w': 125,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9246114292590658',\n",
              "       'text': 'TV shows'},\n",
              "      {'x': 3258,\n",
              "       'y': 3409,\n",
              "       'w': 92,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9997655162696993',\n",
              "       'text': 'Sports'},\n",
              "      {'x': 3406,\n",
              "       'y': 3410,\n",
              "       'w': 103,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8474373564599108',\n",
              "       'text': 'Live TV'},\n",
              "      {'x': 3617,\n",
              "       'y': 3400,\n",
              "       'w': 226,\n",
              "       'h': 45,\n",
              "       'confidence': '0.8174373558377389',\n",
              "       'text': '88 Subscriptions'},\n",
              "      {'x': 4578,\n",
              "       'y': 3410,\n",
              "       'w': 142,\n",
              "       'h': 30,\n",
              "       'confidence': '0.99553294392977',\n",
              "       'text': 'Join Prime'},\n",
              "      {'x': 278,\n",
              "       'y': 3452,\n",
              "       'w': 360,\n",
              "       'h': 36,\n",
              "       'confidence': '0.7047169073709755',\n",
              "       'text': '1-16 of 423 results for \"psych\\\\\\''},\n",
              "      {'x': 1889,\n",
              "       'y': 3459,\n",
              "       'w': 75,\n",
              "       'h': 19,\n",
              "       'confidence': '0.11005944172241902',\n",
              "       'text': 'Soit Dy:'},\n",
              "      {'x': 1971,\n",
              "       'y': 3459,\n",
              "       'w': 84,\n",
              "       'h': 19,\n",
              "       'confidence': '0.29499002159361254',\n",
              "       'text': 'Fejined'},\n",
              "      {'x': 287,\n",
              "       'y': 3548,\n",
              "       'w': 156,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9990852999023038',\n",
              "       'text': 'Department'},\n",
              "      {'x': 740,\n",
              "       'y': 3547,\n",
              "       'w': 136,\n",
              "       'h': 45,\n",
              "       'confidence': '0.9999943228283826',\n",
              "       'text': 'Results'},\n",
              "      {'x': 305,\n",
              "       'y': 3592,\n",
              "       'w': 203,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9998931275118514',\n",
              "       'text': 'Any Department'},\n",
              "      {'x': 305,\n",
              "       'y': 3626,\n",
              "       'w': 90,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9900162228444945',\n",
              "       'text': 'Movies'},\n",
              "      {'x': 417,\n",
              "       'y': 3626,\n",
              "       'w': 39,\n",
              "       'h': 30,\n",
              "       'confidence': '0.692302817065619',\n",
              "       'text': 'TV'},\n",
              "      {'x': 1208,\n",
              "       'y': 3625,\n",
              "       'w': 209,\n",
              "       'h': 41,\n",
              "       'confidence': '0.9998672130865801',\n",
              "       'text': 'Psych Season'},\n",
              "      {'x': 308,\n",
              "       'y': 3660,\n",
              "       'w': 157,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9967419744129417',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 963,\n",
              "       'y': 3640,\n",
              "       'w': 210,\n",
              "       'h': 89,\n",
              "       'confidence': '0.9924588504985821',\n",
              "       'text': 'psych'},\n",
              "      {'x': 1210,\n",
              "       'y': 3670,\n",
              "       'w': 45,\n",
              "       'h': 15,\n",
              "       'confidence': '0.31488314995478933',\n",
              "       'text': 'Dna'},\n",
              "      {'x': 287,\n",
              "       'y': 3721,\n",
              "       'w': 229,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9653939816018824',\n",
              "       'text': 'Customer Reviews'},\n",
              "      {'x': 491,\n",
              "       'y': 3769,\n",
              "       'w': 38,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9531486091757126',\n",
              "       'text': 'Up'},\n",
              "      {'x': 1243,\n",
              "       'y': 3730,\n",
              "       'w': 380,\n",
              "       'h': 77,\n",
              "       'confidence': '0.9748831519376201',\n",
              "       'text': 'Action: Click'},\n",
              "      {'x': 1649,\n",
              "       'y': 3729,\n",
              "       'w': 494,\n",
              "       'h': 88,\n",
              "       'confidence': '0.9910871912900234',\n",
              "       'text': '\\\\\\'Psych Season 1\"'},\n",
              "      {'x': 287,\n",
              "       'y': 3838,\n",
              "       'w': 73,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9568974753211537',\n",
              "       'text': 'Actor'},\n",
              "      {'x': 1204,\n",
              "       'y': 3864,\n",
              "       'w': 307,\n",
              "       'h': 30,\n",
              "       'confidence': '0.6405649787830849',\n",
              "       'text': 'For 534.99to buy season'},\n",
              "      {'x': 1710,\n",
              "       'y': 3868,\n",
              "       'w': 65,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9726776926547237',\n",
              "       'text': 'et al'},\n",
              "      {'x': 283,\n",
              "       'y': 3881,\n",
              "       'w': 164,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9998532018858871',\n",
              "       'text': 'James Roday'},\n",
              "      {'x': 287,\n",
              "       'y': 3916,\n",
              "       'w': 108,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9997481346479044',\n",
              "       'text': 'Dule Hill'},\n",
              "      {'x': 1208,\n",
              "       'y': 3906,\n",
              "       'w': 170,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9976254385110922',\n",
              "       'text': 'Or $0.00 with'},\n",
              "      {'x': 1394,\n",
              "       'y': 3912,\n",
              "       'w': 229,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9971502541214071',\n",
              "       'text': 'Prime membership'},\n",
              "      {'x': 286,\n",
              "       'y': 3950,\n",
              "       'w': 240,\n",
              "       'h': 36,\n",
              "       'confidence': '0.7538909452511954',\n",
              "       'text': 'Timothy Omundson'},\n",
              "      {'x': 286,\n",
              "       'y': 3988,\n",
              "       'w': 188,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9679767153315857',\n",
              "       'text': 'Maggie Lawson'},\n",
              "      {'x': 287,\n",
              "       'y': 4050,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9998992396924596',\n",
              "       'text': 'Price'},\n",
              "      {'x': 1203,\n",
              "       'y': 4036,\n",
              "       'w': 240,\n",
              "       'h': 45,\n",
              "       'confidence': '0.785766285786742',\n",
              "       'text': 'Psych Season 3'},\n",
              "      {'x': 287,\n",
              "       'y': 4093,\n",
              "       'w': 56,\n",
              "       'h': 30,\n",
              "       'confidence': '0.832883894443512',\n",
              "       'text': 'Frce'},\n",
              "      {'x': 1005,\n",
              "       'y': 4066,\n",
              "       'w': 160,\n",
              "       'h': 62,\n",
              "       'confidence': '0.9861756384847814',\n",
              "       'text': 'psych'},\n",
              "      {'x': 1209,\n",
              "       'y': 4085,\n",
              "       'w': 68,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999942779541016',\n",
              "       'text': '2008'},\n",
              "      {'x': 1308,\n",
              "       'y': 4085,\n",
              "       'w': 39,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9281070729500017',\n",
              "       'text': 'CC'},\n",
              "      {'x': 2536,\n",
              "       'y': 4074,\n",
              "       'w': 134,\n",
              "       'h': 62,\n",
              "       'confidence': '0.9999823003925323',\n",
              "       'text': 'prime'},\n",
              "      {'x': 286,\n",
              "       'y': 4127,\n",
              "       'w': 106,\n",
              "       'h': 36,\n",
              "       'confidence': '0.8334278497918738',\n",
              "       'text': 'S0 to $1'},\n",
              "      {'x': 1100,\n",
              "       'y': 4132,\n",
              "       'w': 56,\n",
              "       'h': 30,\n",
              "       'confidence': '0.2602988776779021',\n",
              "       'text': 'Ua'},\n",
              "      {'x': 1390,\n",
              "       'y': 4128,\n",
              "       'w': 52,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999765993614418',\n",
              "       'text': '894'},\n",
              "      {'x': 287,\n",
              "       'y': 4167,\n",
              "       'w': 104,\n",
              "       'h': 30,\n",
              "       'confidence': '0.5475368389681508',\n",
              "       'text': 'S1to $3'},\n",
              "      {'x': 1209,\n",
              "       'y': 4175,\n",
              "       'w': 155,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8170518009386879',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 1696,\n",
              "       'y': 4170,\n",
              "       'w': 387,\n",
              "       'h': 36,\n",
              "       'confidence': '0.8668839692776761',\n",
              "       'text': 'Starring: James Roday , Dule Hill'},\n",
              "      {'x': 287,\n",
              "       'y': 4201,\n",
              "       'w': 104,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8026197172479016',\n",
              "       'text': '53 to $5'},\n",
              "      {'x': 1693,\n",
              "       'y': 4210,\n",
              "       'w': 294,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8163355052910958',\n",
              "       'text': 'and Timothy Omundson'},\n",
              "      {'x': 2545,\n",
              "       'y': 4189,\n",
              "       'w': 190,\n",
              "       'h': 81,\n",
              "       'confidence': '0.999968545852762',\n",
              "       'text': 'Psych'},\n",
              "      {'x': 286,\n",
              "       'y': 4235,\n",
              "       'w': 123,\n",
              "       'h': 36,\n",
              "       'confidence': '0.7592226570413584',\n",
              "       'text': 'S5 to $10'},\n",
              "      {'x': 1209,\n",
              "       'y': 4240,\n",
              "       'w': 47,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9983256631214406',\n",
              "       'text': 'For'},\n",
              "      {'x': 1263,\n",
              "       'y': 4217,\n",
              "       'w': 68,\n",
              "       'h': 55,\n",
              "       'confidence': '0.9996564997575981',\n",
              "       'text': '299'},\n",
              "      {'x': 1331,\n",
              "       'y': 4246,\n",
              "       'w': 28,\n",
              "       'h': 19,\n",
              "       'confidence': '0.15495014034007515',\n",
              "       'text': '[0'},\n",
              "      {'x': 1412,\n",
              "       'y': 4245,\n",
              "       'w': 99,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9581236128935542',\n",
              "       'text': 'episode'},\n",
              "      {'x': 1697,\n",
              "       'y': 4249,\n",
              "       'w': 290,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8343972966348543',\n",
              "       'text': 'Directed by: Mel Damski'},\n",
              "      {'x': 1996,\n",
              "       'y': 4249,\n",
              "       'w': 73,\n",
              "       'h': 30,\n",
              "       'confidence': '0.6928710396108171',\n",
              "       'text': 'Steve'},\n",
              "      {'x': 286,\n",
              "       'y': 4274,\n",
              "       'w': 136,\n",
              "       'h': 36,\n",
              "       'confidence': '0.4999700013841887',\n",
              "       'text': '510 to $15'},\n",
              "      {'x': 1208,\n",
              "       'y': 4278,\n",
              "       'w': 300,\n",
              "       'h': 37,\n",
              "       'confidence': '0.6527953011558812',\n",
              "       'text': 'For 534.99 to buy season'},\n",
              "      {'x': 1697,\n",
              "       'y': 4284,\n",
              "       'w': 82,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7117880086550257',\n",
              "       'text': 'Franks'},\n",
              "      {'x': 1792,\n",
              "       'y': 4288,\n",
              "       'w': 82,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9980833227547308',\n",
              "       'text': 'James'},\n",
              "      {'x': 1868,\n",
              "       'y': 4281,\n",
              "       'w': 211,\n",
              "       'h': 39,\n",
              "       'confidence': '0.8117847887744913',\n",
              "       'text': 'Roday Rodriguez'},\n",
              "      {'x': 2545,\n",
              "       'y': 4292,\n",
              "       'w': 359,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8326778433084102',\n",
              "       'text': 'PRIMETIME EMMY? nominee'},\n",
              "      {'x': 286,\n",
              "       'y': 4309,\n",
              "       'w': 136,\n",
              "       'h': 36,\n",
              "       'confidence': '0.4918433719422026',\n",
              "       'text': '515 to S20'},\n",
              "      {'x': 1208,\n",
              "       'y': 4322,\n",
              "       'w': 170,\n",
              "       'h': 36,\n",
              "       'confidence': '0.7693419679834789',\n",
              "       'text': 'Or $0.00 with'},\n",
              "      {'x': 1393,\n",
              "       'y': 4322,\n",
              "       'w': 231,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9812259560094939',\n",
              "       'text': 'Prime membership'},\n",
              "      {'x': 1710,\n",
              "       'y': 4323,\n",
              "       'w': 65,\n",
              "       'h': 29,\n",
              "       'confidence': '0.9912953690929662',\n",
              "       'text': 'et al'},\n",
              "      {'x': 287,\n",
              "       'y': 4370,\n",
              "       'w': 125,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999849795731132',\n",
              "       'text': 'Condition'},\n",
              "      {'x': 2635,\n",
              "       'y': 4369,\n",
              "       'w': 144,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9972974868276134',\n",
              "       'text': 'Season 1'},\n",
              "      {'x': 287,\n",
              "       'y': 4418,\n",
              "       'w': 61,\n",
              "       'h': 25,\n",
              "       'confidence': '0.999096691608429',\n",
              "       'text': 'New'},\n",
              "      {'x': 287,\n",
              "       'y': 4474,\n",
              "       'w': 95,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999573675136857',\n",
              "       'text': 'Format'},\n",
              "      {'x': 1203,\n",
              "       'y': 4456,\n",
              "       'w': 214,\n",
              "       'h': 45,\n",
              "       'confidence': '0.9999662406675327',\n",
              "       'text': 'Psych Season'},\n",
              "      {'x': 2544,\n",
              "       'y': 4460,\n",
              "       'w': 906,\n",
              "       'h': 41,\n",
              "       'confidence': '0.6211726488937785',\n",
              "       'text': 'James Roday plays the somewhat eccentric Shawn Spencer;'},\n",
              "      {'x': 3478,\n",
              "       'y': 4460,\n",
              "       'w': 97,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9998614429258561',\n",
              "       'text': 'police'},\n",
              "      {'x': 326,\n",
              "       'y': 4517,\n",
              "       'w': 147,\n",
              "       'h': 30,\n",
              "       'confidence': '0.984611954249115',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 869,\n",
              "       'y': 4472,\n",
              "       'w': 186,\n",
              "       'h': 76,\n",
              "       'confidence': '0.8063558468174334',\n",
              "       'text': 'peych'},\n",
              "      {'x': 1209,\n",
              "       'y': 4500,\n",
              "       'w': 68,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9495017332787029',\n",
              "       'text': '2013'},\n",
              "      {'x': 1308,\n",
              "       'y': 4500,\n",
              "       'w': 39,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9165904917167833',\n",
              "       'text': 'CC'},\n",
              "      {'x': 2544,\n",
              "       'y': 4503,\n",
              "       'w': 1087,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9154273680241156',\n",
              "       'text': 'consultant who solves crimes with powers of observation so acute that'},\n",
              "      {'x': 1390,\n",
              "       'y': 4547,\n",
              "       'w': 73,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999718512093189',\n",
              "       'text': '1,326'},\n",
              "      {'x': 2544,\n",
              "       'y': 4538,\n",
              "       'w': 438,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9230218630956153',\n",
              "       'text': \"the police think he'$ psychic\"},\n",
              "      {'x': 3007,\n",
              "       'y': 4538,\n",
              "       'w': 378,\n",
              "       'h': 36,\n",
              "       'confidence': '0.5419409648766186',\n",
              "       'text': 'or S0 he lets them think:'},\n",
              "      {'x': 287,\n",
              "       'y': 4578,\n",
              "       'w': 78,\n",
              "       'h': 25,\n",
              "       'confidence': '0.9939055735363',\n",
              "       'text': 'Genre'},\n",
              "      {'x': 1209,\n",
              "       'y': 4591,\n",
              "       'w': 155,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7177894969122179',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 1695,\n",
              "       'y': 4589,\n",
              "       'w': 267,\n",
              "       'h': 38,\n",
              "       'confidence': '0.9362746908554749',\n",
              "       'text': 'Starring: James Roday'},\n",
              "      {'x': 1974,\n",
              "       'y': 4591,\n",
              "       'w': 86,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9826079371534815',\n",
              "       'text': 'Shawn'},\n",
              "      {'x': 2727,\n",
              "       'y': 4608,\n",
              "       'w': 51,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999151386219184',\n",
              "       'text': '739'},\n",
              "      {'x': 2796,\n",
              "       'y': 4608,\n",
              "       'w': 121,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9604323541602485',\n",
              "       'text': 'IMDb 8.4'},\n",
              "      {'x': 2930,\n",
              "       'y': 4608,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999858140945435',\n",
              "       'text': '2007'},\n",
              "      {'x': 287,\n",
              "       'y': 4621,\n",
              "       'w': 82,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9981046880712884',\n",
              "       'text': 'Action'},\n",
              "      {'x': 391,\n",
              "       'y': 4621,\n",
              "       'w': 130,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9949315883031272',\n",
              "       'text': 'Adventure'},\n",
              "      {'x': 1697,\n",
              "       'y': 4630,\n",
              "       'w': 104,\n",
              "       'h': 30,\n",
              "       'confidence': '0.36887581954279874',\n",
              "       'text': 'Spencer,'},\n",
              "      {'x': 1810,\n",
              "       'y': 4630,\n",
              "       'w': 242,\n",
              "       'h': 30,\n",
              "       'confidence': '0.4473675958028089',\n",
              "       'text': 'Dule Hill and Corbin'},\n",
              "      {'x': 287,\n",
              "       'y': 4656,\n",
              "       'w': 130,\n",
              "       'h': 29,\n",
              "       'confidence': '0.9999192936825748',\n",
              "       'text': 'Animation'},\n",
              "      {'x': 1202,\n",
              "       'y': 4636,\n",
              "       'w': 311,\n",
              "       'h': 56,\n",
              "       'confidence': '0.6855254578212047',\n",
              "       'text': 'For *299 to buy episode'},\n",
              "      {'x': 1697,\n",
              "       'y': 4664,\n",
              "       'w': 104,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999467873719561',\n",
              "       'text': 'Bernsen'},\n",
              "      {'x': 2547,\n",
              "       'y': 4663,\n",
              "       'w': 112,\n",
              "       'h': 39,\n",
              "       'confidence': '0.9939527722190847',\n",
              "       'text': 'Comedy'},\n",
              "      {'x': 2695,\n",
              "       'y': 4663,\n",
              "       'w': 127,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9954190078821036',\n",
              "       'text': 'Suspense'},\n",
              "      {'x': 3095,\n",
              "       'y': 4643,\n",
              "       'w': 383,\n",
              "       'h': 77,\n",
              "       'confidence': '0.9952330939086883',\n",
              "       'text': 'Action: Click'},\n",
              "      {'x': 3631,\n",
              "       'y': 4637,\n",
              "       'w': 632,\n",
              "       'h': 96,\n",
              "       'confidence': '0.5855708824536582',\n",
              "       'text': 'Episode 1 HD $2.99\"'},\n",
              "      {'x': 287,\n",
              "       'y': 4694,\n",
              "       'w': 151,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7662581681295304',\n",
              "       'text': 'Arts, Culture'},\n",
              "      {'x': 465,\n",
              "       'y': 4694,\n",
              "       'w': 177,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9989231041551517',\n",
              "       'text': 'Entertainment'},\n",
              "      {'x': 1208,\n",
              "       'y': 4693,\n",
              "       'w': 300,\n",
              "       'h': 37,\n",
              "       'confidence': '0.530635886841011',\n",
              "       'text': 'For 534.99 to buy season'},\n",
              "      {'x': 1697,\n",
              "       'y': 4703,\n",
              "       'w': 290,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7805488218482368',\n",
              "       'text': 'Directed by: Mel Damski'},\n",
              "      {'x': 1996,\n",
              "       'y': 4703,\n",
              "       'w': 73,\n",
              "       'h': 30,\n",
              "       'confidence': '0.786025840214382',\n",
              "       'text': 'Steve'},\n",
              "      {'x': 287,\n",
              "       'y': 4729,\n",
              "       'w': 104,\n",
              "       'h': 30,\n",
              "       'confidence': '0.99974410161451',\n",
              "       'text': 'Comedy'},\n",
              "      {'x': 1208,\n",
              "       'y': 4737,\n",
              "       'w': 170,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9330388542176832',\n",
              "       'text': 'Or $0.00 with'},\n",
              "      {'x': 1394,\n",
              "       'y': 4742,\n",
              "       'w': 229,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9812735244723495',\n",
              "       'text': 'Prime membership'},\n",
              "      {'x': 1697,\n",
              "       'y': 4738,\n",
              "       'w': 82,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9928392791083546',\n",
              "       'text': 'Franks'},\n",
              "      {'x': 1791,\n",
              "       'y': 4737,\n",
              "       'w': 288,\n",
              "       'h': 36,\n",
              "       'confidence': '0.8558148069885567',\n",
              "       'text': 'James Roday Rodriguez'},\n",
              "      {'x': 287,\n",
              "       'y': 4768,\n",
              "       'w': 164,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8949498504423591',\n",
              "       'text': 'Documentary'},\n",
              "      {'x': 1710,\n",
              "       'y': 4777,\n",
              "       'w': 65,\n",
              "       'h': 30,\n",
              "       'confidence': '0.504750896239558',\n",
              "       'text': 'et al.'},\n",
              "      {'x': 287,\n",
              "       'y': 4803,\n",
              "       'w': 82,\n",
              "       'h': 30,\n",
              "       'confidence': '0.999692224146474',\n",
              "       'text': 'Drama'},\n",
              "      {'x': 2579,\n",
              "       'y': 4797,\n",
              "       'w': 278,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9935780648519309',\n",
              "       'text': 'Watch with Prime'},\n",
              "      {'x': 3145,\n",
              "       'y': 4797,\n",
              "       'w': 127,\n",
              "       'h': 41,\n",
              "       'confidence': '0.9999625575094643',\n",
              "       'text': 'Episode'},\n",
              "      {'x': 3448,\n",
              "       'y': 4793,\n",
              "       'w': 118,\n",
              "       'h': 41,\n",
              "       'confidence': '0.999979511150598',\n",
              "       'text': 'Season'},\n",
              "      {'x': 3677,\n",
              "       'y': 4794,\n",
              "       'w': 232,\n",
              "       'h': 44,\n",
              "       'confidence': '0.96254729144712',\n",
              "       'text': 'More purchase'},\n",
              "      {'x': 287,\n",
              "       'y': 4842,\n",
              "       'w': 95,\n",
              "       'h': 29,\n",
              "       'confidence': '0.9958418326927505',\n",
              "       'text': 'Fantasy'},\n",
              "      {'x': 2578,\n",
              "       'y': 4831,\n",
              "       'w': 423,\n",
              "       'h': 51,\n",
              "       'confidence': '0.7225171684800967',\n",
              "       'text': 'Start your 30-day free trial'},\n",
              "      {'x': 3072,\n",
              "       'y': 4832,\n",
              "       'w': 157,\n",
              "       'h': 45,\n",
              "       'confidence': '0.7316211805694955',\n",
              "       'text': 'HD $2.99'},\n",
              "      {'x': 3379,\n",
              "       'y': 4832,\n",
              "       'w': 174,\n",
              "       'h': 45,\n",
              "       'confidence': '0.6774419478289473',\n",
              "       'text': 'HD $34.99'},\n",
              "      {'x': 3673,\n",
              "       'y': 4836,\n",
              "       'w': 127,\n",
              "       'h': 41,\n",
              "       'confidence': '0.9999857169372777',\n",
              "       'text': 'options'},\n",
              "      {'x': 4359,\n",
              "       'y': 4805,\n",
              "       'w': 56,\n",
              "       'h': 60,\n",
              "       'confidence': '0.24962467663013274',\n",
              "       'text': '08'},\n",
              "      {'x': 287,\n",
              "       'y': 4876,\n",
              "       'w': 86,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9996397523650223',\n",
              "       'text': 'Horror'},\n",
              "      {'x': 1208,\n",
              "       'y': 4875,\n",
              "       'w': 96,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999976542710154',\n",
              "       'text': 'Bosch'},\n",
              "      {'x': 1329,\n",
              "       'y': 4875,\n",
              "       'w': 144,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9667137893626345',\n",
              "       'text': 'Season 3'},\n",
              "      {'x': 283,\n",
              "       'y': 4915,\n",
              "       'w': 164,\n",
              "       'h': 26,\n",
              "       'confidence': '0.8148624472963694',\n",
              "       'text': 'International'},\n",
              "      {'x': 833,\n",
              "       'y': 4912,\n",
              "       'w': 119,\n",
              "       'h': 19,\n",
              "       'confidence': '0.23265289232885192',\n",
              "       'text': 'amizon'},\n",
              "      {'x': 962,\n",
              "       'y': 4911,\n",
              "       'w': 138,\n",
              "       'h': 25,\n",
              "       'confidence': '0.5942612661573402',\n",
              "       'text': 'oRiginal'},\n",
              "      {'x': 1203,\n",
              "       'y': 4914,\n",
              "       'w': 71,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999985694885254',\n",
              "       'text': '2017'},\n",
              "      {'x': 1307,\n",
              "       'y': 4914,\n",
              "       'w': 88,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9957530902424929',\n",
              "       'text': 'TV-MA'},\n",
              "      {'x': 1429,\n",
              "       'y': 4919,\n",
              "       'w': 39,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9248411484234061',\n",
              "       'text': 'CC'},\n",
              "      {'x': 2593,\n",
              "       'y': 4919,\n",
              "       'w': 129,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9995481298657944',\n",
              "       'text': 'Join Prime'},\n",
              "      {'x': 287,\n",
              "       'y': 4950,\n",
              "       'w': 156,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9990344497015597',\n",
              "       'text': 'Music Videos'},\n",
              "      {'x': 469,\n",
              "       'y': 4950,\n",
              "       'w': 112,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9960566367023705',\n",
              "       'text': 'Concerts'},\n",
              "      {'x': 817,\n",
              "       'y': 4921,\n",
              "       'w': 299,\n",
              "       'h': 109,\n",
              "       'confidence': '0.49661450941371776',\n",
              "       'text': 'BOScH'},\n",
              "      {'x': 1407,\n",
              "       'y': 4963,\n",
              "       'w': 56,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9810856580734253',\n",
              "       'text': \"'782\"},\n",
              "      {'x': 287,\n",
              "       'y': 4984,\n",
              "       'w': 117,\n",
              "       'h': 30,\n",
              "       'confidence': '0.999962332222351',\n",
              "       'text': 'Romance'},\n",
              "      {'x': 2544,\n",
              "       'y': 4979,\n",
              "       'w': 183,\n",
              "       'h': 41,\n",
              "       'confidence': '0.9940388788432443',\n",
              "       'text': 'Terms apply'},\n",
              "      {'x': 287,\n",
              "       'y': 5023,\n",
              "       'w': 177,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9518605897530277',\n",
              "       'text': 'Science Fiction'},\n",
              "      {'x': 1209,\n",
              "       'y': 5010,\n",
              "       'w': 155,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7665930388756772',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 1696,\n",
              "       'y': 5006,\n",
              "       'w': 274,\n",
              "       'h': 37,\n",
              "       'confidence': '0.7134498429984544',\n",
              "       'text': 'Starring: Titus Welliver'},\n",
              "      {'x': 1978,\n",
              "       'y': 5010,\n",
              "       'w': 78,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7341447794222655',\n",
              "       'text': 'Jamie'},\n",
              "      {'x': 313,\n",
              "       'y': 5062,\n",
              "       'w': 117,\n",
              "       'h': 26,\n",
              "       'confidence': '0.649215776200284',\n",
              "       'text': 'See more'},\n",
              "      {'x': 1697,\n",
              "       'y': 5045,\n",
              "       'w': 86,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9904024909282815',\n",
              "       'text': 'Hector'},\n",
              "      {'x': 1791,\n",
              "       'y': 5044,\n",
              "       'w': 205,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9954534218674623',\n",
              "       'text': 'Amy Aquino and'},\n",
              "      {'x': 1265,\n",
              "       'y': 5075,\n",
              "       'w': 263,\n",
              "       'h': 26,\n",
              "       'confidence': '0.7838216814208566',\n",
              "       'text': 'Watch now: Free with ads'},\n",
              "      {'x': 1697,\n",
              "       'y': 5084,\n",
              "       'w': 169,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9763734471595076',\n",
              "       'text': 'Madison Lintz'},\n",
              "      {'x': 2548,\n",
              "       'y': 5100,\n",
              "       'w': 140,\n",
              "       'h': 36,\n",
              "       'confidence': '0.926190924083751',\n",
              "       'text': 'Episodes'},\n",
              "      {'x': 2721,\n",
              "       'y': 5096,\n",
              "       'w': 127,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999979273829965',\n",
              "       'text': 'Related'},\n",
              "      {'x': 2881,\n",
              "       'y': 5096,\n",
              "       'w': 114,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9999908534406047',\n",
              "       'text': 'Details'},\n",
              "      {'x': 286,\n",
              "       'y': 5117,\n",
              "       'w': 171,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9996432307628514',\n",
              "       'text': 'Video Length'},\n",
              "      {'x': 1696,\n",
              "       'y': 5117,\n",
              "       'w': 339,\n",
              "       'h': 37,\n",
              "       'confidence': '0.7543648527777784',\n",
              "       'text': 'Directed by: Adam Davidson ,'},\n",
              "      {'x': 1208,\n",
              "       'y': 5135,\n",
              "       'w': 170,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9468598820017944',\n",
              "       'text': 'Or $0.00 with'},\n",
              "      {'x': 1393,\n",
              "       'y': 5135,\n",
              "       'w': 231,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9890128560644751',\n",
              "       'text': 'Prime membership'},\n",
              "      {'x': 325,\n",
              "       'y': 5161,\n",
              "       'w': 71,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9924531240420943',\n",
              "       'text': 'Up to'},\n",
              "      {'x': 417,\n",
              "       'y': 5162,\n",
              "       'w': 104,\n",
              "       'h': 30,\n",
              "       'confidence': '0.999867800303433',\n",
              "       'text': 'Minutes'},\n",
              "      {'x': 1697,\n",
              "       'y': 5157,\n",
              "       'w': 195,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7861779961983838',\n",
              "       'text': 'Alex Zakrzewski ,'},\n",
              "      {'x': 1900,\n",
              "       'y': 5152,\n",
              "       'w': 118,\n",
              "       'h': 36,\n",
              "       'confidence': '0.8732828567329605',\n",
              "       'text': 'Sarah Pia'},\n",
              "      {'x': 326,\n",
              "       'y': 5200,\n",
              "       'w': 203,\n",
              "       'h': 30,\n",
              "       'confidence': '0.5888104485972826',\n",
              "       'text': '11t0 20 Minutes'},\n",
              "      {'x': 1697,\n",
              "       'y': 5192,\n",
              "       'w': 121,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9998939983488226',\n",
              "       'text': 'Anderson'},\n",
              "      {'x': 1827,\n",
              "       'y': 5192,\n",
              "       'w': 65,\n",
              "       'h': 30,\n",
              "       'confidence': '0.6075091678316357',\n",
              "       'text': 'et al.'},\n",
              "      {'x': 326,\n",
              "       'y': 5235,\n",
              "       'w': 203,\n",
              "       'h': 30,\n",
              "       'confidence': '0.70088241925712',\n",
              "       'text': '21 t0 30 Minutes'},\n",
              "      {'x': 4577,\n",
              "       'y': 5238,\n",
              "       'w': 79,\n",
              "       'h': 41,\n",
              "       'confidence': '0.9999500513076782',\n",
              "       'text': 'Sort'},\n",
              "      {'x': 322,\n",
              "       'y': 5274,\n",
              "       'w': 207,\n",
              "       'h': 30,\n",
              "       'confidence': '0.49642419118112946',\n",
              "       'text': '31 to 60 Minutes'},\n",
              "      {'x': 326,\n",
              "       'y': 5309,\n",
              "       'w': 203,\n",
              "       'h': 30,\n",
              "       'confidence': '0.6566842875856942',\n",
              "       'text': '61 to 90 Minutes'},\n",
              "      {'x': 1208,\n",
              "       'y': 5290,\n",
              "       'w': 321,\n",
              "       'h': 41,\n",
              "       'confidence': '0.8773204839451406',\n",
              "       'text': 'Hollywood Homicide'},\n",
              "      {'x': 326,\n",
              "       'y': 5348,\n",
              "       'w': 268,\n",
              "       'h': 29,\n",
              "       'confidence': '0.6416611893769193',\n",
              "       'text': 'More Than 90 Minutes'},\n",
              "      {'x': 1204,\n",
              "       'y': 5335,\n",
              "       'w': 73,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999058842658997',\n",
              "       'text': '2003'},\n",
              "      {'x': 1308,\n",
              "       'y': 5335,\n",
              "       'w': 82,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7845349973247815',\n",
              "       'text': 'PG-13'},\n",
              "      {'x': 1425,\n",
              "       'y': 5335,\n",
              "       'w': 34,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9695600535432867',\n",
              "       'text': 'CC'},\n",
              "      {'x': 2967,\n",
              "       'y': 5341,\n",
              "       'w': 228,\n",
              "       'h': 51,\n",
              "       'confidence': '0.5825681578172452',\n",
              "       'text': '51 E1 - Psych'},\n",
              "      {'x': 1390,\n",
              "       'y': 5378,\n",
              "       'w': 52,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999905709103443',\n",
              "       'text': '609'},\n",
              "      {'x': 287,\n",
              "       'y': 5404,\n",
              "       'w': 95,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9992110868276357',\n",
              "       'text': 'Decade'},\n",
              "      {'x': 2972,\n",
              "       'y': 5403,\n",
              "       'w': 326,\n",
              "       'h': 40,\n",
              "       'confidence': '0.8056074388372636',\n",
              "       'text': 'July 6, 2006 1h 6 min'},\n",
              "      {'x': 3315,\n",
              "       'y': 5404,\n",
              "       'w': 51,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9733342524526458',\n",
              "       'text': '13+'},\n",
              "      {'x': 1208,\n",
              "       'y': 5420,\n",
              "       'w': 157,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9993080250573056',\n",
              "       'text': 'Prime Video'},\n",
              "      {'x': 1697,\n",
              "       'y': 5425,\n",
              "       'w': 272,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7593454392633849',\n",
              "       'text': 'Starring: Harrison Ford'},\n",
              "      {'x': 1978,\n",
              "       'y': 5425,\n",
              "       'w': 65,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8185861189822426',\n",
              "       'text': 'Josh'},\n",
              "      {'x': 326,\n",
              "       'y': 5447,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.992163659623242',\n",
              "       'text': '2020'},\n",
              "      {'x': 417,\n",
              "       'y': 5451,\n",
              "       'w': 82,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999231237255782',\n",
              "       'text': 'Newer'},\n",
              "      {'x': 1697,\n",
              "       'y': 5460,\n",
              "       'w': 112,\n",
              "       'h': 30,\n",
              "       'confidence': '0.5196377349789545',\n",
              "       'text': 'Hartnett .'},\n",
              "      {'x': 1817,\n",
              "       'y': 5455,\n",
              "       'w': 224,\n",
              "       'h': 43,\n",
              "       'confidence': '0.9485102473799144',\n",
              "       'text': 'Isaiah Washington ,'},\n",
              "      {'x': 2052,\n",
              "       'y': 5464,\n",
              "       'w': 30,\n",
              "       'h': 26,\n",
              "       'confidence': '0.7537987281514168',\n",
              "       'text': 'et'},\n",
              "      {'x': 2971,\n",
              "       'y': 5450,\n",
              "       'w': 1336,\n",
              "       'h': 55,\n",
              "       'confidence': '0.7962302505386596',\n",
              "       'text': 'When the Santa Barbara police department hit a wall on a high profile kidnapping case;'},\n",
              "      {'x': 326,\n",
              "       'y': 5486,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8388739736371865',\n",
              "       'text': '2010'},\n",
              "      {'x': 408,\n",
              "       'y': 5486,\n",
              "       'w': 69,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999998807907104',\n",
              "       'text': '2019'},\n",
              "      {'x': 1204,\n",
              "       'y': 5490,\n",
              "       'w': 52,\n",
              "       'h': 30,\n",
              "       'confidence': '0.9999075679051405',\n",
              "       'text': 'For'},\n",
              "      {'x': 1263,\n",
              "       'y': 5471,\n",
              "       'w': 68,\n",
              "       'h': 51,\n",
              "       'confidence': '0.9999871296397141',\n",
              "       'text': '399'},\n",
              "      {'x': 1330,\n",
              "       'y': 5495,\n",
              "       'w': 90,\n",
              "       'h': 25,\n",
              "       'confidence': '0.35750720858567797',\n",
              "       'text': 't0 rent'},\n",
              "      {'x': 1693,\n",
              "       'y': 5499,\n",
              "       'w': 39,\n",
              "       'h': 30,\n",
              "       'confidence': '0.8796454602748326',\n",
              "       'text': 'al:'},\n",
              "      {'x': 2973,\n",
              "       'y': 5503,\n",
              "       'w': 1111,\n",
              "       'h': 30,\n",
              "       'confidence': '0.6758566999967923',\n",
              "       'text': \"thev reluctantlv hire Shawn Spencer the town's newest Dsvchic detective\"},\n",
              "      {'x': 4093,\n",
              "       'y': 5503,\n",
              "       'w': 264,\n",
              "       'h': 30,\n",
              "       'confidence': '0.528841751484451',\n",
              "       'text': 'to held solve the'},\n",
              "      {'x': 1688,\n",
              "       'y': 50,\n",
              "       'w': 168,\n",
              "       'h': 140,\n",
              "       'confidence': '0.9998497554120749',\n",
              "       'text': 'buy'},\n",
              "      {'x': 2217,\n",
              "       'y': 615,\n",
              "       'w': 62,\n",
              "       'h': 42,\n",
              "       'confidence': '0.9920787076168529',\n",
              "       'text': 'shop'},\n",
              "      {'x': 4678,\n",
              "       'y': 615,\n",
              "       'w': 66,\n",
              "       'h': 43,\n",
              "       'confidence': '0.6087168228752607',\n",
              "       'text': 'shop'},\n",
              "      {'x': 2581,\n",
              "       'y': 1286,\n",
              "       'w': 60,\n",
              "       'h': 47,\n",
              "       'confidence': '0.99998946970357',\n",
              "       'text': 'Buy'},\n",
              "      {'x': 86,\n",
              "       'y': 1901,\n",
              "       'w': 64,\n",
              "       'h': 46,\n",
              "       'confidence': '0.9953294396400452',\n",
              "       'text': 'Top-'},\n",
              "      {'x': 2547,\n",
              "       'y': 1900,\n",
              "       'w': 64,\n",
              "       'h': 47,\n",
              "       'confidence': '0.8335041999816895',\n",
              "       'text': 'Top-'},\n",
              "      {'x': 4678,\n",
              "       'y': 3314,\n",
              "       'w': 66,\n",
              "       'h': 42,\n",
              "       'confidence': '0.9968127012252808',\n",
              "       'text': 'shop'},\n",
              "      {'x': 1361,\n",
              "       'y': 4236,\n",
              "       'w': 48,\n",
              "       'h': 42,\n",
              "       'confidence': '0.9998781798318682',\n",
              "       'text': 'buy'},\n",
              "      {'x': 3515,\n",
              "       'y': 4639,\n",
              "       'w': 112,\n",
              "       'h': 95,\n",
              "       'confidence': '0.9999821053874435',\n",
              "       'text': 'Buy'},\n",
              "      {'x': 3082,\n",
              "       'y': 4794,\n",
              "       'w': 60,\n",
              "       'h': 47,\n",
              "       'confidence': '0.9999765305361424',\n",
              "       'text': 'Buy'},\n",
              "      {'x': 3385,\n",
              "       'y': 4794,\n",
              "       'w': 60,\n",
              "       'h': 46,\n",
              "       'confidence': '0.9999933927534448',\n",
              "       'text': 'Buy'}]}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 73, 'y': 34.04, 'w': 305, 'h': 402.96}},\n",
              "    {'type': 'text',\n",
              "     'value': 'User input: \"Search for Psych and buy the first episode\"',\n",
              "     'md': 'User input: \"Search for Psych and buy the first episode\"',\n",
              "     'bBox': {'x': 82, 'y': 123, 'w': 283, 'h': 532}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      ['Agent step 1', 'Agent step 2'],\n",
              "      ['---', '---'],\n",
              "      ['amazon Prime Video Search Amazon', 'EN Hello, sign in Returns 10'],\n",
              "      ['amazon Prime Video Psych', 'EN - Hello, sign in Returns 10'],\n",
              "      ['Account & Lists Orders', \"psych Action: Click 'Psych'\"],\n",
              "      ['Black Friday Week= All Black Friday Deals', \"Action: Type 'Psych'\"],\n",
              "      ['amazon Home shop now', 'All Black Friday Deals'],\n",
              "      ['prime video Home', 'Join Prime'],\n",
              "      ['prime', 'psychological thrillers'],\n",
              "      ['psychic', 'psychic kids'],\n",
              "      ['psych the movie', 'psychology'],\n",
              "      ['psychic investigators', 'YELLO'],\n",
              "      ['psycho', 'GOLDEN GLOBE@ winner'],\n",
              "      ['Buy Episode', 'Watch now HD $2.99'],\n",
              "      ['Freevee (with ads)', 'Available to buy TV-MA'],\n",
              "      ['Popular movies See more', 'Top-rated movies See more'],\n",
              "      ['Featured Originals and Exclusives', 'Neeson'],\n",
              "      ['Agent step 3', 'Agent step 4'],\n",
              "      ['amazon Prime Video psych', 'EN - Hello, sign in Returns 10'],\n",
              "      ['Account Lists Orders', 'EN - Account Lists Orders'],\n",
              "      ['Black Friday Deals Medical Care Best Sellers Prime',\n",
              "       'Amazon Basics Groceries New Releases Music Customer Service Amazon Home Black Friday Week shop now'],\n",
              "      ['Home Store Free with ads Categories My Stuff',\n",
              "       'Settings Getting Started Help'],\n",
              "      ['prime video Home Movies TV shows Sports Live TV 88 Subscriptions',\n",
              "       'Join Prime'],\n",
              "      ['1-16 of 423 results for \"psych\"', 'Sort By: Featured'],\n",
              "      ['Department Results', 'Any Department Movies TV'],\n",
              "      ['psych Psych Season', 'Prime Video'],\n",
              "      ['Customer Reviews', \"Action: Click 'Psych Season 1'\"],\n",
              "      ['Actor', 'For $34.99 to buy season et al'],\n",
              "      ['James Roday', 'Or $0.00 with Prime membership'],\n",
              "      ['Dule Hill', 'Psych Season 3'],\n",
              "      ['Price', 'psych 2008 CC'],\n",
              "      ['Free', '$0 to $1'],\n",
              "      ['$1 to $3', 'Prime Video'],\n",
              "      ['$3 to $5', 'Starring: James Roday, Dule Hill'],\n",
              "      ['$5 to $10', 'and Timothy Omundson'],\n",
              "      ['$10 to $15', 'For $34.99 to buy season'],\n",
              "      ['$15 to $20', 'Or $0.00 with Prime membership'],\n",
              "      ['Condition', 'New'],\n",
              "      ['Format', 'peych'],\n",
              "      ['Psych Season', '2013 CC'],\n",
              "      ['Genre', 'Prime Video 1,326'],\n",
              "      ['Action Adventure', 'For $2.99 to buy episode'],\n",
              "      ['Animation', 'Comedy'],\n",
              "      ['Documentary', 'Drama'],\n",
              "      ['Fantasy', 'Horror'],\n",
              "      ['International', 'Music Videos Concerts'],\n",
              "      ['Romance', 'Science Fiction'],\n",
              "      ['See more', 'Watch now: Free with ads'],\n",
              "      ['Or $0.00 with Prime membership',\n",
              "       'Directed by: Adam Davidson, Alex Zakrzewski, Sarah Pia Anderson et al.'],\n",
              "      ['Video Length', 'Up to Minutes'],\n",
              "      ['11 to 20 Minutes', '21 to 30 Minutes'],\n",
              "      ['31 to 60 Minutes', '61 to 90 Minutes'],\n",
              "      ['More Than 90 Minutes', 'Decade'],\n",
              "      ['2020 Newer', '2010-2019'],\n",
              "      ['Figure 7: Example of a multimodal agent.', '']],\n",
              "     'md': 'User input: \"Search for Psych and buy the first episode\"\\n|Agent step 1|Agent step 2|\\n|---|---|\\n|amazon Prime Video Search Amazon|EN Hello, sign in Returns 10|\\n|amazon Prime Video Psych|EN - Hello, sign in Returns 10|\\n|Account & Lists Orders|psych Action: Click \\'Psych\\'|\\n|Black Friday Week= All Black Friday Deals|Action: Type \\'Psych\\'|\\n|amazon Home shop now|All Black Friday Deals|\\n|prime video Home|Join Prime|\\n|prime|psychological thrillers|\\n|psychic|psychic kids|\\n|psych the movie|psychology|\\n|psychic investigators|YELLO|\\n|psycho|GOLDEN GLOBE@ winner|\\n|Buy Episode|Watch now HD $2.99|\\n|Freevee (with ads)|Available to buy TV-MA|\\n|Popular movies See more|Top-rated movies See more|\\n|Featured Originals and Exclusives|Neeson|\\n|Agent step 3|Agent step 4|\\n|amazon Prime Video psych|EN - Hello, sign in Returns 10|\\n|Account Lists Orders|EN - Account Lists Orders|\\n|Black Friday Deals Medical Care Best Sellers Prime|Amazon Basics Groceries New Releases Music Customer Service Amazon Home Black Friday Week shop now|\\n|Home Store Free with ads Categories My Stuff|Settings Getting Started Help|\\n|prime video Home Movies TV shows Sports Live TV 88 Subscriptions|Join Prime|\\n|1-16 of 423 results for \"psych\"|Sort By: Featured|\\n|Department Results|Any Department Movies TV|\\n|psych Psych Season|Prime Video|\\n|Customer Reviews|Action: Click \\'Psych Season 1\\'|\\n|Actor|For $34.99 to buy season et al|\\n|James Roday|Or $0.00 with Prime membership|\\n|Dule Hill|Psych Season 3|\\n|Price|psych 2008 CC|\\n|Free|$0 to $1|\\n|$1 to $3|Prime Video|\\n|$3 to $5|Starring: James Roday, Dule Hill|\\n|$5 to $10|and Timothy Omundson|\\n|$10 to $15|For $34.99 to buy season|\\n|$15 to $20|Or $0.00 with Prime membership|\\n|Condition|New|\\n|Format|peych|\\n|Psych Season|2013 CC|\\n|Genre|Prime Video 1,326|\\n|Action Adventure|For $2.99 to buy episode|\\n|Animation|Comedy|\\n|Documentary|Drama|\\n|Fantasy|Horror|\\n|International|Music Videos Concerts|\\n|Romance|Science Fiction|\\n|See more|Watch now: Free with ads|\\n|Or $0.00 with Prime membership|Directed by: Adam Davidson, Alex Zakrzewski, Sarah Pia Anderson et al.|\\n|Video Length|Up to Minutes|\\n|11 to 20 Minutes|21 to 30 Minutes|\\n|31 to 60 Minutes|61 to 90 Minutes|\\n|More Than 90 Minutes|Decade|\\n|2020 Newer|2010-2019|\\n|Figure 7: Example of a multimodal agent.| |',\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\"Agent step 1\",\"Agent step 2\"\\n\"---\",\"---\"\\n\"amazon Prime Video Search Amazon\",\"EN Hello, sign in Returns 10\"\\n\"amazon Prime Video Psych\",\"EN - Hello, sign in Returns 10\"\\n\"Account & Lists Orders\",\"psych Action: Click \\'Psych\\'\"\\n\"Black Friday Week= All Black Friday Deals\",\"Action: Type \\'Psych\\'\"\\n\"amazon Home shop now\",\"All Black Friday Deals\"\\n\"prime video Home\",\"Join Prime\"\\n\"prime\",\"psychological thrillers\"\\n\"psychic\",\"psychic kids\"\\n\"psych the movie\",\"psychology\"\\n\"psychic investigators\",\"YELLO\"\\n\"psycho\",\"GOLDEN GLOBE@ winner\"\\n\"Buy Episode\",\"Watch now HD $2.99\"\\n\"Freevee (with ads)\",\"Available to buy TV-MA\"\\n\"Popular movies See more\",\"Top-rated movies See more\"\\n\"Featured Originals and Exclusives\",\"Neeson\"\\n\"Agent step 3\",\"Agent step 4\"\\n\"amazon Prime Video psych\",\"EN - Hello, sign in Returns 10\"\\n\"Account Lists Orders\",\"EN - Account Lists Orders\"\\n\"Black Friday Deals Medical Care Best Sellers Prime\",\"Amazon Basics Groceries New Releases Music Customer Service Amazon Home Black Friday Week shop now\"\\n\"Home Store Free with ads Categories My Stuff\",\"Settings Getting Started Help\"\\n\"prime video Home Movies TV shows Sports Live TV 88 Subscriptions\",\"Join Prime\"\\n\"1-16 of 423 results for \"\"psych\"\"\",\"Sort By: Featured\"\\n\"Department Results\",\"Any Department Movies TV\"\\n\"psych Psych Season\",\"Prime Video\"\\n\"Customer Reviews\",\"Action: Click \\'Psych Season 1\\'\"\\n\"Actor\",\"For $34.99 to buy season et al\"\\n\"James Roday\",\"Or $0.00 with Prime membership\"\\n\"Dule Hill\",\"Psych Season 3\"\\n\"Price\",\"psych 2008 CC\"\\n\"Free\",\"$0 to $1\"\\n\"$1 to $3\",\"Prime Video\"\\n\"$3 to $5\",\"Starring: James Roday, Dule Hill\"\\n\"$5 to $10\",\"and Timothy Omundson\"\\n\"$10 to $15\",\"For $34.99 to buy season\"\\n\"$15 to $20\",\"Or $0.00 with Prime membership\"\\n\"Condition\",\"New\"\\n\"Format\",\"peych\"\\n\"Psych Season\",\"2013 CC\"\\n\"Genre\",\"Prime Video 1,326\"\\n\"Action Adventure\",\"For $2.99 to buy episode\"\\n\"Animation\",\"Comedy\"\\n\"Documentary\",\"Drama\"\\n\"Fantasy\",\"Horror\"\\n\"International\",\"Music Videos Concerts\"\\n\"Romance\",\"Science Fiction\"\\n\"See more\",\"Watch now: Free with ads\"\\n\"Or $0.00 with Prime membership\",\"Directed by: Adam Davidson, Alex Zakrzewski, Sarah Pia Anderson et al.\"\\n\"Video Length\",\"Up to Minutes\"\\n\"11 to 20 Minutes\",\"21 to 30 Minutes\"\\n\"31 to 60 Minutes\",\"61 to 90 Minutes\"\\n\"More Than 90 Minutes\",\"Decade\"\\n\"2020 Newer\",\"2010-2019\"\\n\"Figure 7: Example of a multimodal agent.\",\"\"',\n",
              "     'bBox': {'x': 73, 'y': 34.04, 'w': 466, 'h': 716}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 41,\n",
              "   'text': \"                                    The Amazon Nova Family of Models\\n     User input                                       Response\\nWhat happened to my tire?                        It appears that your tire has a significant puncture, as\\nWhat should | do?                                 evidenced by the deflated state and visible damage. Here's\\n                                                  what you should do:\\n                                                  1. Assess the Damage: Confirm the puncture by checking for\\n                                                      any visible holes or debris embedded in the tire:\\n                                                  2. Check Tire Pressure: Ensure the tire is completely deflated.\\n                                                  3. Remove the Tire: If    re comfortable, you can remove the\\n                                                                       you'\\n                                                      wheel to inspect the puncture more closely:\\n                                                  4. Temporary Fix: Use a tire repair kit to patch the hole if it's\\n                                                      small: This is a temporary solution:\\n                                                  5.  Professional Repair: Visit a tire shop for a proper repair or\\n                                                      replacement: They can assess if the tire can be patched or if\\n                                                      a new tire is needed.\\n                                                  6.  Inspect Other Tires: Check the other tires for any signs of\\n                                                      damage or wear:\\n                                                  Would you like more detailed instructions on any of these\\n                                                 steps?\\n                             Figure 8: Created with Nova Pro. Image source: [88].\\n                                                      41\",\n",
              "   'md': \"# The Amazon Nova Family of Models\\n\\n|User input|Response|\\n|---|---|\\n|What happened to my tire?|It appears that your tire has a significant puncture, as evidenced by the deflated state and visible damage. Here's what you should do:|\\n| |1. Assess the Damage: Confirm the puncture by checking for any visible holes or debris embedded in the tire.\\n2. Check Tire Pressure: Ensure the tire is completely deflated.\\n3. Remove the Tire: If you're comfortable, you can remove the wheel to inspect the puncture more closely.\\n4. Temporary Fix: Use a tire repair kit to patch the hole if it's small. This is a temporary solution.\\n5. Professional Repair: Visit a tire shop for a proper repair or replacement. They can assess if the tire can be patched or if a new tire is needed.\\n6. Inspect Other Tires: Check the other tires for any signs of damage or wear.\\n|\\n| |Would you like more detailed instructions on any of these steps?|\\n\\nFigure 8: Created with Nova Pro. Image source: [88].\",\n",
              "   'images': [{'name': 'img_p40_1.png',\n",
              "     'height': 1778,\n",
              "     'width': 3190,\n",
              "     'x': 72,\n",
              "     'y': 256.64039999999994,\n",
              "     'original_width': 3190,\n",
              "     'original_height': 1778,\n",
              "     'ocr': [{'x': 207,\n",
              "       'y': 62,\n",
              "       'w': 365,\n",
              "       'h': 92,\n",
              "       'confidence': '0.9998985655994233',\n",
              "       'text': 'User input'},\n",
              "      {'x': 1538,\n",
              "       'y': 63,\n",
              "       'w': 337,\n",
              "       'h': 98,\n",
              "       'confidence': '0.9999960381929892',\n",
              "       'text': 'Response'},\n",
              "      {'x': 76,\n",
              "       'y': 265,\n",
              "       'w': 727,\n",
              "       'h': 77,\n",
              "       'confidence': '0.74907757966919',\n",
              "       'text': 'What happened to my tire?'},\n",
              "      {'x': 1428,\n",
              "       'y': 257,\n",
              "       'w': 1405,\n",
              "       'h': 86,\n",
              "       'confidence': '0.8678158402406111',\n",
              "       'text': 'It appears that your tire has a significant puncture, as'},\n",
              "      {'x': 77,\n",
              "       'y': 343,\n",
              "       'w': 481,\n",
              "       'h': 65,\n",
              "       'confidence': '0.7793777700308451',\n",
              "       'text': 'What should | do?'},\n",
              "      {'x': 1429,\n",
              "       'y': 340,\n",
              "       'w': 1547,\n",
              "       'h': 79,\n",
              "       'confidence': '0.8191668091659111',\n",
              "       'text': \"evidenced by the deflated state and visible damage. Here's\"},\n",
              "      {'x': 1431,\n",
              "       'y': 417,\n",
              "       'w': 543,\n",
              "       'h': 72,\n",
              "       'confidence': '0.9317608190189094',\n",
              "       'text': 'what you should do:'},\n",
              "      {'x': 1433,\n",
              "       'y': 501,\n",
              "       'w': 56,\n",
              "       'h': 56,\n",
              "       'confidence': '0.994519250024855',\n",
              "       'text': '1.'},\n",
              "      {'x': 1515,\n",
              "       'y': 488,\n",
              "       'w': 1524,\n",
              "       'h': 85,\n",
              "       'confidence': '0.839888696734239',\n",
              "       'text': 'Assess the Damage: Confirm the puncture by checking for'},\n",
              "      {'x': 1508,\n",
              "       'y': 570,\n",
              "       'w': 1269,\n",
              "       'h': 78,\n",
              "       'confidence': '0.5460592944053829',\n",
              "       'text': 'any visible holes or debris embedded in the tire:'},\n",
              "      {'x': 1431,\n",
              "       'y': 653,\n",
              "       'w': 58,\n",
              "       'h': 56,\n",
              "       'confidence': '0.9984289586003544',\n",
              "       'text': '2.'},\n",
              "      {'x': 1514,\n",
              "       'y': 649,\n",
              "       'w': 1552,\n",
              "       'h': 79,\n",
              "       'confidence': '0.656593935369633',\n",
              "       'text': 'Check Tire Pressure: Ensure the tire is completely deflated.'},\n",
              "      {'x': 1431,\n",
              "       'y': 731,\n",
              "       'w': 58,\n",
              "       'h': 55,\n",
              "       'confidence': '0.9997909580314662',\n",
              "       'text': '3.'},\n",
              "      {'x': 1515,\n",
              "       'y': 727,\n",
              "       'w': 513,\n",
              "       'h': 63,\n",
              "       'confidence': '0.9988775322297723',\n",
              "       'text': 'Remove the Tire: If'},\n",
              "      {'x': 2139,\n",
              "       'y': 726,\n",
              "       'w': 947,\n",
              "       'h': 76,\n",
              "       'confidence': '0.7266379182344462',\n",
              "       'text': 're comfortable, you can remove the'},\n",
              "      {'x': 1512,\n",
              "       'y': 797,\n",
              "       'w': 1156,\n",
              "       'h': 84,\n",
              "       'confidence': '0.8820809333588429',\n",
              "       'text': 'wheel to inspect the puncture more closely:'},\n",
              "      {'x': 1433,\n",
              "       'y': 885,\n",
              "       'w': 53,\n",
              "       'h': 53,\n",
              "       'confidence': '0.9972287063703513',\n",
              "       'text': '4.'},\n",
              "      {'x': 1509,\n",
              "       'y': 875,\n",
              "       'w': 1523,\n",
              "       'h': 87,\n",
              "       'confidence': '0.564719939784979',\n",
              "       'text': \"Temporary Fix: Use a tire repair kit to patch the hole if it's\"},\n",
              "      {'x': 1510,\n",
              "       'y': 954,\n",
              "       'w': 919,\n",
              "       'h': 84,\n",
              "       'confidence': '0.7388713418379286',\n",
              "       'text': 'small: This is a temporary solution:'},\n",
              "      {'x': 1433,\n",
              "       'y': 1040,\n",
              "       'w': 56,\n",
              "       'h': 55,\n",
              "       'confidence': '0.9999289411528949',\n",
              "       'text': '5.'},\n",
              "      {'x': 1514,\n",
              "       'y': 1031,\n",
              "       'w': 1524,\n",
              "       'h': 81,\n",
              "       'confidence': '0.8374793536717562',\n",
              "       'text': 'Professional Repair: Visit a tire shop for a proper repair or'},\n",
              "      {'x': 1509,\n",
              "       'y': 1103,\n",
              "       'w': 1578,\n",
              "       'h': 87,\n",
              "       'confidence': '0.5969811519263037',\n",
              "       'text': 'replacement: They can assess if the tire can be patched or if'},\n",
              "      {'x': 1515,\n",
              "       'y': 1188,\n",
              "       'w': 551,\n",
              "       'h': 63,\n",
              "       'confidence': '0.6936305143138947',\n",
              "       'text': 'a new tire is needed.'},\n",
              "      {'x': 1433,\n",
              "       'y': 1269,\n",
              "       'w': 56,\n",
              "       'h': 55,\n",
              "       'confidence': '0.999904665159737',\n",
              "       'text': '6.'},\n",
              "      {'x': 1511,\n",
              "       'y': 1265,\n",
              "       'w': 1508,\n",
              "       'h': 76,\n",
              "       'confidence': '0.8212998889018067',\n",
              "       'text': 'Inspect Other Tires: Check the other tires for any signs of'},\n",
              "      {'x': 1509,\n",
              "       'y': 1340,\n",
              "       'w': 448,\n",
              "       'h': 80,\n",
              "       'confidence': '0.8497017117035138',\n",
              "       'text': 'damage or wear:'},\n",
              "      {'x': 1434,\n",
              "       'y': 1417,\n",
              "       'w': 1512,\n",
              "       'h': 76,\n",
              "       'confidence': '0.843726429546946',\n",
              "       'text': 'Would you like more detailed instructions on any of these'},\n",
              "      {'x': 1428,\n",
              "       'y': 1495,\n",
              "       'w': 184,\n",
              "       'h': 81,\n",
              "       'confidence': '0.7785940797531822',\n",
              "       'text': 'steps?'},\n",
              "      {'x': 2027,\n",
              "       'y': 734,\n",
              "       'w': 129,\n",
              "       'h': 54,\n",
              "       'confidence': '0.771219254519134',\n",
              "       'text': \"you' \"}]}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['User input', 'Response'],\n",
              "      ['What happened to my tire?',\n",
              "       \"It appears that your tire has a significant puncture, as evidenced by the deflated state and visible damage. Here's what you should do:\"],\n",
              "      ['']],\n",
              "     'md': \"|User input|Response|\\n|What happened to my tire?|It appears that your tire has a significant puncture, as evidenced by the deflated state and visible damage. Here's what you should do:|\\n| |1. Assess the Damage: Confirm the puncture by checking for any visible holes or debris embedded in the tire.\",\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\"User input\",\"Response\"\\n\"What happened to my tire?\",\"It appears that your tire has a significant puncture, as evidenced by the deflated state and visible damage. Here\\'s what you should do:\"\\n\"\"',\n",
              "     'bBox': {'x': 83, 'y': 34.04, 'w': 442, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': \"2. Check Tire Pressure: Ensure the tire is completely deflated.\\n3. Remove the Tire: If you're comfortable, you can remove the wheel to inspect the puncture more closely.\\n4. Temporary Fix: Use a tire repair kit to patch the hole if it's small. This is a temporary solution.\\n5. Professional Repair: Visit a tire shop for a proper repair or replacement. They can assess if the tire can be patched or if a new tire is needed.\\n6. Inspect Other Tires: Check the other tires for any signs of damage or wear.\",\n",
              "     'md': \"2. Check Tire Pressure: Ensure the tire is completely deflated.\\n3. Remove the Tire: If you're comfortable, you can remove the wheel to inspect the puncture more closely.\\n4. Temporary Fix: Use a tire repair kit to patch the hole if it's small. This is a temporary solution.\\n5. Professional Repair: Visit a tire shop for a proper repair or replacement. They can assess if the tire can be patched or if a new tire is needed.\\n6. Inspect Other Tires: Check the other tires for any signs of damage or wear.\",\n",
              "     'bBox': {'x': 282, 'y': 352, 'w': 243, 'h': 101}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      [],\n",
              "      [],\n",
              "      [],\n",
              "      [],\n",
              "      ['',\n",
              "       'Would you like more detailed instructions on any of these steps?']],\n",
              "     'md': \"2. Check Tire Pressure: Ensure the tire is completely deflated.\\n4. Temporary Fix: Use a tire repair kit to patch the hole if it's small. This is a temporary solution.\\n5. Professional Repair: Visit a tire shop for a proper repair or replacement. They can assess if the tire can be patched or if a new tire is needed.\\n6. Inspect Other Tires: Check the other tires for any signs of damage or wear.\\n|\\n| |Would you like more detailed instructions on any of these steps?|\",\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\\n\\n\\n\\n\"\",\"Would you like more detailed instructions on any of these steps?\"',\n",
              "     'bBox': {'x': 83, 'y': 34.04, 'w': 442, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Figure 8: Created with Nova Pro. Image source: [88].',\n",
              "     'md': 'Figure 8: Created with Nova Pro. Image source: [88].',\n",
              "     'bBox': {'x': 198, 'y': 523.04, 'w': 214, 'h': 10}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 42,\n",
              "   'text': '                                             The Amazon Nova Family of Models\\n       User input                                                             Response\\nGroup this sales data by month using pandas and                          import pandas as pd\\nthen create a bar chart of the sales per month:                          import matplotlib.pyplot as plt\\n                                                                           Data\\n                 Month           Sales                                   data =                                    May\\n                 Jan                     1523.13                              Month   [\\'Jan  \\'Jan    Feb  March          June             July\\n                                                                             \\'Sales    July   Aug \"  Dec \\' ] ,\\n                 Jan                         957                                     [1523.13, 957, 133 , 452 , 521, 901, 1334 , 131, 2601, 3005]\\n                 Feb                         133                           Create DataFrame\\n                 March                       452                         df= pd. DataFrame(data)\\n                 May                        521/                         # Group bymonth and sum sales\\n                 June                        901                         monthly_sales = df.groupby Month  [ \\'Sales\\' ] . sum(  . reset_index\\n                 July                      1334]                           Sort months based oncalendar\\n                 July                        131                         monthly_sales [ Month1 = pd. Categorical(\\n                 Aug                        2601                             monthly_sales [ \\'Month\\' ]\\n                                                                             categories= [ \\' Jan     Feb  March  April\\'  May \"            June  July\\n                 Dec                      30051                              ordered-TrueAug \" \\'Sept      \\'Oct  \\'Nov  Dec\\' ] ,\\n                                                                         monthly_sales = monthly_sales. sort_values Month\\'\\n       Generated plot:                                                     Plot barchart\\n                             Total Sales per Month                       plt.figure( figsize=( 10 ,  6) )\\n     3000                                                                plt.bar(monthly_sales [ \\'Month\\' ] , monthly_sales [ \\'Sales\\' ] ,  color=\\'blue\\'\\n                                                                         plt.xlabel  Month\\n     2500                                                                plt.ylabel( \\'Total Sales\\'\\n                                                                         plt.title  Total Sales per Month\\'\\n     2000                                                                plt. show\\n     3\\n    2¹⁵⁰⁰\\n     1000\\n      500\\n             Jan   Feb  March MayMonthJune  July  Aug  Dec\\n                                                 Figure 9: Created with Nova Pro.\\n       User input                                                             Response\\n How many people say the European Union                                  The chart shows that 57% of people say the\\nresponded well to the pandemic?                                          European Union responded well to the pandemic\\n                                                                         This is indicated by the green bar labeled \"Good\" for\\n         Poor marks for America\\'$ response to                            the EU, which extends to a height of 57%. This\\n         coronavirus outbreak                                            suggests that a majority of the people surveyed had\\n         %6 who say each has done a job dealing with the                  apositive view of the EU\\'s response to the\\n         coronavirus outbreak                                            pandemic\\n                                  Bad       Good\\n        Our country               2520            7496\\n                WHO              34              64\\n                 EU             39             57\\n              China        60              37\\n               US     84               15\\n         Note: Percentages are medians hased on 13 countries Those who\\n         did not answer are not shcren\\n         Source: Summer 2020 Global Attitudes Survey. Q10a-e\\n         US. Image Plummets Internationally as Most Say Country Has\\n         Handled Coronavirus Badly\"\\n         PEW RESEARCH CENTER\\n                                  Figure 10: Created with Nova Lite. Image source: ChartQA dataset [50].\\n                                                                   42',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# User input\\n\\nGroup this sales data by month using pandas and then create a bar chart of the sales per month:\\n\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nData\\nMonth           Sales\\nJan                     1523.13\\nJan                         957\\nFeb                         133\\nMarch                       452\\nMay                        521\\nJune                        901\\nJuly                      1334\\nJuly                        131\\nAug                        2601\\nDec                      3005\\nCreate DataFrame\\ndf= pd.DataFrame(data)\\n# Group by month and sum sales\\nmonthly_sales = df.groupby(\\'Month\\')[\\'Sales\\'].sum().reset_index()\\n# Sort months based on calendar\\nmonthly_sales[\\'Month\\'] = pd.Categorical(monthly_sales[\\'Month\\'],\\ncategories=[\\'Jan\\', \\'Feb\\', \\'March\\', \\'April\\', \\'May\\', \\'June\\', \\'July\\', \\'Aug\\', \\'Sept\\', \\'Oct\\', \\'Nov\\', \\'Dec\\'],\\nordered=True)\\nmonthly_sales = monthly_sales.sort_values(\\'Month\\')\\nPlot barchart\\nplt.figure(figsize=(10, 6))\\nplt.bar(monthly_sales[\\'Month\\'], monthly_sales[\\'Sales\\'], color=\\'blue\\')\\nplt.xlabel(\\'Month\\')\\nplt.ylabel(\\'Total Sales\\')\\nplt.title(\\'Total Sales per Month\\')\\nplt.show()\\n# Generated plot:\\n\\n# Total Sales per Month\\n\\n|Month|Sales|\\n|---|---|\\n|Jan|1523.13|\\n|Jan|957|\\n|Feb|133|\\n|March|452|\\n|May|521|\\n|June|901|\\n|July|1334|\\n|July|131|\\n|Aug|2601|\\n|Dec|3005|\\n\\n# User input\\n\\nHow many people say the European Union responded well to the pandemic?\\n\\n# Response\\n\\nThe chart shows that 57% of people say the European Union responded well to the pandemic. This is indicated by the green bar labeled \"Good\" for the EU, which extends to a height of 57%. This suggests that a majority of the people surveyed had a positive view of the EU\\'s response to the pandemic.\\n\\n| |Poor|Good|\\n|---|---|---|\\n|Our country|2520|7496|\\n|WHO|34|64|\\n|EU|39|57|\\n|China|60|37|\\n|US|84|15|\\n\\nNote: Percentages are medians based on 13 countries. Those who did not answer are not shown.\\n\\nSource: Summer 2020 Global Attitudes Survey. Q10a-e\\n\\nUS. Image Plummets Internationally as Most Say Country Has Handled Coronavirus Badly\\n\\nPEW RESEARCH CENTER\\n\\nFigure 10: Created with Nova Lite. Image source: ChartQA dataset [50].',\n",
              "   'images': [{'name': 'img_p41_1.png',\n",
              "     'height': 2328,\n",
              "     'width': 3112,\n",
              "     'x': 72,\n",
              "     'y': 72.49135999999999,\n",
              "     'original_width': 3112,\n",
              "     'original_height': 2328,\n",
              "     'ocr': [{'x': 205,\n",
              "       'y': 62,\n",
              "       'w': 366,\n",
              "       'h': 90,\n",
              "       'confidence': '0.6628547286717683',\n",
              "       'text': 'User input'},\n",
              "      {'x': 1757,\n",
              "       'y': 61,\n",
              "       'w': 340,\n",
              "       'h': 101,\n",
              "       'confidence': '0.9999955324299241',\n",
              "       'text': 'Response'},\n",
              "      {'x': 1651,\n",
              "       'y': 245,\n",
              "       'w': 267,\n",
              "       'h': 51,\n",
              "       'confidence': '0.7061460522444595',\n",
              "       'text': 'import pandas'},\n",
              "      {'x': 1928,\n",
              "       'y': 255,\n",
              "       'w': 43,\n",
              "       'h': 31,\n",
              "       'confidence': '0.9997889351072115',\n",
              "       'text': 'as'},\n",
              "      {'x': 1988,\n",
              "       'y': 249,\n",
              "       'w': 47,\n",
              "       'h': 45,\n",
              "       'confidence': '0.9997774718924124',\n",
              "       'text': 'pd'},\n",
              "      {'x': 69,\n",
              "       'y': 264,\n",
              "       'w': 559,\n",
              "       'h': 78,\n",
              "       'confidence': '0.7450867758499787',\n",
              "       'text': 'Group this sales data'},\n",
              "      {'x': 709,\n",
              "       'y': 260,\n",
              "       'w': 654,\n",
              "       'h': 84,\n",
              "       'confidence': '0.8887413937513079',\n",
              "       'text': 'month using pandas and'},\n",
              "      {'x': 1652,\n",
              "       'y': 300,\n",
              "       'w': 483,\n",
              "       'h': 45,\n",
              "       'confidence': '0.6243432686179393',\n",
              "       'text': 'import matplotlib.pyplot'},\n",
              "      {'x': 2147,\n",
              "       'y': 306,\n",
              "       'w': 43,\n",
              "       'h': 29,\n",
              "       'confidence': '0.999669921840324',\n",
              "       'text': 'as'},\n",
              "      {'x': 2204,\n",
              "       'y': 300,\n",
              "       'w': 69,\n",
              "       'h': 43,\n",
              "       'confidence': '0.9955437103578304',\n",
              "       'text': 'plt'},\n",
              "      {'x': 69,\n",
              "       'y': 341,\n",
              "       'w': 1229,\n",
              "       'h': 73,\n",
              "       'confidence': '0.9485636305676513',\n",
              "       'text': 'then create a bar chart of the sales per month:'},\n",
              "      {'x': 1689,\n",
              "       'y': 403,\n",
              "       'w': 88,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9999737739562988',\n",
              "       'text': 'Data'},\n",
              "      {'x': 1650,\n",
              "       'y': 454,\n",
              "       'w': 88,\n",
              "       'h': 37,\n",
              "       'confidence': '1.0',\n",
              "       'text': 'data'},\n",
              "      {'x': 1750,\n",
              "       'y': 459,\n",
              "       'w': 26,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9959056649123887',\n",
              "       'text': '='},\n",
              "      {'x': 438,\n",
              "       'y': 457,\n",
              "       'w': 164,\n",
              "       'h': 57,\n",
              "       'confidence': '0.9999955751015998',\n",
              "       'text': 'Month'},\n",
              "      {'x': 770,\n",
              "       'y': 456,\n",
              "       'w': 149,\n",
              "       'h': 61,\n",
              "       'confidence': '0.8629570765447315',\n",
              "       'text': 'Sales'},\n",
              "      {'x': 1749,\n",
              "       'y': 502,\n",
              "       'w': 109,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999982940153483',\n",
              "       'text': 'Month'},\n",
              "      {'x': 1912,\n",
              "       'y': 502,\n",
              "       'w': 106,\n",
              "       'h': 40,\n",
              "       'confidence': '0.8769650686514936',\n",
              "       'text': \"['Jan\"},\n",
              "      {'x': 2069,\n",
              "       'y': 500,\n",
              "       'w': 86,\n",
              "       'h': 42,\n",
              "       'confidence': '0.634786133086383',\n",
              "       'text': \"'Jan\"},\n",
              "      {'x': 2224,\n",
              "       'y': 502,\n",
              "       'w': 71,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9970781207084656',\n",
              "       'text': 'Feb'},\n",
              "      {'x': 2362,\n",
              "       'y': 502,\n",
              "       'w': 111,\n",
              "       'h': 40,\n",
              "       'confidence': '0.999998933759633',\n",
              "       'text': 'March'},\n",
              "      {'x': 2681,\n",
              "       'y': 502,\n",
              "       'w': 91,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999806880950928',\n",
              "       'text': 'June'},\n",
              "      {'x': 2839,\n",
              "       'y': 502,\n",
              "       'w': 93,\n",
              "       'h': 45,\n",
              "       'confidence': '0.999989926815033',\n",
              "       'text': 'July'},\n",
              "      {'x': 431,\n",
              "       'y': 543,\n",
              "       'w': 93,\n",
              "       'h': 56,\n",
              "       'confidence': '0.7449458241462708',\n",
              "       'text': 'Jan'},\n",
              "      {'x': 885,\n",
              "       'y': 543,\n",
              "       'w': 213,\n",
              "       'h': 56,\n",
              "       'confidence': '0.797107626599632',\n",
              "       'text': '1523.13'},\n",
              "      {'x': 1946,\n",
              "       'y': 553,\n",
              "       'w': 91,\n",
              "       'h': 43,\n",
              "       'confidence': '0.9999898076057434',\n",
              "       'text': 'July'},\n",
              "      {'x': 2102,\n",
              "       'y': 551,\n",
              "       'w': 79,\n",
              "       'h': 50,\n",
              "       'confidence': '0.5753179469122375',\n",
              "       'text': 'Aug \"'},\n",
              "      {'x': 2243,\n",
              "       'y': 551,\n",
              "       'w': 125,\n",
              "       'h': 45,\n",
              "       'confidence': '0.5684229842662811',\n",
              "       'text': \"Dec ' ] ,\"},\n",
              "      {'x': 1735,\n",
              "       'y': 602,\n",
              "       'w': 123,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999573188462305',\n",
              "       'text': \"'Sales\"},\n",
              "      {'x': 1911,\n",
              "       'y': 601,\n",
              "       'w': 380,\n",
              "       'h': 48,\n",
              "       'confidence': '0.4379503665877404',\n",
              "       'text': '[1523.13, 957, 133 ,'},\n",
              "      {'x': 2304,\n",
              "       'y': 604,\n",
              "       'w': 84,\n",
              "       'h': 43,\n",
              "       'confidence': '0.7427031030109713',\n",
              "       'text': '452 ,'},\n",
              "      {'x': 2403,\n",
              "       'y': 604,\n",
              "       'w': 84,\n",
              "       'h': 43,\n",
              "       'confidence': '0.9999959468841553',\n",
              "       'text': '521,'},\n",
              "      {'x': 2503,\n",
              "       'y': 604,\n",
              "       'w': 84,\n",
              "       'h': 43,\n",
              "       'confidence': '0.8833313777020365',\n",
              "       'text': '901,'},\n",
              "      {'x': 2603,\n",
              "       'y': 604,\n",
              "       'w': 103,\n",
              "       'h': 43,\n",
              "       'confidence': '0.7605743090268744',\n",
              "       'text': '1334 ,'},\n",
              "      {'x': 2721,\n",
              "       'y': 599,\n",
              "       'w': 323,\n",
              "       'h': 50,\n",
              "       'confidence': '0.5750727258563677',\n",
              "       'text': '131, 2601, 3005]'},\n",
              "      {'x': 431,\n",
              "       'y': 628,\n",
              "       'w': 95,\n",
              "       'h': 56,\n",
              "       'confidence': '0.5567623376846313',\n",
              "       'text': 'Jan'},\n",
              "      {'x': 992,\n",
              "       'y': 628,\n",
              "       'w': 103,\n",
              "       'h': 54,\n",
              "       'confidence': '0.999997040503424',\n",
              "       'text': '957'},\n",
              "      {'x': 435,\n",
              "       'y': 707,\n",
              "       'w': 102,\n",
              "       'h': 61,\n",
              "       'confidence': '0.9999993117448777',\n",
              "       'text': 'Feb'},\n",
              "      {'x': 997,\n",
              "       'y': 710,\n",
              "       'w': 101,\n",
              "       'h': 57,\n",
              "       'confidence': '0.7333797812461853',\n",
              "       'text': '133'},\n",
              "      {'x': 1689,\n",
              "       'y': 753,\n",
              "       'w': 327,\n",
              "       'h': 44,\n",
              "       'confidence': '0.7361677703204764',\n",
              "       'text': 'Create DataFrame'},\n",
              "      {'x': 437,\n",
              "       'y': 792,\n",
              "       'w': 163,\n",
              "       'h': 61,\n",
              "       'confidence': '0.7693878741181885',\n",
              "       'text': 'March'},\n",
              "      {'x': 995,\n",
              "       'y': 795,\n",
              "       'w': 103,\n",
              "       'h': 55,\n",
              "       'confidence': '0.9999997246979434',\n",
              "       'text': '452'},\n",
              "      {'x': 1650,\n",
              "       'y': 804,\n",
              "       'w': 50,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999828041291119',\n",
              "       'text': 'df'},\n",
              "      {'x': 1709,\n",
              "       'y': 812,\n",
              "       'w': 26,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9023674810898719',\n",
              "       'text': '='},\n",
              "      {'x': 1748,\n",
              "       'y': 803,\n",
              "       'w': 363,\n",
              "       'h': 49,\n",
              "       'confidence': '0.7882276701622939',\n",
              "       'text': 'pd. DataFrame(data)'},\n",
              "      {'x': 433,\n",
              "       'y': 873,\n",
              "       'w': 123,\n",
              "       'h': 78,\n",
              "       'confidence': '0.9999021307587381',\n",
              "       'text': 'May'},\n",
              "      {'x': 995,\n",
              "       'y': 878,\n",
              "       'w': 103,\n",
              "       'h': 57,\n",
              "       'confidence': '0.6750254034996033',\n",
              "       'text': '521/'},\n",
              "      {'x': 1651,\n",
              "       'y': 909,\n",
              "       'w': 26,\n",
              "       'h': 31,\n",
              "       'confidence': '0.9999995231628986',\n",
              "       'text': '#'},\n",
              "      {'x': 1686,\n",
              "       'y': 903,\n",
              "       'w': 173,\n",
              "       'h': 50,\n",
              "       'confidence': '0.9671685634693481',\n",
              "       'text': 'Group by'},\n",
              "      {'x': 1869,\n",
              "       'y': 906,\n",
              "       'w': 108,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9913080837331406',\n",
              "       'text': 'month'},\n",
              "      {'x': 1988,\n",
              "       'y': 906,\n",
              "       'w': 66,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9899277687072754',\n",
              "       'text': 'and'},\n",
              "      {'x': 2069,\n",
              "       'y': 912,\n",
              "       'w': 62,\n",
              "       'h': 30,\n",
              "       'confidence': '0.7601930499076843',\n",
              "       'text': 'sum'},\n",
              "      {'x': 2146,\n",
              "       'y': 906,\n",
              "       'w': 108,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999279751615774',\n",
              "       'text': 'sales'},\n",
              "      {'x': 431,\n",
              "       'y': 961,\n",
              "       'w': 127,\n",
              "       'h': 56,\n",
              "       'confidence': '0.6494529987323368',\n",
              "       'text': 'June'},\n",
              "      {'x': 992,\n",
              "       'y': 961,\n",
              "       'w': 103,\n",
              "       'h': 56,\n",
              "       'confidence': '0.9999996558724309',\n",
              "       'text': '901'},\n",
              "      {'x': 1650,\n",
              "       'y': 957,\n",
              "       'w': 266,\n",
              "       'h': 45,\n",
              "       'confidence': '0.7842421263353325',\n",
              "       'text': 'monthly_sales'},\n",
              "      {'x': 1928,\n",
              "       'y': 963,\n",
              "       'w': 26,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9978067992508386',\n",
              "       'text': '='},\n",
              "      {'x': 1967,\n",
              "       'y': 953,\n",
              "       'w': 210,\n",
              "       'h': 50,\n",
              "       'confidence': '0.8308851746798424',\n",
              "       'text': 'df.groupby'},\n",
              "      {'x': 2204,\n",
              "       'y': 957,\n",
              "       'w': 111,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9999969339786576',\n",
              "       'text': 'Month'},\n",
              "      {'x': 2325,\n",
              "       'y': 953,\n",
              "       'w': 564,\n",
              "       'h': 50,\n",
              "       'confidence': '0.6548727867452127',\n",
              "       'text': \" [ 'Sales' ] . sum(  . reset_index\"},\n",
              "      {'x': 431,\n",
              "       'y': 1044,\n",
              "       'w': 109,\n",
              "       'h': 73,\n",
              "       'confidence': '0.8996173838283181',\n",
              "       'text': 'July'},\n",
              "      {'x': 964,\n",
              "       'y': 1040,\n",
              "       'w': 136,\n",
              "       'h': 64,\n",
              "       'confidence': '0.456758245268208',\n",
              "       'text': '1334]'},\n",
              "      {'x': 1689,\n",
              "       'y': 1057,\n",
              "       'w': 88,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999857544898987',\n",
              "       'text': 'Sort'},\n",
              "      {'x': 1788,\n",
              "       'y': 1057,\n",
              "       'w': 128,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999234461682571',\n",
              "       'text': 'months'},\n",
              "      {'x': 1927,\n",
              "       'y': 1057,\n",
              "       'w': 108,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999974943349246',\n",
              "       'text': 'based'},\n",
              "      {'x': 2046,\n",
              "       'y': 1061,\n",
              "       'w': 50,\n",
              "       'h': 36,\n",
              "       'confidence': '0.9996966404632243',\n",
              "       'text': 'on'},\n",
              "      {'x': 2107,\n",
              "       'y': 1057,\n",
              "       'w': 166,\n",
              "       'h': 40,\n",
              "       'confidence': '0.8868142003959861',\n",
              "       'text': 'calendar'},\n",
              "      {'x': 1649,\n",
              "       'y': 1107,\n",
              "       'w': 285,\n",
              "       'h': 49,\n",
              "       'confidence': '0.7247811960764724',\n",
              "       'text': 'monthly_sales ['},\n",
              "      {'x': 1946,\n",
              "       'y': 1108,\n",
              "       'w': 111,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9719580316647767',\n",
              "       'text': 'Month'},\n",
              "      {'x': 2070,\n",
              "       'y': 1112,\n",
              "       'w': 17,\n",
              "       'h': 31,\n",
              "       'confidence': '0.9254704002889866',\n",
              "       'text': '1'},\n",
              "      {'x': 2105,\n",
              "       'y': 1116,\n",
              "       'w': 26,\n",
              "       'h': 26,\n",
              "       'confidence': '0.7093282172900786',\n",
              "       'text': '='},\n",
              "      {'x': 2146,\n",
              "       'y': 1108,\n",
              "       'w': 305,\n",
              "       'h': 44,\n",
              "       'confidence': '0.8592975165104563',\n",
              "       'text': 'pd. Categorical('},\n",
              "      {'x': 427,\n",
              "       'y': 1123,\n",
              "       'w': 116,\n",
              "       'h': 78,\n",
              "       'confidence': '0.9537892101519285',\n",
              "       'text': 'July'},\n",
              "      {'x': 995,\n",
              "       'y': 1128,\n",
              "       'w': 100,\n",
              "       'h': 57,\n",
              "       'confidence': '0.9999981417113152',\n",
              "       'text': '131'},\n",
              "      {'x': 1730,\n",
              "       'y': 1159,\n",
              "       'w': 446,\n",
              "       'h': 45,\n",
              "       'confidence': '0.7078350743052692',\n",
              "       'text': \"monthly_sales [ 'Month' ]\"},\n",
              "      {'x': 962,\n",
              "       'y': 1210,\n",
              "       'w': 134,\n",
              "       'h': 61,\n",
              "       'confidence': '1.0',\n",
              "       'text': '2601'},\n",
              "      {'x': 1729,\n",
              "       'y': 1206,\n",
              "       'w': 329,\n",
              "       'h': 49,\n",
              "       'confidence': '0.7535657726891194',\n",
              "       'text': \"categories= [ ' Jan\"},\n",
              "      {'x': 2126,\n",
              "       'y': 1210,\n",
              "       'w': 72,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9999991052683532',\n",
              "       'text': 'Feb'},\n",
              "      {'x': 2265,\n",
              "       'y': 1210,\n",
              "       'w': 108,\n",
              "       'h': 40,\n",
              "       'confidence': '0.813309352413769',\n",
              "       'text': 'March'},\n",
              "      {'x': 2440,\n",
              "       'y': 1204,\n",
              "       'w': 130,\n",
              "       'h': 51,\n",
              "       'confidence': '0.9994682813739889',\n",
              "       'text': \"April'\"},\n",
              "      {'x': 2617,\n",
              "       'y': 1207,\n",
              "       'w': 79,\n",
              "       'h': 48,\n",
              "       'confidence': '0.7584798871236232',\n",
              "       'text': 'May \"'},\n",
              "      {'x': 2761,\n",
              "       'y': 1210,\n",
              "       'w': 88,\n",
              "       'h': 37,\n",
              "       'confidence': '0.8992589358566507',\n",
              "       'text': 'June'},\n",
              "      {'x': 2919,\n",
              "       'y': 1210,\n",
              "       'w': 91,\n",
              "       'h': 42,\n",
              "       'confidence': '0.9171257832017555',\n",
              "       'text': 'July'},\n",
              "      {'x': 1983,\n",
              "       'y': 1258,\n",
              "       'w': 77,\n",
              "       'h': 50,\n",
              "       'confidence': '0.8104181340016374',\n",
              "       'text': 'Aug \"'},\n",
              "      {'x': 2107,\n",
              "       'y': 1256,\n",
              "       'w': 110,\n",
              "       'h': 50,\n",
              "       'confidence': '0.993541185690036',\n",
              "       'text': \"'Sept\"},\n",
              "      {'x': 2270,\n",
              "       'y': 1258,\n",
              "       'w': 86,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9995055198669434',\n",
              "       'text': \"'Oct\"},\n",
              "      {'x': 2409,\n",
              "       'y': 1262,\n",
              "       'w': 80,\n",
              "       'h': 33,\n",
              "       'confidence': '0.9946573551765109',\n",
              "       'text': \"'Nov\"},\n",
              "      {'x': 2561,\n",
              "       'y': 1261,\n",
              "       'w': 126,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9128742919836034',\n",
              "       'text': \"Dec' ] ,\"},\n",
              "      {'x': 437,\n",
              "       'y': 1293,\n",
              "       'w': 107,\n",
              "       'h': 61,\n",
              "       'confidence': '0.9999785264700803',\n",
              "       'text': 'Dec'},\n",
              "      {'x': 962,\n",
              "       'y': 1293,\n",
              "       'w': 137,\n",
              "       'h': 61,\n",
              "       'confidence': '0.5938760741975789',\n",
              "       'text': '30051'},\n",
              "      {'x': 1730,\n",
              "       'y': 1309,\n",
              "       'w': 247,\n",
              "       'h': 40,\n",
              "       'confidence': '0.966733671458014',\n",
              "       'text': 'ordered-True'},\n",
              "      {'x': 1650,\n",
              "       'y': 1411,\n",
              "       'w': 266,\n",
              "       'h': 45,\n",
              "       'confidence': '0.9042375438574491',\n",
              "       'text': 'monthly_sales'},\n",
              "      {'x': 1931,\n",
              "       'y': 1423,\n",
              "       'w': 22,\n",
              "       'h': 19,\n",
              "       'confidence': '0.9945084355656633',\n",
              "       'text': '='},\n",
              "      {'x': 1964,\n",
              "       'y': 1406,\n",
              "       'w': 510,\n",
              "       'h': 54,\n",
              "       'confidence': '0.9758296089470004',\n",
              "       'text': 'monthly_sales. sort_values'},\n",
              "      {'x': 2501,\n",
              "       'y': 1409,\n",
              "       'w': 144,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9378642242023233',\n",
              "       'text': \"Month' \"},\n",
              "      {'x': 222,\n",
              "       'y': 1479,\n",
              "       'w': 435,\n",
              "       'h': 75,\n",
              "       'confidence': '0.878943156828182',\n",
              "       'text': 'Generated plot:'},\n",
              "      {'x': 1689,\n",
              "       'y': 1511,\n",
              "       'w': 88,\n",
              "       'h': 40,\n",
              "       'confidence': '0.999147891998291',\n",
              "       'text': 'Plot'},\n",
              "      {'x': 1788,\n",
              "       'y': 1511,\n",
              "       'w': 70,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999858219572908',\n",
              "       'text': 'bar'},\n",
              "      {'x': 1869,\n",
              "       'y': 1511,\n",
              "       'w': 108,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999981873912962',\n",
              "       'text': 'chart'},\n",
              "      {'x': 678,\n",
              "       'y': 1585,\n",
              "       'w': 252,\n",
              "       'h': 28,\n",
              "       'confidence': '0.8730038287891561',\n",
              "       'text': 'Total Sales per Month'},\n",
              "      {'x': 1650,\n",
              "       'y': 1562,\n",
              "       'w': 460,\n",
              "       'h': 45,\n",
              "       'confidence': '0.4109185573515574',\n",
              "       'text': 'plt.figure( figsize=( 10 ,'},\n",
              "      {'x': 2126,\n",
              "       'y': 1562,\n",
              "       'w': 65,\n",
              "       'h': 40,\n",
              "       'confidence': '0.999287081733639',\n",
              "       'text': '6) )'},\n",
              "      {'x': 207,\n",
              "       'y': 1636,\n",
              "       'w': 55,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9998063445091248',\n",
              "       'text': '3000'},\n",
              "      {'x': 1650,\n",
              "       'y': 1613,\n",
              "       'w': 618,\n",
              "       'h': 45,\n",
              "       'confidence': '0.7269521612266578',\n",
              "       'text': \"plt.bar(monthly_sales [ 'Month' ] ,\"},\n",
              "      {'x': 2284,\n",
              "       'y': 1613,\n",
              "       'w': 461,\n",
              "       'h': 45,\n",
              "       'confidence': '0.7652835623157632',\n",
              "       'text': \"monthly_sales [ 'Sales' ] ,\"},\n",
              "      {'x': 2761,\n",
              "       'y': 1613,\n",
              "       'w': 261,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9642847062319827',\n",
              "       'text': \"color='blue' \"},\n",
              "      {'x': 1649,\n",
              "       'y': 1660,\n",
              "       'w': 210,\n",
              "       'h': 50,\n",
              "       'confidence': '0.981032021858697',\n",
              "       'text': 'plt.xlabel'},\n",
              "      {'x': 1886,\n",
              "       'y': 1662,\n",
              "       'w': 113,\n",
              "       'h': 40,\n",
              "       'confidence': '0.999997121150701',\n",
              "       'text': 'Month'},\n",
              "      {'x': 1650,\n",
              "       'y': 1713,\n",
              "       'w': 346,\n",
              "       'h': 45,\n",
              "       'confidence': '0.9181211015356511',\n",
              "       'text': \"plt.ylabel( 'Total\"},\n",
              "      {'x': 2007,\n",
              "       'y': 1713,\n",
              "       'w': 142,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9599954329019388',\n",
              "       'text': \"Sales' \"},\n",
              "      {'x': 207,\n",
              "       'y': 1738,\n",
              "       'w': 55,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9999409317970276',\n",
              "       'text': '2500'},\n",
              "      {'x': 1650,\n",
              "       'y': 1764,\n",
              "       'w': 188,\n",
              "       'h': 45,\n",
              "       'confidence': '0.9938683596355227',\n",
              "       'text': 'plt.title'},\n",
              "      {'x': 1869,\n",
              "       'y': 1764,\n",
              "       'w': 224,\n",
              "       'h': 40,\n",
              "       'confidence': '0.7499051293116378',\n",
              "       'text': 'Total Sales'},\n",
              "      {'x': 2107,\n",
              "       'y': 1764,\n",
              "       'w': 220,\n",
              "       'h': 45,\n",
              "       'confidence': '0.9096064820340712',\n",
              "       'text': \"per Month' \"},\n",
              "      {'x': 207,\n",
              "       'y': 1838,\n",
              "       'w': 55,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9976677894592285',\n",
              "       'text': '2000'},\n",
              "      {'x': 1650,\n",
              "       'y': 1815,\n",
              "       'w': 169,\n",
              "       'h': 45,\n",
              "       'confidence': '0.7332466243324262',\n",
              "       'text': 'plt. show'},\n",
              "      {'x': 181,\n",
              "       'y': 1885,\n",
              "       'w': 19,\n",
              "       'h': 56,\n",
              "       'confidence': '0.6121551884373275',\n",
              "       'text': '3'},\n",
              "      {'x': 207,\n",
              "       'y': 1940,\n",
              "       'w': 55,\n",
              "       'h': 24,\n",
              "       'confidence': '0.5550395637795608',\n",
              "       'text': '1500'},\n",
              "      {'x': 177,\n",
              "       'y': 1938,\n",
              "       'w': 24,\n",
              "       'h': 52,\n",
              "       'confidence': '0.5174574482498464',\n",
              "       'text': '2'},\n",
              "      {'x': 207,\n",
              "       'y': 2040,\n",
              "       'w': 55,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9992749094963074',\n",
              "       'text': '1000'},\n",
              "      {'x': 219,\n",
              "       'y': 2139,\n",
              "       'w': 43,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9954513335566669',\n",
              "       'text': '500'},\n",
              "      {'x': 350,\n",
              "       'y': 2263,\n",
              "       'w': 38,\n",
              "       'h': 24,\n",
              "       'confidence': '0.9991668448241406',\n",
              "       'text': 'Jan'},\n",
              "      {'x': 474,\n",
              "       'y': 2261,\n",
              "       'w': 40,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9999927733243802',\n",
              "       'text': 'Feb'},\n",
              "      {'x': 586,\n",
              "       'y': 2261,\n",
              "       'w': 65,\n",
              "       'h': 26,\n",
              "       'confidence': '0.7591729262884082',\n",
              "       'text': 'March'},\n",
              "      {'x': 841,\n",
              "       'y': 2263,\n",
              "       'w': 50,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9998754262924194',\n",
              "       'text': 'June'},\n",
              "      {'x': 968,\n",
              "       'y': 2263,\n",
              "       'w': 45,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9995226860046387',\n",
              "       'text': 'July'},\n",
              "      {'x': 1218,\n",
              "       'y': 2261,\n",
              "       'w': 43,\n",
              "       'h': 26,\n",
              "       'confidence': '0.9999847207512171',\n",
              "       'text': 'Dec'},\n",
              "      {'x': 771,\n",
              "       'y': 2290,\n",
              "       'w': 67,\n",
              "       'h': 24,\n",
              "       'confidence': '0.9517245471784188',\n",
              "       'text': 'Month'},\n",
              "      {'x': 641,\n",
              "       'y': 259,\n",
              "       'w': 54,\n",
              "       'h': 82,\n",
              "       'confidence': '0.99369241305034',\n",
              "       'text': 'by'},\n",
              "      {'x': 2542,\n",
              "       'y': 498,\n",
              "       'w': 67,\n",
              "       'h': 52,\n",
              "       'confidence': '0.9999955951683134',\n",
              "       'text': 'May'},\n",
              "      {'x': 442,\n",
              "       'y': 1203,\n",
              "       'w': 96,\n",
              "       'h': 84,\n",
              "       'confidence': '0.9999896761798278',\n",
              "       'text': 'Aug'},\n",
              "      {'x': 720,\n",
              "       'y': 2258,\n",
              "       'w': 43,\n",
              "       'h': 33,\n",
              "       'confidence': '0.9089048504829407',\n",
              "       'text': 'May'},\n",
              "      {'x': 1092,\n",
              "       'y': 2259,\n",
              "       'w': 43,\n",
              "       'h': 32,\n",
              "       'confidence': '0.9987847208976746',\n",
              "       'text': 'Aug'}]},\n",
              "    {'name': 'img_p41_2.png',\n",
              "     'height': 1677,\n",
              "     'width': 3112,\n",
              "     'x': 72,\n",
              "     'y': 449.42674,\n",
              "     'original_width': 3112,\n",
              "     'original_height': 1677,\n",
              "     'ocr': [{'x': 206,\n",
              "       'y': 63,\n",
              "       'w': 365,\n",
              "       'h': 89,\n",
              "       'confidence': '0.7173779620360612',\n",
              "       'text': 'User input'},\n",
              "      {'x': 1758,\n",
              "       'y': 61,\n",
              "       'w': 339,\n",
              "       'h': 99,\n",
              "       'confidence': '0.9999957010176243',\n",
              "       'text': 'Response'},\n",
              "      {'x': 71,\n",
              "       'y': 263,\n",
              "       'w': 1121,\n",
              "       'h': 80,\n",
              "       'confidence': '0.9729644699789103',\n",
              "       'text': 'How many people say the European Union'},\n",
              "      {'x': 1652,\n",
              "       'y': 266,\n",
              "       'w': 1147,\n",
              "       'h': 77,\n",
              "       'confidence': '0.8752973300064123',\n",
              "       'text': 'The chart shows that 57% of people say the'},\n",
              "      {'x': 66,\n",
              "       'y': 333,\n",
              "       'w': 895,\n",
              "       'h': 91,\n",
              "       'confidence': '0.792331710483227',\n",
              "       'text': 'responded well to the pandemic?'},\n",
              "      {'x': 1652,\n",
              "       'y': 334,\n",
              "       'w': 1315,\n",
              "       'h': 88,\n",
              "       'confidence': '0.9537994881216033',\n",
              "       'text': 'European Union responded well to the pandemic'},\n",
              "      {'x': 1652,\n",
              "       'y': 415,\n",
              "       'w': 1378,\n",
              "       'h': 79,\n",
              "       'confidence': '0.9227472362903618',\n",
              "       'text': 'This is indicated by the green bar labeled \"Good\" for'},\n",
              "      {'x': 1649,\n",
              "       'y': 494,\n",
              "       'w': 1221,\n",
              "       'h': 76,\n",
              "       'confidence': '0.9192973521015405',\n",
              "       'text': 'the EU, which extends to a height of 57%. This'},\n",
              "      {'x': 252,\n",
              "       'y': 535,\n",
              "       'w': 949,\n",
              "       'h': 68,\n",
              "       'confidence': '0.588832646391042',\n",
              "       'text': \"Poor marks for America'$ response to\"},\n",
              "      {'x': 251,\n",
              "       'y': 603,\n",
              "       'w': 543,\n",
              "       'h': 55,\n",
              "       'confidence': '0.9908964473696216',\n",
              "       'text': 'coronavirus outbreak'},\n",
              "      {'x': 1648,\n",
              "       'y': 571,\n",
              "       'w': 1372,\n",
              "       'h': 81,\n",
              "       'confidence': '0.6841820721047641',\n",
              "       'text': 'suggests that a majority of the people surveyed had'},\n",
              "      {'x': 253,\n",
              "       'y': 685,\n",
              "       'w': 497,\n",
              "       'h': 53,\n",
              "       'confidence': '0.48357552052554875',\n",
              "       'text': '%6 who say each has done a'},\n",
              "      {'x': 792,\n",
              "       'y': 680,\n",
              "       'w': 375,\n",
              "       'h': 60,\n",
              "       'confidence': '0.6629150608486664',\n",
              "       'text': 'job dealing with the'},\n",
              "      {'x': 1656,\n",
              "       'y': 666,\n",
              "       'w': 30,\n",
              "       'h': 41,\n",
              "       'confidence': '0.9997227384136664',\n",
              "       'text': 'a'},\n",
              "      {'x': 1694,\n",
              "       'y': 644,\n",
              "       'w': 1077,\n",
              "       'h': 84,\n",
              "       'confidence': '0.9777579369347608',\n",
              "       'text': \"positive view of the EU's response to the\"},\n",
              "      {'x': 254,\n",
              "       'y': 740,\n",
              "       'w': 398,\n",
              "       'h': 43,\n",
              "       'confidence': '0.8766075643214496',\n",
              "       'text': 'coronavirus outbreak'},\n",
              "      {'x': 1649,\n",
              "       'y': 717,\n",
              "       'w': 292,\n",
              "       'h': 88,\n",
              "       'confidence': '0.999902048736947',\n",
              "       'text': 'pandemic'},\n",
              "      {'x': 799,\n",
              "       'y': 821,\n",
              "       'w': 79,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999950445645353',\n",
              "       'text': 'Bad'},\n",
              "      {'x': 925,\n",
              "       'y': 821,\n",
              "       'w': 101,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9998270869255066',\n",
              "       'text': 'Good'},\n",
              "      {'x': 261,\n",
              "       'y': 871,\n",
              "       'w': 208,\n",
              "       'h': 50,\n",
              "       'confidence': '0.9555852273924744',\n",
              "       'text': 'Our country'},\n",
              "      {'x': 801,\n",
              "       'y': 874,\n",
              "       'w': 86,\n",
              "       'h': 43,\n",
              "       'confidence': '0.1636164379430478',\n",
              "       'text': '2520'},\n",
              "      {'x': 1137,\n",
              "       'y': 874,\n",
              "       'w': 86,\n",
              "       'h': 40,\n",
              "       'confidence': '0.8047981124765582',\n",
              "       'text': '7496'},\n",
              "      {'x': 373,\n",
              "       'y': 952,\n",
              "       'w': 91,\n",
              "       'h': 38,\n",
              "       'confidence': '0.9939248408949845',\n",
              "       'text': 'WHO'},\n",
              "      {'x': 772,\n",
              "       'y': 952,\n",
              "       'w': 57,\n",
              "       'h': 42,\n",
              "       'confidence': '0.3570665419101715',\n",
              "       'text': '34'},\n",
              "      {'x': 1117,\n",
              "       'y': 952,\n",
              "       'w': 57,\n",
              "       'h': 40,\n",
              "       'confidence': '0.8435346436728003',\n",
              "       'text': '64'},\n",
              "      {'x': 410,\n",
              "       'y': 1030,\n",
              "       'w': 54,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9782828378657022',\n",
              "       'text': 'EU'},\n",
              "      {'x': 748,\n",
              "       'y': 1030,\n",
              "       'w': 59,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999939308592087',\n",
              "       'text': '39'},\n",
              "      {'x': 1086,\n",
              "       'y': 1030,\n",
              "       'w': 54,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9998586423965649',\n",
              "       'text': '57'},\n",
              "      {'x': 359,\n",
              "       'y': 1105,\n",
              "       'w': 105,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999890710298764',\n",
              "       'text': 'China'},\n",
              "      {'x': 653,\n",
              "       'y': 1108,\n",
              "       'w': 59,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9996728718357353',\n",
              "       'text': '60'},\n",
              "      {'x': 996,\n",
              "       'y': 1108,\n",
              "       'w': 54,\n",
              "       'h': 40,\n",
              "       'confidence': '0.6431979711287937',\n",
              "       'text': '37'},\n",
              "      {'x': 388,\n",
              "       'y': 1183,\n",
              "       'w': 69,\n",
              "       'h': 40,\n",
              "       'confidence': '0.8555225097983744',\n",
              "       'text': 'US'},\n",
              "      {'x': 544,\n",
              "       'y': 1185,\n",
              "       'w': 59,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9999972183091315',\n",
              "       'text': '84'},\n",
              "      {'x': 913,\n",
              "       'y': 1185,\n",
              "       'w': 57,\n",
              "       'h': 40,\n",
              "       'confidence': '0.9853182195762988',\n",
              "       'text': '15'},\n",
              "      {'x': 252,\n",
              "       'y': 1273,\n",
              "       'w': 994,\n",
              "       'h': 48,\n",
              "       'confidence': '0.7563140248213207',\n",
              "       'text': 'Note: Percentages are medians hased on 13 countries Those who'},\n",
              "      {'x': 254,\n",
              "       'y': 1319,\n",
              "       'w': 446,\n",
              "       'h': 38,\n",
              "       'confidence': '0.37794979661396966',\n",
              "       'text': 'did not answer are not shcren'},\n",
              "      {'x': 254,\n",
              "       'y': 1362,\n",
              "       'w': 828,\n",
              "       'h': 44,\n",
              "       'confidence': '0.30338914627186014',\n",
              "       'text': 'Source: Summer 2020 Global Attitudes Survey. Q10a-e'},\n",
              "      {'x': 265,\n",
              "       'y': 1402,\n",
              "       'w': 926,\n",
              "       'h': 52,\n",
              "       'confidence': '0.7289514039887456',\n",
              "       'text': 'US. Image Plummets Internationally as Most Say Country Has'},\n",
              "      {'x': 254,\n",
              "       'y': 1450,\n",
              "       'w': 422,\n",
              "       'h': 43,\n",
              "       'confidence': '0.9334844829657865',\n",
              "       'text': 'Handled Coronavirus Badly\"'},\n",
              "      {'x': 254,\n",
              "       'y': 1514,\n",
              "       'w': 385,\n",
              "       'h': 37,\n",
              "       'confidence': '0.9171970273091365',\n",
              "       'text': 'PEW RESEARCH CENTER'}]}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 229, 'y': 34.04, 'w': 149, 'h': 10}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'User input',\n",
              "     'md': '# User input',\n",
              "     'bBox': {'x': 103, 'y': 82, 'w': 55, 'h': 551}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Group this sales data by month using pandas and then create a bar chart of the sales per month:\\n\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nData\\nMonth           Sales\\nJan                     1523.13\\nJan                         957\\nFeb                         133\\nMarch                       452\\nMay                        521\\nJune                        901\\nJuly                      1334\\nJuly                        131\\nAug                        2601\\nDec                      3005\\nCreate DataFrame\\ndf= pd.DataFrame(data)',\n",
              "     'md': 'Group this sales data by month using pandas and then create a bar chart of the sales per month:\\n\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nData\\nMonth           Sales\\nJan                     1523.13\\nJan                         957\\nFeb                         133\\nMarch                       452\\nMay                        521\\nJune                        901\\nJuly                      1334\\nJuly                        131\\nAug                        2601\\nDec                      3005\\nCreate DataFrame\\ndf= pd.DataFrame(data)',\n",
              "     'bBox': {'x': 82, 'y': 109, 'w': 443, 'h': 525}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Group by month and sum sales',\n",
              "     'md': '# Group by month and sum sales',\n",
              "     'bBox': {'x': 138, 'y': 141, 'w': 235, 'h': 187}},\n",
              "    {'type': 'text',\n",
              "     'value': \"monthly_sales = df.groupby('Month')['Sales'].sum().reset_index()\",\n",
              "     'md': \"monthly_sales = df.groupby('Month')['Sales'].sum().reset_index()\",\n",
              "     'bBox': {'x': 138, 'y': 141, 'w': 332, 'h': 187}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Sort months based on calendar',\n",
              "     'md': '# Sort months based on calendar',\n",
              "     'bBox': {'x': 138, 'y': 141, 'w': 276, 'h': 187}},\n",
              "    {'type': 'text',\n",
              "     'value': \"monthly_sales['Month'] = pd.Categorical(monthly_sales['Month'],\\ncategories=['Jan', 'Feb', 'March', 'April', 'May', 'June', 'July', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec'],\\nordered=True)\\nmonthly_sales = monthly_sales.sort_values('Month')\\nPlot barchart\\nplt.figure(figsize=(10, 6))\\nplt.bar(monthly_sales['Month'], monthly_sales['Sales'], color='blue')\\nplt.xlabel('Month')\\nplt.ylabel('Total Sales')\\nplt.title('Total Sales per Month')\\nplt.show()\",\n",
              "     'md': \"monthly_sales['Month'] = pd.Categorical(monthly_sales['Month'],\\ncategories=['Jan', 'Feb', 'March', 'April', 'May', 'June', 'July', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec'],\\nordered=True)\\nmonthly_sales = monthly_sales.sort_values('Month')\\nPlot barchart\\nplt.figure(figsize=(10, 6))\\nplt.bar(monthly_sales['Month'], monthly_sales['Sales'], color='blue')\\nplt.xlabel('Month')\\nplt.ylabel('Total Sales')\\nplt.title('Total Sales per Month')\\nplt.show()\",\n",
              "     'bBox': {'x': 125, 'y': 141, 'w': 401, 'h': 276}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Generated plot:',\n",
              "     'md': '# Generated plot:',\n",
              "     'bBox': {'x': 105, 'y': 295, 'w': 65, 'h': 11}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Total Sales per Month',\n",
              "     'md': '# Total Sales per Month',\n",
              "     'bBox': {'x': 138, 'y': 141, 'w': 235, 'h': 187}},\n",
              "    {'type': 'table',\n",
              "     'rows': [['Month', 'Sales'],\n",
              "      ['Jan', '1523.13'],\n",
              "      ['Jan', '957'],\n",
              "      ['Feb', '133'],\n",
              "      ['March', '452'],\n",
              "      ['May', '521'],\n",
              "      ['June', '901'],\n",
              "      ['July', '1334'],\n",
              "      ['July', '131'],\n",
              "      ['Aug', '2601'],\n",
              "      ['Dec', '3005']],\n",
              "     'md': '|Month|Sales|\\n|Jan|1523.13|\\n|Jan|957|\\n|Feb|133|\\n|March|452|\\n|May|521|\\n|June|901|\\n|July|1334|\\n|July|131|\\n|Aug|2601|\\n|Dec|3005|',\n",
              "     'isPerfectTable': True,\n",
              "     'csv': '\"Month\",\"Sales\"\\n\"Jan\",\"1523.13\"\\n\"Jan\",\"957\"\\n\"Feb\",\"133\"\\n\"March\",\"452\"\\n\"May\",\"521\"\\n\"June\",\"901\"\\n\"July\",\"1334\"\\n\"July\",\"131\"\\n\"Aug\",\"2601\"\\n\"Dec\",\"3005\"',\n",
              "     'bBox': {'x': 82, 'y': 112, 'w': 448, 'h': 309}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'User input',\n",
              "     'md': '# User input',\n",
              "     'bBox': {'x': 103, 'y': 82, 'w': 55, 'h': 551}},\n",
              "    {'type': 'text',\n",
              "     'value': 'How many people say the European Union responded well to the pandemic?',\n",
              "     'md': 'How many people say the European Union responded well to the pandemic?',\n",
              "     'bBox': {'x': 82, 'y': 489, 'w': 436, 'h': 121}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Response',\n",
              "     'md': '# Response',\n",
              "     'bBox': {'x': 336, 'y': 82, 'w': 51, 'h': 392}},\n",
              "    {'type': 'text',\n",
              "     'value': 'The chart shows that 57% of people say the European Union responded well to the pandemic. This is indicated by the green bar labeled \"Good\" for the EU, which extends to a height of 57%. This suggests that a majority of the people surveyed had a positive view of the EU\\'s response to the pandemic.',\n",
              "     'md': 'The chart shows that 57% of people say the European Union responded well to the pandemic. This is indicated by the green bar labeled \"Good\" for the EU, which extends to a height of 57%. This suggests that a majority of the people surveyed had a positive view of the EU\\'s response to the pandemic.',\n",
              "     'bBox': {'x': 134, 'y': 82, 'w': 393, 'h': 528}},\n",
              "    {'type': 'table',\n",
              "     'rows': [[],\n",
              "      ['', 'Poor', 'Good'],\n",
              "      ['---', '---', '---'],\n",
              "      ['Our country', '2520', '7496'],\n",
              "      ['WHO', '34', '64'],\n",
              "      ['EU', '39', '57'],\n",
              "      ['China', '60', '37'],\n",
              "      ['US', '84', '15']],\n",
              "     'md': 'The chart shows that 57% of people say the European Union responded well to the pandemic. This is indicated by the green bar labeled \"Good\" for the EU, which extends to a height of 57%. This suggests that a majority of the people surveyed had a positive view of the EU\\'s response to the pandemic.\\n| |Poor|Good|\\n|---|---|---|\\n|Our country|2520|7496|\\n|WHO|34|64|\\n|EU|39|57|\\n|China|60|37|\\n|US|84|15|',\n",
              "     'isPerfectTable': False,\n",
              "     'csv': '\\n\"\",\"Poor\",\"Good\"\\n\"---\",\"---\",\"---\"\\n\"Our country\",\"2520\",\"7496\"\\n\"WHO\",\"34\",\"64\"\\n\"EU\",\"39\",\"57\"\\n\"China\",\"60\",\"37\"\\n\"US\",\"84\",\"15\"',\n",
              "     'bBox': {'x': 82, 'y': 34.04, 'w': 448, 'h': 716}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Note: Percentages are medians based on 13 countries. Those who did not answer are not shown.\\n\\nSource: Summer 2020 Global Attitudes Survey. Q10a-e\\n\\nUS. Image Plummets Internationally as Most Say Country Has Handled Coronavirus Badly\\n\\nPEW RESEARCH CENTER\\n\\nFigure 10: Created with Nova Lite. Image source: ChartQA dataset [50].',\n",
              "     'md': 'Note: Percentages are medians based on 13 countries. Those who did not answer are not shown.\\n\\nSource: Summer 2020 Global Attitudes Survey. Q10a-e\\n\\nUS. Image Plummets Internationally as Most Say Country Has Handled Coronavirus Badly\\n\\nPEW RESEARCH CENTER\\n\\nFigure 10: Created with Nova Lite. Image source: ChartQA dataset [50].',\n",
              "     'bBox': {'x': 99, 'y': 133, 'w': 352, 'h': 584.04}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False},\n",
              "  {'page': 43,\n",
              "   'text': '                            The Amazon Nova Family of Models\\nD   Correspondence and Contributors\\nPlease direct all correspondences to:\\n                         nova-technical-reportCamazon . com\\nThe Nova family of models were built by the Amazon AGI organization and partner teams.\\n                                         43',\n",
              "   'md': '# The Amazon Nova Family of Models\\n\\n# Correspondence and Contributors\\n\\nPlease direct all correspondences to:\\n\\nnova-technical-report@amazon.com\\n\\nThe Nova family of models were built by the Amazon AGI organization and partner teams.\\n\\n43',\n",
              "   'images': [{'name': 'img_p42_1.png',\n",
              "     'height': 108,\n",
              "     'width': 1538,\n",
              "     'x': 210.239,\n",
              "     'y': 118.69030000000001,\n",
              "     'original_width': 1538,\n",
              "     'original_height': 108,\n",
              "     'ocr': [{'x': 15,\n",
              "       'y': 12,\n",
              "       'w': 1335,\n",
              "       'h': 96,\n",
              "       'confidence': '0.8680535427172303',\n",
              "       'text': 'nova-technical-reportCamazon .'},\n",
              "      {'x': 1375,\n",
              "       'y': 31,\n",
              "       'w': 146,\n",
              "       'h': 62,\n",
              "       'confidence': '0.9942112019913054',\n",
              "       'text': 'com'}]}],\n",
              "   'charts': [],\n",
              "   'items': [{'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'The Amazon Nova Family of Models',\n",
              "     'md': '# The Amazon Nova Family of Models',\n",
              "     'bBox': {'x': 72, 'y': 34.04, 'w': 306, 'h': 48.01}},\n",
              "    {'type': 'heading',\n",
              "     'lvl': 1,\n",
              "     'value': 'Correspondence and Contributors',\n",
              "     'md': '# Correspondence and Contributors',\n",
              "     'bBox': {'x': 72, 'y': 70.04, 'w': 196, 'h': 12}},\n",
              "    {'type': 'text',\n",
              "     'value': 'Please direct all correspondences to:\\n\\nnova-technical-report@amazon.com\\n\\nThe Nova family of models were built by the Amazon AGI organization and partner teams.\\n\\n43',\n",
              "     'md': 'Please direct all correspondences to:\\n\\nnova-technical-report@amazon.com\\n\\nThe Nova family of models were built by the Amazon AGI organization and partner teams.\\n\\n43',\n",
              "     'bBox': {'x': 71, 'y': 70.04, 'w': 363, 'h': 679.99}}],\n",
              "   'status': 'OK',\n",
              "   'links': [],\n",
              "   'width': 612,\n",
              "   'height': 792,\n",
              "   'triggeredAutoMode': False,\n",
              "   'parsingMode': 'accurate',\n",
              "   'structuredData': None,\n",
              "   'noStructuredContent': False,\n",
              "   'noTextContent': False}],\n",
              " 'job_metadata': {'credits_used': 1659.0,\n",
              "  'job_credits_usage': 0,\n",
              "  'job_pages': 0,\n",
              "  'job_auto_mode_triggered_pages': 6,\n",
              "  'job_is_cache_hit': True,\n",
              "  'credits_max': 3000},\n",
              " 'job_id': '52fb78c8-3815-4cca-b12c-4c7c947d36ce',\n",
              " 'file_path': '/content/data/nova_technical_report.pdf'}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for page in json_output[0][\"pages\"]:\n",
        "    print(f\"Page {page['page']}: {page['triggeredAutoMode']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v_G48EeephJ",
        "outputId": "bf345d6f-1ffa-4ded-865c-a63bdd58e9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1: False\n",
            "Page 2: False\n",
            "Page 3: True\n",
            "Page 4: False\n",
            "Page 5: False\n",
            "Page 6: False\n",
            "Page 7: False\n",
            "Page 8: True\n",
            "Page 9: True\n",
            "Page 10: False\n",
            "Page 11: False\n",
            "Page 12: False\n",
            "Page 13: False\n",
            "Page 14: False\n",
            "Page 15: False\n",
            "Page 16: False\n",
            "Page 17: False\n",
            "Page 18: False\n",
            "Page 19: False\n",
            "Page 20: False\n",
            "Page 21: True\n",
            "Page 22: False\n",
            "Page 23: True\n",
            "Page 24: False\n",
            "Page 25: False\n",
            "Page 26: False\n",
            "Page 27: True\n",
            "Page 28: False\n",
            "Page 29: False\n",
            "Page 30: False\n",
            "Page 31: False\n",
            "Page 32: False\n",
            "Page 33: False\n",
            "Page 34: False\n",
            "Page 35: False\n",
            "Page 36: False\n",
            "Page 37: False\n",
            "Page 38: False\n",
            "Page 39: False\n",
            "Page 40: False\n",
            "Page 41: False\n",
            "Page 42: False\n",
            "Page 43: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Triggering on regular expressions"
      ],
      "metadata": {
        "id": "cvRwe07tfNkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agentic_parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    auto_mode=True,\n",
        "    auto_mode_trigger_on_regexp_in_page=\"/(A|a)gent(s|ic)/g\"\n",
        ")\n",
        "\n",
        "agentic_json_output = agentic_parser.get_json_result(file_path)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD4UlOMVfEWH",
        "outputId": "c7df64ea-640d-45ff-87ed-b3e535f7bd9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id bc838b57-5dc3-4706-97d5-0d0696589e3d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for page in agentic_json_output[\"pages\"]:\n",
        "    print(f\"Page {page['page']}: {page['triggeredAutoMode']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bi_rDeEnbCu",
        "outputId": "3739ff09-6557-4172-b8ba-a8380398080b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1: True\n",
            "Page 2: True\n",
            "Page 3: True\n",
            "Page 4: False\n",
            "Page 5: False\n",
            "Page 6: False\n",
            "Page 7: False\n",
            "Page 8: True\n",
            "Page 9: True\n",
            "Page 10: True\n",
            "Page 11: False\n",
            "Page 12: False\n",
            "Page 13: False\n",
            "Page 14: False\n",
            "Page 15: False\n",
            "Page 16: False\n",
            "Page 17: False\n",
            "Page 18: False\n",
            "Page 19: False\n",
            "Page 20: False\n",
            "Page 21: True\n",
            "Page 22: False\n",
            "Page 23: True\n",
            "Page 24: False\n",
            "Page 25: False\n",
            "Page 26: False\n",
            "Page 27: True\n",
            "Page 28: False\n",
            "Page 29: False\n",
            "Page 30: False\n",
            "Page 31: False\n",
            "Page 32: False\n",
            "Page 33: False\n",
            "Page 34: False\n",
            "Page 35: False\n",
            "Page 36: False\n",
            "Page 37: False\n",
            "Page 38: False\n",
            "Page 39: False\n",
            "Page 40: False\n",
            "Page 41: False\n",
            "Page 42: False\n",
            "Page 43: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up a Simple RAG Pipeline"
      ],
      "metadata": {
        "id": "BzOdVcbxngXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = VectorStoreIndex(page_nodes)\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=3)"
      ],
      "metadata": {
        "id": "Hn2kcYOUndfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Give me a comparison graph of time-to-first-token among all models\"\n",
        ")"
      ],
      "metadata": {
        "id": "_Z170Dn0nuAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zfUXpQHn3c4",
        "outputId": "8c99c20b-36c7-41dc-fea4-cc08c31c1c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot create visual content such as graphs. However, I can provide you with a list of the time-to-first-token (TTFT) values for each model, which you can use to create a comparison graph:\n",
            "\n",
            "- Llama 2 7B: 0.29 sec\n",
            "- Nova MICRO: 0.32 sec\n",
            "- Gemini 1.5 Pro (Fast): 0.35 sec\n",
            "- Gemini 1.5 Pro (Standard): 0.35 sec\n",
            "- Mistral 8x7B: 0.36 sec\n",
            "- Llama 2 13B: 0.36 sec\n",
            "- Nova Lite: 0.37 sec\n",
            "- Nova Pro: 0.38 sec\n",
            "- Llama 2 70B: 0.42 sec\n",
            "- GPT-3.5: 0.42 sec\n",
            "- Llama 2 34B: 0.46 sec\n",
            "- Mistral Large 2 (Nov): 0.53 sec\n",
            "- GPT-4 Turbo: 0.62 sec\n",
            "- Llama 2 34B8: 0.72 sec\n",
            "- Claude 2.1 (Fast): 0.72 sec\n",
            "- Claude 2.1 (Standard): 0.87 sec\n",
            "- Gemini 1.5 Pro (Slow): 0.98 sec\n",
            "\n",
            "You can plot these values on a graph to visualize the comparison of time-to-first-token among the models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjT56fwEn36C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}