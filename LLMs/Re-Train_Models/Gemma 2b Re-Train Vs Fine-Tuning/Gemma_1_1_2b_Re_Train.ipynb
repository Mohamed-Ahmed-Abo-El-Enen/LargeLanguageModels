{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CALbbej9DTs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65359695-6ebd-43c9-ee7d-be2d381a47b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.1/296.1 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -qq torch\n",
        "!pip install -qq transformers\n",
        "!pip install -qq accelerate\n",
        "!pip install -qq datasets\n",
        "!pip install -qq bitsandbytes\n",
        "!pip install -qq peft\n",
        "!pip install -qq trl==0.8.6\n",
        "!pip install -qq SentencePiece\n",
        "!pip install -qq wandb -U\n",
        "# !pip install -qq ninja packaging\n",
        "# !pip install -qq -U flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-iveWf8UDdu1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    transformers.set_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(2)"
      ],
      "metadata": {
        "id": "3Ng4tJmCDd_2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import wandb\n",
        "\n",
        "# wandb.login(key=\"YOUR WANDB ACCESS TOKEN\") #######################################\n",
        "\n",
        "# wandb_project = \"Gemma-1.1-2b-it-Squad-Fine-Tuning\"\n",
        "# if len(wandb_project) > 0:\n",
        "#     os.environ[\"WANDB_PROJECT\"] = wandb_project\n"
      ],
      "metadata": {
        "id": "8-NJxn2ZDfTz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub.hf_api import HfFolder\n",
        "import os\n",
        "\n",
        "hf_token = \"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\"\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "HfFolder.save_token(hf_token)"
      ],
      "metadata": {
        "id": "ejgexyRBDgIO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SnB_xC1xDhZm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vcmvajfNDhcL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tImpjCkrDhey"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Squad Dataset"
      ],
      "metadata": {
        "id": "9t8KMlbSDi4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad_v2 = False\n",
        "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")\n",
        "datasets"
      ],
      "metadata": {
        "id": "L2f-vST5Djvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438477fa-b493-4324-fd0e-b838671354c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets[\"train\"]\n",
        "valid_dataset = datasets[\"validation\"]"
      ],
      "metadata": {
        "id": "UXGMBU-VDlin"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "_GLXdhy1DmZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311a31e9-7e17-4a7e-c2f5-c1579c6fbd8b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661182',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IM7rPyy3DnW5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZIqaJTQFDnZb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XKQSU2uVDnet"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruct_prompt(data_point):\n",
        "    prompt_template = \"\"\" Answer the follwing question from the given Context:\n",
        "{context}\n",
        "\n",
        "### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\n",
        "{answer}\"\"\"\n",
        "\n",
        "    data_point[\"prompt\"] = prompt_template.format(context=data_point[\"context\"],\n",
        "                                                  question=data_point[\"question\"],\n",
        "                                                  answer=data_point[\"answers\"]['text'][0])\n",
        "    return data_point"
      ],
      "metadata": {
        "id": "oLxKK1r2D0D3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "column_names = list(train_dataset.features)\n",
        "\n",
        "train_dataset = train_dataset.map(format_instruct_prompt,\n",
        "                                  num_proc=os.cpu_count(),\n",
        "                                  remove_columns=column_names,\n",
        "                                  desc=\"Applying chat template\",)\n",
        "\n",
        "\n",
        "valid_dataset = valid_dataset.map(format_instruct_prompt,\n",
        "                                  num_proc=os.cpu_count(),\n",
        "                                  remove_columns=column_names,\n",
        "                                  desc=\"Applying chat template\",)\n"
      ],
      "metadata": {
        "id": "jBDqcm5BELWd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset[0]"
      ],
      "metadata": {
        "id": "7Fv7KJdIEMo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "042e88ba-9889-4829-860b-03d23a755a65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': ' Answer the follwing question from the given Context:\\nSuper Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\\n\\n### Question:\\nWhich NFL team represented the AFC at Super Bowl 50?\\n\\n### Answer:\\nDenver Broncos'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pc-8HMxWENDH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbF1NaavENXk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Model Repo ID or Path"
      ],
      "metadata": {
        "id": "BHEbOYNVDn6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-1.1-2b-it\""
      ],
      "metadata": {
        "id": "NfYW0q0qDrWT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Gemma tokenizer"
      ],
      "metadata": {
        "id": "SED95HOADuaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "old_tokenizer.pad_token = old_tokenizer.eos_token\n",
        "old_tokenizer.padding_side = 'right'"
      ],
      "metadata": {
        "id": "3U3G-8C3DqAs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 1024\n",
        "\n",
        "old_tokenizer.model_max_length = max_seq_length"
      ],
      "metadata": {
        "id": "38i4-NfpDyVT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = old_tokenizer.tokenize(valid_dataset[0][\"prompt\"])\n",
        "tokens"
      ],
      "metadata": {
        "id": "sZr35yD2D1Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc23f72-b1d0-43b0-e402-69d30b5f1898"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁Answer',\n",
              " '▁the',\n",
              " '▁foll',\n",
              " 'wing',\n",
              " '▁question',\n",
              " '▁from',\n",
              " '▁the',\n",
              " '▁given',\n",
              " '▁Context',\n",
              " ':',\n",
              " '\\n',\n",
              " 'Super',\n",
              " '▁Bowl',\n",
              " '▁',\n",
              " '5',\n",
              " '0',\n",
              " '▁was',\n",
              " '▁an',\n",
              " '▁American',\n",
              " '▁football',\n",
              " '▁game',\n",
              " '▁to',\n",
              " '▁determine',\n",
              " '▁the',\n",
              " '▁champion',\n",
              " '▁of',\n",
              " '▁the',\n",
              " '▁National',\n",
              " '▁Football',\n",
              " '▁League',\n",
              " '▁(',\n",
              " 'NFL',\n",
              " ')',\n",
              " '▁for',\n",
              " '▁the',\n",
              " '▁',\n",
              " '2',\n",
              " '0',\n",
              " '1',\n",
              " '5',\n",
              " '▁season',\n",
              " '.',\n",
              " '▁The',\n",
              " '▁American',\n",
              " '▁Football',\n",
              " '▁Conference',\n",
              " '▁(',\n",
              " 'AFC',\n",
              " ')',\n",
              " '▁champion',\n",
              " '▁Denver',\n",
              " '▁Broncos',\n",
              " '▁defeated',\n",
              " '▁the',\n",
              " '▁National',\n",
              " '▁Football',\n",
              " '▁Conference',\n",
              " '▁(',\n",
              " 'NFC',\n",
              " ')',\n",
              " '▁champion',\n",
              " '▁Carolina',\n",
              " '▁Panthers',\n",
              " '▁',\n",
              " '2',\n",
              " '4',\n",
              " '–',\n",
              " '1',\n",
              " '0',\n",
              " '▁to',\n",
              " '▁earn',\n",
              " '▁their',\n",
              " '▁third',\n",
              " '▁Super',\n",
              " '▁Bowl',\n",
              " '▁title',\n",
              " '.',\n",
              " '▁The',\n",
              " '▁game',\n",
              " '▁was',\n",
              " '▁played',\n",
              " '▁on',\n",
              " '▁February',\n",
              " '▁',\n",
              " '7',\n",
              " ',',\n",
              " '▁',\n",
              " '2',\n",
              " '0',\n",
              " '1',\n",
              " '6',\n",
              " ',',\n",
              " '▁at',\n",
              " '▁Levi',\n",
              " \"'\",\n",
              " 's',\n",
              " '▁Stadium',\n",
              " '▁in',\n",
              " '▁the',\n",
              " '▁San',\n",
              " '▁Francisco',\n",
              " '▁Bay',\n",
              " '▁Area',\n",
              " '▁at',\n",
              " '▁Santa',\n",
              " '▁Clara',\n",
              " ',',\n",
              " '▁California',\n",
              " '.',\n",
              " '▁As',\n",
              " '▁this',\n",
              " '▁was',\n",
              " '▁the',\n",
              " '▁',\n",
              " '5',\n",
              " '0',\n",
              " 'th',\n",
              " '▁Super',\n",
              " '▁Bowl',\n",
              " ',',\n",
              " '▁the',\n",
              " '▁league',\n",
              " '▁emphasized',\n",
              " '▁the',\n",
              " '▁\"',\n",
              " 'golden',\n",
              " '▁anniversary',\n",
              " '\"',\n",
              " '▁with',\n",
              " '▁various',\n",
              " '▁gold',\n",
              " '-',\n",
              " 'themed',\n",
              " '▁initiatives',\n",
              " ',',\n",
              " '▁as',\n",
              " '▁well',\n",
              " '▁as',\n",
              " '▁temporarily',\n",
              " '▁suspending',\n",
              " '▁the',\n",
              " '▁tradition',\n",
              " '▁of',\n",
              " '▁naming',\n",
              " '▁each',\n",
              " '▁Super',\n",
              " '▁Bowl',\n",
              " '▁game',\n",
              " '▁with',\n",
              " '▁Roman',\n",
              " '▁numerals',\n",
              " '▁(',\n",
              " 'under',\n",
              " '▁which',\n",
              " '▁the',\n",
              " '▁game',\n",
              " '▁would',\n",
              " '▁have',\n",
              " '▁been',\n",
              " '▁known',\n",
              " '▁as',\n",
              " '▁\"',\n",
              " 'Super',\n",
              " '▁Bowl',\n",
              " '▁L',\n",
              " '\"),',\n",
              " '▁so',\n",
              " '▁that',\n",
              " '▁the',\n",
              " '▁logo',\n",
              " '▁could',\n",
              " '▁prominently',\n",
              " '▁feature',\n",
              " '▁the',\n",
              " '▁Arabic',\n",
              " '▁numerals',\n",
              " '▁',\n",
              " '5',\n",
              " '0',\n",
              " '.',\n",
              " '\\n\\n',\n",
              " '###',\n",
              " '▁Question',\n",
              " ':',\n",
              " '\\n',\n",
              " 'Which',\n",
              " '▁NFL',\n",
              " '▁team',\n",
              " '▁represented',\n",
              " '▁the',\n",
              " '▁AFC',\n",
              " '▁at',\n",
              " '▁Super',\n",
              " '▁Bowl',\n",
              " '▁',\n",
              " '5',\n",
              " '0',\n",
              " '?',\n",
              " '\\n\\n',\n",
              " '###',\n",
              " '▁Answer',\n",
              " ':',\n",
              " '\\n',\n",
              " 'Denver',\n",
              " '▁Broncos']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spy_VmC8D2Lj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train new tokenizer"
      ],
      "metadata": {
        "id": "WWR6d7uIE1kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Just to not consume much time in tokenizer training we will use 2000 sample for training\n",
        "#### you can delete this and use the full train_dataset cell blow this one\n",
        "\n",
        "tokenizer_train_dataset = train_dataset.select(range(2000))\n",
        "\n",
        "def get_training_corpus(batch_size=1000):\n",
        "    for start_idx in range(0, len(tokenizer_train_dataset), batch_size):\n",
        "        samples = tokenizer_train_dataset[start_idx : start_idx + batch_size]\n",
        "        yield samples[\"prompt\"]\n",
        "\n",
        "training_corpus = get_training_corpus()\n"
      ],
      "metadata": {
        "id": "XV7tzRI5MFUV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_training_corpus(batch_size=1000):\n",
        "#     for start_idx in range(0, len(train_dataset), batch_size):\n",
        "#         samples = train_dataset[start_idx : start_idx + batch_size]\n",
        "#         yield samples[\"prompt\"]\n",
        "\n",
        "# training_corpus = get_training_corpus()"
      ],
      "metadata": {
        "id": "nXict6gBD4TW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, old_tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "A9sdul8uD4YE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 1024\n",
        "\n",
        "new_tokenizer.model_max_length = max_seq_length"
      ],
      "metadata": {
        "id": "DuAyAGQSNbE0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = new_tokenizer.tokenize(valid_dataset[0][\"prompt\"])\n",
        "tokens"
      ],
      "metadata": {
        "id": "OHBqIqHHD5ZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70dab7d1-fab0-4631-f5f2-b28b602a388f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁Answer▁the▁follwing▁question▁from▁the▁given▁Context:\\n',\n",
              " 'Super▁Bowl▁',\n",
              " '50▁',\n",
              " 'was▁',\n",
              " 'an▁American▁',\n",
              " 'football▁',\n",
              " 'game▁',\n",
              " 'to▁determine▁',\n",
              " 'the▁',\n",
              " 'champ',\n",
              " 'ion▁of▁the▁',\n",
              " 'National▁',\n",
              " 'Football▁',\n",
              " 'Le',\n",
              " 'ague▁',\n",
              " '(',\n",
              " 'N',\n",
              " 'F',\n",
              " 'L',\n",
              " ')▁',\n",
              " 'for▁the▁',\n",
              " '2015▁',\n",
              " 'season',\n",
              " '.▁The▁',\n",
              " 'American▁',\n",
              " 'Football▁',\n",
              " 'Conference▁',\n",
              " '(A',\n",
              " 'F',\n",
              " 'C',\n",
              " ')▁',\n",
              " 'champ',\n",
              " 'ion▁',\n",
              " 'D',\n",
              " 'en',\n",
              " 'ver▁',\n",
              " 'Br',\n",
              " 'onc',\n",
              " 'os▁',\n",
              " 'defe',\n",
              " 'ated▁',\n",
              " 'the▁',\n",
              " 'National▁',\n",
              " 'Football▁',\n",
              " 'Conference▁',\n",
              " '(',\n",
              " 'N',\n",
              " 'F',\n",
              " 'C',\n",
              " ')▁',\n",
              " 'champ',\n",
              " 'ion▁',\n",
              " 'Car',\n",
              " 'ol',\n",
              " 'ina▁',\n",
              " 'Pan',\n",
              " 'ther',\n",
              " 's▁',\n",
              " '24',\n",
              " '–',\n",
              " '10▁',\n",
              " 'to▁',\n",
              " 'earn▁',\n",
              " 'their▁third▁',\n",
              " 'Super▁Bowl▁',\n",
              " 'tit',\n",
              " 'le',\n",
              " '.▁The▁',\n",
              " 'game▁',\n",
              " 'was▁',\n",
              " 'play',\n",
              " 'ed▁on▁',\n",
              " 'February▁',\n",
              " '7,▁',\n",
              " '2016,▁',\n",
              " 'at▁L',\n",
              " 'ev',\n",
              " \"i's▁\",\n",
              " 'Stadi',\n",
              " 'um▁',\n",
              " 'in▁the▁',\n",
              " 'S',\n",
              " 'an▁',\n",
              " 'Franc',\n",
              " 'isc',\n",
              " 'o▁',\n",
              " 'Bay▁',\n",
              " 'A',\n",
              " 're',\n",
              " 'a▁',\n",
              " 'at▁',\n",
              " 'S',\n",
              " 'ant',\n",
              " 'a▁',\n",
              " 'Cl',\n",
              " 'ar',\n",
              " 'a,▁',\n",
              " 'Californi',\n",
              " 'a',\n",
              " '.▁A',\n",
              " 's▁',\n",
              " 'this▁was▁the▁',\n",
              " '50',\n",
              " 'th▁',\n",
              " 'Super▁',\n",
              " 'Bowl',\n",
              " ',▁the▁',\n",
              " 'league▁',\n",
              " 'emphas',\n",
              " 'iz',\n",
              " 'ed▁the▁',\n",
              " '\"',\n",
              " 'golden▁',\n",
              " 'ann',\n",
              " 'ivers',\n",
              " 'ary',\n",
              " '\"▁',\n",
              " 'with▁various▁',\n",
              " 'gold',\n",
              " '-themed▁',\n",
              " 'initi',\n",
              " 'ativ',\n",
              " 'es,▁',\n",
              " 'as▁well▁as▁',\n",
              " 'tempor',\n",
              " 'arily▁',\n",
              " 'sus',\n",
              " 'p',\n",
              " 'end',\n",
              " 'ing▁the▁',\n",
              " 'tradition▁of▁',\n",
              " 'nam',\n",
              " 'ing▁',\n",
              " 'each▁',\n",
              " 'Super▁Bowl▁',\n",
              " 'game▁',\n",
              " 'with▁',\n",
              " 'Rom',\n",
              " 'an▁',\n",
              " 'numer',\n",
              " 'als▁',\n",
              " '(',\n",
              " 'under▁',\n",
              " 'which▁the▁',\n",
              " 'game▁',\n",
              " 'would▁',\n",
              " 'have▁been▁',\n",
              " 'known▁as▁\"',\n",
              " 'Super▁Bowl▁',\n",
              " 'L',\n",
              " '\"),▁',\n",
              " 'so▁',\n",
              " 'that▁the▁',\n",
              " 'log',\n",
              " 'o▁',\n",
              " 'could▁',\n",
              " 'promin',\n",
              " 'ently▁',\n",
              " 'featur',\n",
              " 'e▁the▁',\n",
              " 'Ar',\n",
              " 'ab',\n",
              " 'ic▁',\n",
              " 'numer',\n",
              " 'als▁',\n",
              " '50',\n",
              " '.\\n\\n###▁Question:\\nWhich▁',\n",
              " 'N',\n",
              " 'F',\n",
              " 'L',\n",
              " '▁',\n",
              " 'team▁',\n",
              " 'represent',\n",
              " 'ed▁the▁',\n",
              " 'A',\n",
              " 'F',\n",
              " 'C▁',\n",
              " 'at▁Super▁Bowl▁',\n",
              " '50',\n",
              " '?\\n\\n###▁Answer:\\n',\n",
              " 'D',\n",
              " 'en',\n",
              " 'ver▁',\n",
              " 'Br',\n",
              " 'onc',\n",
              " 'os']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQ6sxKovFEjD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWzXI6cvFEl7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5CADbq4FE0a"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model (Use the model as it is)"
      ],
      "metadata": {
        "id": "J5Z1MeNoFMa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "#                                              # attn_implementation=\"flash_attention_2\",\n",
        "#                                              trust_remote_code=True,\n",
        "#                                              device_map=\"auto\",\n",
        "#                                              low_cpu_mem_usage=True,\n",
        "#                                              use_cache=False)"
      ],
      "metadata": {
        "id": "8bOA596fFLti"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model Configration"
      ],
      "metadata": {
        "id": "W9SfgdVgFFep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_id)\n",
        "config"
      ],
      "metadata": {
        "id": "RxRHSsSTFHpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31bff3b2-be51-45a0-f140-b361c004e00d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GemmaConfig {\n",
              "  \"_name_or_path\": \"google/gemma-1.1-2b-it\",\n",
              "  \"architectures\": [\n",
              "    \"GemmaForCausalLM\"\n",
              "  ],\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 2,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"head_dim\": 256,\n",
              "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
              "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
              "  \"hidden_size\": 2048,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 16384,\n",
              "  \"max_position_embeddings\": 8192,\n",
              "  \"model_type\": \"gemma\",\n",
              "  \"num_attention_heads\": 8,\n",
              "  \"num_hidden_layers\": 18,\n",
              "  \"num_key_value_heads\": 1,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"torch_dtype\": \"bfloat16\",\n",
              "  \"transformers_version\": \"4.41.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 256000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Reduce the model hidden layers </br>(may also change {hidden_size, num_attention_heads, ... })"
      ],
      "metadata": {
        "id": "l0pFKIYEFWEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "\n",
        "config.num_hidden_layers = 6\n",
        "model = AutoModelForCausalLM.from_config(config,\n",
        "                                         trust_remote_code=True)"
      ],
      "metadata": {
        "id": "iAt-3jpHFVfj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "xCeVCvCnFxsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f989760c-7fbb-4b02-9a86-828cfff2cb37"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GemmaConfig {\n",
              "  \"_name_or_path\": \"google/gemma-1.1-2b-it\",\n",
              "  \"architectures\": [\n",
              "    \"GemmaForCausalLM\"\n",
              "  ],\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 2,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"head_dim\": 256,\n",
              "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
              "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
              "  \"hidden_size\": 2048,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 16384,\n",
              "  \"max_position_embeddings\": 8192,\n",
              "  \"model_type\": \"gemma\",\n",
              "  \"num_attention_heads\": 8,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"num_key_value_heads\": 1,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"rms_norm_eps\": 1e-06,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"torch_dtype\": \"bfloat16\",\n",
              "  \"transformers_version\": \"4.41.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 256000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4QqeEkOF7Ku"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1SynGJwF7NT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Training Arguments"
      ],
      "metadata": {
        "id": "ah9j5wthF7pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ],
      "metadata": {
        "id": "ElliUDCHFy7F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"Gemma-1.1-2b-it-Squad-Re-Train\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "8yaDn3GrF3Li"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "training_arguments = TrainingArguments(output_dir=output_dir,\n",
        "                                       overwrite_output_dir=True,\n",
        "                                       warmup_steps=5,\n",
        "                                       per_device_train_batch_size=1,\n",
        "                                       per_device_eval_batch_size=1,\n",
        "                                       gradient_accumulation_steps=1,\n",
        "                                       eval_accumulation_steps=1,\n",
        "                                       gradient_checkpointing=True,\n",
        "                                       num_train_epochs=1,\n",
        "                                       learning_rate=2.0e-05,         # Want a small lr for training\n",
        "                                       lr_scheduler_type=\"cosine\",\n",
        "                                       optim=\"paged_adamw_8bit\",\n",
        "                                       weight_decay = 0.01,\n",
        "                                       logging_dir=\"./logs\",          # Directory for storing logs\n",
        "                                       save_strategy=\"steps\",         # Save the model checkpoint every logging step\n",
        "                                       eval_strategy=\"epoch\",         # Evaluate the model every logging step\n",
        "                                       save_total_limit=5,\n",
        "                                       do_eval=True,                  # Perform evaluation at the end of training\n",
        "\n",
        "                                       push_to_hub=True,\n",
        "                                       hub_strategy=\"checkpoint\",\n",
        "                                       hub_token=hf_token,\n",
        "\n",
        "                                       seed=2,\n",
        "\n",
        "                                       logging_steps=100,             # When to start reporting loss\n",
        "                                       save_steps=100,                # Save checkpoints every 100 steps\n",
        "                                       eval_steps=200,                # Evaluate and save checkpoints every 200 steps\n",
        "\n",
        "                                       )\n"
      ],
      "metadata": {
        "id": "C1SL9y0JGFtO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(new_tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "HnCHTgNsGR1Z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(model=model,\n",
        "                     tokenizer=new_tokenizer,\n",
        "                     args=training_arguments,\n",
        "                     data_collator=data_collator,\n",
        "                     max_seq_length=new_tokenizer.model_max_length,\n",
        "                     train_dataset=train_dataset,\n",
        "                     eval_dataset=valid_dataset,\n",
        "                     dataset_text_field=\"prompt\",\n",
        "                     packing=False,\n",
        "                    )\n",
        "\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "iBC2ZmnfGTRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776d1f43-64c6-42ae-f2aa-a391fb5c36ad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start Re-Training"
      ],
      "metadata": {
        "id": "38MA-TmRGZ-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n",
        "trainer_stats"
      ],
      "metadata": {
        "id": "0M_c7bjfGUrV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "925f7983-67c9-4e29-fdd4-0a267bf241af"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamed-ahmed\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240621_095836-v2gg6y20</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohamed-ahmed/huggingface/runs/v2gg6y20' target=\"_blank\">Gemma-1.1-2b-it-Squad-Re-Train</a></strong> to <a href='https://wandb.ai/mohamed-ahmed/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mohamed-ahmed/huggingface' target=\"_blank\">https://wandb.ai/mohamed-ahmed/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mohamed-ahmed/huggingface/runs/v2gg6y20' target=\"_blank\">https://wandb.ai/mohamed-ahmed/huggingface/runs/v2gg6y20</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='87599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  245/87599 11:58 < 71:45:51, 0.34 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/ec/93/ec93b9ef6652e8555b6dabe404df3632f249452af753b58ba26a9e525fd2e7a0/10781a05d56bbed2a3c404e2645fbfb63807907452750d01192a20c91312693f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20240621%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240621T100557Z&X-Amz-Expires=86400&X-Amz-Signature=27a57d8df2b8dfccc84c8afe777afe9f0d5aabfe06e4fe6d38d0e4401690fa1e&X-Amz-SignedHeaders=host&partNumber=161&uploadId=Jfe1Fi4ub4Y11.qUrJgcEG.rZT9z98QtZZZv5STjgI8KMZhghh_IqnYoAyEZbSjxETaoYuliL.Vzf1bvf9QaCvOntEQYKw1nruFb6.h8kbLGGAqD1ZKq1xQH_9_Q7Rxk&x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))\"), '(Request ID: 36308dd3-e3c9-4263-b97c-db2e6bb6c66b)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/ec/93/ec93b9ef6652e8555b6dabe404df3632f249452af753b58ba26a9e525fd2e7a0/10781a05d56bbed2a3c404e2645fbfb63807907452750d01192a20c91312693f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20240621%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240621T100557Z&X-Amz-Expires=86400&X-Amz-Signature=27a57d8df2b8dfccc84c8afe777afe9f0d5aabfe06e4fe6d38d0e4401690fa1e&X-Amz-SignedHeaders=host&partNumber=161&uploadId=Jfe1Fi4ub4Y11.qUrJgcEG.rZT9z98QtZZZv5STjgI8KMZhghh_IqnYoAyEZbSjxETaoYuliL.Vzf1bvf9QaCvOntEQYKw1nruFb6.h8kbLGGAqD1ZKq1xQH_9_Q7Rxk&x-id=UploadPart\n",
            "WARNING:huggingface_hub.utils._http:'(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/ec/93/ec93b9ef6652e8555b6dabe404df3632f249452af753b58ba26a9e525fd2e7a0/10781a05d56bbed2a3c404e2645fbfb63807907452750d01192a20c91312693f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20240621%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240621T100557Z&X-Amz-Expires=86400&X-Amz-Signature=27a57d8df2b8dfccc84c8afe777afe9f0d5aabfe06e4fe6d38d0e4401690fa1e&X-Amz-SignedHeaders=host&partNumber=161&uploadId=Jfe1Fi4ub4Y11.qUrJgcEG.rZT9z98QtZZZv5STjgI8KMZhghh_IqnYoAyEZbSjxETaoYuliL.Vzf1bvf9QaCvOntEQYKw1nruFb6.h8kbLGGAqD1ZKq1xQH_9_Q7Rxk&x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))\"), '(Request ID: 36308dd3-e3c9-4263-b97c-db2e6bb6c66b)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/ec/93/ec93b9ef6652e8555b6dabe404df3632f249452af753b58ba26a9e525fd2e7a0/10781a05d56bbed2a3c404e2645fbfb63807907452750d01192a20c91312693f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20240621%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240621T100557Z&X-Amz-Expires=86400&X-Amz-Signature=27a57d8df2b8dfccc84c8afe777afe9f0d5aabfe06e4fe6d38d0e4401690fa1e&X-Amz-SignedHeaders=host&partNumber=161&uploadId=Jfe1Fi4ub4Y11.qUrJgcEG.rZT9z98QtZZZv5STjgI8KMZhghh_IqnYoAyEZbSjxETaoYuliL.Vzf1bvf9QaCvOntEQYKw1nruFb6.h8kbLGGAqD1ZKq1xQH_9_Q7Rxk&x-id=UploadPart\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c881fd6b6370>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 \u001b[0;31m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                 return inner_training_loop(\n\u001b[0m\u001b[1;32m   1877\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m                     \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3241\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.push_to_hub()\n",
        "print(\"Re-Trained Successfully\")"
      ],
      "metadata": {
        "id": "w7TLc-mHGdPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QuNj-BcNIAt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}