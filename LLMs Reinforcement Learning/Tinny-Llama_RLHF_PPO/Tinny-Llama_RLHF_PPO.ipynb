{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6397196,"sourceType":"datasetVersion","datasetId":3688077}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"53abb305c420411c9bc8c6e11b146872":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb5d6134d8ae419b809067fcbfaf6303","IPY_MODEL_a0e1b6c6f5dc4ea6abbf0f4b9816345d","IPY_MODEL_6204d1c088374261a1db2ed895042ac8"],"layout":"IPY_MODEL_9309962c72e842b0b9e9326309c8bd53"}},"fb5d6134d8ae419b809067fcbfaf6303":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc3f78f7daa844358de65becdf98e156","placeholder":"​","style":"IPY_MODEL_5de3fd1e86f24b4a89b56cb96b0e4fbb","value":"Map: 100%"}},"a0e1b6c6f5dc4ea6abbf0f4b9816345d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd6bef0c80c8400ba5cea8ea0d965c80","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e670029a7f0d482595f7fe55ccc82884","value":1000}},"6204d1c088374261a1db2ed895042ac8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93a9f14a76074367a3f52d213f6faae3","placeholder":"​","style":"IPY_MODEL_2a7f8e2f5abd46cd952fb91d27d96066","value":" 1000/1000 [00:00&lt;00:00, 1427.32 examples/s]"}},"9309962c72e842b0b9e9326309c8bd53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc3f78f7daa844358de65becdf98e156":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5de3fd1e86f24b4a89b56cb96b0e4fbb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd6bef0c80c8400ba5cea8ea0d965c80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e670029a7f0d482595f7fe55ccc82884":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93a9f14a76074367a3f52d213f6faae3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a7f8e2f5abd46cd952fb91d27d96066":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d90732b8b47b4f51a9e0b7847905bf2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7166ea6dfa0c4c9f9e1f91cf28655e39","IPY_MODEL_4e77821d175e4f4b9c9a795a78f211b0","IPY_MODEL_226246f0b4dd4000b7ebcc42b6373d9e"],"layout":"IPY_MODEL_d60958ab5be24fff84f23405d4a36b83"}},"7166ea6dfa0c4c9f9e1f91cf28655e39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9aadb1bc6d14bdcb9a11ca41f78b1b4","placeholder":"​","style":"IPY_MODEL_8d91094d27b645b79309812cde1a2dae","value":"Map: 100%"}},"4e77821d175e4f4b9c9a795a78f211b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_04c2750aa5e74e849e23440e673ca4a2","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d2a9d372ed2a407caa2b8b80ae4140e2","value":1000}},"226246f0b4dd4000b7ebcc42b6373d9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53e53741fd9f424c8596eb80762605a9","placeholder":"​","style":"IPY_MODEL_39a7ce1bd1c7472d88db877fbf4edbde","value":" 1000/1000 [00:05&lt;00:00, 251.69 examples/s]"}},"d60958ab5be24fff84f23405d4a36b83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9aadb1bc6d14bdcb9a11ca41f78b1b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d91094d27b645b79309812cde1a2dae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04c2750aa5e74e849e23440e673ca4a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2a9d372ed2a407caa2b8b80ae4140e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53e53741fd9f424c8596eb80762605a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39a7ce1bd1c7472d88db877fbf4edbde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qq transformers\n!pip install -qq datasets\n!pip install -qq accelerate\n!pip install -qq bitsandbytes\n!pip install -qq peft\n!pip install -qq trl\n!pip install -qq evaluate\n!pip install -qq rouge_score\n!pip install -qq jiwer\n!pip install -qq wandb\n!pip install -qq tensorboard\n!pip install -qq gradio\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rzm2fRMJiTSL","outputId":"c0e7f812-2f5f-4d68-ceac-6a51cb1e7746","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !unzip /content/rlhf_dataset.zip","metadata":{"_kg_hide-output":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"cwhNAnwsiTSO","outputId":"6c0ac4cc-6da7-43dd-ac7b-ea6718beeb94","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"j__WVxpmklWl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"cr_2kGC1iTSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"id":"KmiyCcruiTSP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"SydrCcM0iTSQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\n\ndef set_seed(seed_val=42):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)","metadata":{"id":"J1pG2k1uiTSQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub.hf_api import HfFolder\nimport os\n# from google.colab import userdata\n\n# os.environ[\"HF_TOKEN\"] = userdata.get(\"HUGGINGFACE_TOKEN\")\nos.environ[\"HF_TOKEN\"] = \"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\"\nHfFolder.save_token(os.environ[\"HF_TOKEN\"])\n","metadata":{"id":"nEOXQnqmiTSR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport wandb\n\n\n# os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\nos.environ[\"WANDB_API_KEY\"] = \"2be7c86a28a2bcbeccdfa66844abfdd19b9bdabf\"\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\nwandb_project = \"RLHF_TinyLlama\"\nif len(wandb_project) > 0:\n    os.environ[\"WANDB_PROJECT\"] = wandb_project\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8RDegYWiTSR","outputId":"e738f1e1-184d-45b6-8b95-ec24573cca6c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"VsTegLmMiTSS","outputId":"3f2c8707-4393-4825-8d3a-7128b7068835","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndef clear_gpu():\n    torch.clear_autocast_cache()\n    torch.cuda.ipc_collect()\n    torch.cuda.empty_cache()\n    gc.collect()\n\nif device == \"gpu\":\n    clear_gpu()","metadata":{"id":"dm0QbG1IiTST","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport multiprocessing\n\ndef optimal_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\nnum_cpu_workers = optimal_workers()\nnum_cpu_workers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLHS8c3aiTST","outputId":"5825641f-bc6a-4090-82a4-550b74218a03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef set_seeds(seed: int=2):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seeds(2)","metadata":{"id":"sl2jiOt2iTSU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"nLUV7HXbiTSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"4KICgPRQiTSV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 1 - train policy model for human evaluation","metadata":{"id":"itid05pbiTSV"}},{"cell_type":"code","source":"import pandas as pd\n\n\n#df = pd.read_parquet(\"/content/rlhf_dataset/train_policy.parquet\")\ndf = pd.read_parquet(\"/kaggle/input/rlhf-dataset/train_policy.parquet\")\ndf.head()","metadata":{"id":"vycSoQf5iTSX","colab":{"base_uri":"https://localhost:8080/","height":226},"outputId":"f9840d36-3d42-4b0e-faa1-a42281e17f99","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(df, split='train')\ntrain_dataset","metadata":{"id":"53Em5XA6iTSX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe6b1f2d-67c5-4290-c2f2-506f1ccffad2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n\n@dataclass\nclass CustomDataCollator:\n    def __init__(self, tokenizer, max_length, prompt_col, label_col):\n        self.__tokenizer = tokenizer\n        self.__max_length = max_length\n        self.__prompt_col = prompt_col\n        self.__label_col = label_col\n\n    def __call__(self, samples: List[dict]):\n        prompt_text = [str(s[self.__prompt_col]) for s in samples]\n        label_text = [str(s[self.__label_col]) for s in samples]\n\n        prompt_tokens = self.__tokenizer(prompt_text,\n                                         return_tensors='pt',\n                                         truncation=True,\n                                         max_length=self.__max_length,\n                                         padding=\"max_length\")\n\n        label_tokens = self.__tokenizer(label_text,\n                                        return_tensors='pt',\n                                        truncation=True,\n                                        max_length=self.__max_length,\n                                        padding=\"max_length\")\n\n        output_dict = dict()\n        output_dict[\"input_ids\"] = prompt_tokens['input_ids']\n        output_dict[\"attention_mask\"] = prompt_tokens['attention_mask']\n        output_dict[\"labels\"] = label_tokens['input_ids']\n        return output_dict","metadata":{"id":"pyyya4CLiTSY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nwith_quantization_config = False if device == \"cpu\" else True\nmax_length = 512","metadata":{"id":"gSLaN96ZiTSZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"MBki0FvxiTSZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_data_collator = CustomDataCollator(tokenizer=tokenizer,\n                                          max_length=max_length,\n                                          prompt_col=\"prompt\",\n                                          label_col=\"label\")","metadata":{"id":"VHDAVh5XiTSZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                                bnb_4bit_use_double_quant=True,\n                                bnb_4bit_quant_type=\"nf4\",\n                                bnb_4bit_compute_dtype=torch.float16)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n                                             quantization_config= bnb_config if with_quantization_config else None,\n                                             device_map=\"auto\",\n                                             trust_remote_code=True,\n                                             # torch_dtype=torch.float16\n                                             )\nmodel.config.pad_token_id = model.config.eos_token_id","metadata":{"id":"dyd1-Kv8iTSa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bitsandbytes as bnb\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, TaskType\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n        if \"lm_head\" in lora_module_names: # Needed for 16bit\n            lora_module_names.remove(\"lm_head\")\n\n    return list(lora_module_names)\n\ndef lora_peft_model(model, TaskType):\n    target_modules = find_all_linear_names(model)\n\n    peft_config = LoraConfig(r=128,  # dimension of the updated matrices\n                             lora_alpha=32,  # parameter for scaling\n                             target_modules=target_modules,  # this chooses on which layers QLoRA is applied\n                             lora_dropout=0.05,  # dropout probability for layers\n                             bias=\"none\",\n                             task_type=TaskType)\n\n\n    ## model.enable_input_require_grads()\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()\n    return model","metadata":{"id":"m9Z0sRqOiTSa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fe375e8-efba-4a5c-a64e-acd7a8997f80","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = lora_peft_model(model, TaskType.CAUSAL_LM)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\n\noutput_dir = \"tiny_llama_rlhf_policy_model\"\n\ntraining_arguments = TrainingArguments(output_dir=output_dir,\n                                       learning_rate=1e-5,\n                                       per_device_train_batch_size=16,\n                                       fp16=False,\n                                       gradient_accumulation_steps=1,\n                                       save_strategy=\"steps\",\n                                       warmup_steps=50,\n                                       logging_steps=20,\n                                       max_steps=20, ###########################\n                                       report_to=[\"tensorboard\"],\n                                       remove_unused_columns=False,\n                                       save_total_limit=1,\n                                       )","metadata":{"id":"6yF9w5e3iTSa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(model=model,\n                  args=training_arguments,\n                  train_dataset=train_dataset,\n                  data_collator=custom_data_collator)","metadata":{"id":"mWD6F0OAiTSa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5974c9aa-765b-45a4-fcfc-3ee0fd5a7c71","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"d8mmO8oWiTSb","colab":{"base_uri":"https://localhost:8080/","height":180},"outputId":"74111994-7404-499a-8377-ef7a5ddddd73","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"tiny_llama_rlhf_policy_model\")\ntokenizer.save_pretrained(\"tiny_llama_rlhf_policy_model\")","metadata":{"id":"yY_KUAnkiTSb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f4b191da-39b4-4a42-e1ad-8ce03df62a89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\nfine_tunned_path = \"tiny_llama_rlhf_policy_model\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n                                             device_map=\"auto\",\n                                             torch_dtype=torch.float16)\n\nmerged_model= PeftModel.from_pretrained(model, fine_tunned_path)\nmerged_model = merged_model.merge_and_unload()\n\nmerged_model.save_pretrained(\"policy_merged_model\", safe_serialiaztion=True)\ntokenizer.save_pretrained(\"policy_merged_model\", safe_serialiaztion=True)","metadata":{"id":"K9xyKgQKiTSb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1602f82d-0618-4a12-cc1f-10488f9b8b4e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8eg2THkZiTSc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"lZRAKFCwiTSc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Restart The Kernel to save GPU Memory","metadata":{"id":"p8KRXbUr4prF"}},{"cell_type":"markdown","source":"## Step 2: train reward function","metadata":{"id":"KlNJWLZciTSc"}},{"cell_type":"code","source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"8fMHYpWuyyxg","outputId":"f1aba46f-03af-4b00-8716-8854b7ee2834","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = \"policy_merged_model\"\nwith_quantization_config = False if device == \"cpu\" else True\nmax_length = 512","metadata":{"id":"6KzB5SR4iTSc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# df = pd.read_parquet(\"/content/rlhf_dataset/train.parquet\")\ndf = pd.read_parquet(\"/kaggle/input/rlhf-dataset/train.parquet\")\ndf.head()","metadata":{"id":"3dOVPaKZiTSc","outputId":"744b4f1a-28ae-4cb2-91a6-a38682407a7e","colab":{"base_uri":"https://localhost:8080/","height":226},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\nraw_dataset = Dataset.from_pandas(df, split='train')\nraw_dataset","metadata":{"id":"IYrIu3gBiTSc","outputId":"fa3c288a-2a14-4396-d33f-1675c965a2bf","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"oYS9N50piTSe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                                bnb_4bit_use_double_quant=True,\n                                bnb_4bit_quant_type=\"nf4\",\n                                bnb_4bit_compute_dtype=torch.float16)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH,\n                                                          quantization_config= bnb_config if with_quantization_config else None,\n                                                          device_map=\"auto\",\n                                                          trust_remote_code=True,\n                                                          # torch_dtype=torch.float16\n                                                          )\nmodel.config.pad_token_id = model.config.eos_token_id","metadata":{"id":"MVcvPqLEiTSe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bitsandbytes as bnb\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, TaskType\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n        if \"lm_head\" in lora_module_names: # Needed for 16bit\n            lora_module_names.remove(\"lm_head\")\n\n    return list(lora_module_names)\n\ndef lora_peft_model(model, task_type):\n    target_modules = find_all_linear_names(model)\n\n    peft_config = LoraConfig(r=128,  # dimension of the updated matrices\n                             lora_alpha=32,  # parameter for scaling\n                             target_modules=target_modules,  # this chooses on which layers QLoRA is applied\n                             lora_dropout=0.05,  # dropout probability for layers\n                             bias=\"none\",\n                             task_type=task_type)\n\n\n    ## model.enable_input_require_grads()\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = lora_peft_model(model, TaskType.SEQ_CLS)","metadata":{"id":"6bhLkfxqiTSe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8427e419-4e31-48d3-b2cd-2a1e3812fff5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n\n@dataclass\nclass CustomDataCollator:\n    def __init__(self, tokenizer, max_length, prompt_col, chosen_col, rejected_col):\n        self.__tokenizer = tokenizer\n        self.__max_length = max_length\n        self.__prompt_col = prompt_col\n        self.__chosen_col = chosen_col\n        self.__rejected_col = rejected_col\n\n    def __call__(self, samples: List[dict]):\n        chosen_text = [str(s[self.__prompt_col] + '\\n' + s[self.__chosen_col]) for s in samples]\n        rejected_text = [str(s[self.__prompt_col] + '\\n' + s[self.__rejected_col]) for s in samples]\n\n        chosen_tokens = self.__tokenizer.encode_plus(\n            chosen_text,\n            return_tensors='pt',\n            padding=\"max_length\",\n            max_length=self.__max_length,\n            truncation=True)\n\n        rejected_tokens = self.__tokenizer.encode_plus(\n            rejected_text,\n            return_tensors='pt',\n            padding=\"max_length\",\n            max_length=self.__max_length,\n            truncation=True)\n\n        output_dict = dict()\n        output_dict[\"input_ids_chosen\"] = chosen_tokens['input_ids']\n        output_dict[\"attention_mask_chosen\"] = chosen_tokens['attention_mask']\n\n        output_dict[\"input_ids_rejected\"] = rejected_tokens['input_ids']\n        output_dict[\"attention_mask_rejected\"] = rejected_tokens['attention_mask']\n\n        return output_dict","metadata":{"id":"ywHLmGhRiTSe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_data_collator = CustomDataCollator(tokenizer=tokenizer,\n                                          max_length=max_length,\n                                          prompt_col=\"prompt\",\n                                          chosen_col=\"chosen\",\n                                          rejected_col=\"rejected\")","metadata":{"id":"arZh7GqaiTSe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom trl import RewardConfig\n\noutput_dir = \"tiny_llama_rlhf_reward_model\"\n\ntraining_args = RewardConfig(output_dir=output_dir,\n                            learning_rate=1e-5,\n                            per_device_train_batch_size=2,\n                            per_device_eval_batch_size=1,\n                            gradient_accumulation_steps=1,\n                            fp16=False,\n                            save_strategy=\"steps\",\n                            logging_steps=20,\n                            max_steps=20, ######################################\n                            report_to=[\"tensorboard\"],\n                            remove_unused_columns=False,\n                            save_total_limit=1)","metadata":{"id":"v-kBwRvQiTSe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import RewardTrainer\n\ntrainer = RewardTrainer(model=model,\n                        args=training_args,\n                        tokenizer=tokenizer,\n                        train_dataset=raw_dataset,\n                        data_collator=custom_data_collator)","metadata":{"id":"ZbSDFcd0iTSf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b58cb082-f03f-4ee3-d192-12f629248069","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"-A2GnesJiTSf","colab":{"base_uri":"https://localhost:8080/","height":180},"outputId":"379b5886-7ab4-420f-9f04-cfd355d6c91f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"tiny_llama_rlhf_reward_model\")\ntokenizer.save_pretrained(\"tiny_llama_rlhf_reward_model\")","metadata":{"id":"9bEUR_uKiTSf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1256d34-b6f2-4384-e82c-2a44ff73377a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nfine_tunned_path = \"tiny_llama_rlhf_reward_model\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n                                             device_map=\"auto\",\n                                             torch_dtype=torch.float16)\n\nmerged_model= PeftModel.from_pretrained(model, fine_tunned_path)\nmerged_model = merged_model.merge_and_unload()\n\nmerged_model.save_pretrained(\"reward_merged_model\", safe_serialiaztion=True)\ntokenizer.save_pretrained(\"reward_merged_model\", safe_serialiaztion=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4hVFpFDUy-40","outputId":"e73f5f40-9ec0-43e2-fdfc-a8fbf2fd759b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_score(model, tokenizer, prompt, response):\n    instructions = tokenizer.encode_plus(prompt,\n                                         response,\n                                         padding=\"max_length\",\n                                         max_length=max_length,\n                                         return_tensors=\"pt\",\n                                         truncation=True).to(device)\n\n    with torch.no_grad():\n        outputs = model(**instructions)\n    logits = outputs[0]\n\n    return logits","metadata":{"id":"jMGTuruDiTSf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = raw_dataset[2][\"prompt\"]\nexample_chosen_response = raw_dataset[2][\"chosen\"]\nexample_rejected_response = raw_dataset[2][\"rejected\"]","metadata":{"id":"IPMAMGmAiTSf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"3NMYlX842Ns3","outputId":"e9024c5b-21c5-4ca2-fbf4-623b2ac2b5c8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_chosen_response","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"bFhZOXe32Pnl","outputId":"120856dc-5cbb-4643-c1af-1aec5b34f776","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_rejected_response","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"YlzL6uIQ2SKX","outputId":"dc959cb1-845d-447e-af0d-bb0a45633bfe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss1 = get_score(model=model,\n                  tokenizer=tokenizer,\n                  prompt=prompt,\n                  response=example_chosen_response)\n\nloss2 = get_score(model=model,\n                  tokenizer=tokenizer,\n                  prompt=prompt,\n                  response=example_rejected_response)","metadata":{"id":"jik-3u1WiTSg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad01d403-cd19-4952-f1cb-cf36b5c8700e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\nloss = -nn.functional.logsigmoid(loss1 - loss2).mean()\nloss","metadata":{"id":"cdA5GLI8iTSg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"10ca7ec4-8753-40d1-e07a-1083af458dfc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(torch.max(loss1, axis=-1).indices[0])","metadata":{"id":"w0ooRs-ciTSg","colab":{"base_uri":"https://localhost:8080/","height":162},"outputId":"2a4a20e1-ca1c-41d2-8721-6d367109f26d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(torch.max(loss2, axis=-1).indices[0])","metadata":{"id":"2b7hT9GbiTSg","colab":{"base_uri":"https://localhost:8080/","height":162},"outputId":"8dd91bc5-15ac-473e-e652-f0d26d230bba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"CsnccVC540tD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"KGd93TI94034"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Restart The Kernel to save GPU Memory","metadata":{"id":"lSsPHkHN41OA"}},{"cell_type":"markdown","source":"## Step 3: RL PPO model","metadata":{"id":"udrd1QpkiTSh"}},{"cell_type":"code","source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"lgg6KTQ2fBdi","outputId":"cbebc2c6-d64f-4047-d497-a90e4f614a7e","execution":{"iopub.status.busy":"2024-10-08T19:28:43.439830Z","iopub.execute_input":"2024-10-08T19:28:43.440726Z","iopub.status.idle":"2024-10-08T19:28:45.158027Z","shell.execute_reply.started":"2024-10-08T19:28:43.440660Z","shell.execute_reply":"2024-10-08T19:28:45.157000Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"MODEL_PATH = \"reward_merged_model\"\nmax_length = 512","metadata":{"id":"cL2OZiLTFpVK","execution":{"iopub.status.busy":"2024-10-08T19:28:45.159550Z","iopub.execute_input":"2024-10-08T19:28:45.159982Z","iopub.status.idle":"2024-10-08T19:28:45.164113Z","shell.execute_reply.started":"2024-10-08T19:28:45.159940Z","shell.execute_reply":"2024-10-08T19:28:45.163168Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# df = pd.read_parquet(\"/content/rlhf_dataset/train.parquet\")\ndf = pd.read_parquet(\"/kaggle/input/rlhf-dataset/train.parquet\")\ndataset = Dataset.from_pandas(df)\ndataset","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fV7qYpG2exVO","outputId":"338406e3-9536-45f3-d403-abee70da7a63","execution":{"iopub.status.busy":"2024-10-08T19:28:45.165210Z","iopub.execute_input":"2024-10-08T19:28:45.165633Z","iopub.status.idle":"2024-10-08T19:28:47.564831Z","shell.execute_reply.started":"2024-10-08T19:28:45.165583Z","shell.execute_reply":"2024-10-08T19:28:47.563837Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'chosen', 'rejected'],\n    num_rows: 92534\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.select(range(100))","metadata":{"id":"6qg6Dx_MumqO","execution":{"iopub.status.busy":"2024-10-08T19:28:47.567356Z","iopub.execute_input":"2024-10-08T19:28:47.568334Z","iopub.status.idle":"2024-10-08T19:28:47.574297Z","shell.execute_reply.started":"2024-10-08T19:28:47.568294Z","shell.execute_reply":"2024-10-08T19:28:47.573317Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from trl import PPOConfig\n\n\nconfig = PPOConfig(model_name=MODEL_PATH,\n                   steps=100,\n                   learning_rate=1e-5,\n                   mini_batch_size=4,\n                   batch_size=16,\n                   remove_unused_columns=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdCBt0GCfHUG","outputId":"e73bcde7-e306-4f84-8e74-cb97702fbafd","execution":{"iopub.status.busy":"2024-10-08T19:28:47.575609Z","iopub.execute_input":"2024-10-08T19:28:47.575996Z","iopub.status.idle":"2024-10-08T19:28:52.283105Z","shell.execute_reply.started":"2024-10-08T19:28:47.575959Z","shell.execute_reply":"2024-10-08T19:28:52.282067Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"bptzVTG3gm8x","execution":{"iopub.status.busy":"2024-10-08T19:28:52.284207Z","iopub.execute_input":"2024-10-08T19:28:52.284794Z","iopub.status.idle":"2024-10-08T19:28:52.449385Z","shell.execute_reply.started":"2024-10-08T19:28:52.284758Z","shell.execute_reply":"2024-10-08T19:28:52.448576Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def tokenize(sample):\n    sample[\"input_ids\"] = tokenizer.encode(sample[\"prompt\"],\n                                           return_tensors=\"pt\",\n                                           truncation=True,\n                                           padding=\"max_length\",\n                                           max_length=max_length)[0]\n    return sample\n\ndataset = dataset.map(tokenize, batched=False)\ndataset = dataset.map(lambda x: {\"query\": tokenizer.decode(x[\"input_ids\"])}, batched=False)\ndataset.set_format(\"pytorch\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["53abb305c420411c9bc8c6e11b146872","fb5d6134d8ae419b809067fcbfaf6303","a0e1b6c6f5dc4ea6abbf0f4b9816345d","6204d1c088374261a1db2ed895042ac8","9309962c72e842b0b9e9326309c8bd53","fc3f78f7daa844358de65becdf98e156","5de3fd1e86f24b4a89b56cb96b0e4fbb","bd6bef0c80c8400ba5cea8ea0d965c80","e670029a7f0d482595f7fe55ccc82884","93a9f14a76074367a3f52d213f6faae3","2a7f8e2f5abd46cd952fb91d27d96066","d90732b8b47b4f51a9e0b7847905bf2f","7166ea6dfa0c4c9f9e1f91cf28655e39","4e77821d175e4f4b9c9a795a78f211b0","226246f0b4dd4000b7ebcc42b6373d9e","d60958ab5be24fff84f23405d4a36b83","a9aadb1bc6d14bdcb9a11ca41f78b1b4","8d91094d27b645b79309812cde1a2dae","04c2750aa5e74e849e23440e673ca4a2","d2a9d372ed2a407caa2b8b80ae4140e2","53e53741fd9f424c8596eb80762605a9","39a7ce1bd1c7472d88db877fbf4edbde"]},"id":"nTpktPh3gkuQ","outputId":"6766d69d-fc42-4047-e115-dc7e3868f3ef","execution":{"iopub.status.busy":"2024-10-08T19:28:52.452175Z","iopub.execute_input":"2024-10-08T19:28:52.452867Z","iopub.status.idle":"2024-10-08T19:28:53.404006Z","shell.execute_reply.started":"2024-10-08T19:28:52.452816Z","shell.execute_reply":"2024-10-08T19:28:53.402963Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"436c09c4f012409d9be577824b220595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9886cc3185f4211a8568f13d534b71a"}},"metadata":{}}]},{"cell_type":"code","source":"def collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])","metadata":{"id":"RFOcPDhajo2M","execution":{"iopub.status.busy":"2024-10-08T19:28:53.405440Z","iopub.execute_input":"2024-10-08T19:28:53.406136Z","iopub.status.idle":"2024-10-08T19:28:53.411903Z","shell.execute_reply.started":"2024-10-08T19:28:53.406089Z","shell.execute_reply":"2024-10-08T19:28:53.410829Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from trl import AutoModelForCausalLMWithValueHead\nfrom transformers import BitsAndBytesConfig\n\nwith_quantization_config = False if device == \"cpu\" else True\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                                bnb_4bit_use_double_quant=True,\n                                bnb_4bit_quant_type=\"nf4\",\n                                bnb_4bit_compute_dtype=torch.float16)\n\n\nppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL_PATH,\n                                                             quantization_config= bnb_config if with_quantization_config else None,\n                                                             device_map=\"auto\",\n                                                             trust_remote_code=True,\n                                                              # torch_dtype=torch.float16\n                                                             )\nppo_model.config.pad_token_id = ppo_model.config.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:28:53.413536Z","iopub.execute_input":"2024-10-08T19:28:53.413902Z","iopub.status.idle":"2024-10-08T19:28:55.503596Z","shell.execute_reply.started":"2024-10-08T19:28:53.413860Z","shell.execute_reply":"2024-10-08T19:28:55.502565Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from trl import create_reference_model\n\nref_model = create_reference_model(ppo_model)\nprint(ppo_model)","metadata":{"id":"_Y4XaLdFiTSi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aaf4af68-e449-4288-cd3e-0f711e839a45","execution":{"iopub.status.busy":"2024-10-08T19:28:55.504960Z","iopub.execute_input":"2024-10-08T19:28:55.505354Z","iopub.status.idle":"2024-10-08T19:28:55.763555Z","shell.execute_reply.started":"2024-10-08T19:28:55.505309Z","shell.execute_reply":"2024-10-08T19:28:55.762618Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"AutoModelForCausalLMWithValueHead(\n  (pretrained_model): LlamaForCausalLM(\n    (model): LlamaModel(\n      (embed_tokens): Embedding(32000, 2048)\n      (layers): ModuleList(\n        (0-21): 22 x LlamaDecoderLayer(\n          (self_attn): LlamaSdpaAttention(\n            (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n            (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n            (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (rotary_emb): LlamaRotaryEmbedding()\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n            (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n            (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        )\n      )\n      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n      (rotary_emb): LlamaRotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n  )\n  (v_head): ValueHead(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (summary): Linear(in_features=2048, out_features=1, bias=True)\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import bitsandbytes as bnb\n\noptimizer = bnb.optim.Adam8bit(ppo_model.parameters(), lr=config.learning_rate)","metadata":{"id":"ungJTPYtgaZT","execution":{"iopub.status.busy":"2024-10-08T19:28:55.764645Z","iopub.execute_input":"2024-10-08T19:28:55.764980Z","iopub.status.idle":"2024-10-08T19:28:56.049633Z","shell.execute_reply.started":"2024-10-08T19:28:55.764946Z","shell.execute_reply":"2024-10-08T19:28:56.048846Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from trl import PPOTrainer\n\n\nppo_trainer = PPOTrainer(config=config,\n                           model=ppo_model,\n                           ref_model=ref_model,\n                           tokenizer=tokenizer,\n                           dataset=dataset,\n                           data_collator=collator,\n                           optimizer=optimizer)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crA34C2M5Ic-","outputId":"36460b24-5a3f-41be-e638-81f9daeb9334","execution":{"iopub.status.busy":"2024-10-08T19:28:56.050757Z","iopub.execute_input":"2024-10-08T19:28:56.051085Z","iopub.status.idle":"2024-10-08T19:28:56.072722Z","shell.execute_reply.started":"2024-10-08T19:28:56.051050Z","shell.execute_reply":"2024-10-08T19:28:56.071856Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"generation_kwargs = {\n    \"min_length\": 5,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:28:56.073746Z","iopub.execute_input":"2024-10-08T19:28:56.074059Z","iopub.status.idle":"2024-10-08T19:28:56.078668Z","shell.execute_reply.started":"2024-10-08T19:28:56.074027Z","shell.execute_reply":"2024-10-08T19:28:56.077639Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"reward_kwargs = {\n    \"top_k\": None, # Return all scores.\n    \"function_to_apply\": \"none\", # want the raw logits without softmax.\n    \"batch_size\": 8,\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:28:56.081383Z","iopub.execute_input":"2024-10-08T19:28:56.081723Z","iopub.status.idle":"2024-10-08T19:28:56.087697Z","shell.execute_reply.started":"2024-10-08T19:28:56.081680Z","shell.execute_reply":"2024-10-08T19:28:56.086770Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from trl.core import LengthSampler\n\n\noutput_min_length = 128\noutput_max_length = max_length\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:28:56.088889Z","iopub.execute_input":"2024-10-08T19:28:56.089218Z","iopub.status.idle":"2024-10-08T19:28:56.096567Z","shell.execute_reply.started":"2024-10-08T19:28:56.089185Z","shell.execute_reply":"2024-10-08T19:28:56.095615Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\n\npipe = pipeline(\"text-classification\",\n                model=MODEL_PATH, \n                device=device,\n               )\n\npipe.tokenizer.pad_token = pipe.tokenizer.eos_token\npipe.model.config.pad_token_id = pipe.model.config.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:28:56.097885Z","iopub.execute_input":"2024-10-08T19:28:56.098356Z","iopub.status.idle":"2024-10-08T19:28:59.902025Z","shell.execute_reply.started":"2024-10-08T19:28:56.098285Z","shell.execute_reply":"2024-10-08T19:28:59.901186Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at reward_merged_model and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from trl.core import LengthSampler\n\n\noutput_length_sampler = LengthSampler(124, max_length)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:28:59.904471Z","iopub.execute_input":"2024-10-08T19:28:59.904886Z","iopub.status.idle":"2024-10-08T19:28:59.909644Z","shell.execute_reply.started":"2024-10-08T19:28:59.904839Z","shell.execute_reply":"2024-10-08T19:28:59.908433Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"The operation is running if you see the following metrics appearing:\n\n* objective/kl: minimize kl divergence,\n* ppo/returns/mean: maximize mean returns,\n* ppo/policy/advantages_mean: maximize advantages.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nmax_ppo_steps = 10\nlabel_idx = 0\n\n\nfor step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    # Break when you reach max_steps.\n    if step >= max_ppo_steps:\n        break   \n\n    prompt_tensors = batch[\"input_ids\"]\n\n    response_tensors = []\n\n    for prompt_tensor in prompt_tensors:\n        max_new_tokens = output_length_sampler()        \n            \n        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n        response = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n        \n        response_tensors.append(response.squeeze()[-max_new_tokens:])\n        \n    # This needs to be called \"response\".\n    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n\n    # Compute reward outputs.\n    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]    \n    rewards = pipe(query_response_pairs, **reward_kwargs)\n\n    reward_tensors = [torch.tensor(reward[label_idx][\"score\"]) for reward in rewards]    \n\n    # Run PPO step.\n    stats = ppo_trainer.step(prompt_tensors, response_tensors, reward_tensors)\n    ppo_trainer.log_stats(stats, batch, reward_tensors)\n    \n    print(f'objective/kl: {stats[\"objective/kl\"]}')\n    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n    print('-'.join('' for x in range(100)))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T19:28:59.910710Z","iopub.execute_input":"2024-10-08T19:28:59.911034Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n1it [03:45, 225.53s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 0.0\nppo/returns/mean: 0.1757490634918213\nppo/policy/advantages_mean: 0.001481133047491312\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -12.62 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n  warnings.warn(\n2it [06:33, 191.66s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: -12.618038177490234\nppo/returns/mean: 0.5381687879562378\nppo/policy/advantages_mean: -0.0008931616321206093\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 16\ncompare_results = {}\n\ndf_batch = dataset[0:batch_size]\n\ncompare_results[\"query\"] = df_batch[\"query\"]\nprompt_tensors = df_batch[\"input_ids\"]\n\nresponse_tensors_ref = []\nresponse_tensors = []\n\n# Get response from ppo and base model.\nfor i in tqdm(range(batch_size)):\n    gen_len = output_length_sampler()\n    generation_kwargs[\"max_new_tokens\"] = gen_len\n    \n    response = ref_model.generate(\n        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n        **generation_kwargs\n    ).squeeze()[-gen_len:]\n    response_tensors_ref.append(response)\n\n    response = ppo_model.generate(\n        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n        **generation_kwargs\n    ).squeeze()[-gen_len:]\n    response_tensors.append(response)\n\n# Decode responses.\ncompare_results[\"response_before\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(batch_size)]\ncompare_results[\"response_after\"] = [tokenizer.decode(response_tensors[i]) for i in range(batch_size)]\n\n# Sentiment analysis of query/response pairs before/after.\ntexts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\nrewards_before = pipe(texts_before, **reward_kwargs)\ncompare_results[\"reward_before\"] = [reward[label_idx][\"score\"] for reward in rewards_before]\n\ntexts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\nrewards_after = pipe(texts_after, **reward_kwargs)\ncompare_results[\"reward_after\"] = [reward[label_idx][\"score\"] for reward in rewards_after]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 500)\ndf_compare_results = pd.DataFrame(compare_results)\ndf_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\ndf_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\ndf_compare_results_sorted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ppo_model.save_pretrained(\"RLHF Model\")\ntokenizer.save_pretrained(\"RLHF Model\")","metadata":{"id":"ip68fjcwtLCh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}